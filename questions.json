{
  "11": {
    "question": "A company has many AWS accounts and uses AWS Organizations to manage all of them. A solutions architect must implement a solution that the company can use to share a common network across multiple accounts.The company\u2019s infrastructure team has a dedicated infrastructure account that has a VPC. The infrastructure team must use this account to manage the network. Individual accounts cannot have the ability to manage their own networks. However, individual accounts must be able to create AWS resources within subnets.Which combination of actions should the solutions architect perform to meet these requirements? (Choose two.)",
    "choices": [
      "A. Create a transit gateway in the infrastructure account.",
      "B. Enable resource sharing from the AWS Organizations management account.",
      "C. Create VPCs in each AWS account within the organization in AWS Organizations. Configure the VPCs to share the same CIDR range and subnets as the VPC in the infrastructure account. Peer the VPCs in each individual account with the VPC in the infrastructure account.",
      "D. Create a resource share in AWS Resource Access Manager in the infrastructure account. Select the specific AWS Organizations OU that will use the shared network. Select each subnet to associate with the resource share.",
      "E. Create a resource share in AWS Resource Access Manager in the infrastructure account. Select the specific AWS Organizations OU that will use the shared network. Select each prefix list to associate with the resource share."
    ],
    "answer": "BD",
    "explanation": "1. Clear explanation of why the correct answer (BD) is right:\n\nThe correct answer is B and D because:\n\nB. Enable resource sharing from the AWS Organizations management account:\nThis is required to enable sharing of resources (such as subnets) across AWS accounts within the organization. The question states that individual accounts cannot manage their own networks, so resource sharing is necessary to provide access to the shared network.\n\nD. Create a resource share in AWS Resource Access Manager in the infrastructure account. Select the specific AWS Organizations OU that will use the shared network. Select each subnet to associate with the resource share:\nThis allows the infrastructure account to share specific subnets with the other accounts in the organization. This way, the individual accounts can create resources within those shared subnets without having to manage their own networks.\n\n2. Brief explanations of why the incorrect choices are wrong:\n\nA. Create a transit gateway in the infrastructure account:\nNot necessary, as the question does not mention the need for connecting multiple VPCs. There is only a single VPC in the infrastructure account that needs to be shared.\n\nC. Create VPCs in each AWS account within the organization in AWS Organizations. Configure the VPCs to share the same CIDR range and subnets as the VPC in the infrastructure account. Peer the VPCs in each individual account with the VPC in the infrastructure account:\nThis is not the correct approach because the requirement is for individual accounts to use the shared network in the infrastructure account, not to create their own VPCs and peer them.\n\nE. Create a resource share in AWS Resource Access Manager in the infrastructure account. Select the specific AWS Organizations OU that will use the shared network. Select each prefix list to associate with the resource share:\nPrefix lists are not necessary in this scenario, as the requirement is to share subnets, not specific IP address ranges."
  },
  "12": {
    "question": "A company wants to use a third-party software-as-a-service (SaaS) application. The third-party SaaS application is consumed through several API calls. The third-party SaaS application also runs on AWS inside a VPC.The company will consume the third-party SaaS application from inside a VPC. The company has internal security policies that mandate the use of private connectivity that does not traverse the internet. No resources that run in the company VPC are allowed to be accessed from outside the company\u2019s VPC. All permissions must conform to the principles of least privilege.Which solution meets these requirements?",
    "choices": [
      "A. Create an AWS PrivateLink interface VPC endpoint. Connect this endpoint to the endpoint service that the third-party SaaS application provides. Create a security group to limit the access to the endpoint. Associate the security group with the endpoint.",
      "B. Create an AWS Site-to-Site VPN connection between the third-party SaaS application and the company VPC. Configure network ACLs to limit access across the VPN tunnels.",
      "C. Create a VPC peering connection between the third-party SaaS application and the company VPUpdate route tables by adding the needed routes for the peering connection.",
      "D. Create an AWS PrivateLink endpoint service. Ask the third-party SaaS provider to create an interface VPC endpoint for this endpoint service. Grant permissions for the endpoint service to the specific account of the third-party SaaS provider."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\n\nThe correct answer is A, which recommends creating an AWS PrivateLink interface VPC endpoint and connecting it to the endpoint service provided by the third-party SaaS application. This solution meets the given requirements in the following ways:\n\n- It provides a private, secure connection between the company's VPC and the third-party SaaS application VPC, without the traffic traversing the public internet. This aligns with the company's internal security policy that mandates the use of private connectivity.\n- By creating a security group and associating it with the VPC endpoint, the company can limit and control the access to the third-party SaaS application, following the principle of least privilege.\n- Since the resources in the company's VPC are not allowed to be accessed from outside the VPC, the PrivateLink solution ensures that the connection is contained within the VPC boundaries.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. This option suggests creating an AWS Site-to-Site VPN connection between the third-party SaaS application and the company VPC. While this would provide a private connection, it does not meet the requirement of not traversing the internet, as VPN connections typically use the public internet as the transport medium.\n\nC. Creating a VPC peering connection between the third-party SaaS application and the company VPC is not the best solution, as VPC peering also relies on the public internet for the connection. Additionally, VPC peering does not provide the same level of fine-grained access control as PrivateLink.\n\nD. This option suggests creating an AWS PrivateLink endpoint service and asking the third-party SaaS provider to create an interface VPC endpoint for this service. While this is a valid solution, it places the burden of creating and managing the PrivateLink endpoint on the third-party SaaS provider, which may not be in the company's control. The correct answer (A) is more self-contained and aligns better with the company's internal security policies."
  },
  "13": {
    "question": "A company needs to implement a patching process for its servers. The on-premises servers and Amazon EC2 instances use a variety of tools to perform patching. Management requires a single report showing the patch status of all the servers and instances.Which set of actions should a solutions architect take to meet these requirements?",
    "choices": [
      "A. Use AWS Systems Manager to manage patches on the on-premises servers and EC2 instances. Use Systems Manager to generate patch compliance reports.",
      "B. Use AWS OpsWorks to manage patches on the on-premises servers and EC2 instances. Use Amazon QuickSight integration with OpsWorks to generate patch compliance reports.",
      "C. Use an Amazon EventBridge rule to apply patches by scheduling an AWS Systems Manager patch remediation job. Use Amazon Inspector to generate patch compliance reports.",
      "D. Use AWS OpsWorks to manage patches on the on-premises servers and EC2 instances. Use AWS X-Ray to post the patch status to AWS Systems Manager OpsCenter to generate patch compliance reports."
    ],
    "answer": "A",
    "explanation": "1. Clear explanation of why the correct answer (A) is right:\n\nThe correct answer is A because AWS Systems Manager provides a unified way to manage patch compliance across both on-premises servers and Amazon EC2 instances. It can automate the process of applying patches and also generate comprehensive patch compliance reports, which meets the requirements stated in the question. Systems Manager is designed specifically for managing patches and configurations across hybrid environments, making it the most suitable choice compared to the other options.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nB. AWS OpsWorks and Amazon QuickSight are not the best choices for this use case. OpsWorks is primarily for orchestrating server configuration and deployment, not specifically for patch management. QuickSight is a business intelligence service, which may not be the most appropriate tool for generating patch compliance reports.\n\nC. Using Amazon EventBridge and AWS Inspector is not the best solution. EventBridge can be used to schedule patch remediation jobs, but it does not provide comprehensive patch management and reporting capabilities. Inspector is an AWS security assessment service, not a patch management tool.\n\nD. Using AWS OpsWorks, AWS X-Ray, and AWS Systems Manager OpsCenter is an overly complex solution for this use case. OpsWorks is not the best tool for patch management, and X-Ray is a distributed tracing service, not designed for patch reporting. OpsCenter is more suitable for incident management, not patch compliance reporting."
  },
  "14": {
    "question": "A company is running an application on several Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer. The load on the application varies throughout the day, and EC2 instances are scaled in and out on a regular basis. Log files from the EC2 instances are copied to a central Amazon S3 bucket every 15 minutes. The security team discovers that log files are missing from some of the terminated EC2 instances.Which set of actions will ensure that log files are copied to the central S3 bucket from the terminated EC2 instances?",
    "choices": [
      "A. Create a script to copy log files to Amazon S3, and store the script in a file on the EC2 instance. Create an Auto Scaling lifecycle hook and an Amazon EventBridge rule to detect lifecycle events from the Auto Scaling group. Invoke an AWS Lambda function on the autoscaling:EC2_INSTANCE_TERMINATING transition to send ABANDON to the Auto Scaling group to prevent termination, run the script to copy the log files, and terminate the instance using the AWS SDK.",
      "B. Create an AWS Systems Manager document with a script to copy log files to Amazon S3. Create an Auto Scaling lifecycle hook and an Amazon EventBridge rule to detect lifecycle events from the Auto Scaling group. Invoke an AWS Lambda function on the autoscaling:EC2_INSTANCE_TERMINATING transition to call the AWS Systems Manager API SendCommand operation to run the document to copy the log files and send CONTINUE to the Auto Scaling group to terminate the instance.",
      "C. Change the log delivery rate to every 5 minutes. Create a script to copy log files to Amazon S3, and add the script to EC2 instance user data. Create an Amazon EventBridge rule to detect EC2 instance termination. Invoke an AWS Lambda function from the EventBridge rule that uses the AWS CLI to run the user-data script to copy the log files and terminate the instance.",
      "D. Create an AWS Systems Manager document with a script to copy log files to Amazon S3. Create an Auto Scaling lifecycle hook that publishes a message to an Amazon Simple Notification Service (Amazon SNS) topic. From the SNS notification, call the AWS Systems Manager API SendCommand operation to run the document to copy the log files and send ABANDON to the Auto Scaling group to terminate the instance."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\nThe correct answer is B, as it provides the most comprehensive and reliable solution to ensure that log files are copied to the central S3 bucket from the terminated EC2 instances.\n\nThe key aspects of this solution are:\n- It creates an AWS Systems Manager document with a script to copy log files to Amazon S3. This ensures that the log file copying logic is centrally managed and can be easily maintained.\n- It creates an Auto Scaling lifecycle hook and an Amazon EventBridge rule to detect lifecycle events from the Auto Scaling group, specifically the autoscaling:EC2_INSTANCE_TERMINATING transition.\n- It invokes an AWS Lambda function on the autoscaling:EC2_INSTANCE_TERMINATING transition, which then calls the AWS Systems Manager API SendCommand operation to run the document and copy the log files.\n- The Lambda function then sends the CONTINUE signal to the Auto Scaling group, allowing the instance to be terminated after the log files have been copied.\n\nThis approach ensures that the log files are copied to the central S3 bucket before the instance is terminated, addressing the issue of missing log files.\n\n2. Explanations of why the other choices are incorrect:\nA. This solution involves creating a custom script on the EC2 instance, which is less reliable than using a centrally managed AWS Systems Manager document. Additionally, the ABANDON signal to the Auto Scaling group would prevent the instance from being terminated, which is not the desired behavior.\n\nC. Changing the log delivery rate to every 5 minutes may not be a practical solution, as it could incur additional costs and complexity. Additionally, relying on user data to execute the script is not as robust as using a centralized AWS Systems Manager document.\n\nD. While using an AWS Systems Manager document and an SNS topic to invoke the script is a valid approach, it is less efficient than directly invoking the document from the Lambda function triggered by the Auto Scaling lifecycle hook, as in the correct answer (B)."
  },
  "15": {
    "question": "A company is using multiple AWS accounts. The DNS records are stored in a private hosted zone for Amazon Route 53 in Account A. The company\u2019s applications and databases are running in Account B.A solutions architect will deploy a two-tier application in a new VPC. To simplify the configuration, the db.example.com CNAME record set for the Amazon RDS endpoint was created in a private hosted zone for Amazon Route 53.During deployment, the application failed to start. Troubleshooting revealed that db.example.com is not resolvable on the Amazon EC2 instance. The solutions architect confirmed that the record set was created correctly in Route 53.Which combination of steps should the solutions architect take to resolve this issue? (Choose two.)",
    "choices": [
      "A. Deploy the database on a separate EC2 instance in the new VPC. Create a record set for the instance\u2019s private IP in the private hosted zone.",
      "B. Use SSH to connect to the application tier EC2 instance. Add an RDS endpoint IP address to the /etc/resolv.conf file.",
      "C. Create an authorization to associate the private hosted zone in Account A with the new VPC in Account B.",
      "D. Create a private hosted zone for the example com domain in Account B. Configure Route 53 replication between AWS accounts.",
      "E. Associate a new VPC in Account B with a hosted zone in Account A. Delete the association authorization in Account A."
    ],
    "answer": "CE",
    "explanation": "1. Clear explanation of why the correct answer is C and E:\n\nThe correct answer is C and E because the issue is related to the DNS resolution of the RDS endpoint from the EC2 instance in a different AWS account. To resolve this, the solutions architect needs to take the following steps:\n\nC. Create an authorization to associate the private hosted zone in Account A with the new VPC in Account B.\nThis is necessary because the private hosted zone containing the DB endpoint record is in Account A, while the application is running in Account B. The VPC in Account B needs to be associated with the private hosted zone in Account A to be able to resolve the DNS records.\n\nE. Associate a new VPC in Account B with a hosted zone in Account A. Delete the association authorization in Account A.\nAfter the VPC in Account B is associated with the hosted zone in Account A, the association authorization in Account A should be deleted to prevent any further associations.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nA. Deploy the database on a separate EC2 instance in the new VPC. Create a record set for the instance's private IP in the private hosted zone.\nThis is incorrect because the issue is not about the database deployment, but rather the DNS resolution of the RDS endpoint from the application tier in a different account.\n\nB. Use SSH to connect to the application tier EC2 instance. Add an RDS endpoint IP address to the /etc/resolv.conf file.\nThis is incorrect because it would only work as a temporary solution. The DNS resolution issue needs to be addressed at the infrastructure level by associating the VPC in Account B with the private hosted zone in Account A.\n\nD. Create a private hosted zone for the example.com domain in Account B. Configure Route 53 replication between AWS accounts.\nThis is incorrect because the issue is specific to the private hosted zone in Account A, not the public domain hosted zone. Replicating the hosted zone between accounts would not resolve the DNS resolution problem."
  },
  "16": {
    "question": "A company used Amazon EC2 instances to deploy a web fleet to host a blog site. The EC2 instances are behind an Application Load Balancer (ALB) and are configured in an Auto Scaling group. The web application stores all blog content on an Amazon EFS volume.The company recently added a feature for bloggers to add video to their posts, attracting 10 times the previous user traffic. At peak times of day, users report buffering and timeout issues while attempting to reach the site or watch videos.Which is the MOST cost-efficient and scalable deployment that will resolve the issues for users?",
    "choices": [
      "A. Reconfigure Amazon EFS to enable maximum I/O.",
      "B. Update the blog site to use instance store volumes for storage. Copy the site contents to the volumes at launch and to Amazon S3 at shutdown.",
      "C. Configure an Amazon CloudFront distribution. Point the distribution to an S3 bucket, and migrate the videos from EFS to Amazon S3.",
      "D. Set up an Amazon CloudFront distribution for all site contents, and point the distribution at the ALB."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C. Configure an Amazon CloudFront distribution, point it to an S3 bucket, and migrate the videos from EFS to Amazon S3.\n\nThis is the most cost-efficient and scalable deployment to resolve the issues for users. Here's why:\n\n- Amazon CloudFront is a content delivery network (CDN) that can cache and deliver content to users with low latency and high data transfer speeds. By using CloudFront, the videos can be served from edge locations closer to the users, reducing buffering and timeout issues.\n- Migrating the video content from the Amazon EFS volume to Amazon S3 is more scalable and cost-effective. S3 is designed for scalable storage and can handle high levels of user traffic, whereas EFS may not be as cost-effective for serving high-volume video content.\n- Storing the videos in an S3 bucket and using CloudFront to deliver them is a more cost-efficient solution compared to continuing to use the EFS volume, which can be more expensive for serving large media files.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Reconfigure Amazon EFS to enable maximum I/O:\n- Enabling maximum I/O on an existing EFS file system is not possible. The maximum I/O performance can only be set when creating a new file system.\n- Increasing the I/O performance may not necessarily resolve the buffering and timeout issues, as the underlying problem is the high volume of video content being served from the EFS volume.\n\nB. Update the blog site to use instance store volumes for storage:\n- Using instance store volumes for storage is generally more expensive than using Amazon EBS or Amazon S3 for storage.\n- Copying the site contents to the instance store volumes at launch and to Amazon S3 at shutdown adds complexity and is not a scalable solution for handling the increased video traffic.\n\nD. Set up an Amazon CloudFront distribution for all site contents, and point the distribution at the ALB:\n- This solution would provide performance improvements, but it would be more costly than the correct answer (C).\n- Serving all site contents, including the blog pages, through the CloudFront distribution would incur additional costs compared to serving only"
  },
  "17": {
    "question": "A company has a web application that allows users to upload short videos. The videos are stored on Amazon EBS volumes and analyzed by custom recognition software for categorization.The website contains static content that has variable traffic with peaks in certain months. The architecture consists of Amazon EC2 instances running in an Auto Scaling group for the web application and EC2 instances running in an Auto Scaling group to process an Amazon SQS queue. The company wants to re-architect the application to reduce operational overhead using AWS managed services where possible and remove dependencies on third-party software.Which solution meets these requirements?",
    "choices": [
      "A. Use Amazon ECS containers for the web application and Spot instances for the Auto Scaling group that processes the SQS queue. Replace the custom software with Amazon Rekognition to categorize the videos.",
      "B. Store the uploaded videos in Amazon EFS and mount the file system to the EC2 instances for the web application. Process the SQS queue with an AWS Lambda function that calls the Amazon Rekognition API to categorize the videos.",
      "C. Host the web application in Amazon S3. Store the uploaded videos in Amazon S3. Use S3 event notification to publish events to the SQS queue. Process the SQS queue with an AWS Lambda function that calls the Amazon Rekognition API to categorize the videos.",
      "D. Use AWS Elastic Beanstalk to launch EC2 instances in an Auto Scaling group for the web application and launch a worker environment to process the SQS queue. Replace the custom software with Amazon Rekognition to categorize the videos."
    ],
    "answer": "C",
    "explanation": "1. Clear explanation of why the correct answer (C) is right:\n\nThe correct answer is C because it leverages multiple AWS managed services to reduce operational overhead and remove dependencies on third-party software, as required in the question.\n\nSpecifically:\n\n- Hosting the web application in Amazon S3 provides a highly available, scalable, and cost-effective solution for serving the static content without the need to manage EC2 instances.\n- Storing the uploaded videos in Amazon S3 eliminates the need to manage EBS volumes and provides a durable storage solution.\n- Using S3 event notifications to publish events to an SQS queue, and then processing the queue with an AWS Lambda function that calls Amazon Rekognition, removes the need for the custom recognition software and automates the video categorization process.\n\nThis solution effectively utilizes managed services like S3, SQS, and Lambda to simplify the architecture and reduce operational overhead, which aligns with the requirements stated in the question.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nA. This solution uses EC2 instances and Spot instances, which still require more operational overhead compared to managed services. Additionally, the use of custom recognition software is not addressed.\n\nB. While using Amazon EFS and an EC2-based SQS queue processor is better than the initial architecture, it still requires managing EC2 instances. The question specifically mentions a requirement to use managed services where possible.\n\nD. While AWS Elastic Beanstalk can help manage the EC2 instances for the web application, it still requires managing the worker environment for the SQS queue. The question requires using managed services to reduce operational overhead, which Elastic Beanstalk does not fully address."
  },
  "18": {
    "question": "A company has a web application that allows users to upload short videos. The videos are stored on Amazon EBS volumes and analyzed by custom recognition software for categorization.The website contains static content that has variable traffic with peaks in certain months. The architecture consists of Amazon EC2 instances running in an Auto Scaling group for the web application and EC2 instances running in an Auto Scaling group to process an Amazon SQS queue. The company wants to re-architect the application to reduce operational overhead using AWS managed services where possible and remove dependencies on third-party software.Which solution meets these requirements?",
    "choices": [
      "A. Use Amazon ECS containers for the web application and Spot instances for the Auto Scaling group that processes the SQS queue. Replace the custom software with Amazon Rekognition to categorize the videos.",
      "B. Store the uploaded videos in Amazon EFS and mount the file system to the EC2 instances for the web application. Process the SQS queue with an AWS Lambda function that calls the Amazon Rekognition API to categorize the videos.",
      "C. Host the web application in Amazon S3. Store the uploaded videos in Amazon S3. Use S3 event notification to publish events to the SQS queue. Process the SQS queue with an AWS Lambda function that calls the Amazon Rekognition API to categorize the videos.",
      "D. Use AWS Elastic Beanstalk to launch EC2 instances in an Auto Scaling group for the web application and launch a worker environment to process the SQS queue. Replace the custom software with Amazon Rekognition to categorize the videos."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C because it uses a combination of fully managed AWS services to address the requirements of the web application, which include:\n\n- Hosting the web application in Amazon S3: This allows the static content of the web application to be served directly from a highly scalable, durable, and secure storage service, reducing the operational overhead of managing EC2 instances.\n- Storing the uploaded videos in Amazon S3: Similarly, using S3 to store the uploaded videos removes the need to manage EC2 instances and EBS volumes.\n- Using S3 event notifications to trigger an AWS Lambda function: This allows the video categorization process to be handled by a serverless function, which can automatically scale to handle the processing load without the need to manage EC2 instances.\n- Processing the SQS queue with an AWS Lambda function: The use of SQS and a Lambda function to process the queue also removes the need to manage EC2 instances for this component.\n\nOverall, this solution leverages fully managed AWS services to reduce the operational overhead and remove dependencies on third-party software, as requested in the requirements.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This solution uses EC2 instances and Spot instances, which still require some operational overhead for management and scaling. Additionally, while Amazon ECS is a managed service, it still requires some operational tasks such as managing Docker containers. The question does not mention the use of Fargate, which would provide a more serverless approach.\n\nB. Storing the videos in Amazon EFS still requires the management of EC2 instances to mount the file system, which does not fully address the requirement to reduce operational overhead. Additionally, using EC2 instances to process the SQS queue does not take advantage of a fully serverless approach.\n\nD. While AWS Elastic Beanstalk is a managed service that simplifies the deployment and scaling of web applications, it still requires the management of underlying EC2 instances. The question specifically states a requirement to use managed services where possible, and Elastic Beanstalk does not fully address this requirement."
  },
  "19": {
    "question": "A company has a serverless application comprised of Amazon CloudFront, Amazon API Gateway, and AWS Lambda functions. The current deployment process of the application code is to create a new version number of the Lambda function and run an AWS CLI script to update. If the new function version has errors, another CLI script reverts by deploying the previous working version of the function. The company would like to decrease the time to deploy new versions of the application logic provided by the Lambda functions, and also reduce the time to detect and revert when errors are identified.How can this be accomplished?",
    "choices": [
      "A. Create and deploy nested AWS CloudFormation stacks with the parent stack consisting of the AWS CloudFront distribution and API Gateway, and the child stack containing the Lambda function. For changes to Lambda, create an AWS CloudFormation change set and deploy; if errors are triggered, revert the AWS CloudFormation change set to the previous version.",
      "B. Use AWS SAM and built-in AWS CodeDeploy to deploy the new Lambda version, gradually shift traffic to the new version, and use pre-traffic and post-traffic test functions to verify code. Rollback if Amazon CloudWatch alarms are triggered.",
      "C. Refactor the AWS CLI scripts into a single script that deploys the new Lambda version. When deployment is completed, the script tests execute. If errors are detected, revert to the previous Lambda version.",
      "D. Create and deploy an AWS CloudFormation stack that consists of a new API Gateway endpoint that references the new Lambda version. Change the CloudFront origin to the new API Gateway endpoint, monitor errors and if detected, change the AWS CloudFront origin to the previous API Gateway endpoint."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B because it leverages AWS Serverless Application Model (SAM) and AWS CodeDeploy to streamline the deployment process and reduce the time to detect and revert errors. \n\nAWS SAM simplifies the creation, testing, and deployment of serverless applications, including Lambda functions. By using AWS SAM, the company can define their serverless application, including the Lambda functions, in a declarative AWS CloudFormation template. \n\nAWS CodeDeploy, which is integrated with AWS SAM, then handles the deployment process. This includes gradually shifting traffic to the new version of the Lambda function, running pre-traffic and post-traffic tests to verify the new code, and automatically rolling back the deployment if any issues are detected (e.g., through CloudWatch alarms).\n\nThis approach addresses the key requirements of the company:\n- Decreasing the time to deploy new versions of the application logic (Lambda functions) through the automated deployment process.\n- Reducing the time to detect and revert errors by leveraging the built-in verification and rollback functionality of AWS CodeDeploy.\n\n2. Explanations of the incorrect choices:\n\nA. This approach using nested CloudFormation stacks is not the most efficient solution, as CloudFormation deployments can be slow and manual. The manual creation of change sets and reverting them is also not as automated as the correct answer.\n\nC. While refactoring the AWS CLI scripts into a single script that deploys the new Lambda version and tests it is a possible solution, it is still a manual and less automated approach compared to the correct answer. It also does not provide the same level of gradual traffic shifting and automatic rollback capabilities as the AWS SAM and CodeDeploy solution.\n\nD. This approach of changing the CloudFront origin to a new API Gateway endpoint that references the new Lambda version is not the most efficient solution. It would still require manual steps to detect and revert errors, and does not provide the same level of automation and deployment control as the correct answer."
  },
  "20": {
    "question": "A company is planning to store a large number of archived documents and make the documents available to employees through the corporate intranet. Employees will access the system by connecting through a client VPN service that is attached to a VPC. The data must not be accessible to the public.The documents that the company is storing are copies of data that is held on physical media elsewhere. The number of requests will be low. Availability and speed of retrieval are not concerns of the company.Which solution will meet these requirements at the LOWEST cost?",
    "choices": [
      "A. Create an Amazon S3 bucket. Configure the S3 bucket to use the S3 One Zone-Infrequent Access (S3 One Zone-IA) storage class as default. Configure the S3 bucket for website hosting. Create an S3 interface endpoint. Configure the S3 bucket to allow access only through that endpoint.",
      "B. Launch an Amazon EC2 instance that runs a web server. Attach an Amazon Elastic File System (Amazon EFS) file system to store the archived data in the EFS One Zone-Infrequent Access (EFS One Zone-IA) storage class Configure the instance security groups to allow access only from private networks.",
      "C. Launch an Amazon EC2 instance that runs a web server Attach an Amazon Elastic Block Store (Amazon EBS) volume to store the archived data. Use the Cold HDD (sc1) volume type. Configure the instance security groups to allow access only from private networks.",
      "D. Create an Amazon S3 bucket. Configure the S3 bucket to use the S3 Glacier Deep Archive storage class as default. Configure the S3 bucket for website hosting. Create an S3 interface endpoint. Configure the S3 bucket to allow access only through that endpoint."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\n\nThe correct answer is A because it meets the requirements at the lowest cost:\n\n- It uses Amazon S3 with the One Zone-IA storage class, which is the lowest-cost storage option for infrequently accessed data. This aligns with the requirement that availability and speed of retrieval are not concerns.\n- By configuring the S3 bucket to use an S3 interface endpoint, it ensures the data is only accessible through the corporate VPC and VPN, meeting the requirement of not being publicly accessible.\n- The S3 bucket is configured for website hosting, allowing employees to access the documents through the corporate intranet, as required.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. This option uses Amazon EFS with the One Zone-IA storage class, which is more expensive than S3 One Zone-IA.\nC. This option uses Amazon EBS with the Cold HDD (sc1) volume type, which is more expensive than S3 One Zone-IA for infrequently accessed data.\nD. This option uses Amazon S3 with the Glacier Deep Archive storage class, which is slower to retrieve data (hours) compared to S3 One Zone-IA (milliseconds to seconds). This may not meet the requirement of making the documents available to employees through the corporate intranet."
  },
  "21": {
    "question": "A company is using an on-premises Active Directory service for user authentication. The company wants to use the same authentication service to sign in to the company\u2019s AWS accounts, which are using AWS Organizations. AWS Site-to-Site VPN connectivity already exists between the on-premises environment and all the company\u2019s AWS accounts.The company\u2019s security policy requires conditional access to the accounts based on user groups and roles. User identities must be managed in a single location.Which solution will meet these requirements?",
    "choices": [
      "A. Configure AWS IAM Identity Center (AWS Single Sign-On) to connect to Active Directory by using SAML 2.0. Enable automatic provisioning by using the System for Cross-domain Identity Management (SCIM) v2.0 protocol. Grant access to the AWS accounts by using attribute-based access controls (ABACs).",
      "B. Configure AWS IAM Identity Center (AWS Single Sign-On) by using IAM Identity Center as an identity source. Enable automatic provisioning by using the System for Cross-domain Identity Management (SCIM) v2.0 protocol. Grant access to the AWS accounts by using IAM Identity Center permission sets.",
      "C. In one of the company\u2019s AWS accounts, configure AWS Identity and Access Management (IAM) to use a SAML 2.0 identity provider. Provision IAM users that are mapped to the federated users. Grant access that corresponds to appropriate groups in Active Directory. Grant access to the required AWS accounts by using cross-account IAM users.",
      "D. In one of the company\u2019s AWS accounts, configure AWS Identity and Access Management (IAM) to use an OpenID Connect (OIDC) identity provider. Provision IAM roles that grant access to the AWS account for the federated users that correspond to appropriate groups in Active Directory. Grant access to the required AWS accounts by using cross-account IAM roles."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\n\nThe correct answer is A because it addresses all the requirements mentioned in the question:\n\n- It configures AWS IAM Identity Center (AWS SSO) to connect to the on-premises Active Directory using SAML 2.0, which allows for user authentication using the same identity source.\n- It enables automatic provisioning using the System for Cross-domain Identity Management (SCIM) v2.0 protocol, allowing for user identities to be managed in a single location.\n- It grants access to the AWS accounts using attribute-based access controls (ABACs), which enables the implementation of the company's security policy for conditional access based on user groups and roles.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. This option configures AWS IAM Identity Center (AWS SSO) using IAM Identity Center as the identity source, which does not meet the requirement of using the on-premises Active Directory for user authentication.\n\nC. This option configures IAM to use a SAML 2.0 identity provider, which is a valid solution. However, it does not mention the use of SCIM for automatic provisioning, and it grants access using cross-account IAM users, which may not be as flexible as using ABACs.\n\nD. This option configures IAM to use an OpenID Connect (OIDC) identity provider, which is not compatible with the on-premises Active Directory. The question specifically states that the company's security policy requires the use of a single identity source, which this option does not meet."
  },
  "22": {
    "question": "A software company has deployed an application that consumes a REST API by using Amazon API Gateway, AWS Lambda functions, and an Amazon DynamoDB table. The application is showing an increase in the number of errors during PUT requests. Most of the PUT calls come from a small number of clients that are authenticated with specific API keys.A solutions architect has identified that a large number of the PUT requests originate from one client. The API is noncritical, and clients can tolerate retries of unsuccessful calls. However, the errors are displayed to customers and are causing damage to the API\u2019s reputation.What should the solutions architect recommend to improve the customer experience?",
    "choices": [
      "A. Implement retry logic with exponential backoff and irregular variation in the client application. Ensure that the errors are caught and handled with descriptive error messages.",
      "B. Implement API throttling through a usage plan at the API Gateway level. Ensure that the client application handles code 429 replies without error.",
      "C. Turn on API caching to enhance responsiveness for the production stage. Run 10-minute load tests. Verify that the cache capacity is appropriate for the workload.",
      "D. Implement reserved concurrency at the Lambda function level to provide the resources that are needed during sudden increases in traffic."
    ],
    "answer": "B",
    "explanation": "1. Explanation of why the correct answer (B) is right:\n\nThe correct answer is B, \"Implement API throttling through a usage plan at the API Gateway level. Ensure that the client application handles code 429 replies without error.\"\n\nThis is the best solution because it directly addresses the root cause of the problem, which is a small number of clients making a large number of PUT requests, leading to an increase in errors. By implementing API throttling through a usage plan at the API Gateway level, the solutions architect can limit the number of requests that a single client can make, which will help to reduce the number of errors and improve the customer experience.\n\nAdditionally, ensuring that the client application handles the code 429 replies (Too Many Requests) without error is important, as it will prevent the errors from being displayed to customers and damaging the API's reputation.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. \"Implement retry logic with exponential backoff and irregular variation in the client application. Ensure that the errors are caught and handled with descriptive error messages.\"\nThis solution does not address the root cause of the problem, which is a small number of clients making a large number of requests. Retry logic may actually exacerbate the problem by increasing the overall number of requests, leading to more errors.\n\nC. \"Turn on API caching to enhance responsiveness for the production stage. Run 10-minute load tests. Verify that the cache capacity is appropriate for the workload.\"\nAPI caching can improve responsiveness, but it does not address the issue of a single client making a large number of requests. The load tests may not be long enough to uncover the problem, and the cache capacity may not be appropriate for the workload.\n\nD. \"Implement reserved concurrency at the Lambda function level to provide the resources that are needed during sudden increases in traffic.\"\nImplementing reserved concurrency at the Lambda function level can help to provide the resources needed during sudden increases in traffic, but it does not address the root cause of the problem, which is a single client making a large number of requests. This solution may not be effective in this scenario."
  },
  "23": {
    "question": "A company is running a data-intensive application on AWS. The application runs on a cluster of hundreds of Amazon EC2 instances. A shared file system also runs on several EC2 instances that store 200\u00a0TB of data. The application reads and modifies the data on the shared file system and generates a report. The job runs once monthly, reads a subset of the files from the shared file system, and takes about 72\u00a0hours to complete. The compute instances scale in an Auto Scaling group, but the instances that host the shared file system run continuously. The compute and storage instances are all in the same AWS Region.A solutions architect needs to reduce costs by replacing the shared file system instances. The file system must provide high performance access to the needed data for the duration of the 72-hour run.Which solution will provide the LARGEST overall cost reduction while meeting these requirements?",
    "choices": [
      "A. Migrate the data from the existing shared file system to an Amazon S3 bucket that uses the S3 Intelligent-Tiering storage class. Before the job runs each month, use Amazon FSx for Lustre to create a new file system with the data from Amazon S3 by using lazy loading. Use the new file system as the shared storage for the duration of the job. Delete the file system when the job is complete.",
      "B. Migrate the data from the existing shared file system to a large Amazon Elastic Block Store (Amazon EBS) volume with Multi-Attach enabled. Attach the EBS volume to each of the instances by using a user data script in the Auto Scaling group launch template. Use the EBS volume as the shared storage for the duration of the job. Detach the EBS volume when the job is complete",
      "C. Migrate the data from the existing shared file system to an Amazon S3 bucket that uses the S3 Standard storage class. Before the job runs each month, use Amazon FSx for Lustre to create a new file system with the data from Amazon S3 by using batch loading. Use the new file system as the shared storage for the duration of the job. Delete the file system when the job is complete.",
      "D. Migrate the data from the existing shared file system to an Amazon S3 bucket. Before the job runs each month, use AWS Storage Gateway to create a file gateway with the data from Amazon S3. Use the file gateway as the shared storage for the job. Delete the file gateway when the job is complete."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\n\nThe correct answer is A because it provides the largest overall cost reduction while meeting the requirements. Migrating the data to an S3 Intelligent-Tiering storage class and using Amazon FSx for Lustre with lazy loading is the most cost-effective solution.\n\nThe key advantages of this approach are:\n- Intelligent-Tiering automatically moves data to the most cost-effective storage class based on access patterns, reducing storage costs.\n- Amazon FSx for Lustre with lazy loading only retrieves the necessary data from S3 when the job runs, minimizing the amount of data transferred and reducing costs.\n- Creating the FSx for Lustre file system before the job runs and deleting it after the job completes avoids the need to keep the file system instances running continuously, reducing compute costs.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. Migrating the data to a large Amazon EBS volume with Multi-Attach is not the best solution because:\n- The question states there are hundreds of EC2 instances, but an EBS volume is limited to a maximum of 16 Nitro-based instances attached at the same time.\n- Attaching and detaching the EBS volume to hundreds of instances would be operationally complex and error-prone.\n\nC. Migrating the data to an S3 Standard storage class and using Amazon FSx for Lustre with batch loading is not the best solution because:\n- Batch loading the entire 200 TB dataset from S3 to the FSx for Lustre file system before each job run would be inefficient and costly, as it would transfer more data than necessary.\n\nD. Migrating the data to S3 and using an AWS Storage Gateway file gateway is not the best solution because:\n- AWS Storage Gateway is primarily designed for on-premises data access, not for cloud-native workloads running on AWS.\n- Using a file gateway in this scenario would be more complex and less cost-effective than the FSx for Lustre solution in option A."
  },
  "24": {
    "question": "A company is developing a new service that will be accessed using TCP on a static port. A solutions architect must ensure that the service is highly available, has redundancy across Availability Zones, and is accessible using the DNS name my.service.com, which is publicly accessible. The service must use fixed address assignments so other companies can add the addresses to their allow lists.Assuming that resources are deployed in multiple Availability Zones in a single Region, which solution will meet these requirements?",
    "choices": [
      "A. Create Amazon EC2 instances with an Elastic IP address for each instance. Create a Network Load Balancer (NLB) and expose the static TCP port. Register EC2 instances with the NLB. Create a new name server record set named my.service.com, and assign the Elastic IP addresses of the EC2 instances to the record set. Provide the Elastic IP addresses of the EC2 instances to the other companies to add to their allow lists.",
      "B. Create an Amazon ECS cluster and a service definition for the application. Create and assign public IP addresses for the ECS cluster. Create a Network Load Balancer (NLB) and expose the TCP port. Create a target group and assign the ECS cluster name to the NLCreate a new A record set named my.service.com, and assign the public IP addresses of the ECS cluster to the record set. Provide the public IP addresses of the ECS cluster to the other companies to add to their allow lists.",
      "C. Create Amazon EC2 instances for the service. Create one Elastic IP address for each Availability Zone. Create a Network Load Balancer (NLB) and expose the assigned TCP port. Assign the Elastic IP addresses to the NLB for each Availability Zone. Create a target group and register the EC2 instances with the NLB. Create a new A (alias) record set named my.service.com, and assign the NLB DNS name to the record set.",
      "D. Create an Amazon ECS cluster and a service definition for the application. Create and assign public IP address for each host in the cluster. Create an Application Load Balancer (ALB) and expose the static TCP port. Create a target group and assign the ECS service definition name to the ALB. Create a new CNAME record set and associate the public IP addresses to the record set. Provide the Elastic IP addresses of the Amazon EC2 instances to the other companies to add to their allow lists."
    ],
    "answer": "C",
    "explanation": "1. Explanation of why the correct answer (C) is right:\n\nThe correct answer is option C because it meets all the requirements specified in the question:\n\na. High availability and redundancy across Availability Zones:\n   - By creating Amazon EC2 instances and using a Network Load Balancer (NLB), the solution ensures high availability and redundancy across Availability Zones.\n   - The NLB can distribute traffic across the EC2 instances in different Availability Zones, providing redundancy and failover capabilities.\n\nb. Accessibility using the DNS name my.service.com:\n   - The solution creates a new A (alias) record set named \"my.service.com\" and assigns the NLB DNS name to it.\n   - This allows the service to be accessed using the DNS name \"my.service.com\", which is publicly accessible.\n\nc. Fixed address assignments for allow listing:\n   - The solution creates Elastic IP addresses for each Availability Zone and assigns them to the NLB.\n   - These Elastic IP addresses can be provided to other companies to add to their allow lists, as they are fixed and do not change.\n\n2. Explanations of why the incorrect choices are wrong:\n\na. Option A is incorrect because:\n   - It creates Elastic IP addresses for each EC2 instance, which can be difficult to manage and does not provide the desired DNS name accessibility.\n   - It does not leverage the benefits of an NLB, which is more suitable for a TCP-based service.\n\nb. Option B is incorrect because:\n   - It uses an Amazon ECS cluster with public IP addresses, which may not provide the desired fixed IP addresses for allow listing by other companies.\n   - The use of public IP addresses may not be the best approach for a TCP-based service, as it may not provide the same level of control and manageability as using Elastic IP addresses.\n\nc. Option D is incorrect because:\n   - It uses an Application Load Balancer (ALB), which is designed for HTTP/HTTPS traffic and may not be the best fit for a TCP-based service.\n   - It involves creating public IP addresses for each host in the ECS cluster, which can be complex and may not provide the desired fixed IP addresses for allow listing."
  },
  "25": {
    "question": "A company uses an on-premises data analytics platform. The system is highly available in a fully redundant configuration across 12 servers in the company\u2019s data center.The system runs scheduled jobs, both hourly and daily, in addition to one-time requests from users. Scheduled jobs can take between 20 minutes and 2 hours to finish running and have tight SLAs. The scheduled jobs account for 65% of the system usage. User jobs typically finish running in less than 5 minutes and have no SLA. The user jobs account for 35% of system usage. During system failures, scheduled jobs must continue to meet SLAs. However, user jobs can be delayed.A solutions architect needs to move the system to Amazon EC2 instances and adopt a consumption-based model to reduce costs with no long-term commitments. The solution must maintain high availability and must not affect the SLAs.Which solution will meet these requirements MOST cost-effectively?",
    "choices": [
      "A. Split the 12 instances across two Availability Zones in the chosen AWS Region. Run two instances in each Availability Zone as On-Demand Instances with Capacity Reservations. Run four instances in each Availability Zone as Spot Instances.",
      "B. Split the 12 instances across three Availability Zones in the chosen AWS Region. In one of the Availability Zones, run all four instances as On-Demand Instances with Capacity Reservations. Run the remaining instances as Spot Instances.",
      "C. Split the 12 instances across three Availability Zones in the chosen AWS Region. Run two instances in each Availability Zone as On-Demand Instances with a Savings Plan. Run two instances in each Availability Zone as Spot Instances.",
      "D. Split the 12 instances across three Availability Zones in the chosen AWS Region. Run three instances in each Availability Zone as On-Demand Instances with Capacity Reservations. Run one instance in each Availability Zone as a Spot Instance."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (D):\n\nThe correct answer is D because it provides the most cost-effective solution while maintaining high availability and meeting the SLA requirements for the scheduled jobs.\n\nThe key points are:\n\n- Split the 12 instances across three Availability Zones (AZs) to ensure high availability.\n- Run 3 instances in each AZ as On-Demand Instances with Capacity Reservations. This guarantees the availability and performance of the scheduled jobs, which account for 65% of the system usage and have tight SLAs.\n- Run 1 instance in each AZ as a Spot Instance. This provides a cost-effective solution for the user jobs, which account for 35% of the system usage and have no SLA.\n- The combination of On-Demand Instances with Capacity Reservations and Spot Instances ensures the system can meet the SLA requirements for the scheduled jobs while reducing costs for the user jobs.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option is not the most cost-effective because it uses more expensive On-Demand Instances than necessary. The SLA requirements can be met with fewer On-Demand Instances, as shown in the correct answer.\n\nB. This option is not the most cost-effective because it uses more expensive On-Demand Instances than necessary. Additionally, running all the instances in one AZ as On-Demand Instances with Capacity Reservations is not the most efficient use of resources.\n\nC. This option is not the most cost-effective because Savings Plans require long-term commitments, which may not be the best approach for a system with varying workloads and usage patterns."
  },
  "26": {
    "question": "A security engineer determined that an existing application retrieves credentials to an Amazon RDS for MySQL database from an encrypted file in Amazon S3. For the next version of the application, the security engineer wants to implement the following application design changes to improve security:The database must use strong, randomly generated passwords stored in a secure AWS managed service.The application resources must be deployed through AWS CloudFormation.The application must rotate credentials for the database every 90\u00a0days.A solutions architect will generate a CloudFormation template to deploy the application.Which resources specified in the CloudFormation template will meet the security engineer\u2019s requirements with the LEAST amount of operational overhead?",
    "choices": [
      "A. Generate the database password as a secret resource using AWS Secrets Manager. Create an AWS Lambda function resource to rotate the database password. Specify a Secrets Manager RotationSchedule resource to rotate the database password every 90\u00a0days.",
      "B. Generate the database password as a SecureString parameter type using AWS Systems Manager Parameter Store. Create an AWS Lambda function resource to rotate the database password. Specify a Parameter Store RotationSchedule resource to rotate the database password every 90\u00a0days.",
      "C. Generate the database password as a secret resource using AWS Secrets Manager. Create an AWS Lambda function resource to rotate the database password. Create an Amazon EventBridge scheduled rule resource to trigger the Lambda function password rotation every 90\u00a0days.",
      "D. Generate the database password as a SecureString parameter type using AWS Systems Manager Parameter Store. Specify an AWS AppSync DataSource resource to automatically rotate the database password every 90\u00a0days."
    ],
    "answer": "A",
    "explanation": "Here's a clear explanation of the correct answer and why the other choices are incorrect:\n\n1. Correct Answer: A\n\nThe correct answer is A because it meets all the security requirements with the least amount of operational overhead.\n\nExplanation:\n- Secrets Manager generates strong, randomly generated passwords and stores them securely as secrets.\n- Secrets Manager provides a built-in rotation feature, which can automatically rotate the database password every 90 days without the need for a custom Lambda function.\n- Deploying the application resources through AWS CloudFormation ensures consistency and traceability.\n\n2. Incorrect Choices:\n\nB. This option is incorrect because the AWS Systems Manager Parameter Store does not have a built-in rotation feature like Secrets Manager. Creating a custom Lambda function to rotate the password and a separate RotationSchedule resource would introduce more operational overhead.\n\nC. This option is incorrect because it does not use CloudFormation to deploy the application resources. Instead, it relies on an Amazon EventBridge scheduled rule to trigger the Lambda function for password rotation, which is more complex than the built-in rotation feature in Secrets Manager.\n\nD. This option is incorrect because the AWS::AppSync::DataSource resource is used to create data sources for resolvers in AWS AppSync, not to manage database credentials. Additionally, AWS AppSync does not provide a built-in credential rotation feature like Secrets Manager.\n\nIn summary, the correct answer (A) leverages the managed rotation feature of AWS Secrets Manager, which reduces operational overhead and aligns with the security requirements specified in the question."
  },
  "27": {
    "question": "A company is storing data in several Amazon DynamoDB tables. A solutions architect must use a serverless architecture to make the data accessible publicly through a simple API over HTTPS. The solution must scale automatically in response to demand.Which solutions meet these requirements? (Choose two.)",
    "choices": [
      "A. Create an Amazon API Gateway REST API. Configure this API with direct integrations to DynamoDB by using API Gateway\u2019s AWS integration type.",
      "B. Create an Amazon API Gateway HTTP API. Configure this API with direct integrations to Dynamo DB by using API Gateway\u2019s AWS integration type.",
      "C. Create an Amazon API Gateway HTTP API. Configure this API with integrations to AWS Lambda functions that return data from the DynamoDB tables.",
      "D. Create an accelerator in AWS Global Accelerator. Configure this accelerator with AWS Lambda@Edge function integrations that return data from the DynamoDB tables.",
      "E. Create a Network Load Balancer. Configure listener rules to forward requests to the appropriate AWS Lambda functions."
    ],
    "answer": "AC",
    "explanation": "1. Clear explanation of the correct answer (A and C):\n\nThe correct answer is A and C, for the following reasons:\n\nA. Create an Amazon API Gateway REST API. Configure this API with direct integrations to DynamoDB by using API Gateway's AWS integration type.\nThis option meets the requirements by using API Gateway to create a REST API that can directly integrate with DynamoDB tables, allowing the data to be accessed publicly through a simple API over HTTPS. The direct integration with DynamoDB allows the solution to scale automatically in response to demand.\n\nC. Create an Amazon API Gateway HTTP API. Configure this API with integrations to AWS Lambda functions that return data from the DynamoDB tables.\nThis option also meets the requirements by using API Gateway to create an HTTP API that integrates with AWS Lambda functions. The Lambda functions can then handle the logic of retrieving data from the DynamoDB tables and returning it through the API, providing a serverless architecture that can scale automatically.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. Create an Amazon API Gateway HTTP API. Configure this API with direct integrations to DynamoDB by using API Gateway's AWS integration type.\nThis option is incorrect because DynamoDB tables are not publicly accessible by default, and the question specifies that the data must be accessible publicly. The direct integration with DynamoDB would not meet this requirement.\n\nD. Create an accelerator in AWS Global Accelerator. Configure this accelerator with AWS Lambda@Edge function integrations that return data from the DynamoDB tables.\nThis option is incorrect because it does not use API Gateway, which is a requirement specified in the question. While using Global Accelerator and Lambda@Edge can improve performance and latency, they do not directly address the requirement of creating a simple API over HTTPS.\n\nE. Create a Network Load Balancer. Configure listener rules to forward requests to the appropriate AWS Lambda functions.\nThis option is incorrect because it does not use API Gateway, which is a requirement specified in the question. While a Network Load Balancer can be used to scale and route requests to Lambda functions, it does not provide the same level of functionality and features as API Gateway for creating and managing APIs."
  },
  "28": {
    "question": "A company has registered 10 new domain names. The company uses the domains for online marketing. The company needs a solution that will redirect online visitors to a specific URL for each domain. All domains and target URLs are defined in a JSON document. All DNS records are managed by Amazon Route 53.A solutions architect must implement a redirect service that accepts HTTP and HTTPS requests.Which combination of steps should the solutions architect take to meet these requirements with the LEAST amount of operational effort? (Choose three.)",
    "choices": [
      "A. Create a dynamic webpage that runs on an Amazon EC2 instance. Configure the webpage to use the JSON document in combination with the event message to look up and respond with a redirect URL.",
      "B. Create an Application Load Balancer that includes HTTP and HTTPS listeners.",
      "C. Create an AWS Lambda function that uses the JSON document in combination with the event message to look up and respond with a redirect URL.",
      "D. Use an Amazon API Gateway API with a custom domain to publish an AWS Lambda function.",
      "E. Create an Amazon CloudFront distribution. Deploy a Lambda@Edge function.",
      "F. Create an SSL certificate by using AWS Certificate Manager (ACM). Include the domains as Subject Alternative Names."
    ],
    "answer": "CEF",
    "explanation": "1. Explanation of the correct answer (CEF):\n\nC. Create an AWS Lambda function that uses the JSON document in combination with the event message to look up and respond with a redirect URL.\nThis is the core of the solution, as the Lambda function can handle the lookup of the target URLs from the JSON document and respond with the appropriate redirect URL. This reduces operational effort compared to using a web server.\n\nE. Create an Amazon CloudFront distribution. Deploy a Lambda@Edge function.\nBy using CloudFront with a Lambda@Edge function, the solution can handle the redirection at the edge, closer to the users, which further reduces operational effort. The Lambda@Edge function can perform the same lookup and redirection logic as the standalone Lambda function in option C.\n\nF. Create an SSL certificate by using AWS Certificate Manager (ACM). Include the domains as Subject Alternative Names.\nThis ensures that the redirect service can handle both HTTP and HTTPS requests, as required by the company. The SSL certificate will enable HTTPS support for the redirect service.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Create a dynamic webpage that runs on an Amazon EC2 instance. Configure the webpage to use the JSON document in combination with the event message to look up and respond with a redirect URL.\nThis would require managing and scaling an EC2 instance, which increases operational effort compared to using a serverless solution like Lambda.\n\nB. Create an Application Load Balancer that includes HTTP and HTTPS listeners.\nWhile an ALB can handle the HTTP and HTTPS listeners, it does not provide the lookup and redirection functionality. You would still need to implement a separate service (like the Lambda function in option C) to handle the lookup and redirection logic.\n\nD. Use an Amazon API Gateway API with a custom domain to publish an AWS Lambda function.\nThis approach would add the complexity of managing an API Gateway, which increases operational effort compared to a direct Lambda function integration (as in option C).\n\nIn summary, the correct answer (CEF) leverages serverless and edge computing technologies (Lambda, Lambda@Edge, CloudFront) to implement the redirect service with the least amount of operational effort, while also ensuring support for both HTTP and HTTPS requests."
  },
  "29": {
    "question": "A company that has multiple AWS accounts is using AWS Organizations. The company\u2019s AWS accounts host VPCs, Amazon EC2 instances, and containers.The company\u2019s compliance team has deployed a security tool in each VPC where the company has deployments. The security tools run on EC2 instances and send information to the AWS account that is dedicated for the compliance team. The company has tagged all the compliance-related resources with a key of \u201ccostCenter\u201d and a value or \u201ccompliance\u201d.The company wants to identify the cost of the security tools that are running on the EC2 instances so that the company can charge the compliance team\u2019s AWS account. The cost calculation must be as accurate as possible.What should a solutions architect do to meet these requirements?",
    "choices": [
      "A. In the management account of the organization, activate the costCenter user-defined tag. Configure monthly AWS Cost and Usage Reports to save to an Amazon S3 bucket in the management account. Use the tag breakdown in the report to obtain the total cost for the costCenter tagged resources.",
      "B. In the member accounts of the organization, activate the costCenter user-defined tag. Configure monthly AWS Cost and Usage Reports to save to an Amazon S3 bucket in the management account. Schedule a monthly AWS Lambda function to retrieve the reports and calculate the total cost for the costCenter tagged resources.",
      "C. In the member accounts of the organization activate the costCenter user-defined tag. From the management account, schedule a monthly AWS Cost and Usage Report. Use the tag breakdown in the report to calculate the total cost for the costCenter tagged resources.",
      "D. Create a custom report in the organization view in AWS Trusted Advisor. Configure the report to generate a monthly billing summary for the costCenter tagged resources in the compliance team\u2019s AWS account."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\n\nThe correct answer is A because the organization's management account should be the one to activate the user-defined \"costCenter\" tag. This is necessary to ensure that the tag is available for all member accounts in the organization, and the cost allocation can be accurately calculated.\n\nBy configuring the monthly AWS Cost and Usage Reports to save to an Amazon S3 bucket in the management account, the company can then use the tag breakdown in the report to obtain the total cost for the \"costCenter\" tagged resources. This provides the most accurate cost calculation, as it leverages the centralized reporting from the management account.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. This option is incorrect because the user-defined tag should be activated in the management account, not the member accounts. Configuring the reports and calculating the costs in the member accounts would require more overhead and may not provide the same level of accuracy as the centralized approach.\n\nC. This option is incorrect because the user-defined tag should be activated in the management account, not the member accounts. Scheduling the monthly AWS Cost and Usage Report from the management account and using the tag breakdown in the report is the correct approach, as described in the correct answer (A).\n\nD. This option is incorrect because it suggests creating a custom report in the organization view in AWS Trusted Advisor, which is not the appropriate tool for this use case. The AWS Cost and Usage Report, as described in the correct answer (A), is the recommended solution for accurately identifying the cost of the security tools based on the \"costCenter\" tag."
  },
  "30": {
    "question": "A solutions architect is reviewing a company's process for taking snapshots of Amazon RDS DB instances. The company takes automatic snapshots every day and retains the snapshots for 7 days.The solutions architect needs to recommend a solution that takes snapshots every 6 hours and retains the snapshots for 30 days. The company uses AWS Organizations to manage all of its AWS accounts. The company needs a consolidated view of the health of the RDS snapshots.Which solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      "A. Turn on the cross-account management feature in AWS Backup. Create a backup plan that specifies the frequency and retention requirements. Add a tag to the DB instances. Apply the backup plan by using tags. Use AWS Backup to monitor the status of the backups.",
      "B. Turn on the cross-account management feature in Amazon RDS. Create a snapshot global policy that specifies the frequency and retention requirements. Use the RDS console in the management account to monitor the status of the backups.",
      "C. Turn on the cross-account management feature in AWS CloudFormation. From the management account, deploy a CloudFormation stack set that contains a backup plan from AWS Backup that specifies the frequency and retention requirements. Create an AWS Lambda function in the management account to monitor the status of the backups. Create an Amazon EventBridge rule in each account to run the Lambda function on a schedule.",
      "D. Configure AWS Backup in each account. Create an Amazon Data Lifecycle Manager lifecycle policy that specifies the frequency and retention requirements. Specify the DB instances as the target resource Use the Amazon Data Lifecycle Manager console in each member account to monitor the status of the backups."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\n\nThe correct answer is A because it provides the solution with the least operational overhead while meeting the requirements.\n\nAWS Backup's cross-account management feature allows you to create and manage backups across multiple AWS accounts from a central management account. This eliminates the need to configure backup settings in each individual account, reducing operational overhead.\n\nAWS Backup allows you to create a backup plan that specifies the frequency (6 hours) and retention period (30 days) for the RDS DB instance snapshots. By applying the backup plan using tags, the snapshots are automatically taken and retained as per the specified requirements, minimizing manual intervention.\n\nAdditionally, AWS Backup provides a consolidated view of the health and status of the backups across all accounts, which meets the requirement for a centralized monitoring solution.\n\n2. Explanations of the incorrect choices:\n\nB. This option uses the cross-account management feature of Amazon RDS, which is limited to managing snapshot policies and does not provide the same level of centralized control and monitoring as AWS Backup. It also does not allow for the specific frequency and retention requirements specified in the question.\n\nC. This option uses AWS CloudFormation and AWS Lambda to achieve the desired functionality, which adds complexity and operational overhead compared to the AWS Backup solution. It requires setting up and managing additional resources, such as the CloudFormation stack set and the Lambda function, which increases the complexity and maintenance required.\n\nD. This option uses Amazon Data Lifecycle Manager (DLM) to manage the snapshots, which is limited to a single AWS account. It does not provide the cross-account management capabilities of AWS Backup, and it would require configuring the DLM policy in each individual account, leading to higher operational overhead."
  },
  "31": {
    "question": "An enterprise company wants to allow its developers to purchase third-party software through AWS Marketplace. The company uses an AWS Organizations account structure with full features enabled, and has a shared services account in each organizational unit (OU) that will be used by procurement managers. The procurement team\u2019s policy indicates that developers should be able to obtain third-party software from an approved list only and use Private Marketplace in AWS Marketplace to achieve this requirement. The procurement team wants administration of Private Marketplace to be restricted to a role named procurement-manager-role, which could be assumed by procurement managers. Other IAM users, groups, roles, and account administrators in the company should be denied Private Marketplace administrative access.What is the MOST efficient way to design an architecture to meet these requirements?",
    "choices": [
      "A. Create an IAM role named procurement-manager-role in all AWS accounts in the organization. Add the PowerUserAccess managed policy to the role. Apply an inline policy to all IAM users and roles in every AWS account to deny permissions on the AWSPrivateMarketplaceAdminFullAccess managed policy.",
      "B. Create an IAM role named procurement-manager-role in all AWS accounts in the organization. Add the AdministratorAccess managed policy to the role. Define a permissions boundary with the AWSPrivateMarketplaceAdminFullAccess managed policy and attach it to all the developer roles.",
      "C. Create an IAM role named procurement-manager-role in all the shared services accounts in the organization. Add the AWSPrivateMarketplaceAdminFullAccess managed policy to the role. Create an organization root-level SCP to deny permissions to administer Private Marketplace to everyone except the role named procurement-manager-role. Create another organization root-level SCP to deny permissions to create an IAM role named procurement-manager-role to everyone in the organization.",
      "D. Create an IAM role named procurement-manager-role in all AWS accounts that will be used by developers. Add the AWSPrivateMarketplaceAdminFullAccess managed policy to the role. Create an SCP in Organizations to deny permissions to administer Private Marketplace to everyone except the role named procurement-manager-role. Apply the SCP to all the shared services accounts in the organization."
    ],
    "answer": "C",
    "explanation": "Explanation:\n\n1. Why the correct answer (C) is right:\n\nThe most efficient way to design an architecture to meet these requirements is option C. This approach leverages the following:\n\na. Create an IAM role named 'procurement-manager-role' in all the shared services accounts in the organization and add the 'AWSPrivateMarketplaceAdminFullAccess' managed policy to the role. This ensures that the procurement managers have the necessary permissions to administer the Private Marketplace.\n\nb. Create an organization root-level Service Control Policy (SCP) to deny permissions to administer the Private Marketplace to everyone except the 'procurement-manager-role'. This SCP applies to the entire organization, effectively restricting Private Marketplace administrative access to only the procurement managers.\n\nc. Create another organization root-level SCP to deny permissions to create an IAM role named 'procurement-manager-role' to everyone in the organization. This additional SCP further strengthens the security by preventing unauthorized creation of the privileged role.\n\nBy using the organization-wide SCP approach, this solution efficiently and effectively restricts Private Marketplace administrative access to the designated procurement managers, while denying such access to all other IAM users, groups, roles, and account administrators in the company.\n\n2. Why the incorrect choices are wrong:\n\nA. This option uses an IAM role-based approach, but it applies the 'PowerUserAccess' policy, which is too broad and grants more permissions than necessary. Additionally, the inline policy to deny 'AWSPrivateMarketplaceAdminFullAccess' is applied to all IAM users and roles, which is less efficient than using an SCP.\n\nB. This option uses a permissions boundary approach, which is not the most efficient solution. The 'AdministratorAccess' policy is also too broad, and applying the permissions boundary to all developer roles is more complex than the SCP approach in option C.\n\nD. This option places the 'procurement-manager-role' in the developer accounts, which increases the risk of mismanagement. Additionally, applying the SCP only to the shared services accounts does not adequately restrict access across the entire organization, as required by the question."
  },
  "32": {
    "question": "A company is in the process of implementing AWS Organizations to constrain its developers to use only Amazon EC2, Amazon S3, and Amazon DynamoDB. The developers account resides in a dedicated organizational unit (OU). The solutions architect has implemented the following SCP on the developers account:When this policy is deployed, IAM users in the developers account are still able to use AWS services that are not listed in the policy.What should the solutions architect do to eliminate the developers\u2019 ability to use services outside the scope of this policy?",
    "choices": [
      "A. Create an explicit deny statement for each AWS service that should be constrained.",
      "B. Remove the FullAWSAccess SCP from the developers account\u2019s OU.",
      "C. Modify the FullAWSAccess SCP to explicitly deny all services.",
      "D. Add an explicit deny statement using a wildcard to the end of the SCP."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B, \"Remove the FullAWSAccess SCP from the developers account's OU.\" This is the right approach because the FullAWSAccess SCP is applied by default when AWS Organizations is enabled. This SCP grants full access to all AWS services, which is the opposite of the requirement to constrain developers to use only Amazon EC2, Amazon S3, and Amazon DynamoDB.\n\nBy removing the FullAWSAccess SCP from the developers account's OU, the solutions architect can then apply a custom SCP that explicitly allows only the desired services (EC2, S3, and DynamoDB), effectively eliminating the developers' ability to use other AWS services.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. \"Create an explicit deny statement for each AWS service that should be constrained\": This is not the best approach because it would require the solutions architect to maintain a long list of denied services, which can become cumbersome and error-prone as new services are added to AWS.\n\nC. \"Modify the FullAWSAccess SCP to explicitly deny all services\": This is not the right approach because the FullAWSAccess SCP is a broad policy that grants full access by default. Modifying it to deny all services would not achieve the desired goal of constraining the developers to a specific set of services.\n\nD. \"Add an explicit deny statement using a wildcard to the end of the SCP\": This is not the correct solution because the SCP should be designed to explicitly allow only the required services (EC2, S3, and DynamoDB), rather than attempting to deny all other services. Using a wildcard deny statement can lead to unintended consequences and may not be the most maintainable approach."
  },
  "33": {
    "question": "A company is hosting a monolithic REST-based API for a mobile app on five Amazon EC2 instances in public subnets of a VPC. Mobile clients connect to the API by using a domain name that is hosted on Amazon Route 53. The company has created a Route 53 multivalue answer routing policy with the IP addresses of all the EC2 instances. Recently, the app has been overwhelmed by large and sudden increases to traffic. The app has not been able to keep up with the traffic.A solutions architect needs to implement a solution so that the app can handle the new and varying load.Which solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      "A. Separate the API into individual AWS Lambda functions. Configure an Amazon API Gateway REST API with Lambda integration for the backend. Update the Route 53 record to point to the API Gateway API.",
      "B. Containerize the API logic. Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. Run the containers in the cluster by using Amazon EC2. Create a Kubernetes ingress. Update the Route 53 record to point to the Kubernetes ingress.",
      "C. Create an Auto Scaling group. Place all the EC2 instances in the Auto Scaling group. Configure the Auto Scaling group to perform scaling actions that are based on CPU utilization. Create an AWS Lambda function that reacts to Auto Scaling group changes and updates the Route 53 record.",
      "D. Create an Application Load Balancer (ALB) in front of the API. Move the EC2 instances to private subnets in the VPC. Add the EC2 instances as targets for the ALB. Update the Route 53 record to point to the ALB."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (D):\n\nThe correct answer is D because it provides the least operational overhead solution to handle the varying load on the API. Here's why:\n\n- Creating an Application Load Balancer (ALB) in front of the API and moving the EC2 instances to private subnets offloads the load balancing and scalability logic to the managed AWS service (ALB). This reduces the operational overhead compared to other options.\n- The ALB can automatically scale and distribute incoming traffic across the EC2 instances, handling the varying load without the need for manual intervention.\n- Updating the Route 53 record to point to the ALB further simplifies the setup, as clients will continue to connect to the same domain name, and the ALB will handle the load balancing.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Separating the API into individual AWS Lambda functions and using API Gateway may reduce the operational overhead, but it introduces significant development overhead. Refactoring the monolithic API into a serverless architecture is a complex task and may not be the least operationally intensive solution in this case.\n\nB. Containerizing the API and running it on Amazon EKS would introduce more operational complexity compared to the ALB solution. Managing the Kubernetes cluster, scaling, and updating the Kubernetes ingress would require more ongoing effort.\n\nC. Creating an Auto Scaling group and a Lambda function to update the Route 53 record would be more operationally intensive than the ALB solution. The Auto Scaling group and the custom Lambda function would need to be managed and monitored, adding to the operational overhead.\n\nIn summary, the correct answer (D) is the least operationally intensive solution because it leverages the managed AWS service (ALB) to handle the load balancing and scalability, reducing the need for manual intervention and ongoing management compared to the other options."
  },
  "34": {
    "question": "A company has created an OU in AWS Organizations for each of its engineering teams. Each OU owns multiple AWS accounts. The organization has hundreds of AWS accounts.A solutions architect must design a solution so that each OU can view a breakdown of usage costs across its AWS accounts.Which solution meets these requirements?",
    "choices": [
      "A. Create an AWS Cost and Usage Report (CUR) for each OU by using AWS Resource Access Manager. Allow each team to visualize the CUR through an Amazon QuickSight dashboard.",
      "B. Create an AWS Cost and Usage Report (CUR) from the AWS Organizations management account. Allow each team to visualize the CUR through an Amazon QuickSight dashboard.",
      "C. Create an AWS Cost and Usage Report (CUR) in each AWS Organizations member account. Allow each team to visualize the CUR through an Amazon QuickSight dashboard.",
      "D. Create an AWS Cost and Usage Report (CUR) by using AWS Systems Manager. Allow each team to visualize the CUR through Systems Manager OpsCenter dashboards."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is option B because it best meets the requirements of the problem statement. Creating an AWS Cost and Usage Report (CUR) from the AWS Organizations management account allows for a centralized view of the usage costs across all the member accounts. This is the most efficient approach, as it avoids the need to create and manage individual CURs for each OU or member account. By allowing each team to visualize the CUR through an Amazon QuickSight dashboard, the solution provides a user-friendly way for the teams to access and analyze the cost breakdown data.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option is incorrect because it requires using AWS Resource Access Manager to create a CUR for each OU, which is more complex and resource-intensive than the centralized approach in option B. Additionally, visualizing the CUR through an Amazon QuickSight dashboard is not a sufficient solution, as it does not provide a centralized view of the cost data.\n\nC. This option is incorrect because creating a CUR in each AWS Organizations member account is not scalable and would require significant overhead to manage and aggregate the data. It is more efficient to create a single CUR from the management account, as in option B.\n\nD. This option is incorrect because it suggests using AWS Systems Manager and OpsCenter dashboards for visualizing the CUR, which is not the most appropriate solution. Amazon QuickSight is a better choice for visualizing and analyzing cost data, as it is specifically designed for this purpose.\n\nIn summary, option B is the correct answer because it provides a centralized, scalable, and user-friendly solution for the given requirements, while the other options have limitations in terms of complexity, scalability, or suitability for the task at hand."
  },
  "35": {
    "question": "A company needs to architect a hybrid DNS solution. This solution will use an Amazon Route 53 private hosted zone for the domain cloud.example.com for the resources stored within VPCs.The company has the following DNS resolution requirements:On-premises systems should be able to resolve and connect to cloud.example.com.All VPCs should be able to resolve cloud.example.com.There is already an AWS Direct Connect connection between the on-premises corporate network and AWS Transit Gateway.Which architecture should the company use to meet these requirements with the HIGHEST performance?",
    "choices": [
      "A. Associate the private hosted zone to all the VPCs. Create a Route 53 inbound resolver in the shared services VPC. Attach all VPCs to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the inbound resolver.",
      "B. Associate the private hosted zone to all the VPCs. Deploy an Amazon EC2 conditional forwarder in the shared services VPC. Attach all VPCs to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the conditional forwarder.",
      "C. Associate the private hosted zone to the shared services VPCreate a Route 53 outbound resolver in the shared services VPAttach all VPCs to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the outbound resolver.",
      "D. Associate the private hosted zone to the shared services VPC. Create a Route 53 inbound resolver in the shared services VPC. Attach the shared services VPC to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the inbound resolver."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\n\nThe correct answer is A because it meets all the requirements stated in the question:\n\n- It associates the private hosted zone for cloud.example.com to all the VPCs, ensuring that all VPCs can resolve the domain.\n- It creates a Route 53 inbound resolver in the shared services VPC, which allows on-premises systems to resolve and connect to cloud.example.com by forwarding the DNS queries to the inbound resolver.\n- It attaches all VPCs to the transit gateway, enabling communication between the on-premises network and the VPCs.\n- It creates forwarding rules in the on-premises DNS server for cloud.example.com, pointing to the inbound resolver, allowing on-premises systems to resolve the domain.\n\nThis architecture ensures the highest performance by using the Route 53 inbound resolver, which is a managed service provided by AWS, rather than relying on a custom EC2 instance as a conditional forwarder (as in option B).\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. This option uses an EC2 instance as a conditional forwarder, which may not provide the highest performance compared to the managed Route 53 inbound resolver in option A.\n\nC. This option does not associate the private hosted zone to all the VPCs, which is a requirement stated in the question. It only associates the private hosted zone to the shared services VPC, which means the other VPCs would not be able to resolve cloud.example.com.\n\nD. This option also does not associate the private hosted zone to all the VPCs, which is a requirement stated in the question. It only associates the private hosted zone to the shared services VPC, which means the other VPCs would not be able to resolve cloud.example.com."
  },
  "36": {
    "question": "A company\u2019s solutions architect is reviewing a web application that runs on AWS. The application references static assets in an Amazon S3 bucket in the us-east-1 Region. The company needs resiliency across multiple AWS Regions. The company already has created an S3 bucket in a second Region.Which solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      "A. Configure the application to write each object to both S3 buckets. Set up an Amazon Route 53 public hosted zone with a record set by using a weighted routing policy for each S3 bucket. Configure the application to reference the objects by using the Route 53 DNS name.",
      "B. Create an AWS Lambda function to copy objects from the S3 bucket in us-east-1 to the S3 bucket in the second Region. Invoke the Lambda function each time an object is written to the S3 bucket in us-east-1. Set up an Amazon CloudFront distribution with an origin group that contains the two S3 buckets as origins.",
      "C. Configure replication on the S3 bucket in us-east-1 to replicate objects to the S3 bucket in the second Region. Set up an Amazon CloudFront distribution with an origin group that contains the two S3 buckets as origins.",
      "D. Configure replication on the S3 bucket in us-east-1 to replicate objects to the S3 bucket in the second Region. If failover is required, update the application code to load S3 objects from the S3 bucket in the second Region."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C, which involves the following steps:\n\n1. Configure replication on the S3 bucket in `us-east-1` to replicate objects to the S3 bucket in the second Region.\n2. Set up an Amazon CloudFront distribution with an origin group that contains the two S3 buckets as origins.\n\nThis solution meets the requirements with the least operational overhead for the following reasons:\n\n- S3 Cross-Region Replication (CRR) automatically replicates objects between the two S3 buckets in different Regions, providing resiliency across multiple Regions.\n- CloudFront's origin group feature automatically handles failover between the two S3 buckets, providing high availability and low latency for the static assets.\n- This solution is highly automated and requires minimal maintenance, as the replication and failover are handled by the AWS services.\n\n2. Explanations of the incorrect choices:\n\nA. This option involves manual configuration of Route 53 and application changes to reference the Route 53 DNS name, which adds operational overhead.\n\nB. This option requires a custom Lambda function to copy objects between the S3 buckets, which adds more operational complexity compared to the S3 CRR solution.\n\nD. This option requires manual updates to the application code to handle failover to the second Region, which adds operational overhead compared to the automated CloudFront failover in the correct answer."
  },
  "37": {
    "question": "A company has a web application that allows users to upload short videos. The videos are stored on Amazon EBS volumes and analyzed by custom recognition software for categorization.The website contains static content that has variable traffic with peaks in certain months. The architecture consists of Amazon EC2 instances running in an Auto Scaling group for the web application and EC2 instances running in an Auto Scaling group to process an Amazon SQS queue. The company wants to re-architect the application to reduce operational overhead using AWS managed services where possible and remove dependencies on third-party software.Which solution meets these requirements?",
    "choices": [
      "A. Use Amazon ECS containers for the web application and Spot instances for the Auto Scaling group that processes the SQS queue. Replace the custom software with Amazon Rekognition to categorize the videos.",
      "B. Store the uploaded videos in Amazon EFS and mount the file system to the EC2 instances for the web application. Process the SQS queue with an AWS Lambda function that calls the Amazon Rekognition API to categorize the videos.",
      "C. Host the web application in Amazon S3. Store the uploaded videos in Amazon S3. Use S3 event notification to publish events to the SQS queue. Process the SQS queue with an AWS Lambda function that calls the Amazon Rekognition API to categorize the videos.",
      "D. Use AWS Elastic Beanstalk to launch EC2 instances in an Auto Scaling group for the web application and launch a worker environment to process the SQS queue. Replace the custom software with Amazon Rekognition to categorize the videos."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C because it leverages multiple AWS managed services to reduce operational overhead and eliminate dependencies on third-party software, as required by the question.\n\nKey points:\n- Hosting the web application in Amazon S3 makes it highly available, scalable, and able to handle variable traffic without the need to manage EC2 instances.\n- Storing the uploaded videos in Amazon S3 provides a scalable and reliable storage solution.\n- Using S3 event notifications to trigger an AWS Lambda function that calls the Amazon Rekognition API for video categorization eliminates the need for custom recognition software.\n- Processing the SQS queue with a Lambda function removes the need to manage EC2 instances for this task.\n\nThis solution effectively uses a combination of S3, SQS, Lambda, and Rekognition to create a serverless architecture that can scale and adapt to the company's requirements without the need for manual server management.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This solution uses EC2 instances and Spot instances, which still require operational overhead for managing the infrastructure. Additionally, the question does not mention the use of AWS Fargate, which would be necessary to truly reduce the operational overhead of container management.\n\nB. While using Amazon EFS can provide a shared file system for the EC2 instances, the question states that the company wants to remove dependencies on third-party software. Replacing the custom recognition software with a Lambda function that calls Amazon Rekognition is a better approach.\n\nD. Hosting the web application in AWS Elastic Beanstalk is a viable option, but it still requires the management of EC2 instances, which the question states the company wants to avoid. The question also specifically mentions the desire to use managed services where possible, and Elastic Beanstalk does not fully eliminate the need for infrastructure management."
  },
  "38": {
    "question": "A company is using AWS Organizations to manage multiple AWS accounts. For security purposes, the company requires the creation of an Amazon Simple Notification Service (Amazon SNS) topic that enables integration with a third-party alerting system in all the Organizations member accounts.A solutions architect used an AWS CloudFormation template to create the SNS topic and stack sets to automate the deployment of CloudFormation stacks. Trusted access has been enabled in Organizations.What should the solutions architect do to deploy the CloudFormation StackSets in all AWS accounts?",
    "choices": [
      "A. Create a stack set in the Organizations member accounts. Use service-managed permissions. Set deployment options to deploy to an organization. Use CloudFormation StackSets drift detection.",
      "B. Create stacks in the Organizations member accounts. Use self-service permissions. Set deployment options to deploy to an organization. Enable the CloudFormation StackSets automatic deployment.",
      "C. Create a stack set in the Organizations management account. Use service-managed permissions. Set deployment options to deploy to the organization. Enable CloudFormation StackSets automatic deployment.",
      "D. Create stacks in the Organizations management account. Use service-managed permissions. Set deployment options to deploy to the organization. Enable CloudFormation StackSets drift detection."
    ],
    "answer": "C",
    "explanation": "1. Clear explanation of why the correct answer (C) is right:\n\nThe correct answer is C because it aligns best with the requirements and capabilities of AWS CloudFormation StackSets in the context of AWS Organizations.\n\nIn this scenario, the company requires the creation of an Amazon SNS topic in all the AWS Organizations member accounts. Using CloudFormation StackSets is an efficient way to automate the deployment of this resource across the organization.\n\nBy creating the stack set in the Organizations management account and using service-managed permissions, the solutions architect can ensure that the stack is automatically deployed to all member accounts, including any new accounts that may be added in the future. The \"deploy to organization\" deployment option ensures that the stack is deployed to all targeted accounts, and enabling automatic deployment removes the need for manual intervention.\n\n2. Brief explanations of why the incorrect choices are wrong:\n\nA. This option is incorrect because it involves creating the stack set in the member accounts, rather than the management account. The management account is the central point of control for the organization, and it is the recommended location to create and manage the stack set.\n\nB. This option is incorrect because it involves creating individual stacks in the member accounts, rather than a single stack set. This would require more manual effort and would not provide the same level of automation and centralized management as using a stack set.\n\nD. This option is incorrect because it involves creating individual stacks in the management account, rather than a stack set. While enabling drift detection is a useful feature, it does not address the primary requirement of automating the deployment of the SNS topic across all member accounts.\n\nIn summary, the correct answer (C) is the most appropriate solution because it leverages the capabilities of CloudFormation StackSets and the centralized management capabilities of AWS Organizations to efficiently and automatically deploy the required SNS topic across all member accounts."
  },
  "39": {
    "question": "A company wants to migrate its workloads from on premises to AWS. The workloads run on Linux and Windows. The company has a large on-premises infrastructure that consists of physical machines and VMs that host numerous applications.The company must capture details about the system configuration, system performance, running processes, and network connections of its on-premises workloads. The company also must divide the on-premises applications into groups for AWS migrations. The company needs recommendations for Amazon EC2 instance types so that the company can run its workloads on AWS in the most cost-effective manner.Which combination of steps should a solutions architect take to meet these requirements? (Choose three.)",
    "choices": [
      "A. Assess the existing applications by installing AWS Application Discovery Agent on the physical machines and VMs.",
      "B. Assess the existing applications by installing AWS Systems Manager Agent on the physical machines and VMs.",
      "C. Group servers into applications for migration by using AWS Systems Manager Application Manager.",
      "D. Group servers into applications for migration by using AWS Migration Hub.",
      "E. Generate recommended instance types and associated costs by using AWS Migration Hub.",
      "F. Import data about server sizes into AWS Trusted Advisor. Follow the recommendations for cost optimization."
    ],
    "answer": "ADE",
    "explanation": "1. Explanation of the correct answer:\n\nThe correct answer is A, D, and E.\n\nA. Assess the existing applications by installing AWS Application Discovery Agent on the physical machines and VMs.\nThis is the correct choice because the AWS Application Discovery Agent is specifically designed to collect detailed information about the on-premises workloads, including system configuration, performance, running processes, and network connections. This data is essential for planning the migration to AWS.\n\nD. Group servers into applications for migration by using AWS Migration Hub.\nThis is the correct choice because AWS Migration Hub provides a centralized way to group the on-premises servers into applications, which is necessary for the migration planning and execution. The grouping of servers into applications is a crucial step in the migration process.\n\nE. Generate recommended instance types and associated costs by using AWS Migration Hub.\nThis is the correct choice because AWS Migration Hub can analyze the collected data from the AWS Application Discovery Agent and provide recommendations for the most cost-effective Amazon EC2 instance types to run the migrated workloads on AWS. This helps the company optimize the costs of running their workloads on AWS.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. Assess the existing applications by installing AWS Systems Manager Agent on the physical machines and VMs.\nThis choice is incorrect because the AWS Systems Manager Agent is primarily used for managing and automating tasks on EC2 instances, not for capturing detailed application and performance data during an assessment phase. The AWS Application Discovery Agent is more appropriate for this purpose.\n\nC. Group servers into applications for migration by using AWS Systems Manager Application Manager.\nThis choice is incorrect because AWS Systems Manager Application Manager is not the recommended tool for grouping servers into applications for migration. AWS Migration Hub is the more appropriate tool for this task, as it is specifically designed for migration planning and execution.\n\nF. Import data about server sizes into AWS Trusted Advisor. Follow the recommendations for cost optimization.\nThis choice is incorrect because AWS Trusted Advisor does not have the capability to import data about server sizes. Trusted Advisor is designed to provide recommendations based on the existing AWS environment, not for analyzing on-premises data. The correct approach is to use AWS Migration Hub to generate the recommended instance types and associated costs."
  },
  "40": {
    "question": "A company is hosting an image-processing service on AWS in a VPC. The VPC extends across two Availability Zones. Each Availability Zone contains one public subnet and one private subnet.The service runs on Amazon EC2 instances in the private subnets. An Application Load Balancer in the public subnets is in front of the service. The service needs to communicate with the internet and does so through two NAT gateways. The service uses Amazon S3 for image storage. The EC2 instances retrieve approximately 1 \u0422\u0412 of data from an S3 bucket each day.The company has promoted the service as highly secure. A solutions architect must reduce cloud expenditures as much as possible without compromising the service\u2019s security posture or increasing the time spent on ongoing operations.Which solution will meet these requirements?",
    "choices": [
      "A. Replace the NAT gateways with NAT instances. In the VPC route table, create a route from the private subnets to the NAT instances.",
      "B. Move the EC2 instances to the public subnets. Remove the NAT gateways.",
      "C. Set up an S3 gateway VPC endpoint in the VPAttach an endpoint policy to the endpoint to allow the required actions on the S3 bucket.",
      "D. Attach an Amazon Elastic File System (Amazon EFS) volume to the EC2 instances. Host the images on the EFS volume."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C. Setting up an S3 gateway VPC endpoint in the VPC and attaching an endpoint policy to the endpoint is the best solution to meet the requirements of reducing cloud expenditures without compromising the service's security posture or increasing the time spent on ongoing operations.\n\nThe key reasons why C is the right answer:\n\n- Gateway VPC endpoints for Amazon S3 are not charged for data processing or data transfer, unlike NAT gateways. This can significantly reduce the cloud expenditures, especially considering the 1 TB of data being retrieved from S3 daily.\n- By using the gateway endpoint, the traffic between the EC2 instances and S3 stays within the AWS network, avoiding the need to go through the internet and the NAT gateways. This improves the security posture of the service.\n- Configuring and maintaining a gateway VPC endpoint is relatively simple and requires minimal ongoing operations, aligning with the requirement to avoid increasing the time spent on ongoing operations.\n- The endpoint policy can be used to allow only the required actions on the specific S3 bucket, maintaining the desired security posture.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Replace the NAT gateways with NAT instances: This would not reduce the cloud expenditures, as NAT instances still incur data processing and data transfer charges, similar to NAT gateways. It also increases the operational overhead, as NAT instances require more management and maintenance.\n\nB. Move the EC2 instances to the public subnets and remove the NAT gateways: This would not be a highly secure solution, as the EC2 instances would have direct internet access, increasing the attack surface. Additionally, the data transfer between the EC2 instances and S3 would still incur costs, as it would have to go through the internet gateway.\n\nD. Attach an Amazon Elastic File System (Amazon EFS) volume to the EC2 instances and host the images on the EFS volume: This solution does not address the requirement of reducing cloud expenditures, as EFS can be more expensive than using S3 for image storage, especially for large data volumes like 1 TB per day. It also introduces additional operational complexity"
  },
  "41": {
    "question": "A company recently deployed an application on AWS. The application uses Amazon DynamoDB. The company measured the application load and configured the RCUs and WCUs on the DynamoDB table to match the expected peak load. The peak load occurs once a week for a 4-hour period and is double the average load. The application load is close to the average load for the rest of the week. The access pattern includes many more writes to the table than reads of the table.A solutions architect needs to implement a solution to minimize the cost of the table.Which solution will meet these requirements?",
    "choices": [
      "A. Use AWS Application Auto Scaling to increase capacity during the peak period. Purchase reserved RCUs and WCUs to match the average load.",
      "B. Configure on-demand capacity mode for the table.",
      "C. Configure DynamoDB Accelerator (DAX) in front of the table. Reduce the provisioned read capacity to match the new peak load on the table.",
      "D. Configure DynamoDB Accelerator (DAX) in front of the table. Configure on-demand capacity mode for the table."
    ],
    "answer": "A",
    "explanation": "Explanation:\n\n1. Correct Answer: A. Use AWS Application Auto Scaling to increase capacity during the peak period. Purchase reserved RCUs and WCUs to match the average load.\n\nThis is the correct answer because it addresses the key requirements:\n\n- The application has a known usage pattern, with a 4-hour peak period once a week that is double the average load.\n- The access pattern includes many more writes than reads.\n- The goal is to minimize the cost of the DynamoDB table.\n\nBy using AWS Application Auto Scaling, the solution can automatically scale up the provisioned capacity (RCUs and WCUs) during the peak 4-hour period to handle the increased load. This avoids over-provisioning capacity for the rest of the week.\n\nTo handle the average load, the solution purchases reserved RCUs and WCUs. This provides a lower cost for the baseline capacity requirements.\n\nThe combination of auto-scaling during peaks and reserved capacity for the average load allows this solution to meet the cost optimization requirement.\n\n2. Incorrect Choices:\n\nB. Configure on-demand capacity mode for the table.\n- On-demand capacity mode is not the best choice here, as the usage pattern is known. On-demand is better suited for unpredictable or highly variable workloads.\n- Given the known peak loads, provisioned capacity mode with auto-scaling can provide a more cost-effective solution.\n\nC. Configure DynamoDB Accelerator (DAX) in front of the table. Reduce the provisioned read capacity to match the new peak load on the table.\n- While DAX can help improve read performance, it does not address the write-heavy workload requirement.\n- Reducing the provisioned read capacity to match the peak load may not be sufficient, as the application has a higher write load.\n\nD. Configure DynamoDB Accelerator (DAX) in front of the table. Configure on-demand capacity mode for the table.\n- Similar to option B, on-demand capacity mode is not the best choice for this known usage pattern.\n- Configuring DAX may help with read performance, but it does not address the write-heavy workload or the cost optimization requirement.\n\nIn summary, the correct answer (A) leverages the strengths of AWS Application Auto Scaling"
  },
  "42": {
    "question": "A software as a service (SaaS) company provides a media software solution to customers. The solution is hosted on 50 VPCs across various AWS Regions and AWS accounts. One of the VPCs is designated as a management VPC. The compute resources in the VPCs work independently.The company has developed a new feature that requires all 50 VPCs to be able to communicate with each other. The new feature also requires one-way access from each customer's VPC to the company's management VPC. The management VPC hosts a compute resource that validates licenses for the media software solution.The number of VPCs that the company will use to host the solution will continue to increase as the solution grows.Which combination of steps will provide the required VPC connectivity with the LEAST operational overhead? (Choose two.)",
    "choices": [
      "A. Create a transit gateway. Attach all the company's VPCs and relevant subnets to the transit gateway.",
      "B. Create VPC peering connections between all the company's VPCs.",
      "C. Create a Network Load Balancer (NLB) that points to the compute resource for license validation. Create an AWS PrivateLink endpoint service that is available to each customer's VPAssociate the endpoint service with the NLB.",
      "D. Create a VPN appliance in each customer's VPC. Connect the company's management VPC to each customer's VPC by using AWS Site-to-Site VPN.",
      "E. Create a VPC peering connection between the company's management VPC and each customer's VPC."
    ],
    "answer": "AC",
    "explanation": "1. Explanation of the correct answer (A and C):\n\nA. Create a transit gateway. Attach all the company's VPCs and relevant subnets to the transit gateway.\n- This is the correct choice because a transit gateway provides a scalable and centralized way to manage VPC connectivity. It allows the company to establish a mesh network where all the VPCs can communicate with each other, reducing the operational overhead compared to VPC peering.\n- Transit gateways also support peering between transit gateways in different regions, which is useful for the company as the number of VPCs is expected to increase across regions.\n\nC. Create a Network Load Balancer (NLB) that points to the compute resource for license validation. Create an AWS PrivateLink endpoint service that is available to each customer's VPC. Associate the endpoint service with the NLB.\n- This is the correct choice because it provides a secure and scalable way for the customer VPCs to access the license validation compute resource in the management VPC, without the need for internet gateways, NAT gateways, or VPNs.\n- AWS PrivateLink simplifies network configuration and reduces operational overhead by allowing the customer VPCs to access the management VPC resources directly.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. Create VPC peering connections between all the company's VPCs.\n- This is incorrect because creating VPC peering connections between all the VPCs would be highly complex and operationally intensive, especially as the number of VPCs increases. VPC peering does not scale well and can be difficult to manage.\n\nD. Create a VPN appliance in each customer's VPC. Connect the company's management VPC to each customer's VPC by using AWS Site-to-Site VPN.\n- This is incorrect because it would require significant operational overhead to manage the VPN appliances in each customer VPC and the VPN connections to the management VPC. It also introduces additional points of failure and complexity.\n\nE. Create a VPC peering connection between the company's management VPC and each customer's VPC.\n- This is incorrect because it would require creating and managing a separate VPC peering connection for each customer VPC, which would become"
  },
  "43": {
    "question": "A company is using Amazon OpenSearch Service to analyze data. The company loads data into an OpenSearch Service cluster with 10 data nodes from an Amazon S3 bucket that uses S3 Standard storage. The data resides in the cluster for 1 month for read-only analysis. After 1 month, the company deletes the index that contains the data from the cluster. For compliance purposes, the company must retain a copy of all input data.The company is concerned about ongoing costs and asks a solutions architect to recommend a new solution.Which solution will meet these requirements MOST cost-effectively?",
    "choices": [
      "A. Replace all the data nodes with UltraWarm nodes to handle the expected capacity. Transition the input data from S3 Standard to S3 Glacier Deep Archive when the company loads the data into the cluster.",
      "B. Reduce the number of data nodes in the cluster to 2 Add UltraWarm nodes to handle the expected capacity. Configure the indexes to transition to UltraWarm when OpenSearch Service ingests the data. Transition the input data to S3 Glacier Deep Archive after 1 month by using an S3 Lifecycle policy.",
      "C. Reduce the number of data nodes in the cluster to 2. Add UltraWarm nodes to handle the expected capacity. Configure the indexes to transition to UltraWarm when OpenSearch Service ingests the data. Add cold storage nodes to the cluster Transition the indexes from UltraWarm to cold storage. Delete the input data from the S3 bucket after 1 month by using an S3 Lifecycle policy.",
      "D. Reduce the number of data nodes in the cluster to 2. Add instance-backed data nodes to handle the expected capacity. Transition the input data from S3 Standard to S3 Glacier Deep Archive when the company loads the data into the cluster."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B because it provides the most cost-effective solution that meets the requirements. The key aspects of this solution are:\n\n- Reducing the number of data nodes in the cluster to 2, which reduces the overall compute costs.\n- Adding UltraWarm nodes to handle the expected capacity. UltraWarm nodes are a lower-cost storage tier in Amazon OpenSearch Service, which is well-suited for infrequently accessed data.\n- Configuring the indexes to transition to UltraWarm when the data is ingested. This ensures that the data is stored in the lower-cost UltraWarm tier from the beginning.\n- Transitioning the input data from S3 Standard to S3 Glacier Deep Archive after 1 month using an S3 Lifecycle policy. This reduces the storage costs for the long-term retention of the data, as S3 Glacier Deep Archive is a much lower-cost storage class.\n\nThis solution is the most cost-effective as it optimizes both the OpenSearch Service cluster and the S3 storage to minimize ongoing costs, while still meeting the requirement to retain the data for compliance purposes.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Replacing all the data nodes with UltraWarm nodes is not supported in Amazon OpenSearch Service. UltraWarm is a complementary storage tier and cannot be used as the sole storage tier for the cluster.\n\nC. This solution involves additional complexity in configuring the indexes to transition between different storage tiers (UltraWarm and cold storage), which may increase operational overhead. Additionally, deleting the input data from the S3 bucket after 1 month violates the requirement to retain a copy of all input data for compliance purposes.\n\nD. Transitioning the input data from S3 Standard to S3 Glacier Deep Archive when loading the data into the cluster contradicts the requirement to retain the data for 1 month for read-only analysis. The data needs to be available in the OpenSearch Service cluster for the 1-month period before being transitioned to lower-cost storage."
  },
  "44": {
    "question": "A company has 10 accounts that are part of an organization in AWS Organizations. AWS Config is configured in each account. All accounts belong to either the Prod OU or the NonProd OU.The company has set up an Amazon EventBridge rule in each AWS account to notify an Amazon Simple Notification Service (Amazon SNS) topic when an Amazon EC2 security group inbound rule is created with 0.0.0.0/0 as the source. The company\u2019s security team is subscribed to the SNS topic.For all accounts in the NonProd OU, the security team needs to remove the ability to create a security group inbound rule that includes 0.0.0.0/0 as the source.Which solution will meet this requirement with the LEAST operational overhead?",
    "choices": [
      "A. Modify the EventBridge rule to invoke an AWS Lambda function to remove the security group inbound rule and to publish to the SNS topic. Deploy the updated rule to the NonProd OU.",
      "B. Add the vpc-sg-open-only-to-authorized-ports AWS Config managed rule to the NonProd OU.",
      "C. Configure an SCP to allow the ec2:AuthorizeSecurityGroupIngress action when the value of the aws:SourceIp condition key is not 0.0.0.0/0. Apply the SCP to the NonProd OU.",
      "D. Configure an SCP to deny the ec2:AuthorizeSecurityGroupIngress action when the value of the aws:SourceIp condition key is 0.0.0.0/0. Apply the SCP to the NonProd OU."
    ],
    "answer": "D",
    "explanation": "1. Clear explanation of why the correct answer is right:\n\nThe correct answer is D. Configure an SCP to deny the ec2:AuthorizeSecurityGroupIngress action when the value of the aws:SourceIp condition key is 0.0.0.0/0, and apply the SCP to the NonProd OU.\n\nThis is the best solution because it directly addresses the requirement to remove the ability to create a security group inbound rule that includes 0.0.0.0/0 as the source. By configuring an SCP to deny the ec2:AuthorizeSecurityGroupIngress action in this scenario, it prevents the creation of such rules in the first place, without the need for any additional actions or monitoring. This is the most operationally efficient solution, as it requires the least amount of ongoing management and maintenance.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nA. Modify the EventBridge rule to invoke an AWS Lambda function to remove the security group inbound rule and to publish to the SNS topic. Deploy the updated rule to the NonProd OU.\nThis solution is more operationally intensive than the correct answer, as it requires the creation and deployment of an additional Lambda function to remove the security group inbound rule. It also does not prevent the creation of the rule in the first place, which is the requirement.\n\nB. Add the vpc-sg-open-only-to-authorized-ports AWS Config managed rule to the NonProd OU.\nThis solution does not directly address the requirement to remove the ability to create a security group inbound rule with 0.0.0.0/0 as the source. The vpc-sg-open-only-to-authorized-ports rule is a monitoring rule and does not prevent the creation of such rules.\n\nC. Configure an SCP to allow the ec2:AuthorizeSecurityGroupIngress action when the value of the aws:SourceIp condition key is not 0.0.0.0/0. Apply the SCP to the NonProd OU.\nThis solution does not meet the requirement of removing the ability to create a security group inbound rule with 0.0.0.0/0 as the source. It only allows the action when the"
  },
  "45": {
    "question": "A company hosts a Git repository in an on-premises data center. The company uses webhooks to invoke functionality that runs in the AWS Cloud. The company hosts the webhook logic on a set of Amazon EC2 instances in an Auto Scaling group that the company set as a target for an Application Load Balancer (ALB). The Git server calls the ALB for the configured webhooks. The company wants to move the solution to a serverless architecture.Which solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      "A. For each webhook, create and configure an AWS Lambda function URL. Update the Git servers to call the individual Lambda function URLs.",
      "B. Create an Amazon API Gateway HTTP API. Implement each webhook logic in a separate AWS Lambda function. Update the Git servers to call the API Gateway endpoint.",
      "C. Deploy the webhook logic to AWS App Runner. Create an ALB, and set App Runner as the target. Update the Git servers to call the ALB endpoint.",
      "D. Containerize the webhook logic. Create an Amazon Elastic Container Service (Amazon ECS) cluster, and run the webhook logic in AWS Fargate. Create an Amazon API Gateway REST API, and set Fargate as the target. Update the Git servers to call the API Gateway endpoint."
    ],
    "answer": "B",
    "explanation": "1. Explanation of why the correct answer (B) is right:\n\nOption B is the best choice with the least operational overhead because it leverages the fully managed services of Amazon API Gateway and AWS Lambda to implement the webhook functionality.\n\n- Amazon API Gateway is a fully managed service that provides an easy-to-use interface for creating, publishing, maintaining, and securing APIs. This removes the need to manage the underlying infrastructure and scaling for the API endpoints.\n- AWS Lambda is a serverless compute service that automatically scales and manages the infrastructure for running the webhook logic. This eliminates the need to provision and manage EC2 instances.\n- By implementing each webhook logic in a separate Lambda function and exposing them through the API Gateway, the solution becomes highly modular and easy to maintain. New webhooks can be added or existing ones updated without impacting the overall system.\n- The API Gateway also provides additional features like authentication, authorization, caching, throttling, and monitoring, further reducing the operational overhead.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Using individual Lambda function URLs:\n- This approach would require updating the Git servers to call multiple endpoints, which increases complexity compared to a single API Gateway endpoint.\n- It lacks the additional features and management capabilities provided by API Gateway, such as authentication, authorization, and monitoring.\n\nC. Using AWS App Runner:\n- App Runner is a fully managed service for containerized applications, but it is not a target for an Application Load Balancer (ALB), which is a requirement in the question.\n- Deploying the webhook logic to App Runner and then setting up an ALB to route traffic to it would add more operational overhead compared to the API Gateway and Lambda-based solution.\n\nD. Using Amazon ECS and API Gateway:\n- Containerizing the webhook logic and deploying it to an ECS cluster with Fargate would add more operational overhead compared to the serverless Lambda-based solution.\n- Setting up and managing the ECS cluster and Fargate tasks requires more operational effort than using the fully managed Lambda and API Gateway services.\n- While this approach does use API Gateway, the additional complexity of the container infrastructure makes it less optimal than the direct Lambda and API Gateway integration in option B.\n\nIn summary, option B is the best choice because it provides the lowest operational overhead by leveraging the"
  },
  "46": {
    "question": "A company is planning to migrate 1,000 on-premises servers to AWS. The servers run on several VMware clusters in the company\u2019s data center. As part of the migration plan, the company wants to gather server metrics such as CPU details, RAM usage, operating system information, and running processes. The company then wants to query and analyze the data.Which solution will meet these requirements?",
    "choices": [
      "A. Deploy and configure the AWS Agentless Discovery Connector virtual appliance on the on-premises hosts. Configure Data Exploration in AWS Migration Hub. Use AWS Glue to perform an ETL job against the data. Query the data by using Amazon S3 Select.",
      "B. Export only the VM performance information from the on-premises hosts. Directly import the required data into AWS Migration Hub. Update any missing information in Migration Hub. Query the data by using Amazon QuickSight.",
      "C. Create a script to automatically gather the server information from the on-premises hosts. Use the AWS CLI to run the put-resource-attributes command to store the detailed server data in AWS Migration Hub. Query the data directly in the Migration Hub console.",
      "D. Deploy the AWS Application Discovery Agent to each on-premises server. Configure Data Exploration in AWS Migration Hub. Use Amazon Athena to run predefined queries against the data in Amazon S3."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer:\n\nThe correct answer is D: Deploy the AWS Application Discovery Agent to each on-premises server. Configure Data Exploration in AWS Migration Hub. Use Amazon Athena to run predefined queries against the data in Amazon S3.\n\nThis solution is the best fit for the given requirements because:\n\n- The AWS Application Discovery Agent can collect detailed metrics from the on-premises servers, including CPU, RAM, operating system information, and running processes. This provides the comprehensive server data needed for the migration.\n- The data collected by the Discovery Agent is automatically uploaded to AWS Migration Hub, which provides a centralized view of the migration process.\n- The Data Exploration feature in Migration Hub allows you to analyze the collected data, including running pre-defined queries.\n- Amazon Athena can be used to run more advanced SQL-based queries against the data stored in Amazon S3, enabling further analysis and insights.\n\nThis approach is efficient and scalable, as it automates the data collection and analysis process, and provides a robust and flexible solution to meet the company's requirements.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This solution uses the AWS Agentless Discovery Connector, which does not provide the same level of detailed metrics as the Discovery Agent. Additionally, using AWS Glue for ETL and S3 Select for querying may not be the most efficient approach compared to Athena.\n\nB. Exporting only the VM performance information and importing it into Migration Hub directly does not cover the required process-level details. Additionally, using QuickSight for querying may not be the best fit for the company's data exploration and analysis needs.\n\nC. Creating a custom script to gather and store the data in Migration Hub may be a time-consuming and less reliable approach, as it does not provide the same level of automation and scalability as the Discovery Agent. It also does not offer the advanced querying capabilities of Athena."
  },
  "47": {
    "question": "A company is building a serverless application that runs on an AWS Lambda function that is attached to a VPC. The company needs to integrate the application with a new service from an external provider. The external provider supports only requests that come from public IPv4 addresses that are in an allow list.The company must provide a single public IP address to the external provider before the application can start using the new service.Which solution will give the application the ability to access the new service?",
    "choices": [
      "A. Deploy a NAT gateway. Associate an Elastic IP address with the NAT gateway. Configure the VPC to use the NAT gateway.",
      "B. Deploy an egress-only internet gateway. Associate an Elastic IP address with the egress-only internet gateway. Configure the elastic network interface on the Lambda function to use the egress-only internet gateway.",
      "C. Deploy an internet gateway. Associate an Elastic IP address with the internet gateway. Configure the Lambda function to use the internet gateway.",
      "D. Deploy an internet gateway. Associate an Elastic IP address with the internet gateway. Configure the default route in the public VPC route table to use the internet gateway."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\nThe correct answer is A: Deploy a NAT gateway, associate an Elastic IP address with it, and configure the VPC to use the NAT gateway.\n\nThis is the right solution because:\n- The external provider requires the application to use a public IPv4 address from an allow list to access the new service.\n- By deploying a NAT gateway and associating an Elastic IP address with it, the Lambda function within the VPC can use the NAT gateway's public IP address to make outbound requests to the external provider.\n- Configuring the VPC to use the NAT gateway ensures that all outbound traffic from the Lambda function is routed through the NAT gateway, which has the public IP address that the external provider can whitelist.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. Deploy an egress-only internet gateway:\nThis is not the correct solution because an egress-only internet gateway is used to provide IPv6 internet access for resources in a private subnet, but it does not provide a public IPv4 address that can be used to make outbound requests to the external provider.\n\nC. Deploy an internet gateway:\nThis is not the correct solution because an internet gateway is used to provide internet access for resources in a public subnet, but the Lambda function is in a private subnet within the VPC. The Lambda function would not be able to use the internet gateway's public IP address for outbound requests.\n\nD. Deploy an internet gateway and configure the default route in the public VPC route table:\nThis is not the correct solution because the Lambda function is in a private subnet, not a public subnet. Configuring the default route in the public VPC route table would not help the Lambda function access the external provider's service, as it would not be able to use the internet gateway's public IP address."
  },
  "48": {
    "question": "A solutions architect has developed a web application that uses an Amazon API Gateway Regional endpoint and an AWS Lambda function. The consumers of the web application are all close to the AWS Region where the application will be deployed. The Lambda function only queries an Amazon Aurora MySQL database. The solutions architect has configured the database to have three read replicas.During testing, the application does not meet performance requirements. Under high load, the application opens a large number of database connections. The solutions architect must improve the application\u2019s performance.Which actions should the solutions architect take to meet these requirements? (Choose two.)",
    "choices": [
      "A. Use the cluster endpoint of the Aurora database.",
      "B. Use RDS Proxy to set up a connection pool to the reader endpoint of the Aurora database.",
      "C. Use the Lambda Provisioned Concurrency feature.",
      "D. Move the code for opening the database connection in the Lambda function outside of the event handler.",
      "E. Change the API Gateway endpoint to an edge-optimized endpoint."
    ],
    "answer": "BD",
    "explanation": "1. Clear explanation of why the correct answer is right:\n\nThe correct answers are B and D:\n\nB. Use RDS Proxy to set up a connection pool to the reader endpoint of the Aurora database.\nThis is the right choice because RDS Proxy can help improve the performance of the application by managing the database connection pool. RDS Proxy acts as an intermediary between the application and the database, and it can efficiently route connections to the available read replicas, reducing the number of connections that need to be opened and closed for each request.\n\nD. Move the code for opening the database connection in the Lambda function outside of the event handler.\nThis is the right choice because it can help improve the performance of the application by allowing the database connection to be reused across multiple requests. By moving the connection-opening code outside of the event handler, the connection can be maintained and reused, instead of being opened and closed for each individual request, which can be time-consuming and resource-intensive.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nA. Use the cluster endpoint of the Aurora database.\nThis is not the right choice because the problem is not related to the database endpoint, but rather to the number of connections being opened to the database. Using the cluster endpoint would not help reduce the number of connections.\n\nC. Use the Lambda Provisioned Concurrency feature.\nThis is not the right choice because the problem is related to the number of connections to the database, not the number of instances running the Lambda function. Provisioned Concurrency would not help with the connection management issue.\n\nE. Change the API Gateway endpoint to an edge-optimized endpoint.\nThis is not the right choice because the problem is related to the number of connections to the database, not the location of the API Gateway endpoint. Changing the API Gateway endpoint to an edge-optimized endpoint would not help with the connection management issue."
  },
  "49": {
    "question": "A company requires that all internal application connectivity use private IP addresses. To facilitate this policy, a solutions architect has created interface endpoints to connect to AWS Public services. Upon testing, the solutions architect notices that the service names are resolving to public IP addresses, and that internal services cannot connect to the interface endpoints.Which step should the solutions architect take to resolve this issue?",
    "choices": [
      "A. Update the subnet route table with a route to the interface endpoint.",
      "B. Enable the private DNS option on the VPC attributes.",
      "C. Configure the security group on the interface endpoint to allow connectivity to the AWS services.",
      "D. Configure an Amazon Route 53 private hosted zone with a conditional forwarder for the internal application."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (Option B):\n\nThe correct answer is Option B: Enable the private DNS option on the VPC attributes.\n\nWhen you create an AWS PrivateLink interface endpoint, AWS automatically provides a private DNS name for the service that resolves to the private IP addresses of the interface endpoint. However, by default, the private DNS option is disabled on the VPC, which means that DNS queries for the service name will be resolved using the public DNS instead of the private DNS provided by the interface endpoint.\n\nBy enabling the private DNS option on the VPC attributes, the VPC is instructed to use the private DNS names provided by the interface endpoints for the specified AWS services. This ensures that the service names resolve to the private IP addresses of the interface endpoints, allowing internal services within the VPC to connect to the AWS services using private IP addresses, as per the company's policy.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Update the subnet route table with a route to the interface endpoint:\nThis option is incorrect because updating the subnet route table would not resolve the issue of the service names resolving to public IP addresses. The route table determines where traffic is forwarded, but it does not affect DNS resolution.\n\nC. Configure the security group on the interface endpoint to allow connectivity to the AWS services:\nThis option is incorrect because the issue is not related to the security group configuration. The problem is that the service names are resolving to public IP addresses, preventing internal services from connecting to the interface endpoints.\n\nD. Configure an Amazon Route 53 private hosted zone with a conditional forwarder for the internal application:\nThis option is incorrect because it would not solve the specific problem described in the question. The issue is related to the DNS resolution of the interface endpoint service names, which can be resolved by enabling the private DNS option on the VPC attributes (Option B)."
  },
  "50": {
    "question": "A company wants to migrate its data analytics environment from on premises to AWS. The environment consists of two simple Node.js applications. One of the applications collects sensor data and loads it into a MySQL database. The other application aggregates the data into reports. When the aggregation jobs run, some of the load jobs fail to run correctly.The company must resolve the data loading issue. The company also needs the migration to occur without interruptions or changes for the company\u2019s customers.What should a solutions architect do to meet these requirements?",
    "choices": [
      "A. Set up an Amazon Aurora MySQL database as a replication target for the on-premises database. Create an Aurora Replica for the Aurora MySQL database, and move the aggregation jobs to run against the Aurora Replica. Set up collection endpoints as AWS Lambda functions behind a Network Load Balancer (NLB), and use Amazon RDS Proxy to write to the Aurora MySQL database. When the databases are synced, disable the replication job and restart the Aurora Replica as the primary instance. Point the collector DNS record to the NLB.",
      "B. Set up an Amazon Aurora MySQL database. Use AWS Database Migration Service (AWS DMS) to perform continuous data replication from the on-premises database to Aurora. Move the aggregation jobs to run against the Aurora MySQL database. Set up collection endpoints behind an Application Load Balancer (ALB) as Amazon EC2 instances in an Auto Scaling group. When the databases are synced, point the collector DNS record to the ALDisable the AWS DMS sync task after the cutover from on premises to AWS.",
      "C. Set up an Amazon Aurora MySQL database. Use AWS Database Migration Service (AWS DMS) to perform continuous data replication from the on-premises database to Aurora. Create an Aurora Replica for the Aurora MySQL database, and move the aggregation jobs to run against the Aurora Replica. Set up collection endpoints as AWS Lambda functions behind an Application Load Balancer (ALB), and use Amazon RDS Proxy to write to the Aurora MySQL database. When the databases are synced, point the collector DNS record to the ALB. Disable the AWS DMS sync task after the cutover from on premises to AWS.",
      "D. Set up an Amazon Aurora MySQL database. Create an Aurora Replica for the Aurora MySQL database, and move the aggregation jobs to run against the Aurora Replica. Set up collection endpoints as an Amazon Kinesis data stream. Use Amazon Kinesis Data Firehose to replicate the data to the Aurora MySQL database. When the databases are synced, disable the replication job and restart the Aurora Replica as the primary instance. Point the collector DNS record to the Kinesis data stream."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C because it addresses all the key requirements mentioned in the question:\n\n- It sets up an Amazon Aurora MySQL database and uses AWS Database Migration Service (AWS DMS) to perform continuous data replication from the on-premises database to Aurora. This ensures a smooth migration without interruptions or changes for the company's customers.\n- It creates an Aurora Replica for the Aurora MySQL database and moves the aggregation jobs to run against the Aurora Replica. This helps resolve the data loading issue by separating the aggregation jobs from the main database.\n- It sets up the collection endpoints as AWS Lambda functions behind an Application Load Balancer (ALB), and uses Amazon RDS Proxy to write to the Aurora MySQL database. This allows for efficient and reliable data collection and loading, without disrupting the customer experience.\n- When the databases are synced, it points the collector DNS record to the ALB, and then disables the AWS DMS sync task after the cutover from on-premises to AWS.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option uses a Network Load Balancer (NLB) instead of an Application Load Balancer (ALB) for the collection endpoints, which is not suitable as NLB only supports private IP addresses, instances, and ALBs, and not Lambda functions. Additionally, it does not use AWS DMS for continuous data replication, which is a crucial requirement.\n\nB. This option does not create an Aurora Replica for the Aurora MySQL database, which is necessary to separate the aggregation jobs from the main database and resolve the data loading issue. It also sets up the collection endpoints as Amazon EC2 instances in an Auto Scaling group, which may not be the most efficient solution compared to using Lambda functions behind an ALB.\n\nD. This option does not use AWS DMS for continuous data replication, which is a key requirement. Instead, it sets up the collection endpoints as an Amazon Kinesis data stream and uses Amazon Kinesis Data Firehose to replicate the data to the Aurora MySQL database. This approach may introduce additional complexity and does not align with the requirement to use Node.js applications for data collection and aggregation."
  },
  "51": {
    "question": "A health insurance company stores personally identifiable information (PII) in an Amazon S3 bucket. The company uses server-side encryption with S3 managed encryption keys (SSE-S3) to encrypt the objects. According to a new requirement, all current and future objects in the S3 bucket must be encrypted by keys that the company\u2019s security team manages. The S3 bucket does not have versioning enabled.Which solution will meet these requirements?",
    "choices": [
      "A. In the S3 bucket properties, change the default encryption to SSE-S3 with a customer managed key. Use the AWS CLI to re-upload all objects in the S3 bucket. Set an S3 bucket policy to deny unencrypted PutObject requests.",
      "B. In the S3 bucket properties, change the default encryption to server-side encryption with AWS KMS managed encryption keys (SSE-KMS). Set an S3 bucket policy to deny unencrypted PutObject requests. Use the AWS CLI to re-upload all objects in the S3 bucket.",
      "C. In the S3 bucket properties, change the default encryption to server-side encryption with AWS KMS managed encryption keys (SSE-KMS). Set an S3 bucket policy to automatically encrypt objects on GetObject and PutObject requests.",
      "D. In the S3 bucket properties, change the default encryption to AES-256 with a customer managed key. Attach a policy to deny unencrypted PutObject requests to any entities that access the S3 bucket. Use the AWS CLI to re-upload all objects in the S3 bucket."
    ],
    "answer": "B",
    "explanation": "Explanation of the Correct Answer (B):\n\nThe correct answer is B because it meets the requirements specified in the question:\n\n1. The S3 bucket's default encryption is changed to server-side encryption with AWS KMS managed encryption keys (SSE-KMS). This allows the company's security team to manage the encryption keys used to encrypt the objects in the S3 bucket, as per the new requirement.\n\n2. The S3 bucket policy is set to deny unencrypted PutObject requests. This ensures that all new objects uploaded to the S3 bucket are encrypted using the SSE-KMS encryption.\n\n3. The AWS CLI is used to re-upload all objects in the S3 bucket. This ensures that all current objects are also encrypted using the SSE-KMS encryption, as required.\n\nExplanations of the Incorrect Choices:\n\nA. Incorrect because it uses SSE-S3 with a customer-managed key, which does not allow the company's security team to manage the encryption keys.\n\nC. Incorrect because it does not specify how the company's security team will manage the encryption keys. Additionally, the bucket policy to automatically encrypt objects on GetObject and PutObject requests does not address the requirement to re-upload all current objects in the S3 bucket.\n\nD. Incorrect because it uses AES-256 with a customer-managed key, which does not allow the company's security team to manage the encryption keys. Additionally, it does not specify how the objects will be re-uploaded to the S3 bucket."
  },
  "52": {
    "question": "A company is running a web application in the AWS Cloud. The application consists of dynamic content that is created on a set of Amazon EC2 instances. The EC2 instances run in an Auto Scaling group that is configured as a target group for an Application Load Balancer (ALB).The company is using an Amazon CloudFront distribution to distribute the application globally. The CloudFront distribution uses the ALB as an origin. The company uses Amazon Route 53 for DNS and has created an A record of www.example.com for the CloudFront distribution.A solutions architect must configure the application so that itis highly available and fault tolerant.Which solution meets these requirements?",
    "choices": [
      "A. Provision a full, secondary application deployment in a different AWS Region. Update the Route 53 A record to be a failover record. Add both of the CloudFront distributions as values. Create Route 53 health checks.",
      "B. Provision an ALB, an Auto Scaling group, and EC2 instances in a different AWS Region. Update the CloudFront distribution, and create a second origin for the new ALCreate an origin group for the two origins. Configure one origin as primary and one origin as secondary.",
      "C. Provision an Auto Scaling group and EC2 instances in a different AWS Region. Create a second target for the new Auto Scaling group in the ALB. Set up the failover routing algorithm on the ALB.",
      "D. Provision a full, secondary application deployment in a different AWS Region. Create a second CloudFront distribution, and add the new application setup as an origin. Create an AWS Global Accelerator accelerator. Add both of the CloudFront distributions as endpoints."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B because it provides a highly available and fault-tolerant solution for the web application deployed in the AWS cloud.\n\nThe key elements of this solution are:\n\n- Provisioning an ALB, an Auto Scaling group, and EC2 instances in a different AWS Region\n- Updating the CloudFront distribution to create a second origin for the new ALB\n- Creating an origin group for the two origins, with one configured as the primary and the other as the secondary\n\nThis setup ensures that if the primary origin (the ALB and its associated resources in the first region) experiences an issue, CloudFront can automatically failover to the secondary origin (the ALB and its resources in the second region). This provides redundancy and high availability for the application, as it can continue to serve requests even if one of the regions experiences a disruption.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This solution uses Route 53 failover records, which can result in increased latency and DNS resolution time for clients, as they need to wait for the DNS failover to complete before being routed to the secondary application deployment.\n\nC. This solution only provides redundancy for the Auto Scaling group and EC2 instances, but not for the load balancer (ALB), which is a critical component of the application. If the primary ALB experiences an issue, the application will still be unavailable.\n\nD. While this solution uses AWS Global Accelerator to provide global acceleration and failover, it is an overkill for this scenario, as the CloudFront distribution can already provide the necessary global distribution and failover capabilities when configured with multiple origins.\n\nIn summary, the correct answer (B) is the most appropriate solution because it leverages the native failover capabilities of CloudFront and ALB to ensure high availability and fault tolerance for the web application, without introducing unnecessary complexity or potential performance issues."
  },
  "53": {
    "question": "A company has an organization in AWS Organizations that has a large number of AWS accounts. One of the AWS accounts is designated as a transit account and has a transit gateway that is shared with all of the other AWS accounts. AWS Site-to-Site VPN connections are configured between all of the company\u2019s global offices and the transit account. The company has AWS Config enabled on all of its accounts.The company\u2019s networking team needs to centrally manage a list of internal IP address ranges that belong to the global offices. Developers will reference this list to gain access to their applications securely.Which solution meets these requirements with the LEAST amount of operational overhead?",
    "choices": [
      "A. Create a JSON file that is hosted in Amazon S3 and that lists all of the internal IP address ranges. Configure an Amazon Simple Notification Service (Amazon SNS) topic in each of the accounts that can be invoked when the JSON file is updated. Subscribe an AWS Lambda function to the SNS topic to update all relevant security group rules with the updated IP address ranges.",
      "B. Create a new AWS Config managed rule that contains all of the internal IP address ranges. Use the rule to check the security groups in each of the accounts to ensure compliance with the list of IP address ranges. Configure the rule to automatically remediate any noncompliant security group that is detected.",
      "C. In the transit account, create a VPC prefix list with all of the internal IP address ranges. Use AWS Resource Access Manager to share the prefix list with all of the other accounts. Use the shared prefix list to configure security group rules in the other accounts.",
      "D. In the transit account, create a security group with all of the internal IP address ranges. Configure the security groups in the other accounts to reference the transit account\u2019s security group by using a nested security group reference of \u201c/sg-1a2b3c4d\u201d."
    ],
    "answer": "C",
    "explanation": "1. Explanation for the correct answer (C):\n\nThe correct answer is option C because it provides the least amount of operational overhead for centrally managing a list of internal IP address ranges for the company's global offices. Here's why:\n\n- In the transit account, a VPC prefix list is created with all the internal IP address ranges. This allows for centralized management of the IP address ranges in a single location.\n- The VPC prefix list is then shared with all the other accounts using AWS Resource Access Manager (RAM). This eliminates the need for manual updates to security group rules in each individual account.\n- Developers can then reference the shared VPC prefix list to configure security group rules in their accounts, ensuring secure access to their applications.\n- The use of a shared VPC prefix list also allows for compliance checks to be performed using AWS Config, and any non-compliant security groups can be automatically remediated.\n\n2. Explanations for the incorrect choices:\n\nA. This option requires manually updating a JSON file hosted in S3, and then configuring an SNS topic and a Lambda function to update the security group rules in each account. This approach introduces more operational overhead compared to the VPC prefix list solution.\n\nB. While this option uses an AWS Config managed rule to check for compliance, it still requires manual updates to the security group rules in each account. This can lead to operational overhead, especially as the number of accounts and IP address ranges grows.\n\nD. This option requires manually updating the security group in the transit account and using a nested security group reference, which can be more complex and error-prone compared to the VPC prefix list solution."
  },
  "54": {
    "question": "A company runs a new application as a static website in Amazon S3. The company has deployed the application to a production AWS account and uses Amazon CloudFront to deliver the website. The website calls an Amazon API Gateway REST API. An AWS Lambda function backs each API method.The company wants to create a CSV report every 2 weeks to show each API Lambda function\u2019s recommended configured memory, recommended cost, and the price difference between current configurations and the recommendations. The company will store the reports in an S3 bucket.Which solution will meet these requirements with the LEAST development time?",
    "choices": [
      "A. Create a Lambda function that extracts metrics data for each API Lambda function from Amazon CloudWatch Logs for the 2-week period. Collate the data into tabular format. Store the data as a .csv file in an S3 bucket. Create an Amazon EventBridge rule to schedule the Lambda function to run every 2 weeks.",
      "B. Opt in to AWS Compute Optimizer. Create a Lambda function that calls the ExportLambdaFunctionRecommendations operation. Export the .csv file to an S3 bucket. Create an Amazon EventBridge rule to schedule the Lambda function to run every 2 weeks.",
      "C. Opt in to AWS Compute Optimizer. Set up enhanced infrastructure metrics. Within the Compute Optimizer console, schedule a job to export the Lambda recommendations to a .csv file. Store the file in an S3 bucket every 2 weeks.",
      "D. Purchase the AWS Business Support plan for the production account. Opt in to AWS Compute Optimizer for AWS Trusted Advisor checks. In the Trusted Advisor console, schedule a job to export the cost optimization checks to a .csv file. Store the file in an S3 bucket every 2 weeks."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (Option B):\n\nOption B is the correct answer because it involves the least development time to meet the requirements. The key points are:\n\n- Opting in to AWS Compute Optimizer allows you to leverage the built-in service to generate the recommended memory configurations and cost comparisons for the Lambda functions, without having to develop custom logic to extract this data.\n- Creating a Lambda function to call the `ExportLambdaFunctionRecommendations` operation provided by Compute Optimizer is a straightforward way to generate the CSV report, again without needing to develop custom data processing logic.\n- Scheduling the Lambda function to run every 2 weeks using Amazon EventBridge is a simple way to automate the report generation and storage in an S3 bucket.\n\nOverall, this approach minimizes development time by utilizing the existing AWS Compute Optimizer service and its APIs, rather than requiring custom code to extract the necessary metrics and generate the report.\n\n2. Explanations of the incorrect choices:\n\nOption A: This option requires developing a custom Lambda function to extract metrics data from CloudWatch Logs, collate the data into a CSV format, and then store it in an S3 bucket. This adds more development time compared to the Compute Optimizer-based solution in Option B.\n\nOption C: This option requires setting up enhanced infrastructure metrics, which adds an extra setup step compared to Option B. Additionally, scheduling the export job within the Compute Optimizer console may be less flexible than automating the process with a scheduled Lambda function.\n\nOption D: This option involves purchasing the AWS Business Support plan, which adds an unnecessary cost overhead. Additionally, using the Trusted Advisor console to schedule the export job is less flexible than automating the process with a Lambda function.\n\nIn summary, Option B is the correct answer because it leverages the existing AWS Compute Optimizer service and its APIs, which requires the least amount of custom development to meet the stated requirements."
  },
  "55": {
    "question": "Accompany is building an application to collect and transmit sensor data from a factory. The application will use AWS IoT Core to send data from hundreds of devices to an Amazon S3 data lake. The company must enrich the data before loading the data into Amazon S3.The application will transmit the sensor data every 5 seconds. New sensor data must be available in Amazon S3 less than 30 minutes after the application collects the data. No other applications are processing the sensor data from AWS IoT Core.Which solution will meet these requirements MOST cost-effectively?",
    "choices": [
      "A. Create a topic in AWS IoT Core to ingest the sensor data. Create an AWS Lambda function to enrich the data and to write the data to Amazon S3. Configure an AWS IoT rule action to invoke the Lambda function.",
      "B. Use AWS IoT Core Basic Ingest to ingest the sensor data. Configure an AWS IoT rule action to write the data to Amazon Kinesis Data Firehose. Set the Kinesis Data Firehose buffering interval to 900 seconds. Use Kinesis Data Firehose to invoke an AWS Lambda function to enrich the data, Configure Kinesis Data Firehose to deliver the data to Amazon S3.",
      "C. Create a topic in AWS IoT Core to ingest the sensor data. Configure an AWS IoT rule action to send the data to an Amazon Timestream table. Create an AWS Lambda, function to read the data from Timestream. Configure the Lambda function to enrich the data and to write the data to Amazon S3.",
      "D. Use AWS loT Core Basic Ingest to ingest the sensor data. Configure an AWS IoT rule action to write the data to Amazon Kinesis Data Streams. Create a consumer AWS Lambda function to process the data from Kinesis Data Streams and to enrich the data. Call the S3 PutObject API operation from the Lambda function to write the data to Amazon S3."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B because it provides the most cost-effective solution to meet the given requirements. Here's why:\n\n- Using AWS IoT Core Basic Ingest is more cost-effective than creating a custom topic in AWS IoT Core (as in options A and C), as Basic Ingest has a lower cost per message.\n- Configuring an AWS IoT rule action to write the data to Amazon Kinesis Data Firehose is a good choice because Kinesis Data Firehose is designed for ingesting and delivering streaming data, which aligns with the requirement of transmitting sensor data every 5 seconds.\n- Setting the Kinesis Data Firehose buffering interval to 900 seconds (15 minutes) ensures that the data is available in Amazon S3 within the 30-minute requirement, while also optimizing the number of S3 PUT requests and reducing the cost of Lambda function invocations for data enrichment.\n- Using Kinesis Data Firehose to invoke an AWS Lambda function to enrich the data is a cost-effective approach, as it allows you to process the data in batches rather than individually.\n- Configuring Kinesis Data Firehose to deliver the enriched data to Amazon S3 is an efficient way to meet the requirement of storing the data in the data lake.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option is less cost-effective than the correct answer (B) because it requires creating a custom topic in AWS IoT Core and invoking a Lambda function for each incoming message, which can lead to higher costs due to the increased number of Lambda function invocations.\n\nC. This option is less cost-effective than the correct answer (B) because it requires creating a custom topic in AWS IoT Core, writing the data to an Amazon Timestream table, and then reading the data from Timestream and enriching it in a separate Lambda function. This approach involves more resources and can lead to higher costs.\n\nD. This option is less cost-effective than the correct answer (B) because it requires using Amazon Kinesis Data Streams, which has a higher cost than Kinesis Data Firehose. Additionally, the need to create a separate"
  },
  "56": {
    "question": "A large education company recently introduced Amazon Workspaces to provide access to internal applications across multiple universities. The company is storing user profiles on an Amazon FSx for Windows File Server file system. The file system is configured with a DNS alias and is connected to a self-managed Active Directory. As more users begin to use the Workspaces, login time increases to unacceptable levels.An investigation reveals a degradation in performance of the file system. The company created the file system on HDD storage with a throughput of 16 MBps. A solutions architect must improve the performance of the file system during a defined maintenance window.What should the solutions architect do to meet these requirements with the LEAST administrative effort?",
    "choices": [
      "A. Use AWS Backup to create a point-in-time backup of the file system. Restore the backup to a new FSx for Windows File Server file system. Select SSD as the storage type. Select 32 MBps as the throughput capacity. When the backup and restore process is completed, adjust the DNS alias accordingly. Delete the original file system.",
      "B. Disconnect users from the file system. In the Amazon FSx console, update the throughput capacity to 32 MBps. Update the storage type to SSD. Reconnect users to the file system.",
      "C. Deploy an AWS DataSync agent onto a new Amazon EC2 instance. Create a task. Configure the existing file system as the source location. Configure a new FSx for Windows File Server file system with SSD storage and 32 MBps of throughput as the target location. Schedule the task. When the task is completed, adjust the DNS alias accordingly. Delete the original file system.",
      "D. Enable shadow copies on the existing file system by using a Windows PowerShell command. Schedule the shadow copy job to create a point-in-time backup of the file system. Choose to restore previous versions. Create a new FSx for Windows File Server file system with SSD storage and 32 MBps of throughput. When the copy job is completed, adjust the DNS alias. Delete the original file system."
    ],
    "answer": "B",
    "explanation": "Explanation:\n\n1. Correct answer: B\n\nThe correct answer is B because it is the option that requires the least administrative effort to improve the performance of the Amazon FSx for Windows File Server file system.\n\nThe key points are:\n- The question states the need to improve performance during a defined maintenance window, which suggests minimal downtime is preferred.\n- Option B allows the solutions architect to directly update the throughput capacity of the existing file system from 16 MBps to 32 MBps, without the need to create a new file system or perform a backup and restore operation.\n- This is the most straightforward and efficient way to address the performance issue, as it does not require complex steps like backup, restore, or data migration.\n\n2. Incorrect answers:\n\nA. This option involves creating a backup of the file system, restoring it to a new file system with SSD storage and higher throughput, and updating the DNS alias. While this would improve performance, it requires more administrative effort and downtime compared to option B.\n\nC. This option uses AWS DataSync to migrate the data to a new file system with SSD storage and higher throughput. This is a valid approach, but it requires additional setup and configuration of the DataSync agent and task, making it more complex than option B.\n\nD. This option uses the Windows PowerShell to enable shadow copies and then restore the file system to a new file system with SSD storage and higher throughput. While it is a viable solution, it requires more steps and customization compared to the direct updates in option B.\n\nIn summary, option B is the correct answer because it allows the solutions architect to directly update the throughput capacity of the existing file system, which is the least administratively complex approach to improve performance during the maintenance window."
  },
  "57": {
    "question": "A solutions architect is planning to migrate critical Microsoft SQL Server databases to AWS. Because the databases are legacy systems, the solutions architect will move the databases to a modern data architecture. The solutions architect must migrate the databases with near-zero downtime.Which solution will meet these requirements?",
    "choices": [
      "A. Use AWS Application Migration Service and the AWS Schema Conversion Tool (AWS SCT). Perform an in-place upgrade before the migration. Export the migrated data to Amazon Aurora Serverless after cutover. Repoint the applications to Amazon Aurora.",
      "B. Use AWS Database Migration Service (AWS DMS) to rehost the database. Set Amazon S3 as a target. Set up change data capture (CDC) replication. When the source and destination are fully synchronized, load the data from Amazon S3 into an Amazon RDS for Microsoft SQL Server DB instance.",
      "C. Use native database high availability tools. Connect the source system to an Amazon RDS for Microsoft SQL Server DB instance. Configure replication accordingly. When data replication is finished, transition the workload to an Amazon RDS for Microsoft SQL Server DB instance.",
      "D. Use AWS Application Migration Service. Rehost the database server on Amazon EC2. When data replication is finished, detach the database and move the database to an Amazon RDS for Microsoft SQL Server DB instance. Reattach the database and then cut over all networking."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\nThe correct answer is C, which involves using native database high availability tools to migrate the critical Microsoft SQL Server databases to AWS with near-zero downtime. This approach leverages the built-in replication capabilities of SQL Server, which can directly replicate data to an Amazon RDS for Microsoft SQL Server DB instance. By using native replication tools, the migration process can be seamless, with minimal downtime, as the data is continuously synchronized between the source and target environments.\n\n2. Explanations of the incorrect choices:\n\nA. This option uses the AWS Application Migration Service and the AWS Schema Conversion Tool (AWS SCT), which may not be the most appropriate choice for a database-only migration. It also involves an in-place upgrade, which can introduce additional complexity and risk. Finally, the step of exporting the migrated data to Amazon Aurora Serverless and then repointing the applications to Amazon Aurora is not directly relevant to the given requirements, which focus on migrating the databases to Amazon RDS for Microsoft SQL Server.\n\nB. While AWS DMS can be a suitable tool for database migrations, the approach of using Amazon S3 as an intermediate storage solution is not optimal for this scenario. Staging the data in S3 and then loading it into the target RDS instance can introduce additional latency and potential downtime during the final cutover. Native replication tools, as mentioned in the correct answer, can provide a more direct and seamless migration process.\n\nD. This option involves rehosting the database server on Amazon EC2, followed by detaching the database and moving it to an Amazon RDS for Microsoft SQL Server DB instance. This approach may not provide the same level of seamless data replication and near-zero downtime as the native SQL Server replication tools used in the correct answer (C).\n\nIn summary, the correct answer (C) is the most appropriate solution because it leverages the native database high availability tools, which can directly replicate data to the target Amazon RDS for Microsoft SQL Server DB instance, thereby achieving the desired near-zero downtime during the migration process."
  },
  "58": {
    "question": "A company has a monolithic application that is critical to the company\u2019s business. The company hosts the application on an Amazon EC2 instance that runs Amazon Linux 2. The company\u2019s application team receives a directive from the legal department to back up the data from the instance\u2019s encrypted Amazon Elastic Block Store (Amazon EBS) volume to an Amazon S3 bucket. The application team does not have the administrative SSH key pair for the instance. The application must continue to serve the users.Which solution will meet these requirements?",
    "choices": [
      "A. Attach a role to the instance with permission to write to Amazon S3. Use the AWS Systems Manager Session Manager option to gain access to the instance and run commands to copy data into Amazon S3.",
      "B. Create an image of the instance with the reboot option turned on. Launch a new EC2 instance from the image. Attach a role to the new instance with permission to write to Amazon S3. Run a command to copy data into Amazon S3.",
      "C. Take a snapshot of the EBS volume by using Amazon Data Lifecycle Manager (Amazon DLM). Copy the data to Amazon S3.",
      "D. Create an image of the instance. Launch a new EC2 instance from the image. Attach a role to the new instance with permission to write to Amazon S3. Run a command to copy data into Amazon S3."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\n\nThe correct answer is A. Attach a role to the instance with permission to write to Amazon S3. Use the AWS Systems Manager Session Manager option to gain access to the instance and run commands to copy data into Amazon S3.\n\nThis solution meets the requirements because:\n- It allows the application team to access the instance without the SSH key pair, using the AWS Systems Manager Session Manager.\n- The IAM role attached to the instance provides the necessary permissions to write data to the Amazon S3 bucket.\n- The application can continue to serve users during the backup process since the data is being copied from the instance, not the EBS volume directly.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. Create an image of the instance with the reboot option turned on. Launch a new EC2 instance from the image. Attach a role to the new instance with permission to write to Amazon S3. Run a command to copy data into Amazon S3.\nThis option is incorrect because it requires stopping the current instance, creating a new instance, and then copying the data. This would impact the running application, which is not allowed as per the requirements.\n\nC. Take a snapshot of the EBS volume by using Amazon Data Lifecycle Manager (Amazon DLM). Copy the data to Amazon S3.\nThis option is incorrect because while it can create a snapshot of the EBS volume, it does not directly copy the data to the Amazon S3 bucket. Snapshots are stored in Amazon S3, but they are not accessible as regular S3 objects, so additional steps would be required to access and copy the data.\n\nD. Create an image of the instance. Launch a new EC2 instance from the image. Attach a role to the new instance with permission to write to Amazon S3. Run a command to copy data into Amazon S3.\nThis option is incorrect because it requires stopping the current instance, creating a new instance, and then copying the data. This would impact the running application, which is not allowed as per the requirements."
  },
  "59": {
    "question": "A solutions architect needs to copy data from an Amazon S3 bucket m an AWS account to a new S3 bucket in a new AWS account. The solutions architect must implement a solution that uses the AWS CLI.Which combination of steps will successfully copy the data? (Choose three.)",
    "choices": [
      "A. Create a bucket policy to allow the source bucket to list its contents and to put objects and set object ACLs in the destination bucket. Attach the bucket policy to the destination bucket.",
      "B. Create a bucket policy to allow a user in the destination account to list the source bucket\u2019s contents and read the source bucket\u2019s objects. Attach the bucket policy to the source bucket.",
      "C. Create an IAM policy in the source account. Configure the policy to allow a user in the source account to list contents and get objects in the source bucket, and to list contents, put objects, and set object ACLs in the destination bucket. Attach the policy to the user.",
      "D. Create an IAM policy in the destination account. Configure the policy to allow a user in the destination account to list contents and get objects in the source bucket, and to list contents, put objects, and set objectACLs in the destination bucket. Attach the policy to the user.",
      "E. Run the aws s3 sync command as a user in the source account. Specify the source and destination buckets to copy the data.",
      "F. Run the aws s3 sync command as a user in the destination account. Specify the source and destination buckets to copy the data."
    ],
    "answer": "BDF",
    "explanation": "1. Explanation of the correct answer (BDF):\n\nB. This step is necessary to allow the user in the destination account to list the contents of the source bucket and read the objects in the source bucket. This bucket policy grants the required permissions on the source bucket.\n\nD. This IAM policy in the destination account grants the user the necessary permissions to list contents, get objects from the source bucket, and list contents, put objects, and set object ACLs in the destination bucket. This ensures the user in the destination account has the required permissions to copy the data.\n\nF. The aws s3 sync command needs to be executed using the IAM user credentials from the destination account. This ensures the copied objects in the destination bucket have the appropriate permissions for the user in the destination account, rather than inheriting the permissions from the source account.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Creating a bucket policy on the destination bucket to allow the source bucket to list its contents and put objects is not the correct approach, as the user who needs to perform the copy operation is in the destination account, not the source account.\n\nC. Creating an IAM policy in the source account is not necessary, as the user who needs to perform the copy operation is in the destination account, not the source account.\n\nE. Running the aws s3 sync command as a user in the source account is not the correct approach, as the copied objects in the destination bucket will still have the source account permissions and may not be accessible by the destination account users."
  },
  "60": {
    "question": "A company built an application based on AWS Lambda deployed in an AWS CloudFormation stack. The last production release of the web application introduced an issue that resulted in an outage lasting several minutes. A solutions architect must adjust the deployment process to support a canary release.Which solution will meet these requirements?",
    "choices": [
      "A. Create an alias for every new deployed version of the Lambda function. Use the AWS CLI update-alias command with the routing-config parameter to distribute the load.",
      "B. Deploy the application into a new CloudFormation stack. Use an Amazon Route 53 weighted routing policy to distribute the load.",
      "C. Create a version for every new deployed Lambda function. Use the AWS CLI update-function-configuration command with the routing-config parameter to distribute the load.",
      "D. Configure AWS CodeDeploy and use CodeDeployDefault.OneAtATime in the Deployment configuration to distribute the load."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\n\nThe correct answer is A. Create an alias for every new deployed version of the Lambda function and use the AWS CLI `update-alias` command with the `routing-config` parameter to distribute the load.\n\nThis is the right solution because it allows for a canary release by gradually shifting traffic from the old version to the new version of the Lambda function. By creating an alias for each new deployed version, you can use the `routing-config` parameter to specify the percentage of traffic that should be routed to each version. This provides a controlled and incremental way to roll out the new version, allowing you to monitor its performance and easily roll back if any issues are detected.\n\n2. Explanations of the incorrect choices:\n\nB. Deploy the application into a new CloudFormation stack and use an Amazon Route 53 weighted routing policy to distribute the load:\nThis is not the correct solution because it involves creating a separate CloudFormation stack for the new version, which is a more complex and time-consuming process compared to creating an alias for a new version of the Lambda function. Additionally, using Route 53 weighted routing policy would add an extra layer of complexity that is not necessary for a Lambda-based application.\n\nC. Create a version for every new deployed Lambda function and use the AWS CLI `update-function-configuration` command with the `routing-config` parameter to distribute the load:\nThis is not the correct solution because, while it would allow for a canary release, creating a new version for every deployment is a more complex and time-consuming process compared to creating an alias for a new version of the Lambda function.\n\nD. Configure AWS CodeDeploy and use `CodeDeployDefault.OneAtATime` in the Deployment configuration to distribute the load:\nThis is not the correct solution because while AWS CodeDeploy does support deployments to Lambda functions, it does not provide the same level of control over traffic routing as using the `update-alias` command with the `routing-config` parameter. The `CodeDeployDefault.OneAtATime` configuration is a deployment strategy for EC2 instances and does not apply to Lambda functions."
  },
  "61": {
    "question": "A finance company hosts a data lake in Amazon S3. The company receives financial data records over SFTP each night from several third parties. The company runs its own SFTP server on an Amazon EC2 instance in a public subnet of a VPC. After the files are uploaded, they are moved to the data lake by a cron job that runs on the same instance. The SFTP server is reachable on DNS sftp.example.com through the use of Amazon Route 53.What should a solutions architect do to improve the reliability and scalability of the SFTP solution?",
    "choices": [
      "A. Move the EC2 instance into an Auto Scaling group. Place the EC2 instance behind an Application Load Balancer (ALB). Update the DNS record sftp.example.com in Route 53 to point to the ALB.",
      "B. Migrate the SFTP server to AWS Transfer for SFTP. Update the DNS record sftp.example.com in Route 53 to point to the server endpoint hostname.",
      "C. Migrate the SFTP server to a file gateway in AWS Storage Gateway. Update the DNS record sftp.example.com in Route 53 to point to the file gateway endpoint.",
      "D. Place the EC2 instance behind a Network Load Balancer (NLB). Update the DNS record sftp.example.com in Route 53 to point to the NLB."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B, \"Migrate the SFTP server to AWS Transfer for SFTP. Update the DNS record sftp.example.com in Route 53 to point to the server endpoint hostname.\" This is the best solution for improving the reliability and scalability of the SFTP solution.\n\nAWS Transfer for SFTP is a fully managed SFTP service provided by AWS. By migrating the SFTP server to AWS Transfer for SFTP, the company can offload the management of the SFTP server to AWS. This provides several benefits:\n\n- High availability: AWS Transfer for SFTP is a highly available and scalable service, ensuring that the SFTP server is always accessible.\n- Scalability: AWS Transfer for SFTP can automatically scale to handle increasing data transfer volumes without the need for manual intervention.\n- Security: AWS Transfer for SFTP provides built-in security features, such as encryption and authentication, without the need for the company to manage these aspects.\n- Reduced operational overhead: The company no longer needs to manage the SFTP server, including tasks such as software updates, patching, and infrastructure management.\n\nBy updating the DNS record sftp.example.com in Route 53 to point to the AWS Transfer for SFTP server endpoint hostname, the company can ensure that the SFTP server is reachable on the DNS, providing a seamless user experience.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Move the EC2 instance into an Auto Scaling group. Place the EC2 instance behind an Application Load Balancer (ALB). Update the DNS record sftp.example.com in Route 53 to point to the ALB.\nThis option is incorrect because an Application Load Balancer (ALB) is not suitable for SFTP traffic, which uses the TCP protocol. ALBs are designed for HTTP/HTTPS traffic, which uses the TCP/IP protocol. SFTP, on the other hand, requires a different load balancing solution, such as a Network Load Balancer (NLB).\n\nC. Migrate the SFTP server to a file gateway in AWS Storage Gateway. Update the DNS record sftp.example.com in Route 53 "
  },
  "62": {
    "question": "A company wants to migrate an application to Amazon EC2 from VMware Infrastructure that runs in an on-premises data center. A solutions architect must preserve the software and configuration settings during the migration.What should the solutions architect do to meet these requirements?",
    "choices": [
      "A. Configure the AWS DataSync agent to start replicating the data store to Amazon FSx for Windows File Server. Use the SMB share to host the VMware data store. Use VM Import/Export to move the VMs to Amazon EC2.",
      "B. Use the VMware vSphere client to export the application as an image in Open Virtualization Format (OVF) format. Create an Amazon S3 bucket to store the image in the destination AWS Region. Create and apply an IAM role for VM Import. Use the AWS CLI to run the EC2 import command.",
      "C. Configure AWS Storage Gateway for files service to export a Common Internet File System (CIFS) share. Create a backup copy to the shared folder. Sign in to the AWS Management Console and create an AMI from the backup copy. Launch an EC2 instance that is based on the AMI.",
      "D. Create a managed-instance activation for a hybrid environment in AWS Systems Manager. Download and install Systems Manager Agent on the on-premises VM. Register the VM with Systems Manager to be a managed instance. Use AWS Backup to create a snapshot of the VM and create an AMI. Launch an EC2 instance that is based on the AMI."
    ],
    "answer": "B",
    "explanation": "1. Clear explanation of why the correct answer (B) is right:\n\nThe correct answer is B because it outlines the steps necessary to preserve the software and configuration settings of the on-premises application during the migration to Amazon EC2.\n\nThe key steps are:\n- Use the VMware vSphere client to export the application as an Open Virtualization Format (OVF) image. OVF is a standard format that preserves the software and configuration settings of the virtual machine.\n- Create an Amazon S3 bucket to store the OVF image in the destination AWS Region.\n- Create and apply an IAM role for VM Import to grant the necessary permissions for the import process.\n- Use the AWS CLI to run the EC2 import command, which will import the OVF image into Amazon EC2, preserving the application's software and configuration.\n\nThis approach ensures that the solutions architect can migrate the on-premises application to Amazon EC2 while maintaining the integrity of the software and configuration settings.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nA. This option uses AWS DataSync and Amazon FSx for Windows File Server, which do not preserve the software and configuration settings of the application. It also mentions using VM Import/Export, but this approach is not sufficient on its own to maintain the application's settings.\n\nC. This option uses AWS Storage Gateway to export a CIFS share, but it does not mention preserving the software and configuration settings of the application. Creating a backup copy and then creating an AMI from it may not accurately capture the full application environment.\n\nD. This option uses AWS Systems Manager and AWS Backup to create a snapshot of the on-premises VM, but it does not explicitly mention preserving the software and configuration settings of the application. The approach of creating an AMI from the snapshot may not be sufficient to maintain the application's integrity."
  },
  "63": {
    "question": "A video processing company has an application that downloads images from an Amazon S3 bucket, processes the images, stores a transformed image in a second S3 bucket, and updates metadata about the image in an Amazon DynamoDB table. The application is written in Node.js and runs by using an AWS Lambda function. The Lambda function is invoked when a new image is uploaded to Amazon S3.The application ran without incident for a while. However, the size of the images has grown significantly. The Lambda function is now failing frequently with timeout errors. The function timeout is set to its maximum value. A solutions architect needs to refactor the application\u2019s architecture to prevent invocation failures. The company does not want to manage the underlying infrastructure.Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
    "choices": [
      "A. Modify the application deployment by building a Docker image that contains the application code. Publish the image to Amazon Elastic Container Registry (Amazon ECR).",
      "B. Create a new Amazon Elastic Container Service (Amazon ECS) task definition with a compatibility type of AWS Fargate. Configure the task definition to use the new image in Amazon Elastic Container Registry (Amazon ECR). Adjust the Lambda function to invoke an ECS task by using the ECS task definition when a new file arrives in Amazon S3.",
      "C. Create an AWS Step Functions state machine with a Parallel state to invoke the Lambda function. Increase the provisioned concurrency of the Lambda function.",
      "D. Create a new Amazon Elastic Container Service (Amazon ECS) task definition with a compatibility type of Amazon EC2. Configure the task definition to use the new image in Amazon Elastic Container Registry (Amazon ECR). Adjust the Lambda function to invoke an ECS task by using the ECS task definition when a new file arrives in Amazon S3.",
      "E. Modify the application to store images on Amazon Elastic File System (Amazon EFS) and to store metadata on an Amazon RDS DB instance. Adjust the Lambda function to mount the EFS file share."
    ],
    "answer": "AB",
    "explanation": "1. Explanation of the correct answer (A and B):\n\nThe correct answer is A and B because they address the key requirements of the problem:\n\nA. Modify the application deployment by building a Docker image that contains the application code. Publish the image to Amazon Elastic Container Registry (Amazon ECR).\nThis step is necessary to package the application code in a container, which provides a consistent and predictable runtime environment. Containerizing the application also makes it easier to deploy and manage the application, especially as the image size has grown.\n\nB. Create a new Amazon Elastic Container Service (Amazon ECS) task definition with a compatibility type of AWS Fargate. Configure the task definition to use the new image in Amazon Elastic Container Registry (Amazon ECR). Adjust the Lambda function to invoke an ECS task by using the ECS task definition when a new file arrives in Amazon S3.\nThis step is necessary to run the containerized application on Fargate, which is a serverless compute engine for containers. Fargate eliminates the need to provision and manage the underlying infrastructure, which aligns with the company's requirement of not wanting to manage the underlying infrastructure. By invoking an ECS task from the Lambda function, the application can leverage the additional resources and processing power provided by Fargate, which should help prevent the timeout errors that the Lambda function is currently experiencing.\n\n2. Explanations of why the incorrect choices are wrong:\n\nC. Create an AWS Step Functions state machine with a Parallel state to invoke the Lambda function. Increase the provisioned concurrency of the Lambda function.\nThis option does not address the root cause of the timeout errors, which is the growing size of the images. Increasing the provisioned concurrency of the Lambda function may help, but it does not fundamentally solve the problem of the function timing out.\n\nD. Create a new Amazon Elastic Container Service (Amazon ECS) task definition with a compatibility type of Amazon EC2. Configure the task definition to use the new image in Amazon Elastic Container Registry (Amazon ECR). Adjust the Lambda function to invoke an ECS task by using the ECS task definition when a new file arrives in Amazon S3.\nThis option is incorrect because it uses Amazon EC2 as the launch type for the ECS task, which means the company would still have to manage the underlying infrastructure,"
  },
  "64": {
    "question": "A company has an organization in AWS Organizations. The company is using AWS Control Tower to deploy a landing zone for the organization. The company wants to implement governance and policy enforcement. The company must implement a policy that will detect Amazon RDS DB instances that are not encrypted at rest in the company\u2019s production OU.Which solution will meet this requirement?",
    "choices": [
      "A. Turn on mandatory guardrails in AWS Control Tower. Apply the mandatory guardrails to the production OU.",
      "B. Enable the appropriate guardrail from the list of strongly recommended guardrails in AWS Control Tower. Apply the guardrail to the production OU.",
      "C. Use AWS Config to create a new mandatory guardrail. Apply the rule to all accounts in the production OU.",
      "D. Create a custom SCP in AWS Control Tower. Apply the SCP to the production OU."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B, which states to \"Enable the appropriate guardrail from the list of strongly recommended guardrails in AWS Control Tower. Apply the guardrail to the production OU.\"\n\nThis is the correct solution because AWS Control Tower provides a set of \"strongly recommended guardrails\" that can be used to enforce governance and policy rules, including the ability to detect Amazon RDS DB instances that are not encrypted at rest. The \"Encrypt Amazon RDS instances\" guardrail is specifically designed to address this requirement.\n\nBy enabling this guardrail and applying it to the production OU, the company can ensure that any RDS instances deployed in the production environment will be automatically monitored and any instances that are not encrypted at rest will be detected and flagged for remediation.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. \"Turn on mandatory guardrails in AWS Control Tower. Apply the mandatory guardrails to the production OU.\"\nThis is incorrect because mandatory guardrails in AWS Control Tower are pre-defined by AWS and cannot be customized. The \"Encrypt Amazon RDS instances\" guardrail is not a mandatory guardrail, but rather a strongly recommended one, which can be selectively enabled.\n\nC. \"Use AWS Config to create a new mandatory guardrail. Apply the rule to all accounts in the production OU.\"\nThis is incorrect because AWS Config does not provide the concept of \"mandatory guardrails\" like AWS Control Tower does. While AWS Config can be used to create custom rules, it does not have the same level of integration and enforcement capabilities as the guardrails provided by AWS Control Tower.\n\nD. \"Create a custom SCP in AWS Control Tower. Apply the SCP to the production OU.\"\nThis is incorrect because AWS Control Tower does not provide a feature called \"custom SCP\" (Service Control Policy). Instead, it uses the concept of guardrails, which are pre-defined or customizable policies that can be applied to the organization or specific OUs."
  },
  "65": {
    "question": "A startup company hosts a fleet of Amazon EC2 instances in private subnets using the latest Amazon Linux 2 AMI. The company\u2019s engineers rely heavily on SSH access to the instances for troubleshooting.The company\u2019s existing architecture includes the following:\u2022\tA VPC with private and public subnets, and a NAT gateway.\u2022\tSite-to-Site VPN for connectivity with the on-premises environment.\u2022\tEC2 security groups with direct SSH access from the on-premises environment.The company needs to increase security controls around SSH access and provide auditing of commands run by the engineers.Which strategy should a solutions architect use?",
    "choices": [
      "A. Install and configure EC2 Instance Connect on the fleet of EC2 instances. Remove all security group rules attached to EC2 instances that allow inbound TCP on port 22. Advise the engineers to remotely access the instances by using the EC2 Instance Connect CLI.",
      "B. Update the EC2 security groups to only allow inbound TCP on port 22 to the IP addresses of the engineer\u2019s devices. Install the Amazon CloudWatch agent on all EC2 instances and send operating system audit logs to CloudWatch Logs.",
      "C. Update the EC2 security groups to only allow inbound TCP on port 22 to the IP addresses of the engineer\u2019s devices. Enable AWS Config for EC2 security group resource changes. Enable AWS Firewall Manager and apply a security group policy that automatically remediates changes to rules.",
      "D. Create an IAM role with the AmazonSSMManagedInstanceCore managed policy attached. Attach the IAM role to all the EC2 instances. Remove all security group rules attached to the EC2 instances that allow inbound TCP on port 22. Have the engineers install the AWS Systems Manager Session Manager plugin for their devices and remotely access the instances by using the start-session API call from Systems Manager."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (D):\n\nThe correct answer is D because it leverages AWS Systems Manager to provide secure and auditable SSH access to the EC2 instances. By creating an IAM role with the AmazonSSMManagedInstanceCore managed policy and attaching it to the EC2 instances, the instances can be managed through the Systems Manager service. This allows engineers to remotely access the instances using the Systems Manager Session Manager feature, which provides the following benefits:\n\n- Secure access: Session Manager establishes a secure, encrypted connection to the instances without the need for direct SSH access or IP-based security group rules.\n- Auditing: Session Manager automatically logs all commands executed on the instances, providing a comprehensive audit trail.\n- No additional infrastructure: This approach does not require any additional infrastructure setup like EC2 Instance Connect or CloudWatch Logs, making it a more straightforward solution.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option uses EC2 Instance Connect, which requires additional configuration and infrastructure setup, and does not provide auditing capabilities.\n\nB. This option relies on IP-based security group rules, which can be difficult to manage and may not be as secure as using IAM roles. Additionally, it focuses on operating system audit logs, rather than specifically auditing SSH access.\n\nC. This option also relies on IP-based security group rules and focuses on automatically remediating changes to the rules, rather than providing secure and auditable access to the instances.\n\nIn summary, the correct answer (D) is the most comprehensive and efficient solution, leveraging AWS Systems Manager to provide secure, auditable, and hassle-free SSH access to the EC2 instances, without the need for additional infrastructure or complex IP-based security group management."
  },
  "66": {
    "question": "A company that uses AWS Organizations allows developers to experiment on AWS. As part of the landing zone that the company has deployed, developers use their company email address to request an account. The company wants to ensure that developers are not launching costly services or running services unnecessarily. The company must give developers a fixed monthly budget to limit their AWS costs.Which combination of steps will meet these requirements? (Choose three.)",
    "choices": [
      "A. Create an SCP to set a fixed monthly account usage limit. Apply the SCP to the developer accounts.",
      "B. Use AWS Budgets to create a fixed monthly budget for each developer\u2019s account as part of the account creation process.",
      "C. Create an SCP to deny access to costly services and components. Apply the SCP to the developer accounts.",
      "D. Create an IAM policy to deny access to costly services and components. Apply the IAM policy to the developer accounts.",
      "E. Create an AWS Budgets alert action to terminate services when the budgeted amount is reached. Configure the action to terminate all services.",
      "F. Create an AWS Budgets alert action to send an Amazon Simple Notification Service (Amazon SNS) notification when the budgeted amount is reached. Invoke an AWS Lambda function to terminate all services."
    ],
    "answer": "BCF",
    "explanation": "1. Explanation of the correct answer (BCF):\n\nThe correct answer is BCF because it addresses the key requirements of the scenario:\n\na) Creating an AWS Budgets to set a fixed monthly budget for each developer's account (B) - This directly meets the requirement of giving developers a fixed monthly budget to limit their AWS costs.\n\nb) Creating an SCP to deny access to costly services and components (C) - This preventive measure ensures that developers cannot launch costly services in the first place, complementing the budgeting approach.\n\nc) Creating an AWS Budgets alert action to send an Amazon SNS notification when the budgeted amount is reached, and invoking an AWS Lambda function to terminate all services (F) - This provides an automated enforcement mechanism to stop services once the budget limit is reached, ensuring that developers do not run services unnecessarily.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Create an SCP to set a fixed monthly account usage limit. Apply the SCP to the developer accounts.\n   - This is not the correct approach because SCPs cannot be used to set a fixed monthly budget. SCPs are used to define permissions guardrails, not budget limits.\n\nD. Create an IAM policy to deny access to costly services and components. Apply the IAM policy to the developer accounts.\n   - While IAM policies can be used to restrict access to services, it is not the preferred approach compared to using SCPs. SCPs are applied at the organizational level and take precedence over IAM policies, making them more effective for enforcing service restrictions.\n\nE. Create an AWS Budgets alert action to terminate services when the budgeted amount is reached. Configure the action to terminate all services.\n   - Terminating all services when the budget is reached is an overly aggressive approach and may not be desired. The requirement is to prevent running unnecessary services, not to terminate all services. A more appropriate approach is to use the AWS Budgets alert action to trigger a notification or an AWS Lambda function to take appropriate action, as shown in the correct answer (F)."
  },
  "67": {
    "question": "A company has applications in an AWS account that is named Source. The account is in an organization in AWS Organizations. One of the applications uses AWS Lambda functions and stores inventory data in an Amazon Aurora database. The application deploys the Lambda functions by using a deployment package. The company has configured automated backups for Aurora.The company wants to migrate the Lambda functions and the Aurora database to a new AWS account that is named Target. The application processes critical data, so the company must minimize downtime.Which solution will meet these requirements?",
    "choices": [
      "A. Download the Lambda function deployment package from the Source account. Use the deployment package and create new Lambda functions in the Target account. Share the automated Aurora DB cluster snapshot with the Target account.",
      "B. Download the Lambda function deployment package from the Source account. Use the deployment package and create new Lambda functions in the Target account. Share the Aurora DB cluster with the Target account by using AWS Resource Access Manager {AWS RAM). Grant the Target account permission to clone the Aurora DB cluster.",
      "C. Use AWS Resource Access Manager (AWS RAM) to share the Lambda functions and the Aurora DB cluster with the Target account. Grant the Target account permission to clone the Aurora DB cluster.",
      "D. Use AWS Resource Access Manager (AWS RAM) to share the Lambda functions with the Target account. Share the automated Aurora DB cluster snapshot with the Target account."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (Option B):\n\nThe correct answer is Option B because it provides the most effective solution to migrate the Lambda functions and the Aurora database to the new Target account while minimizing downtime.\n\nThe key points of this solution are:\n\n- Download the Lambda function deployment package from the Source account and use it to create new Lambda functions in the Target account. This ensures the Lambda functions can be easily recreated in the new account.\n\n- Use AWS Resource Access Manager (AWS RAM) to share the Aurora DB cluster with the Target account. This allows the Target account to access and clone the existing Aurora DB cluster.\n\n- Grant the Target account permission to clone the Aurora DB cluster. This enables the Target account to create a copy of the database, which can then be used while the original continues running in the Source account, minimizing downtime.\n\n2. Explanations of why the incorrect choices are wrong:\n\nOption A:\n- Downloading the Lambda function deployment package and creating new Lambda functions in the Target account is correct, but this solution does not mention sharing the Aurora DB cluster with the Target account.\n- Sharing the automated Aurora DB cluster snapshot is not possible, as automated snapshots cannot be shared across accounts.\n\nOption C:\n- Using AWS RAM to share both the Lambda functions and the Aurora DB cluster with the Target account is not the best approach, as it does not specify how the data will be migrated to minimize downtime.\n- Granting the Target account permission to clone the Aurora DB cluster is missing from this solution.\n\nOption D:\n- Using AWS RAM to share the Lambda functions with the Target account is correct, but sharing the automated Aurora DB cluster snapshot is not the best solution, as it does not allow for a live migration of the database.\n- This solution does not mention how the Target account will access and use the Aurora DB cluster.\n\nIn summary, Option B is the best solution because it combines the use of the Lambda function deployment package, AWS RAM to share the Aurora DB cluster, and the ability to clone the database, which allows for a seamless migration to the Target account while minimizing downtime."
  },
  "68": {
    "question": "A company runs a Python script on an Amazon EC2 instance to process data. The script runs every 10 minutes. The script ingests files from an Amazon S3 bucket and processes the files. On average, the script takes approximately 5 minutes to process each file The script will not reprocess a file that the script has already processed.The company reviewed Amazon CloudWatch metrics and noticed that the EC2 instance is idle for approximately 40% of the time because of the file processing speed. The company wants to make the workload highly available and scalable. The company also wants to reduce long-term management overhead.Which solution will meet these requirements MOST cost-effectively?",
    "choices": [
      "A. Migrate the data processing script to an AWS Lambda function. Use an S3 event notification to invoke the Lambda function to process the objects when the company uploads the objects.",
      "B. Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure Amazon S3 to send event notifications to the SQS queue. Create an EC2 Auto Scaling group with a minimum size of one instance. Update the data processing script to poll the SQS queue. Process the S3 objects that the SQS message identifies.",
      "C. Migrate the data processing script to a container image. Run the data processing container on an EC2 instance. Configure the container to poll the S3 bucket for new objects and to process the resulting objects.",
      "D. Migrate the data processing script to a container image that runs on Amazon Elastic Container Service (Amazon ECS) on AWS Fargate. Create an AWS Lambda function that calls the Fargate RunTaskAPI operation when the container processes the file. Use an S3 event notification to invoke the Lambda function."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\nThe correct answer is A, migrating the data processing script to an AWS Lambda function and using an S3 event notification to invoke the Lambda function to process the objects when the company uploads the objects. This is the most cost-effective solution for the given requirements:\n\n- Serverless Lambda functions provide high availability and scalability without the need for managing underlying infrastructure, which aligns with the requirement to reduce long-term management overhead.\n- Lambda functions are charged only for the duration of execution, which is likely to be more cost-effective than running a dedicated EC2 instance for the full duration of the data processing script, especially considering the 40% idle time.\n- Leveraging S3 event notifications to trigger the Lambda function ensures that the script only runs when new data is available, further improving cost-effectiveness.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. Using Amazon SQS queue:\n- This option adds an additional service (SQS) and requires managing the EC2 Auto Scaling group, which goes against the requirement to reduce long-term management overhead.\n- The script already has built-in logic to prevent reprocessing of files, making the SQS queue redundant.\n\nC. Migrating the script to a container image on an EC2 instance:\n- This option still requires managing the underlying EC2 instance, which does not align with the requirement to reduce long-term management overhead.\n- It may not be as cost-effective as the serverless Lambda function, especially considering the 40% idle time.\n\nD. Migrating the script to a container image on Amazon ECS on AWS Fargate:\n- While Fargate removes the need to manage the underlying infrastructure, it may not be as cost-effective as the Lambda function, as the script only runs for 5 minutes every 10 minutes.\n- The additional complexity of using a Lambda function to trigger the Fargate task may not be necessary for this use case."
  },
  "69": {
    "question": "A financial services company in North America plans to release a new online web application to its customers on AWS. The company will launch the application in the us-east-1 Region on Amazon EC2 instances. The application must be highly available and must dynamically scale to meet user traffic. The company also wants to implement a disaster recovery environment for the application in the us-west-1 Region by using active-passive failover.Which solution will meet these requirements?",
    "choices": [
      "A. Create a VPC in us-east-1 and a VPC in us-west-1. Configure VPC peering. In the us-east-1 VPC, create an Application Load Balancer (ALB) that extends across multiple Availability Zones in both VPCs. Create an Auto Scaling group that deploys the EC2 instances across the multiple Availability Zones in both VPCs. Place the Auto Scaling group behind the ALB.",
      "B. Create a VPC in us-east-1 and a VPC in us-west-1. In the us-east-1 VPC, create an Application Load Balancer (ALB) that extends across multiple Availability Zones in that VPC. Create an Auto Scaling group that deploys the EC2 instances across the multiple Availability Zones in the us-east-1 VPC. Place the Auto Scaling group behind the ALSet up the same configuration in the us-west-1 VPC. Create an Amazon Route 53 hosted zone. Create separate records for each ALEnable health checks to ensure high availability between Regions.",
      "C. Create a VPC in us-east-1 and a VPC in us-west-1. In the us-east-1 VPC, create an Application Load Balancer (ALB) that extends across multiple Availability Zones in that VPCreate an Auto Scaling group that deploys the EC2 instances across the multiple Availability Zones in the us-east-1 VPPlace the Auto Scaling group behind the ALB. Set up the same configuration in the us-west-1 VPCreate an Amazon Route 53 hosted zone. Create separate records for each ALB. Enable health checks and configure a failover routing policy for each record.",
      "D. Create a VPC in us-east-1 and a VPC in us-west-1. Configure VPC peering. In the us-east-1 VPC, create an Application Load Balancer (ALB) that extends across multiple Availability Zones in both VPCs. Create an Auto Scaling group that deploys the EC2 instances across the multiple Availability Zones in both VPCs. Place the Auto Scaling group behind the ALB. Create an Amazon Route 53 hosted zone. Create a record for the ALB."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C because it fully meets the requirements of the scenario:\n\n- It creates a VPC in us-east-1 and a VPC in us-west-1, ensuring geographical separation for disaster recovery.\n- In the us-east-1 VPC, it creates an Application Load Balancer (ALB) that extends across multiple Availability Zones, and an Auto Scaling group that deploys EC2 instances across those Availability Zones. This ensures high availability and scalability in the primary us-east-1 region.\n- It sets up the same configuration (ALB and Auto Scaling group) in the us-west-1 VPC, creating a disaster recovery environment.\n- It creates an Amazon Route 53 hosted zone with separate records for each ALB, and enables health checks and configures a failover routing policy for each record.\n- The failover routing policy ensures automatic failover from the primary us-east-1 region to the secondary us-west-1 region in case of an outage, providing the required disaster recovery solution.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This choice does not include the necessary configuration for the failover routing policy in Route 53, so it does not fully implement the disaster recovery environment with active-passive failover.\n\nB. This choice sets up the configuration in each region separately, but does not include the failover routing policy in Route 53, so it does not provide the automatic failover mechanism required for the disaster recovery environment.\n\nD. This choice is similar to A, where it sets up the infrastructure across both regions but does not include the failover routing policy in Route 53, so it does not fully implement the disaster recovery environment with active-passive failover."
  },
  "70": {
    "question": "A company has an environment that has a single AWS account. A solutions architect is reviewing the environment to recommend what the company could improve specifically in terms of access to the AWS Management Console. The company\u2019s IT support workers currently access the console for administrative tasks, authenticating with named IAM users that have been mapped to their job role.The IT support workers no longer want to maintain both their Active Directory and IAM user accounts. They want to be able to access the console by using their existing Active Directory credentials. The solutions architect is using AWS IAM Identity Center (AWS Single Sign-On) to implement this functionality.Which solution will meet these requirements MOST cost-effectively?",
    "choices": [
      "A. Create an organization in AWS Organizations. Turn on the IAM Identity Center feature in Organizations. Create and configure a directory in AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD) with a two-way trust to the company\u2019s on-premises Active Directory. Configure IAM Identity Center and set the AWS Managed Microsoft AD directory as the identity source. Create permission sets and map them to the existing groups within the AWS Managed Microsoft AD directory.",
      "B. Create an organization in AWS Organizations. Turn on the IAM Identity Center feature in Organizations. Create and configure an AD Connector to connect to the company\u2019s on-premises Active Directory. Configure IAM Identity Center and select the AD Connector as the identity source. Create permission sets and map them to the existing groups within the company\u2019s Active Directory.",
      "C. Create an organization in AWS Organizations. Turn on all features for the organization. Create and configure a directory in AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD) with a two-way trust to the company\u2019s on-premises Active Directory. Configure IAM Identity Center and select the AWS Managed Microsoft AD directory as the identity source. Create permission sets and map them to the existing groups within the AWS Managed Microsoft AD directory.",
      "D. Create an organization in AWS Organizations. Turn on all features for the organization. Create and configure an AD Connector to connect to the company\u2019s on-premises Active Directory. Configure IAM Identity Center and set the AD Connector as the identity source. Create permission sets and map them to the existing groups within the company\u2019s Active Directory."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (D):\n\nOption D is the most cost-effective solution to meet the requirements. It states that the solution architect should:\n\n- Create an organization in AWS Organizations\n- Turn on all features for the organization\n- Create and configure an AD Connector to connect to the company's on-premises Active Directory\n- Configure IAM Identity Center and set the AD Connector as the identity source\n- Create permission sets and map them to the existing groups within the company's Active Directory\n\nThis approach is the most cost-effective because it leverages the existing on-premises Active Directory without the need to create a new directory in AWS Directory Service. By using the AD Connector, the IT support workers can access the AWS Management Console using their existing Active Directory credentials, eliminating the need to maintain separate IAM user accounts.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option involves creating an AWS Managed Microsoft AD directory, which would incur additional costs compared to using the existing on-premises Active Directory.\n\nB. This option is incorrect because it does not mention enabling all features in AWS Organizations, which is required to integrate IAM Identity Center with on-premises Active Directory.\n\nC. This option is incorrect because it involves creating an AWS Managed Microsoft AD directory, which is unnecessary and would incur additional costs compared to using the existing on-premises Active Directory."
  },
  "71": {
    "question": "A video streaming company recently launched a mobile app for video sharing. The app uploads various files to an Amazon S3 bucket in the us-east-1 Region. The files range in size from 1 GB to 10 GB.Users who access the app from Australia have experienced uploads that take long periods of time. Sometimes the files fail to completely upload for these users. A solutions architect must improve the app\u2019s performance for these uploads.Which solutions will meet these requirements? (Choose two.)",
    "choices": [
      "A. Enable S3 Transfer Acceleration on the S3 bucket. Configure the app to use the Transfer Acceleration endpoint for uploads.",
      "B. Configure an S3 bucket in each Region to receive the uploads. Use S3 Cross-Region Replication to copy the files to the distribution S3 bucket.",
      "C. Set up Amazon Route 53 with latency-based routing to route the uploads to the nearest S3 bucket Region.",
      "D. Configure the app to break the video files into chunks. Use a multipart upload to transfer files to Amazon S3.",
      "E. Modify the app to add random prefixes to the files before uploading."
    ],
    "answer": "AD",
    "explanation": "1. Clear explanation of why the correct answer (A and D) is right:\n\nA. Enable S3 Transfer Acceleration on the S3 bucket. Configure the app to use the Transfer Acceleration endpoint for uploads.\nThis solution addresses the performance issue by leveraging Amazon S3 Transfer Acceleration, which uses a globally distributed network of edge locations to route data to the S3 bucket more efficiently, reducing latency and improving upload speeds, especially for users in distant regions like Australia.\n\nD. Configure the app to break the video files into chunks. Use a multipart upload to transfer files to Amazon S3.\nThis solution addresses the reliability issue by using multipart uploads, which allow the app to upload large files (1-10 GB) in smaller parts. If a part fails, the app can retry that part instead of having to restart the entire upload. This improves the reliability of the uploads, especially for large files that are more prone to failures.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nB. Configure an S3 bucket in each Region to receive the uploads. Use S3 Cross-Region Replication to copy the files to the distribution S3 bucket.\nThis solution does not address the performance issue, as it does not improve the upload speed from Australia to the S3 bucket in the us-east-1 region.\n\nC. Set up Amazon Route 53 with latency-based routing to route the uploads to the nearest S3 bucket Region.\nWhile this solution can route the uploads to the nearest S3 bucket, it does not guarantee faster upload speeds or better reliability, as it does not address the underlying network latency issues.\n\nE. Modify the app to add random prefixes to the files before uploading.\nAdding random prefixes to the files before uploading will not improve the upload performance or reliability, as it does not address the root causes of the issues."
  },
  "72": {
    "question": "A company operates a fleet of servers on premises and operates a fleet of Amazon EC2 instances in its organization in AWS Organizations. The company's AWS accounts contain hundreds of VPCs. The company wants to connect its AWS accounts to its on-premises network. AWS Site-to-Site VPN connections are already established to a single AWS account. The company wants to control which VPCs can communicate with other VPCs.Which combination of steps will achieve this level of control with the LEAST operational effort? (Choose three.)",
    "choices": [
      "A. Create a transit gateway in an AWS account. Share the transit gateway across accounts by using AWS Resource Access Manager (AWS RAM).",
      "B. Configure attachments to all VPCs and VPNs.",
      "C. Setup transit gateway route tables. Associate the VPCs and VPNs with the route tables.",
      "D. Configure VPC peering between the VPCs.",
      "E. Configure attachments between the VPCs and VPNs.",
      "F. Setup route tables on the VPCs and VPNs."
    ],
    "answer": "ABC",
    "explanation": "1. Explanation of the correct answer (ABC):\n\nA. Create a transit gateway in an AWS account. Share the transit gateway across accounts by using AWS Resource Access Manager (AWS RAM).\nThis is the correct choice as it enables central control and management of connectivity between VPCs and on-premises network. The transit gateway acts as a hub, simplifying connectivity compared to individual VPC peering.\n\nB. Configure attachments to all VPCs and VPNs.\nThis is a correct choice as the attachments need to be configured between the VPCs, VPNs, and the transit gateway to enable communication.\n\nC. Setup transit gateway route tables. Associate the VPCs and VPNs with the route tables.\nThis is a correct choice as the transit gateway route tables control which VPCs can communicate with each other, providing the required level of control.\n\n2. Explanations of why the incorrect choices are wrong:\n\nD. Configure VPC peering between the VPCs.\nThis is incorrect as VPC peering would be too complex and operationally intensive to manage for hundreds of VPCs. The transit gateway provides a more scalable and centralized approach.\n\nE. Configure attachments between the VPCs and VPNs.\nThis is incorrect as the attachments should be configured between the VPCs, VPNs, and the transit gateway, not directly between the VPCs and VPNs.\n\nF. Setup route tables on the VPCs and VPNs.\nThis is incorrect as the route tables should be configured on the transit gateway, not on the individual VPCs and VPNs. The transit gateway route tables provide the central control required."
  },
  "73": {
    "question": "A company is building a solution in the AWS Cloud. Thousands or devices will connect to the solution and send data. Each device needs to be able to send and receive data in real time over the MQTT protocol. Each device must authenticate by using a unique X.509 certificate.Which solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      "A. Set up AWS IoT Core. For each device, create a corresponding Amazon MQ queue and provision a certificate. Connect each device to Amazon MQ.",
      "B. Create a Network Load Balancer (NLB) and configure it with an AWS Lambda authorizer. Run an MQTT broker on Amazon EC2 instances in an Auto Scaling group. Set the Auto Scaling group as the target for the NLConnect each device to the NLB.",
      "C. Set up AWS IoT Core. For each device, create a corresponding AWS IoT thing and provision a certificate. Connect each device to AWS IoT Core.",
      "D. Set up an Amazon API Gateway HTTP API and a Network Load Balancer (NLB). Create integration between API Gateway and the NLB. Configure a mutual TLS certificate authorizer on the HTTP API. Run an MQTT broker on an Amazon EC2 instance that the NLB targets. Connect each device to the NLB."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is option C, which is to set up AWS IoT Core, create a corresponding AWS IoT thing and provision a certificate for each device, and then connect each device to AWS IoT Core. This is the best solution because:\n\n- AWS IoT Core is a fully managed service that is purpose-built for securely connecting IoT devices to the cloud and managing their communication. It natively supports the MQTT protocol and device authentication using X.509 certificates.\n- By using AWS IoT Core, the company does not have to manage the infrastructure, scale the MQTT brokers, or handle the complexities of device authentication and authorization. All of these tasks are handled by the managed service, which reduces the operational overhead.\n- AWS IoT Core is designed to scale to support thousands of devices, which meets the requirement of the company.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Setting up Amazon MQ queues and connecting each device to a queue would require significant operational overhead to manage the queues and ensure that each device is properly authenticated and connected. This approach does not leverage the features of a purpose-built IoT service like AWS IoT Core.\n\nB. Using a Network Load Balancer (NLB) with a Lambda authorizer and running an MQTT broker on EC2 instances introduces more operational complexity and overhead compared to using AWS IoT Core. The company would need to manage the infrastructure, scale the MQTT brokers, and handle the complexities of device authentication.\n\nD. Using an Amazon API Gateway HTTP API with a mutual TLS certificate authorizer and running an MQTT broker on EC2 instances also introduces more operational complexity and overhead compared to using AWS IoT Core. The company would need to manage the infrastructure, scale the MQTT brokers, and handle the complexities of device authentication."
  },
  "74": {
    "question": "A company is using an organization in AWS Organizations to manage hundreds of AWS accounts. A solutions architect is working on a solution to provide baseline protection for the Open Web Application Security Project (OWASP) top 10 web application vulnerabilities. The solutions architect is using AWS WAF for all existing and new Amazon CloudFront distributions that are deployed within the organization.Which combination of steps should the solutions architect take to provide the baseline protection? (Choose three.)",
    "choices": [
      "A. Enable AWS Config in all accounts",
      "B. Enable Amazon GuardDuty in all accounts",
      "C. Enable all features for the organization",
      "D. Use AWS Firewall Manager to deploy AWS WAF rules in all accounts for all CloudFront distributions",
      "E. Use AWS Shield Advanced to deploy AWS WAF rules in all accounts for all CloudFront distributions",
      "F. Use AWS Security Hub to deploy AWS WAF rules in all accounts for all CloudFront distributions"
    ],
    "answer": "ACD",
    "explanation": "1. Explanation of the correct answer (ACD):\n\nA. Enable AWS Config in all accounts:\nAWS Config is a must-have prerequisite for using AWS Firewall Manager to deploy AWS WAF rules across the organization. AWS Config allows Firewall Manager to detect newly created resources, such as CloudFront distributions, and apply the appropriate WAF rules.\n\nC. Enable all features for the organization:\nAWS Firewall Manager requires the organization to have \"All Features\" enabled in AWS Organizations. This allows Firewall Manager to manage and apply policies across all accounts in the organization.\n\nD. Use AWS Firewall Manager to deploy AWS WAF rules in all accounts for all CloudFront distributions:\nAWS Firewall Manager is the recommended service to centrally manage and deploy AWS WAF rules across all accounts and CloudFront distributions within the organization. Firewall Manager simplifies the process of applying consistent security policies throughout the organization.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. Enable Amazon GuardDuty in all accounts:\nWhile Amazon GuardDuty is a valuable security service, it is not a requirement for deploying AWS WAF rules using Firewall Manager. GuardDuty provides threat detection, but does not directly manage the deployment of WAF rules.\n\nE. Use AWS Shield Advanced to deploy AWS WAF rules in all accounts for all CloudFront distributions:\nAWS Shield Advanced is a DDoS protection service and does not provide the functionality to deploy AWS WAF rules across the organization. AWS Firewall Manager is the correct service to use for centralized WAF rule management.\n\nF. Use AWS Security Hub to deploy AWS WAF rules in all accounts for all CloudFront distributions:\nAWS Security Hub is a security posture management service that aggregates security findings from various AWS services. It does not have the capability to directly deploy AWS WAF rules across the organization. AWS Firewall Manager is the appropriate service for this task."
  },
  "75": {
    "question": "A company needs to architect a hybrid DNS solution. This solution will use an Amazon Route 53 private hosted zone for the domain cloud.example.com for the resources stored within VPCs.The company has the following DNS resolution requirements:On-premises systems should be able to resolve and connect to cloud.example.com.All VPCs should be able to resolve cloud.example.com.There is already an AWS Direct Connect connection between the on-premises corporate network and AWS Transit Gateway.Which architecture should the company use to meet these requirements with the HIGHEST performance?",
    "choices": [
      "A. Associate the private hosted zone to all the VPCs. Create a Route 53 inbound resolver in the shared services VPC. Attach all VPCs to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the inbound resolver.",
      "B. Associate the private hosted zone to all the VPCs. Deploy an Amazon EC2 conditional forwarder in the shared services VPC. Attach all VPCs to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the conditional forwarder.",
      "C. Associate the private hosted zone to the shared services VPCreate a Route 53 outbound resolver in the shared services VPAttach all VPCs to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the outbound resolver.",
      "D. Associate the private hosted zone to the shared services VPC. Create a Route 53 inbound resolver in the shared services VPC. Attach the shared services VPC to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the inbound resolver."
    ],
    "answer": "A",
    "explanation": "1. Clear explanation of why the correct answer (A) is right:\n\nThe correct answer is A because it meets all the requirements specified in the question:\n\n- It associates the private hosted zone for cloud.example.com to all the VPCs, ensuring that all VPCs can resolve the domain.\n- It creates a Route 53 inbound resolver in the shared services VPC, which allows the on-premises systems to resolve and connect to cloud.example.com by forwarding DNS queries to the inbound resolver.\n- It attaches all VPCs to the transit gateway and creates forwarding rules in the on-premises DNS server for cloud.example.com that point to the inbound resolver. This ensures that the on-premises systems can resolve and connect to the cloud.example.com domain.\n\nThis architecture provides the highest performance by using the Route 53 inbound resolver, which is a fully managed service that can handle DNS resolution efficiently, rather than relying on an EC2-based conditional forwarder or an outbound resolver.\n\n2. Brief explanations of why the incorrect choices are wrong:\n\nB. The EC2 conditional forwarder will not meet the highest performance requirement, as it is a less scalable and less efficient solution compared to the Route 53 inbound resolver.\n\nC. This solution is missing the association of the private hosted zone to all the VPCs, which is a requirement stated in the question. All VPCs need to be able to resolve the cloud.example.com domain.\n\nD. This solution is also missing the association of the private hosted zone to all the VPCs, which is a requirement stated in the question. All VPCs need to be able to resolve the cloud.example.com domain."
  },
  "76": {
    "question": "A company is running a traditional web application on Amazon EC2 instances. The company needs to refactor the application as microservices that run on containers. Separate versions of the application exist in two distinct environments: production and testing. Load for the application is variable, but the minimum load and the maximum load are known. A solutions architect needs to design the updated application with a serverless architecture that minimizes operational complexity.Which solution will meet these requirements MOST cost-effectively?",
    "choices": [
      "A. Upload the container images to AWS Lambda as functions. Configure a concurrency limit for the associated Lambda functions to handle the expected peak load. Configure two separate Lambda integrations within Amazon API Gateway: one for production and one for testing.",
      "B. Upload the container images to Amazon Elastic Container Registry (Amazon ECR). Configure two auto scaled Amazon Elastic Container Service (Amazon ECS) clusters with the Fargate launch type to handle the expected load. Deploy tasks from the ECR images. Configure two separate Application Load Balancers to direct traffic to the ECS clusters.",
      "C. Upload the container images to Amazon Elastic Container Registry (Amazon ECR). Configure two auto scaled Amazon Elastic Kubernetes Service (Amazon EKS) clusters with the Fargate launch type to handle the expected load. Deploy tasks from the ECR images. Configure two separate Application Load Balancers to direct traffic to the EKS clusters.",
      "D. Upload the container images to AWS Elastic Beanstalk. In Elastic Beanstalk, create separate environments and deployments for production and testing. Configure two separate Application Load Balancers to direct traffic to the Elastic Beanstalk deployments."
    ],
    "answer": "B",
    "explanation": "1. Clear explanation of why the correct answer (B) is right:\n\nThe correct answer is B because it best meets the requirements of the problem statement:\n\n- Serverless architecture: The use of Amazon ECS with the Fargate launch type provides a serverless container orchestration solution, minimizing operational complexity.\n- Separate environments: The question states the company needs separate versions of the application in production and testing environments. Answer B addresses this by configuring two separate ECS clusters and Application Load Balancers.\n- Scalability: The auto-scaling capabilities of ECS with Fargate can handle the variable load, from minimum to maximum, as specified in the requirements.\n- Cost-effectiveness: Compared to the other options, ECS with Fargate is the most cost-effective serverless approach, as it eliminates the need to manage the underlying infrastructure.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nA. Using AWS Lambda functions to run the containerized application is not the most cost-effective solution. Lambda has limitations on the size of container images (10GB), which may not be sufficient for a full-fledged application. Additionally, managing separate Lambda integrations within API Gateway for production and testing adds unnecessary complexity.\n\nC. Using Amazon EKS (Elastic Kubernetes Service) with Fargate is also a valid approach, but it is more complex to manage and operate compared to the simpler ECS solution in answer B. EKS requires more overhead in terms of Kubernetes cluster management, which adds to the operational complexity.\n\nD. Elastic Beanstalk is a PaaS (Platform-as-a-Service) solution, which provides less control and flexibility compared to the container-based approach in answer B. Elastic Beanstalk also does not offer the same level of serverless, auto-scaling capabilities as ECS with Fargate.\n\nIn summary, answer B is the most cost-effective and operationally efficient solution that meets the requirements of a serverless architecture with separate production and testing environments, as well as the ability to handle variable load."
  },
  "77": {
    "question": "Example Corp. has an on-premises data center and a VPC named VPC A in the Example Corp. AWS account. The on-premises network connects to VPC A through an AWS Site-To-Site VPN. The on-premises servers can properly access VPC A. Example Corp. just acquired AnyCompany, which has a VPC named VPC B. There is no IP address overlap among these networks. Example Corp. has peered VPC A and VPC B.Example Corp. wants to connect from its on-premise servers to VPC B. Example Corp. has properly set up the network ACL and security groups.Which solution will meet this requirement with the LEAST operational effort?",
    "choices": [
      "A. Create a transit gateway. Attach the Site-to-Site VPN, VPC A, and VPC B to the transit gateway. Update the transit gateway route tables for all networks to add IP range routes for all other networks.",
      "B. Create a transit gateway. Create a Site-to-Site VPN connection between the on-premises network and VPC B, and connect the VPN connection to the transit gateway. Add a route to direct traffic to the peered VPCs, and add an authorization rule to give clients access to the VPCs A and B.",
      "C. Update the route tables for the Site-to-Site VPN and both VPCs for all three networks. Configure BGP propagation for all three networks. Wait for up to 5 minutes for BGP propagation to finish.",
      "D. Modify the Site-to-Site VPN\u2019s virtual private gateway definition to include VPC A and VPC B. Split the two routers of the virtual private getaway between the two VPCs."
    ],
    "answer": "A",
    "explanation": "1. Clear explanation of why the correct answer (A) is right:\n\nThe correct answer is A because the use of an AWS Transit Gateway (TGW) provides the simplest and most scalable solution to connect the on-premises network to both VPC A and VPC B. \n\nWith the TGW, you can attach the existing Site-to-Site VPN, VPC A, and VPC B to the TGW. This allows for transitive routing between all three networks through the centralized TGW, eliminating the need to manage complex routing configurations across multiple VPCs and on-premises networks. The TGW also provides high availability and scalability, making it the recommended approach for interconnecting multiple VPCs.\n\nBy updating the TGW route tables to include IP ranges for all connected networks, traffic can flow seamlessly between the on-premises network and both VPCs, with the least operational effort.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nB. This choice suggests creating a separate Site-to-Site VPN connection between the on-premises network and VPC B, and attaching it to the TGW. While this would work, it adds unnecessary complexity by requiring the management of multiple VPN connections. The correct approach is to attach all networks (on-premises and both VPCs) directly to the centralized TGW.\n\nC. Updating the route tables for the Site-to-Site VPN and both VPCs individually is more operationally intensive than using the TGW. It also does not provide the same level of scalability and high availability as the TGW solution.\n\nD. Modifying the existing Site-to-Site VPN's virtual private gateway to include both VPC A and VPC B is not the recommended approach. It would be more complex to manage and does not leverage the benefits of the TGW, such as transitive routing and centralized network management."
  },
  "78": {
    "question": "A company recently completed the migration from an on-premises data center to the AWS Cloud by using a replatforming strategy. One of the migrated servers is running a legacy Simple Mail Transfer Protocol (SMTP) service that a critical application relies upon. The application sends outbound email messages to the company\u2019s customers. The legacy SMTP server does not support TLS encryption and uses TCP port 25. The application can use SMTP only.The company decides to use Amazon Simple Email Service (Amazon SES) and to decommission the legacy SMTP server. The company has created and validated the SES domain. The company has lifted the SES limits.What should the company do to modify the application to send email messages from Amazon SES?",
    "choices": [
      "A. Configure the application to connect to Amazon SES by using TLS Wrapper. Create an IAM role that has ses:SendEmail and ses:SendRawEmail permissions. Attach the IAM role to an Amazon EC2 instance.",
      "B. Configure the application to connect to Amazon SES by using STARTTLS. Obtain Amazon SES SMTP credentials. Use the credentials to authenticate with Amazon SES.",
      "C. Configure the application to use the SES API to send email messages. Create an IAM role that has ses:SendEmail and ses:SendRawEmail permissions. Use the IAM role as a service role for Amazon SES.",
      "D. Configure the application to use AWS SDKs to send email messages. Create an IAM user for Amazon SES. Generate API access keys. Use the access keys to authenticate with Amazon SES."
    ],
    "answer": "B",
    "explanation": "1. Explanation of why the correct answer (B) is right:\n\nThe correct answer is B because it aligns best with the given scenario and requirements:\n\n- The application can only use SMTP to send emails, so it needs to connect to Amazon SES using the SMTP interface.\n- Amazon SES supports STARTTLS, which allows the application to establish a secure connection over the existing port 25 used by the legacy SMTP server.\n- Obtaining the Amazon SES SMTP credentials and using them to authenticate with the Amazon SES SMTP endpoint is the recommended approach, as it does not require modifying the application to use a different authentication mechanism (like IAM roles or API access keys).\n\n2. Explanations of why each incorrect choice is wrong:\n\nA. This option requires the use of a TLS Wrapper, which is not necessary since Amazon SES supports STARTTLS. Additionally, creating an IAM role with specific permissions and attaching it to an EC2 instance is an unnecessary complication when the application can simply use the Amazon SES SMTP credentials.\n\nC. This option requires the application to use the Amazon SES API directly, which is not possible since the application can only use SMTP. The use of an IAM role as a service role for Amazon SES is also not required in this scenario.\n\nD. This option involves creating an IAM user and using API access keys to authenticate with Amazon SES. However, the question states that the application can only use SMTP, so this approach is not suitable."
  },
  "79": {
    "question": "A company recently acquired several other companies. Each company has a separate AWS account with a different billing and reporting method. The acquiring company has consolidated all the accounts into one organization in AWS Organizations. However, the acquiring company has found it difficult to generate a cost report that contains meaningful groups for all the teams.The acquiring company\u2019s finance team needs a solution to report on costs for all the companies through a self-managed application.Which solution will meet these requirements?",
    "choices": [
      "A. Create an AWS Cost and Usage Report for the organization. Define tags and cost categories in the report. Create a table in Amazon Athena. Create an Amazon QuickSight dataset based on the Athena table. Share the dataset with the finance team.",
      "B. Create an AWS Cost and Usage Report for the organization. Define tags and cost categories in the report. Create a specialized template in AWS Cost Explorer that the finance department will use to build reports.",
      "C. Create an Amazon QuickSight dataset that receives spending information from the AWS Price List Query API. Share the dataset with the finance team.",
      "D. Use the AWS Price List Query API to collect account spending information. Create a specialized template in AWS Cost Explorer that the finance department will use to build reports."
    ],
    "answer": "A",
    "explanation": "1. Clear explanation of why the correct answer (A) is right:\n\nThe correct answer is A because it provides the most comprehensive solution to the given requirements. By creating an AWS Cost and Usage Report for the organization and defining tags and cost categories, the acquiring company can get detailed cost data for all the consolidated accounts. Then, by creating a table in Amazon Athena and an Amazon QuickSight dataset based on this table, the finance team can easily query and generate reports on the costs for all the companies. Sharing the QuickSight dataset with the finance team allows them to access and analyze the cost data through a self-managed application, which meets the stated requirement.\n\n2. Brief explanations of why the incorrect choices are wrong:\n\nB. This option is incorrect because it does not provide a way to query and generate reports on the costs for all the companies. While it mentions creating a specialized template in AWS Cost Explorer, the question specifically states that the finance team needs a solution to report on costs through a self-managed application, which Cost Explorer does not provide.\n\nC. This option is incorrect because it only provides spending information from the AWS Price List Query API and does not provide detailed cost reporting for the different companies. The finance team requires a more comprehensive solution to analyze the costs across all the consolidated accounts.\n\nD. This option is incorrect because it only uses the AWS Price List Query API and does not provide a way to query and generate reports on the costs for all the companies. Like option C, it does not meet the requirement of the finance team to have a self-managed application for cost reporting."
  },
  "80": {
    "question": "A company runs an IoT platform on AWS. IoT sensors in various locations send data to the company\u2019s Node.js API servers on Amazon EC2 instances running behind an Application Load Balancer. The data is stored in an Amazon RDS MySQL DB instance that uses a 4 TB General Purpose SSD volume.The number of sensors the company has deployed in the field has increased over time, and is expected to grow significantly. The API servers are consistently overloaded and RDS metrics show high write latency.Which of the following steps together will resolve the issues permanently and enable growth as new sensors are provisioned, while keeping this platform cost-efficient? (Choose two.)",
    "choices": [
      "A. Resize the MySQL General Purpose SSD storage to 6 TB to improve the volume\u2019s IOPS.",
      "B. Re-architect the database tier to use Amazon Aurora instead of an RDS MySQL DB instance and add read replicas.",
      "C. Leverage Amazon Kinesis Data Streams and AWS Lambda to ingest and process the raw data.",
      "D. Use AWS X-Ray to analyze and debug application issues and add more API servers to match the load.",
      "E. Re-architect the database tier to use Amazon DynamoDB instead of an RDS MySQL DB instance."
    ],
    "answer": "CE",
    "explanation": "1. Explanation of the correct answer:\n\nThe correct answer is C and B.\n\nC. Leverage Amazon Kinesis Data Streams and AWS Lambda to ingest and process the raw data.\nThis is the correct choice because the API servers are consistently overloaded, indicating that they are unable to handle the incoming data efficiently. By using Amazon Kinesis Data Streams and AWS Lambda, the platform can offload the data ingestion and processing tasks from the API servers, improving their performance and scalability. Kinesis Data Streams can handle the high volume of data from the IoT sensors, and Lambda can be used to process the data in real-time, reducing the load on the API servers.\n\nB. Re-architect the database tier to use Amazon Aurora instead of an RDS MySQL DB instance and add read replicas.\nThis is the correct choice because the RDS MySQL DB instance is experiencing high write latency, which can be a bottleneck for the overall platform performance. Migrating to Amazon Aurora, which is a highly scalable and resilient database service, can help address the write latency issues. Additionally, adding read replicas can help distribute the read load, further improving the platform's performance and scalability.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Resize the MySQL General Purpose SSD storage to 6 TB to improve the volume's IOPS.\nThis is not the correct choice because simply increasing the storage size of the RDS MySQL DB instance may not resolve the high write latency issue. The underlying problem is likely related to the database architecture and the ability to handle the increasing data volume, not just the storage capacity.\n\nD. Use AWS X-Ray to analyze and debug application issues and add more API servers to match the load.\nThis is not the correct choice because while adding more API servers can help distribute the load, it does not address the root cause of the overloading issue. Using AWS X-Ray to analyze and debug the application may help identify the bottlenecks, but it does not provide a comprehensive solution to the scalability and performance challenges.\n\nE. Re-architect the database tier to use Amazon DynamoDB instead of an RDS MySQL DB instance.\nThis is not the correct choice because RDS MySQL and DynamoDB are fundamentally different database systems. RDS MySQL is a"
  },
  "81": {
    "question": "A company is building an electronic document management system in which users upload their documents. The application stack is entirely serverless and runs on AWS in the eu-central-1 Region. The system includes a web application that uses an Amazon CloudFront distribution for delivery with Amazon S3 as the origin. The web application communicates with Amazon API Gateway Regional endpoints. The API Gateway APIs call AWS Lambda functions that store metadata in an Amazon Aurora Serverless database and put the documents into an S3 bucket.The company is growing steadily and has completed a proof of concept with its largest customer. The company must improve latency outside of Europe.Which combination of actions will meet these requirements? (Choose two.)",
    "choices": [
      "A. Enable S3 Transfer Acceleration on the S3 bucket. Ensure that the web application uses the Transfer Acceleration signed URLs.",
      "B. Create an accelerator in AWS Global Accelerator. Attach the accelerator to the CloudFront distribution.",
      "C. Change the API Gateway Regional endpoints to edge-optimized endpoints.",
      "D. Provision the entire stack in two other locations that are spread across the world. Use global databases on the Aurora Serverless cluster.",
      "E. Add an Amazon RDS proxy between the Lambda functions and the Aurora Serverless database."
    ],
    "answer": "AC",
    "explanation": "1. Explanation of the correct answer:\n\nA. Enable S3 Transfer Acceleration on the S3 bucket. Ensure that the web application uses the Transfer Acceleration signed URLs:\nThis option is correct because S3 Transfer Acceleration can help improve upload latency for users outside of Europe. By enabling Transfer Acceleration, the user uploads are routed to the nearest AWS edge location, which then transfers the data to the S3 bucket in the eu-central-1 region more efficiently, reducing the latency experienced by users.\n\nC. Change the API Gateway Regional endpoints to edge-optimized endpoints:\nThis option is also correct because by changing the API Gateway endpoints to edge-optimized, the API responses will be cached at the nearest AWS edge location, reducing the latency experienced by users when making API calls, especially those outside of Europe.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. Create an accelerator in AWS Global Accelerator. Attach the accelerator to the CloudFront distribution:\nThis is incorrect because the question states that the web application uses a CloudFront distribution, and Global Accelerator is designed to work with other AWS services, not CloudFront. Global Accelerator would not provide any additional benefits in this scenario.\n\nD. Provision the entire stack in two other locations that are spread across the world. Use global databases on the Aurora Serverless cluster:\nThis is incorrect because the question states that the application is entirely serverless and runs on AWS in the eu-central-1 region. Provisioning the entire stack in multiple locations would require significant architectural changes and may not be necessary if the other options (A and C) are implemented.\n\nE. Add an Amazon RDS proxy between the Lambda functions and the Aurora Serverless database:\nThis is incorrect because the question does not indicate any issues with the communication between the Lambda functions and the Aurora Serverless database. Adding an RDS proxy would not directly address the requirement to improve latency outside of Europe."
  },
  "82": {
    "question": "An adventure company has launched a new feature on its mobile app. Users can use the feature to upload their hiking and rafting photos and videos anytime. The photos and videos are stored in Amazon S3 Standard storage in an S3 bucket and are served through Amazon CloudFront.The company needs to optimize the cost of the storage. A solutions architect discovers that most of the uploaded photos and videos are accessed infrequently after 30 days. However, some of the uploaded photos and videos are accessed frequently after 30 days. The solutions architect needs to implement a solution that maintains millisecond retrieval availability of the photos and videos at the lowest possible cost.Which solution will meet these requirements?",
    "choices": [
      "A. Configure S3 Intelligent-Tiering on the S3 bucket.",
      "B. Configure an S3 Lifecycle policy to transition image objects and video objects from S3 Standard to S3 Glacier Deep Archive after 30 days.",
      "C. Replace Amazon S3 with an Amazon Elastic File System (Amazon EFS) file system that is mounted on Amazon EC2 instances.",
      "D. Add a Cache-Control: max-age header to the S3 image objects and S3 video objects. Set the header to 30 days."
    ],
    "answer": "A",
    "explanation": "1. Explanation of why the correct answer (A) is right:\n\nThe correct answer is A. Configure S3 Intelligent-Tiering on the S3 bucket.\n\nS3 Intelligent-Tiering is the ideal solution for this scenario because it automatically moves objects between two access tiers (frequent access and infrequent access) based on changing access patterns. This allows for cost optimization without requiring manual intervention.\n\nSince the question states that most of the uploaded photos and videos are accessed infrequently after 30 days, but some are accessed frequently, S3 Intelligent-Tiering can automatically move the infrequently accessed objects to the infrequent access tier, which has a lower storage cost. At the same time, it will keep the frequently accessed objects in the frequent access tier, maintaining millisecond retrieval availability.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. Configure an S3 Lifecycle policy to transition image objects and video objects from S3 Standard to S3 Glacier Deep Archive after 30 days.\nThis is incorrect because S3 Glacier Deep Archive has a higher latency for data retrieval, which does not meet the requirement of maintaining millisecond retrieval availability.\n\nC. Replace Amazon S3 with an Amazon Elastic File System (Amazon EFS) file system that is mounted on Amazon EC2 instances.\nThis is incorrect because Amazon EFS is a file storage service for use with Amazon EC2 instances, not a cost-effective solution for storing and retrieving large amounts of data like photos and videos.\n\nD. Add a Cache-Control: max-age header to the S3 image objects and S3 video objects. Set the header to 30 days.\nThis is incorrect because adding a Cache-Control header only controls the caching behavior of the objects and does not address the cost optimization requirements. It does not automatically move the infrequently accessed objects to a lower-cost storage tier."
  },
  "83": {
    "question": "A company has set up its entire infrastructure on AWS. The company uses Amazon EC2 instances to host its ecommerce website and uses Amazon S3 to store static data. Three engineers at the company handle the cloud administration and development through one AWS account. Occasionally, an engineer alters an EC2 security group configuration of another engineer and causes noncompliance issues in the environment.A solutions architect must set up a system that tracks changes that the engineers make. The system must send alerts when the engineers make noncompliant changes to the security settings for the EC2 instances.What is the FASTEST way for the solutions architect to meet these requirements?",
    "choices": [
      "A. Set up AWS Organizations for the company. Apply SCPs to govern and track noncompliant security group changes that are made to the AWS account.",
      "B. Enable AWS CloudTrail to capture the changes to EC2 security groups. Enable Amazon CloudWatch rules to provide alerts when noncompliant security settings are detected.",
      "C. Enable SCPs on the AWS account to provide alerts when noncompliant security group changes are made to the environment.",
      "D. Enable AWS Config on the EC2 security groups to track any noncompliant changes. Send the changes as alerts through an Amazon Simple Notification Service (Amazon SNS) topic."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (D):\n\nThe correct answer is D. Enable AWS Config on the EC2 security groups to track any noncompliant changes and send the changes as alerts through an Amazon Simple Notification Service (Amazon SNS) topic.\n\nAWS Config is the fastest way to meet the requirements in this scenario for the following reasons:\n\n- AWS Config can directly monitor the configuration of EC2 security groups and detect any noncompliant changes in near real-time (typically within 10 minutes).\n- AWS Config can be configured to send notifications through Amazon SNS whenever a noncompliant change is detected, allowing the engineers to be alerted immediately.\n- Setting up AWS Config is relatively straightforward and does not require the additional steps of configuring CloudTrail, CloudWatch Logs, CloudWatch Metric Filters, and CloudWatch Alarms, as would be the case with option B.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Set up AWS Organizations for the company. Apply SCPs to govern and track noncompliant security group changes that are made to the AWS account.\n- This option is not the fastest solution, as setting up AWS Organizations and applying Service Control Policies (SCPs) takes more time and effort than the AWS Config approach.\n- SCPs are used to govern and restrict what actions can be performed within an AWS organization, not for real-time monitoring and alerting of noncompliant changes.\n\nB. Enable AWS CloudTrail to capture the changes to EC2 security groups. Enable Amazon CloudWatch rules to provide alerts when noncompliant security settings are detected.\n- While this option can also meet the requirements, it is not the fastest solution. Setting up CloudTrail, CloudWatch Logs, CloudWatch Metric Filters, and CloudWatch Alarms requires more steps and configuration than the AWS Config approach.\n- The question specifically asks for the \"FASTEST way\" to meet the requirements, and the AWS Config option is simpler and quicker to implement.\n\nC. Enable SCPs on the AWS account to provide alerts when noncompliant security group changes are made to the environment.\n- This option is incorrect because SCPs are not designed for real-time monitoring and alerting of noncompliant changes. SCPs are used to govern and restrict"
  },
  "84": {
    "question": "A company has multiple AWS accounts. The company recently had a security audit that revealed many unencrypted Amazon Elastic Block Store (Amazon EBS) volumes attached to Amazon EC2 instances.A solutions architect must encrypt the unencrypted volumes and ensure that unencrypted volumes will be detected automatically in the future. Additionally, the company wants a solution that can centrally manage multiple AWS accounts with a focus on compliance and security.Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
    "choices": [
      "A. Create an organization in AWS Organizations. Set up AWS Control Tower, and turn on the strongly recommended controls (guardrails). Join all accounts to the organization. Categorize the AWS accounts into OUs.",
      "B. Use the AWS CLI to list all the unencrypted volumes in all the AWS accounts. Run a script to encrypt all the unencrypted volumes in place.",
      "C. Create a snapshot of each unencrypted volume. Create a new encrypted volume from the unencrypted snapshot. Detach the existing volume, and replace it with the encrypted volume.",
      "D. Create an organization in AWS Organizations. Set up AWS Control Tower, and turn on the mandatory controls (guardrails). Join all accounts to the organization. Categorize the AWS accounts into OUs.",
      "E. Turn on AWS CloudTrail. Configure an Amazon EventBridge rule to detect and automatically encrypt unencrypted volumes."
    ],
    "answer": "AC",
    "explanation": "1. Explanation of the correct answer:\n\nThe correct answer is A and C.\n\nA is correct because setting up an AWS Organization with AWS Control Tower is the best way to centrally manage multiple AWS accounts and enforce compliance and security policies across all accounts. The \"strongly recommended\" controls in AWS Control Tower include the ability to detect whether encryption is enabled for Amazon EBS volumes attached to Amazon EC2 instances. This helps automatically identify any unencrypted volumes and ensures that encryption is enforced going forward.\n\nC is also correct because it outlines the best way to encrypt the existing unencrypted volumes. By creating snapshots of the unencrypted volumes, creating new encrypted volumes from the snapshots, and then replacing the original unencrypted volumes, the solutions architect can efficiently encrypt all the volumes without data loss.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB is incorrect because using the AWS CLI to list and encrypt unencrypted volumes manually across multiple accounts is not a scalable or efficient solution, especially for a large number of volumes. It also does not provide a way to automatically detect and encrypt new unencrypted volumes in the future.\n\nD is incorrect because the \"mandatory\" controls in AWS Control Tower are not sufficient to meet the requirements. The question specifically states that the \"strongly recommended\" controls are needed to detect unencrypted volumes.\n\nE is incorrect because while setting up an Amazon EventBridge rule to detect and automatically encrypt unencrypted volumes is a valid approach, it does not address the requirement for centralized management of multiple AWS accounts and enforcement of compliance and security policies. The AWS Organizations and AWS Control Tower solution in option A is a more comprehensive approach to meet all the requirements."
  },
  "85": {
    "question": "A company has its cloud infrastructure on AWS. A solutions architect needs to define the infrastructure as code. The infrastructure is currently deployed in one AWS Region. The company\u2019s business expansion plan includes deployments in multiple Regions across multiple AWS accounts.What should the solutions architect do to meet these requirements?",
    "choices": [
      "A. Use AWS CloudFormation templates. Add IAM policies to control the various accounts, Deploy the templates across the multiple Regions.",
      "B. Use AWS Organizations. Deploy AWS CloudFormation templates from the management account Use AWS Control Tower to manage deployments across accounts.",
      "C. Use AWS Organizations and AWS CloudFormation StackSets. Deploy a Cloud Formation template from an account that has the necessary IAM permissions.",
      "D. Use nested stacks with AWS CloudFormation templates. Change the Region by using nested stacks."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C. Use AWS Organizations and AWS CloudFormation StackSets. Deploy a Cloud Formation template from an account that has the necessary IAM permissions.\n\nThe key reasons why this is the correct choice:\n\n- AWS Organizations allows you to manage multiple AWS accounts from a central location, making it ideal for handling deployments across multiple accounts.\n- CloudFormation StackSets enable you to deploy stacks across multiple accounts and Regions from a single CloudFormation template, which is a perfect fit for the requirement of deploying to multiple Regions.\n- By using an account with the necessary IAM permissions, you can ensure that the CloudFormation template can be deployed across the different accounts and Regions as needed.\n\n2. Explanations of why the other choices are incorrect:\n\nA. Use AWS CloudFormation templates. Add IAM policies to control the various accounts, Deploy the templates across the multiple Regions.\n- This option does not address the requirement of managing deployments across multiple accounts. IAM policies alone cannot provide the centralized management capabilities needed.\n\nB. Use AWS Organizations. Deploy AWS CloudFormation templates from the management account Use AWS Control Tower to manage deployments across accounts.\n- While AWS Organizations and AWS Control Tower can help manage deployments across multiple accounts, the question specifically asks about using CloudFormation, which is not mentioned in this choice.\n\nD. Use nested stacks with AWS CloudFormation templates. Change the Region by using nested stacks.\n- Nested stacks can be used to manage complex infrastructures, but they do not provide the cross-account and cross-Region deployment capabilities required in the question. Nested stacks are more suitable for managing dependencies within a single account and Region."
  },
  "86": {
    "question": "A company operates a fleet of servers on premises and operates a fleet of Amazon EC2 instances in its organization in AWS Organizations. The company's AWS accounts contain hundreds of VPCs. The company wants to connect its AWS accounts to its on-premises network. AWS Site-to-Site VPN connections are already established to a single AWS account. The company wants to control which VPCs can communicate with other VPCs.Which combination of steps will achieve this level of control with the LEAST operational effort? (Choose three.)",
    "choices": [
      "A. Create a transit gateway in an AWS account. Share the transit gateway across accounts by using AWS Resource Access Manager (AWS RAM).",
      "B. Configure attachments to all VPCs and VPNs.",
      "C. Setup transit gateway route tables. Associate the VPCs and VPNs with the route tables.",
      "D. Configure VPC peering between the VPCs.",
      "E. Configure attachments between the VPCs and VPNs.",
      "F. Setup route tables on the VPCs and VPNs."
    ],
    "answer": "ABC",
    "explanation": "1. Explanation of the correct answer (ABC):\n\nA. Create a transit gateway in an AWS account. Share the transit gateway across accounts by using AWS Resource Access Manager (AWS RAM).\nThis is the recommended approach to connect multiple VPCs and on-premises networks in a scalable and manageable way. The transit gateway acts as a central hub, allowing you to control the network traffic between the connected VPCs and VPNs.\n\nC. Setup transit gateway route tables. Associate the VPCs and VPNs with the route tables.\nAfter creating the transit gateway, you need to set up the route tables to control the traffic flow between the connected VPCs and VPNs. This allows you to specify which VPCs can communicate with each other.\n\nB. Configure attachments to all VPCs and VPNs.\nThis is a necessary step to connect the VPCs and VPNs to the transit gateway. By attaching the VPCs and VPNs to the transit gateway, you can enable network connectivity and apply the desired routing control.\n\n2. Explanations of why the incorrect choices are wrong:\n\nD. Configure VPC peering between the VPCs.\nVPC peering is not a feasible option for hundreds of VPCs, as it would require configuring and managing multiple peering connections, which can become operationally intensive.\n\nE. Configure attachments between the VPCs and VPNs.\nThis is an invalid choice, as the attachments should be made between the VPCs/VPNs and the transit gateway, not directly between the VPCs and VPNs.\n\nF. Setup route tables on the VPCs and VPNs.\nWhile route tables are necessary, they should be configured on the transit gateway, not on the individual VPCs and VPNs. The transit gateway route tables allow you to control the communication between the connected VPCs and VPNs."
  },
  "87": {
    "question": "A company has an application that runs on Amazon EC2 instances. A solutions architect is designing VPC infrastructure in an AWS Region where the application needs to access an Amazon Aurora DB Cluster. The EC2 instances are all associated with the same security group. The DB cluster is associated with its own security group.The solutions architect needs to add rules to the security groups to provide the application with least privilege access to the DB Cluster.Which combination of steps will meet these requirements? (Choose two.)",
    "choices": [
      "A. Add an inbound rule to the EC2 instances' security group. Specify the DB cluster's security group as the source over the default Aurora port.",
      "B. Add an outbound rule to the EC2 instances' security group. Specify the DB cluster's security group as the destination over the default Aurora port.",
      "C. Add an inbound rule to the DB cluster's security group. Specify the EC2 instances' security group as the source over the default Aurora port.",
      "D. Add an outbound rule to the DB cluster's security group. Specify the EC2 instances' security group as the destination over the default Aurora port.",
      "E. Add an outbound rule to the DB cluster's security group. Specify the EC2 instances' security group as the destination over the ephemeral ports."
    ],
    "answer": "BC",
    "explanation": "Clear explanation of the correct answer:\n\nThe correct answer is B and C.\n\nB. Add an outbound rule to the EC2 instances' security group. Specify the DB cluster's security group as the destination over the default Aurora port.\nThis allows the EC2 instances to initiate outbound connections to the Aurora DB cluster on the default Aurora port. This is necessary to establish connectivity from the application running on the EC2 instances to the DB cluster.\n\nC. Add an inbound rule to the DB cluster's security group. Specify the EC2 instances' security group as the source over the default Aurora port.\nThis allows the Aurora DB cluster to accept inbound connections from the EC2 instances on the default Aurora port. This is necessary to complete the connectivity between the application and the DB cluster.\n\nExplanations of why each incorrect choice is wrong:\n\nA. Add an inbound rule to the EC2 instances' security group. Specify the DB cluster's security group as the source over the default Aurora port.\nThis would allow incoming connections to the EC2 instances on the default Aurora port, but it would not establish the necessary outbound connectivity from the instances to the DB cluster.\n\nD. Add an outbound rule to the DB cluster's security group. Specify the EC2 instances' security group as the destination over the default Aurora port.\nThis would allow the DB cluster to initiate outbound connections to the EC2 instances on the default Aurora port, but it would not establish the necessary inbound connectivity from the instances to the DB cluster.\n\nE. Add an outbound rule to the DB cluster's security group. Specify the EC2 instances' security group as the destination over the ephemeral ports.\nThis would allow the DB cluster to initiate outbound connections to the EC2 instances on ephemeral ports, but it would not establish the necessary inbound connectivity from the instances to the DB cluster on the default Aurora port."
  },
  "88": {
    "question": "A company is running a workload that consists of thousands of Amazon EC2 instances. The workload is running in a VPC that contains several public subnets and private subnets. The public subnets have a route for 0.0.0.0/0 to an existing internet gateway. The private subnets have a route for 0.0.0.0/0 to an existing NAT gateway.A solutions architect needs to migrate the entire fleet of EC2 instances to use IPv6. The EC2 instances that are in private subnets must not be accessible from the public internet.What should the solutions architect do to meet these requirements?",
    "choices": [
      "A. Update the existing VPC, and associate a custom IPv6 CIDR block with the VPC and all subnets. Update all the VPC route tables, and add a route for ::/0 to the internet gateway.",
      "B. Update the existing VPC, and associate an Amazon-provided IPv6 CIDR block with the VPC and all subnets. Update the VPC route tables for all private subnets, and add a route for ::/0 to the NAT gateway.",
      "C. Update the existing VPC, and associate an Amazon-provided IPv6 CIDR block with the VPC and all subnets. Create an egress-only internet gateway. Update the VPC route tables for all private subnets, and add a route for ::/0 to the egress-only internet gateway.",
      "D. Update the existing VPC, and associate a custom IPV6 CIDR block with the VPC and all subnets. Create a new NAT gateway, and enable IPV6 support. Update the VPC route tables for all private subnets, and add a route for ::/0 to the IPv6-enabled NAT gateway."
    ],
    "answer": "C",
    "explanation": "1. Clear explanation of why the correct answer (C) is right:\n\nThe correct answer is C because it meets the requirements specified in the question:\n- Associating an Amazon-provided IPv6 CIDR block with the VPC and all subnets allows the EC2 instances to use IPv6 without additional configuration.\n- Creating an egress-only internet gateway allows traffic from the private subnets to exit the VPC to the public internet, but prevents incoming traffic (ingress) from reaching the private subnets. This ensures that the EC2 instances in the private subnets are not accessible from the public internet.\n- Updating the VPC route tables for all private subnets and adding a route for ::/0 to the egress-only internet gateway ensures that IPv6 traffic from the private subnets can exit the VPC through the egress-only internet gateway.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nA. This option uses a custom IPv6 CIDR block instead of the Amazon-provided one, which is unnecessary. Additionally, it uses an internet gateway for the private subnets, which would make the EC2 instances in those subnets accessible from the public internet, contradicting the requirement.\n\nB. This option uses the Amazon-provided IPv6 CIDR block correctly, but it uses a NAT gateway for the private subnets, which would also make the EC2 instances in those subnets accessible from the public internet, contradicting the requirement.\n\nD. This option uses a custom IPv6 CIDR block instead of the Amazon-provided one, which is unnecessary. While it enables IPv6 support for the NAT gateway, it still does not prevent the EC2 instances in the private subnets from being accessible from the public internet."
  },
  "89": {
    "question": "A company is using AWS CloudFormation to deploy its infrastructure. The company is concerned that, if a production CloudFormation stack is deleted, important data stored in Amazon RDS databases or Amazon EBS volumes might also be deleted.How can the company prevent users from accidentally deleting data in this way?",
    "choices": [
      "A. Modify the CloudFormation templates to add a DeletionPolicy attribute to RDS and EBS resources.",
      "B. Configure a stack policy that disallows the deletion of RDS and EBS resources.",
      "C. Modify IAM policies lo deny deleting RDS and EBS resources that are tagged with an \"aws:cloudformation:stack-name\" tag.",
      "D. Use AWS Config rules to prevent deleting RDS and EBS resources."
    ],
    "answer": "A",
    "explanation": "1. Clear explanation of why the correct answer (A) is right:\n\nThe correct answer is A, which is to modify the CloudFormation templates to add a DeletionPolicy attribute to RDS and EBS resources. This is the right approach because the DeletionPolicy attribute allows you to control the behavior of resources when the CloudFormation stack is deleted. By setting the DeletionPolicy to \"Retain\" for the RDS and EBS resources, you can ensure that these important data stores are not accidentally deleted along with the stack. This preserves the data and ensures that it is not lost when the stack is deleted.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nB. Configure a stack policy that disallows the deletion of RDS and EBS resources:\nThis is not the correct approach because stack policies apply to the entire stack, not individual resources. A stack policy cannot be used to selectively protect specific resources within the stack, such as RDS and EBS.\n\nC. Modify IAM policies to deny deleting RDS and EBS resources that are tagged with an \"aws:cloudformation:stack-name\" tag:\nThis approach is not correct because it relies on resource tags, which can be modified or removed by users. The DeletionPolicy attribute is a more robust and reliable way to protect resources within a CloudFormation stack.\n\nD. Use AWS Config rules to prevent deleting RDS and EBS resources:\nThis approach is not correct because AWS Config rules are designed to monitor and assess the compliance of your resources, not to actively prevent specific actions from being taken. The DeletionPolicy attribute is a more direct and effective way to protect resources within a CloudFormation stack."
  },
  "90": {
    "question": "A company has deployed applications to thousands of Amazon EC2 instances in an AWS account. A security audit discovers that several unencrypted Amazon Elastic Block Store (Amazon EBS) volumes are attached to the EC2 instances. The company\u2019s security policy requires the EBS volumes to be encrypted.The company needs to implement an automated solution to encrypt the EBS volumes. The solution also must prevent development teams from creating unencrypted EBS volumes.Which solution will meet these requirements?",
    "choices": [
      "A. Configure the AWS Config managed rule that identifies unencrypted EBS volumes. Configure an automatic remediation action. Associate an AWS Systems Manager Automation runbook that includes the steps to create a new encrypted EBS volume. Create an AWS Key Management Service (AWS KMS) customer managed key. In the key policy, include a statement to deny the creation of unencrypted EBS volumes.",
      "B. Use AWS Systems Manager Fleet Manager to create a list of unencrypted EBS volumes, Create a Systems Manager Automation runbook that includes the steps to create a new encrypted EBS volume. Create an SCP to deny the creation of unencrypted EBS volumes.",
      "C. Use AWS Systems Manager Fleet Manager to create a list of unencrypted EBS volumes. Create a Systems Manager Automation runbook that includes the steps to create a new encrypted EBS volume. Modify the AWS account setting for EBS encryption to always encrypt new EBS volumes.",
      "D. Configure the AWS Config managed rule that identifies unencrypted EBS volumes. Configure an automatic remediation action. Associate an AWS Systems Manager Automation runbook that includes the steps to create a new encrypted EBS volume. Modify the AWS account setting for EBS encryption to always encrypt new EBS volumes."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (Option D):\n\nOption D is the correct answer because it meets the requirements of the scenario:\n\na) It configures the AWS Config managed rule to identify unencrypted EBS volumes.\nb) It configures an automatic remediation action to create a new encrypted EBS volume and replace the old one.\nc) It associates an AWS Systems Manager Automation runbook to perform the steps for creating the new encrypted EBS volume.\nd) It modifies the AWS account setting for EBS encryption to always encrypt new EBS volumes, which prevents the creation of unencrypted EBS volumes in the future.\n\nThis comprehensive approach addresses both the existing unencrypted volumes and the prevention of new unencrypted volumes, fulfilling the company's security policy requirements.\n\n2. Explanations of why the incorrect choices are wrong:\n\na) Option A is incorrect because while it addresses the issue of existing unencrypted volumes, it does not prevent the creation of new unencrypted EBS volumes. The key policy modification to deny the creation of unencrypted EBS volumes is not a viable solution, as it cannot be enforced at the account level.\n\nb) Option B is incorrect because it uses AWS Systems Manager Fleet Manager to create a list of unencrypted EBS volumes, which is not the appropriate tool for identifying unencrypted volumes. Additionally, it does not mention modifying the AWS account setting to always encrypt new EBS volumes, which is a crucial requirement.\n\nc) Option C is incorrect because it uses AWS Systems Manager Fleet Manager to create a list of unencrypted EBS volumes, which is not the appropriate tool. While it mentions modifying the AWS account setting to always encrypt new EBS volumes, it does not include the necessary steps to remediate the existing unencrypted volumes."
  },
  "91": {
    "question": "A company used Amazon EC2 instances to deploy a web fleet to host a blog site. The EC2 instances are behind an Application Load Balancer (ALB) and are configured in an Auto Scaling group. The web application stores all blog content on an Amazon EFS volume.The company recently added a feature for bloggers to add video to their posts, attracting 10 times the previous user traffic. At peak times of day, users report buffering and timeout issues while attempting to reach the site or watch videos.Which is the MOST cost-efficient and scalable deployment that will resolve the issues for users?",
    "choices": [
      "A. Reconfigure Amazon EFS to enable maximum I/O.",
      "B. Update the blog site to use instance store volumes for storage. Copy the site contents to the volumes at launch and to Amazon S3 at shutdown.",
      "C. Configure an Amazon CloudFront distribution. Point the distribution to an S3 bucket, and migrate the videos from EFS to Amazon S3.",
      "D. Set up an Amazon CloudFront distribution for all site contents, and point the distribution at the ALB."
    ],
    "answer": "C",
    "explanation": "1. Clear explanation of why the correct answer (C) is right:\n\nThe correct answer is C, which is to configure an Amazon CloudFront distribution and point it to an S3 bucket, while migrating the videos from the Amazon EFS volume to Amazon S3.\n\nThis is the most cost-efficient and scalable deployment to resolve the buffering and timeout issues experienced by users. Here's why:\n\n- Amazon CloudFront is a Content Delivery Network (CDN) that can cache and serve the video content from edge locations closer to the users, reducing latency and improving the user experience.\n- Migrating the video content from the Amazon EFS volume to Amazon S3 provides a more scalable and cost-effective storage solution. S3 is designed for high-performance, scalable, and durable object storage, making it well-suited for handling the increased video traffic.\n- By offloading the video content to CloudFront and S3, the load on the Amazon EFS volume and the EC2 instances behind the Application Load Balancer (ALB) is reduced, allowing them to focus on serving the blog content more efficiently.\n\n2. Brief explanations of why the incorrect choices are wrong:\n\nA. Reconfigure Amazon EFS to enable maximum I/O: This option may not resolve the issues, as increasing the IOPS of the EFS volume could negatively impact the latency, and it's not a cost-efficient solution for handling the increased video traffic.\n\nB. Update the blog site to use instance store volumes for storage: This option is not the most cost-efficient or scalable solution. Instance store volumes are more expensive and have limited storage capacity, and the need to copy the site contents to S3 at shutdown adds complexity and potential for data loss.\n\nD. Set up an Amazon CloudFront distribution for all site contents and point the distribution at the ALB: This option provides performance improvements, but it's not as cost-efficient as option C. Serving all site content, including the blog pages, through CloudFront would incur higher costs compared to serving only the video content through the CDN."
  },
  "92": {
    "question": "A company is running a workload that consists of thousands of Amazon EC2 instances. The workload is running in a VPC that contains several public subnets and private subnets. The public subnets have a route for 0.0.0.0/0 to an existing internet gateway. The private subnets have a route for 0.0.0.0/0 to an existing NAT gateway.A solutions architect needs to migrate the entire fleet of EC2 instances to use IPv6. The EC2 instances that are in private subnets must not be accessible from the public internet.What should the solutions architect do to meet these requirements?",
    "choices": [
      "A. Update the existing VPC, and associate a custom IPv6 CIDR block with the VPC and all subnets. Update all the VPC route tables, and add a route for ::/0 to the internet gateway.",
      "B. Update the existing VPC, and associate an Amazon-provided IPv6 CIDR block with the VPC and all subnets. Update the VPC route tables for all private subnets, and add a route for ::/0 to the NAT gateway.",
      "C. Update the existing VPC, and associate an Amazon-provided IPv6 CIDR block with the VPC and all subnets. Create an egress-only internet gateway. Update the VPC route tables for all private subnets, and add a route for ::/0 to the egress-only internet gateway.",
      "D. Update the existing VPC, and associate a custom IPV6 CIDR block with the VPC and all subnets. Create a new NAT gateway, and enable IPV6 support. Update the VPC route tables for all private subnets, and add a route for ::/0 to the IPv6-enabled NAT gateway."
    ],
    "answer": "C",
    "explanation": "1. Clear explanation of why the correct answer (C) is right:\n\nThe correct answer is C because it outlines the necessary steps to migrate the EC2 instances to use IPv6 while ensuring the instances in the private subnets are not accessible from the public internet.\n\nKey points:\n- Associating an Amazon-provided IPv6 CIDR block with the VPC and all subnets allows the EC2 instances to use IPv6 without additional configuration.\n- Creating an egress-only internet gateway allows outbound traffic from the VPC to the public internet, but blocks inbound traffic, ensuring the private subnet instances are not accessible from the public internet.\n- Updating the VPC route tables for the private subnets to route IPv6 traffic (::/0) to the egress-only internet gateway allows the instances to access the public internet while maintaining the required isolation.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nA. This choice suggests using a custom IPv6 CIDR block and routing IPv6 traffic (::/0) to the internet gateway. This would make the private subnet instances accessible from the public internet, which is not the requirement.\n\nB. This choice suggests using an Amazon-provided IPv6 CIDR block and routing IPv6 traffic (::/0) to the NAT gateway. While this would allow outbound internet access, it does not prevent inbound traffic from reaching the private subnet instances.\n\nD. This choice suggests using a custom IPv6 CIDR block and an IPv6-enabled NAT gateway. While this could provide the required functionality, it is more complex than the egress-only internet gateway solution in choice C, which is the more straightforward and recommended approach."
  },
  "93": {
    "question": "A finance company is running its business-critical application on current-generation Linux EC2 instances. The application includes a self-managed MySQL database performing heavy I/O operations. The application is working fine to handle a moderate amount of traffic during the month. However, it slows down during the final three days of each month due to month-end reporting, even though the company is using Elastic Load Balancers and Auto Scaling within its infrastructure to meet the increased demand.Which of the following actions would allow the database to handle the month-end load with the LEAST impact on performance?",
    "choices": [
      "A. Pre-warming Elastic Load Balancers, using a bigger instance type, changing all Amazon EBS volumes to GP2 volumes.",
      "B. Performing a one-time migration of the database cluster to Amazon RDS, and creating several additional read replicas to handle the load during end of month.",
      "C. Using Amazon CloudWatch with AWS Lambda to change the type, size, or IOPS of Amazon EBS volumes in the cluster based on a specific CloudWatch metric.",
      "D. Replacing all existing Amazon EBS volumes with new PIOPS volumes that have the maximum available storage size and I/O per second by taking snapshots before the end of the month and reverting back afterwards."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B - Performing a one-time migration of the database cluster to Amazon RDS, and creating several additional read replicas to handle the load during end of the month.\n\nThis is the best solution for the following reasons:\n\n- Migrating the database to Amazon RDS will offload the management of the underlying infrastructure, such as software patching, backups, and monitoring, which reduces the operational overhead for the company.\n- RDS provides the ability to easily scale the database by adding read replicas to handle the increased read traffic during the end-of-month reporting. Read replicas can be quickly provisioned and scaled up to handle the spike in traffic.\n- RDS offers built-in high availability and failover capabilities, which can help ensure the database remains highly available during the peak load period.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Pre-warming Elastic Load Balancers, using a bigger instance type, changing all Amazon EBS volumes to GP2 volumes.\n- This solution may help to some extent, but it is unlikely to be sufficient to handle the heavy I/O load during the end-of-month reporting. Upgrading the instance type and EBS volumes alone may not provide enough performance improvement to handle the spike in traffic.\n\nC. Using Amazon CloudWatch with AWS Lambda to change the type, size, or IOPS of Amazon EBS volumes in the cluster based on a specific CloudWatch metric.\n- This solution is too complex and may not be able to react quickly enough to the sudden surge in traffic. Manually resizing the EBS volumes based on CloudWatch metrics may not be a scalable or reliable solution for handling the end-of-month load.\n\nD. Replacing all existing Amazon EBS volumes with new PIOPS volumes that have the maximum available storage size and I/O per second by taking snapshots before the end of the month and reverting back afterwards.\n- This solution is not the most efficient or cost-effective way to handle the end-of-month load. Provisioning the maximum available storage and IOPS for the EBS volumes may be overkill and can be expensive. Additionally, the need to take snapshots and revert the volumes back may introduce additional complexity and potential downtime."
  },
  "94": {
    "question": "A company has mounted sensors to collect information about environmental parameters such as humidity and light throughout all the company's factories. The company needs to stream and analyze the data in the AWS Cloud in real time. If any of the parameters fall out of acceptable ranges, the factory operations team must receive a notification immediately.Which solution will meet these requirements?",
    "choices": [
      "A. Stream the data to an Amazon Kinesis Data Firehose delivery stream. Use AWS Step Functions to consume and analyze the data in the Kinesis Data Firehose delivery stream. Use Amazon Simple Notification Service (Amazon SNS) to notify the operations team.",
      "B. Stream the data to an Amazon Managed Streaming for Apache Kafka (Amazon MSK) cluster. Set up a trigger in Amazon MSK to invoke an AWS Fargate task to analyze the data. Use Amazon Simple Email Service (Amazon SES) to notify the operations team.",
      "C. Stream the data to an Amazon Kinesis data stream. Create an AWS Lambda function to consume the Kinesis data stream and to analyze the data. Use Amazon Simple Notification Service (Amazon SNS) to notify the operations team.",
      "D. Stream the data to an Amazon Kinesis Data Analytics application. Use an automatically scaled and containerized service in Amazon Elastic Container Service (Amazon ECS) to consume and analyze the data. Use Amazon Simple Email Service (Amazon SES) to notify the operations team."
    ],
    "answer": "C",
    "explanation": "1. Clear explanation of why the correct answer (C) is right:\n\nThe correct answer is C because it best meets all the requirements mentioned in the question:\n\n- Streaming the data to an Amazon Kinesis data stream provides real-time data ingestion, which is necessary for the requirement of \"stream and analyze the data in the AWS Cloud in real time.\"\n- Creating an AWS Lambda function to consume the Kinesis data stream and analyze the data allows for real-time processing and monitoring of the environmental parameters.\n- Using Amazon Simple Notification Service (Amazon SNS) to notify the operations team when any parameters fall out of acceptable ranges satisfies the requirement of \"the factory operations team must receive a notification immediately.\"\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nA. This choice uses Amazon Kinesis Data Firehose, which is a near-real-time solution, not a true real-time solution. Additionally, using AWS Step Functions to consume and analyze the data is an overly complex solution when a simpler Lambda function can achieve the same result.\n\nB. This choice uses Amazon Managed Streaming for Apache Kafka (Amazon MSK), which is a valid real-time streaming solution. However, the requirement does not mention the need for a trigger-based solution, and using Amazon SES for notifications is not the correct choice, as SNS is better suited for this use case.\n\nD. This choice uses Amazon Kinesis Data Analytics, which is a valid real-time analytics solution. However, the requirement does not mention the need for an automatically scaled and containerized service in Amazon ECS, and using Amazon SES for notifications is again not the correct choice."
  },
  "95": {
    "question": "A company is refactoring its on-premises order-processing platform in the AWS Cloud. The platform includes a web front end that is hosted on a fleet of VMs, RabbitMQ to connect the front end to the backend, and a Kubernetes cluster to run a containerized backend system to process the orders. The company does not want to make any major changes to the application.Which solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      "A. Create an AMI of the web server VM. Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer. Set up Amazon MQ to replace the on-premises messaging queue. Configure Amazon Elastic Kubernetes Service (Amazon EKS) to host the order-processing backend.",
      "B. Create a custom AWS Lambda runtime to mimic the web server environment. Create an Amazon API Gateway API to replace the front-end web servers. Set up Amazon MQ to replace the on-premises messaging queue. Configure Amazon Elastic Kubernetes Service (Amazon EKS) to host the order-processing backend.",
      "C. Create an AMI of the web server VM. Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer. Set up Amazon MQ to replace the on-premises messaging queue. Install Kubernetes on a fleet of different EC2 instances to host the order-processing backend.",
      "D. Create an AMI of the web server VM. Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer. Set up an Amazon Simple Queue Service (Amazon SQS) queue to replace the on-premises messaging queue. Configure Amazon Elastic Kubernetes Service (Amazon EKS) to host the order-processing backend."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (Option A):\n\nOption A is the correct answer because it provides the least operational overhead for the company while allowing them to lift and shift their existing on-premises order-processing platform to the AWS Cloud with minimal changes.\n\nKey points:\n- Creating an AMI of the web server VM allows the company to launch identical EC2 instances with the same environment as the on-premises web servers, ensuring a smooth transition.\n- Using an EC2 Auto Scaling group with an Application Load Balancer provides automatic scaling and high availability for the web front end, reducing the need for manual infrastructure management.\n- Replacing the on-premises RabbitMQ messaging queue with Amazon MQ, a managed message broker service, eliminates the need to maintain the messaging infrastructure.\n- Configuring Amazon Elastic Kubernetes Service (EKS) to host the order-processing backend allows the company to run their existing Kubernetes cluster in the AWS Cloud without significant changes to the application.\n\n2. Explanations of why the incorrect choices are wrong:\n\nOption B:\n- Creating a custom AWS Lambda runtime and using Amazon API Gateway would require significant changes to the application, which goes against the requirement of \"not wanting to make any major changes to the application\".\n- This approach may not be compatible with the existing codebase and would introduce more operational overhead compared to the lift-and-shift approach in Option A.\n\nOption C:\n- Installing Kubernetes on a fleet of EC2 instances would also require changes to the application and may not be compatible with the existing codebase.\n- While this is a valid approach, it does not provide the same level of managed services and reduced operational overhead as the EKS solution in Option A.\n\nOption D:\n- Using Amazon SQS instead of Amazon MQ may not provide the same level of messaging capabilities required by the order-processing platform.\n- SQS is a basic queue service, while Amazon MQ is a fully managed message broker service that is compatible with the existing RabbitMQ infrastructure, providing a more seamless transition."
  },
  "96": {
    "question": "A solutions architect needs to implement a client-side encryption mechanism for objects that will be stored in a new Amazon S3 bucket. The solutions architect created a CMK that is stored in AWS Key Management Service (AWS KMS) for this purpose.The solutions architect created the following IAM policy and attached it to an IAM role:During tests, the solutions architect was able to successfully get existing test objects in the S3 bucket. However, attempts to upload a new object resulted in an error message. The error message stated that the action was forbidden.Which action must the solutions architect add to the IAM policy to meet all the requirements?",
    "choices": [
      "A. kms:GenerateDataKey",
      "B. kms:GetKeyPolicy",
      "C. kms:GetPublicKey",
      "D. kms:Sign"
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A. kms:GenerateDataKey):\nThe correct answer is A. kms:GenerateDataKey because the solutions architect needs to generate a data key using the CMK stored in AWS KMS for the client-side encryption of objects in the S3 bucket. The IAM policy must grant the necessary permissions to the IAM role to generate the data key, which is used to encrypt the data before it is uploaded to S3. Without the kms:GenerateDataKey permission, the IAM role would not be able to generate the data key, resulting in the \"forbidden\" error when attempting to upload a new object.\n\n2. Explanations of why the incorrect choices are wrong:\nB. kms:GetKeyPolicy: This action allows retrieving the key policy for a CMK, but it is not necessary for the client-side encryption use case. The key policy does not directly impact the ability to generate a data key for encrypting the S3 object.\nC. kms:GetPublicKey: This action allows retrieving the public key of a CMK, which is not relevant for the client-side encryption scenario. Client-side encryption typically uses symmetric encryption with a data key, not asymmetric encryption with a public/private key pair.\nD. kms:Sign: This action allows signing a message using a CMK, but it is not required for the client-side encryption of S3 objects. Signing is not a necessary step in the client-side encryption process."
  },
  "97": {
    "question": "A company has developed a web application. The company is hosting the application on a group of Amazon EC2 instances behind an Application Load Balancer. The company wants to improve the security posture of the application and plans to use AWS WAF web ACLs. The solution must not adversely affect legitimate traffic to the application.How should a solutions architect configure the web ACLs to meet these requirements?",
    "choices": [
      "A. Set the action of the web ACL rules to Count. Enable AWS WAF logging. Analyze the requests for false positives. Modify the rules to avoid any false positive. Over time, change the action of the web ACL rules from Count to Block.",
      "B. Use only rate-based rules in the web ACLs, and set the throttle limit as high as possible. Temporarily block all requests that exceed the limit. Define nested rules to narrow the scope of the rate tracking.",
      "C. Set the action of the web ACL rules to Block. Use only AWS managed rule groups in the web ACLs. Evaluate the rule groups by using Amazon CloudWatch metrics with AWS WAF sampled requests or AWS WAF logs.",
      "D. Use only custom rule groups in the web ACLs, and set the action to Allow. Enable AWS WAF logging. Analyze the requests for false positives. Modify the rules to avoid any false positive. Over time, change the action of the web ACL rules from Allow to Block."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\n\nThe correct answer is A because it outlines the best approach to configure the web ACLs to improve the security posture of the web application without adversely affecting legitimate traffic.\n\nThe key steps are:\n\n1. Set the action of the web ACL rules to \"Count\" mode. This allows the company to monitor the incoming traffic and its behavior without actually blocking any requests. This is crucial to avoid disrupting legitimate traffic while evaluating the effectiveness of the rules.\n\n2. Enable AWS WAF logging to capture the requests that match the web ACL rules. This logging data can be analyzed to identify any false positives (legitimate requests being incorrectly flagged as malicious).\n\n3. Analyze the requests and modify the web ACL rules to avoid any false positives. This iterative process of analyzing the logs and refining the rules is essential to ensure the web ACLs are accurately detecting and mitigating threats without impacting valid traffic.\n\n4. Over time, once the company is satisfied with the accuracy of the web ACL rules, they can gradually change the action from \"Count\" to \"Block\" to start actively blocking malicious traffic.\n\nThis approach allows the company to implement the AWS WAF web ACLs in a controlled and incremental manner, prioritizing the security of the application without disrupting its availability and usability for legitimate users.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. Using only rate-based rules with a high throttle limit can be problematic. While rate-based rules can help mitigate certain types of attacks, such as DDoS, they may not be effective against other types of threats, such as SQL injection or cross-site scripting (XSS) attacks. Additionally, setting a high throttle limit may not provide sufficient protection against malicious traffic.\n\nC. Setting the action of the web ACL rules to \"Block\" from the start, and using only AWS-managed rule groups, can lead to false positives and unnecessarily block legitimate traffic. The AWS-managed rule groups may not be tailored to the specific requirements of the company's web application, and blocking traffic without proper evaluation can adversely affect the application's availability.\n\nD. Using only custom rule groups with the action set to \"Allow\" can create security vulnerabilities. While"
  },
  "98": {
    "question": "A company has an organization that has many AWS accounts in AWS Organizations. A solutions architect must improve how the company manages common security group rules for the AWS accounts in the organization.The company has a common set of IP CIDR ranges in an allow list in each AWS account to allow access to and from the company\u2019s on-premises network. Developers within each account are responsible for adding new IP CIDR ranges to their security groups. The security team has its own AWS account. Currently, the security team notifies the owners of the other AWS accounts when changes are made to the allow list.The solutions architect must design a solution that distributes the common set of CIDR ranges across all accounts.Which solution meets these requirements with the LEAST amount of operational overhead?",
    "choices": [
      "A. Set up an Amazon Simple Notification Service (Amazon SNS) topic in the security team's AWS account. Deploy an AWS Lambda function in each AWS account. Configure the Lambda function to run every time an SNS topic receives a message. Configure the Lambda function to take an IP address as input and add it to a list of security groups in the account. Instruct the security team to distribute changes by publishing messages to its SNS topic.",
      "B. Create new customer-managed prefix lists in each AWS account within the organization. Populate the prefix lists in each account with all internal CIDR ranges. Notify the owner of each AWS account to allow the new customer-managed prefix list IDs in their accounts in their security groups. Instruct the security team to share updates with each AWS account owner.",
      "C. Create a new customer-managed prefix list in the security team\u2019s AWS account. Populate the customer-managed prefix list with all internal CIDR ranges. Share the customer-managed prefix list with the organization by using AWS Resource Access Manager. Notify the owner of each AWS account to allow the new customer-managed prefix list ID in their security groups.",
      "D. Create an IAM role in each account in the organization. Grant permissions to update security groups. Deploy an AWS Lambda function in the security team\u2019s AWS account. Configure the Lambda function to take a list of internal IP addresses as input, assume a role in each organization account, and add the list of IP addresses to the security groups in each account."
    ],
    "answer": "C",
    "explanation": "1. Explanation of why the correct answer (C) is right:\n\nThe correct answer is C, which involves creating a customer-managed prefix list in the security team's AWS account, populating it with the internal CIDR ranges, and then sharing the prefix list with the organization using AWS Resource Access Manager (RAM).\n\nThis solution is the most optimal in terms of operational overhead for the following reasons:\n\n- It centralizes the management of the common set of CIDR ranges in the security team's AWS account, eliminating the need for each individual account to maintain their own version of the allow list.\n- By using a customer-managed prefix list, the security team can easily update the allowed CIDR ranges in a single location, and the changes will be automatically reflected in all the accounts that have access to the prefix list.\n- Sharing the prefix list with the organization using RAM allows the security team to distribute the common set of CIDR ranges to all the accounts, without requiring the security team to manually notify each account owner or deploy any additional infrastructure (such as AWS Lambda functions) in each account.\n- The account owners are responsible for allowing the shared prefix list in their security groups, which is a straightforward and scalable process, reducing the operational overhead for the security team.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This solution requires setting up an Amazon SNS topic in the security team's AWS account and deploying an AWS Lambda function in each AWS account. This increases the operational overhead as the security team needs to manage the SNS topic and ensure the Lambda functions are properly configured in each account.\n\nB. This solution requires creating new customer-managed prefix lists in each AWS account within the organization. This increases the operational overhead as the security team needs to create and maintain multiple prefix lists, and notify each account owner to allow the new prefix list IDs in their security groups.\n\nD. This solution requires creating an IAM role in each account in the organization and deploying an AWS Lambda function in the security team's AWS account. This increases the operational overhead as the security team needs to set up and maintain the IAM roles in each account, as well as the additional complexity of the Lambda function.\n\nIn summary, the correct answer (C) is the most optimal solution as it centralizes the management of the common CIDR ranges, leverages the AWS Resource"
  },
  "99": {
    "question": "A large company is migrating its entire IT portfolio to AWS. Each business unit in the company has a standalone AWS account that supports both development and test environments. New accounts to support production workloads will be needed soon.The finance department requires a centralized method for payment but must maintain visibility into each group's spending to allocate costs.The security team requires a centralized mechanism to control IAM usage in all the company\u2019s accounts.What combination of the following options meets the company\u2019s needs with the LEAST effort? (Choose two.)",
    "choices": [
      "A. Use a collection of parameterized AWS CloudFormation templates defining common IAM permissions that are launched into each account. Require all new and existing accounts to launch the appropriate stacks to enforce the least privilege model.",
      "B. Use AWS Organizations to create a new organization from a chosen payer account and define an organizational unit hierarchy. Invite the existing accounts to join the organization and create new accounts using Organizations.",
      "C. Require each business unit to use its own AWS accounts. Tag each AWS account appropriately and enable Cost Explorer to administer chargebacks.",
      "D. Enable all features of AWS Organizations and establish appropriate service control policies that filter IAM permissions for sub-accounts.",
      "E. Consolidate all of the company's AWS accounts into a single AWS account. Use tags for billing purposes and the IAM\u2019s Access Advisor feature to enforce the least privilege model."
    ],
    "answer": "BD",
    "explanation": "1. Explanation of the correct answer:\n\nThe correct answer is BD - Use AWS Organizations to create a new organization from a chosen payer account and define an organizational unit hierarchy. Enable all features of AWS Organizations and establish appropriate service control policies that filter IAM permissions for sub-accounts.\n\nThis combination of options meets the company's needs with the least effort for the following reasons:\n\na) AWS Organizations provides a centralized mechanism to control IAM usage across all of the company's accounts, meeting the security team's requirement. By establishing service control policies (SCPs), the security team can enforce least-privilege access across all sub-accounts.\n\nb) AWS Organizations also enables cost allocation and visibility into spending for each business unit, allowing the finance department to administer chargebacks effectively.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Using a collection of parameterized CloudFormation templates to enforce IAM permissions does not provide a centralized mechanism for IAM control across all accounts. It requires manual effort to maintain and deploy the templates in each account.\n\nC. Requiring each business unit to use its own AWS accounts and relying on tags and Cost Explorer does not provide a centralized mechanism for IAM control, which is a key requirement from the security team.\n\nE. Consolidating all accounts into a single AWS account and using tags for billing does not address the need for centralized IAM control. The IAM's Access Advisor feature alone is not sufficient to enforce the least-privilege model across multiple business units."
  },
  "100": {
    "question": "A company is running a containerized application in the AWS Cloud. The application is running by using Amazon Elastic Container Service (Amazon ECS) on a set of Amazon EC2 instances. The EC2 instances run in an Auto Scaling group.The company uses Amazon Elastic Container Registry (Amazon ECR) to store its container images. When a new image version is uploaded, the new image version receives a unique tag.The company needs a solution that inspects new image versions for common vulnerabilities and exposures. The solution must automatically delete new image tags that have Critical or High severity findings. The solution also must notify the development team when such a deletion occurs.Which solution meets these requirements?",
    "choices": [
      "A. Configure scan on push on the repository. Use Amazon EventBridge to invoke an AWS Step Functions state machine when a scan is complete for images that have Critical or High severity findings. Use the Step Functions state machine to delete the image tag for those images and to notify the development team through Amazon Simple Notification Service (Amazon SNS).",
      "B. Configure scan on push on the repository. Configure scan results to be pushed to an Amazon Simple Queue Service (Amazon SQS) queue. Invoke an AWS Lambda function when a new message is added to the SQS queue. Use the Lambda function to delete the image tag for images that have Critical or High severity findings. Notify the development team by using Amazon Simple Email Service (Amazon SES).",
      "C. Schedule an AWS Lambda function to start a manual image scan every hour. Configure Amazon EventBridge to invoke another Lambda function when a scan is complete. Use the second Lambda function to delete the image tag for images that have Critical or High severity findings. Notify the development team by using Amazon Simple Notification Service (Amazon SNS).",
      "D. Configure periodic image scan on the repository. Configure scan results to be added to an Amazon Simple Queue Service (Amazon SQS) queue. Invoke an AWS Step Functions state machine when a new message is added to the SQS queue. Use the Step Functions state machine to delete the image tag for images that have Critical or High severity findings. Notify the development team by using Amazon Simple Email Service (Amazon SES)."
    ],
    "answer": "A",
    "explanation": "1. Explanation of why the correct answer (A) is right:\n\nAnswer A is the correct choice because it meets all the requirements specified in the question:\n\n- It configures \"scan on push\" on the ECR repository, which allows scanning new image versions for vulnerabilities as soon as they are pushed.\n- It uses Amazon EventBridge to trigger an AWS Step Functions state machine when a scan is complete and finds critical or high severity findings.\n- The Step Functions state machine is used to automatically delete the image tag for the vulnerable images and notify the development team through Amazon SNS.\n\nThis approach ensures that vulnerable images are automatically detected and removed, and the development team is notified, without any manual intervention.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. This option suggests using Amazon SQS to store the scan results, which is not a recommended approach. Scan results should be processed directly, not put in a queue. Additionally, using Amazon SES for notifications is not the best choice, as Amazon SNS is the more appropriate service for this use case.\n\nC. Scheduling a Lambda function to perform manual scans hourly is not efficient, as it introduces unnecessary delay in detecting vulnerabilities. The question specifically asks for a solution that automatically deletes the vulnerable images, which this option does not provide.\n\nD. This option uses a similar approach to B, but with AWS Step Functions instead of Lambda. However, it still relies on Amazon SQS, which is not the recommended way to handle the scan results. Additionally, using Amazon SES for notifications is not the best choice.\n\nIn summary, Answer A is the correct choice because it fully meets the requirements by using the appropriate AWS services (ECR, EventBridge, Step Functions, SNS) to automatically detect, delete, and notify about vulnerable container images."
  },
  "101": {
    "question": "A company is running applications on AWS in a multi-account environment. The company's sales team and marketing team use separate AWS accounts in AWS Organizations.The sales team stores petabytes of data in an Amazon S3 bucket. The marketing team uses Amazon QuickSight for data visualizations. The marketing team needs access to data that the sates team stores in the S3 bucket. The company has encrypted the S3 bucket with an AWS Key Management Service (AWS KMS) key. The marketing team has already created the IAM service role for QuickSight to provide QuickSight access in the marketing AWS account. The company needs a solution that will provide secure access to the data in the S3 bucket across AWS accounts.Which solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      "A. Create a new S3 bucket in the marketing account. Create an S3 replication rule in the sales account to copy the objects to the new S3 bucket in the marketing account. Update the QuickSight permissions in the marketing account to grant access to the new S3 bucket.",
      "B. Create an SCP to grant access to the S3 bucket to the marketing account. Use AWS Resource Access Manager (AWS RAM) to share the KMS key from the sates account with the marketing account. Update the QuickSight permissions in the marketing account to grant access to the S3 bucket.",
      "C. Update the S3 bucket policy in the marketing account to grant access to the QuickSight role. Create a KMS grant for the encryption key that is used in the S3 bucket. Grant decrypt access to the QuickSight role. Update the QuickSight permissions in the marketing account to grant access to the S3 bucket.",
      "D. Create an IAM role in the sales account and grant access to the S3 bucket. From the marketing account, assume the IAM role in the sales account to access the S3 bucket. Update the QuickSight rote, to create a trust relationship with the new IAM role in the sales account."
    ],
    "answer": "D",
    "explanation": "Here's a clear explanation of the correct answer and brief explanations of why the incorrect choices are wrong:\n\n1. Explanation of the correct answer (D):\n   - The correct answer is D because it provides the least operational overhead by leveraging IAM roles for cross-account access.\n   - The solution involves:\n     1. Creating an IAM role in the sales account and granting it access to the S3 bucket.\n     2. From the marketing account, assuming the IAM role in the sales account to access the S3 bucket.\n     3. Updating the QuickSight role in the marketing account to create a trust relationship with the new IAM role in the sales account.\n   - This approach is the most efficient as it avoids the need to copy data, share KMS keys, or modify the S3 bucket policy, which would increase operational complexity and costs.\n\n2. Explanations of why the incorrect choices are wrong:\n   - A. Creating a new S3 bucket in the marketing account and replicating data is not the least operational overhead, as it would lead to data duplication and increased storage costs.\n   - B. Creating an SCP to grant access and using AWS RAM to share the KMS key is not the least operational overhead, as it does not provide a secure way to share the KMS key between accounts and could also lead to data duplication.\n   - C. Updating the S3 bucket policy in the marketing account is not correct, as the S3 bucket is in the sales account, and the marketing team cannot directly modify the policy on the sales team's bucket.\n\nIn summary, the correct answer (D) is the most efficient solution as it leverages IAM roles for secure cross-account access, without the need for data duplication or additional storage costs, and it works seamlessly with the existing KMS encryption."
  },
  "102": {
    "question": "A company has multiple AWS accounts. The company recently had a security audit that revealed many unencrypted Amazon Elastic Block Store (Amazon EBS) volumes attached to Amazon EC2 instances.A solutions architect must encrypt the unencrypted volumes and ensure that unencrypted volumes will be detected automatically in the future. Additionally, the company wants a solution that can centrally manage multiple AWS accounts with a focus on compliance and security.Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
    "choices": [
      "A. Create an organization in AWS Organizations. Set up AWS Control Tower, and turn on the strongly recommended controls (guardrails). Join all accounts to the organization. Categorize the AWS accounts into OUs.",
      "B. Use the AWS CLI to list all the unencrypted volumes in all the AWS accounts. Run a script to encrypt all the unencrypted volumes in place.",
      "C. Create a snapshot of each unencrypted volume. Create a new encrypted volume from the unencrypted snapshot. Detach the existing volume, and replace it with the encrypted volume.",
      "D. Create an organization in AWS Organizations. Set up AWS Control Tower, and turn on the mandatory controls (guardrails). Join all accounts to the organization. Categorize the AWS accounts into OUs.",
      "E. Turn on AWS CloudTrail. Configure an Amazon EventBridge rule to detect and automatically encrypt unencrypted volumes."
    ],
    "answer": "AC",
    "explanation": "1. Explanation of the correct answer:\n\nA. This option is correct because setting up an AWS Organization with AWS Control Tower allows the solutions architect to centrally manage multiple AWS accounts and enforce security and compliance guardrails across all the accounts. The strongly recommended controls in AWS Control Tower include a control to detect whether Amazon EBS volumes attached to Amazon EC2 instances are encrypted, which meets the requirement of automatically detecting unencrypted volumes in the future.\n\nC. This option is correct because it provides a more efficient and automated way to encrypt the existing unencrypted EBS volumes. By creating snapshots of the unencrypted volumes, creating new encrypted volumes from the snapshots, and then replacing the original unencrypted volumes, the solutions architect can encrypt all the volumes without having to manually encrypt each one.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. This option is incorrect because using the AWS CLI to list and encrypt all the unencrypted volumes manually across multiple accounts is not a scalable or efficient solution, especially for a large number of accounts and volumes.\n\nD. This option is incorrect because the mandatory controls in AWS Control Tower do not include the control to detect whether Amazon EBS volumes are encrypted. The strongly recommended controls are required to meet the requirement of automatically detecting unencrypted volumes.\n\nE. This option is incorrect because while turning on AWS CloudTrail and configuring an Amazon EventBridge rule to detect and automatically encrypt unencrypted volumes is a valid approach, it does not address the requirement of centrally managing multiple AWS accounts with a focus on compliance and security. AWS Control Tower provides a more comprehensive solution for this requirement."
  },
  "103": {
    "question": "A publishing company's design team updates the icons and other static assets that an ecommerce web application uses. The company serves the icons and assets from an Amazon S3 bucket that is hosted in the company's production account. The company also uses a development account that members of the design team can access.After the design team tests the static assets in the development account, the design team needs to load the assets into the S3 bucket in the production account. A solutions architect must provide the design team with access to the production account without exposing other parts of the web application to the risk of unwanted changes.Which combination of steps will meet these requirements? (Choose three.)",
    "choices": [
      "A. In the production account, create a new IAM policy that allows read and write access to the S3 bucket.",
      "B. In the development account, create a new IAM policy that allows read and write access to the S3 bucket.",
      "C. In the production account, create a role Attach the new policy to the role. Define the development account as a trusted entity.",
      "D. In the development account, create a role. Attach the new policy to the role Define the production account as a trusted entity.",
      "E. In the development account, create a group that contains all the IAM users of the design team Attach a different IAM policy to the group to allow the sts:AssumeRole action on the role In the production account.",
      "F. In the development account, create a group that contains all the IAM users of the design team Attach a different IAM policy to the group to allow the sts:AssumeRole action on the role in the development account."
    ],
    "answer": "ACE",
    "explanation": "1. Explanation of the correct answer (A, C, E):\n\nA. Creating a new IAM policy in the production account that allows read and write access to the S3 bucket is correct because it grants the necessary permissions for the design team to upload and update the static assets in the S3 bucket.\n\nC. Creating a role in the production account, attaching the new policy to the role, and defining the development account as a trusted entity is correct because it allows the design team members from the development account to assume the role and access the S3 bucket in the production account, without giving them direct access to the production account. This helps limit the risk of unwanted changes to other parts of the web application.\n\nE. Creating a group in the development account that contains all the IAM users of the design team and attaching a policy that allows the sts:AssumeRole action on the role in the production account is correct because it enables the design team members to assume the role created in the production account, granting them access to the S3 bucket while still maintaining separation of concerns between the development and production environments.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. Creating a new IAM policy in the development account that allows read and write access to the S3 bucket is incorrect because the design team needs to access the S3 bucket in the production account, not the development account.\n\nD. Creating a role in the development account, attaching the new policy to the role, and defining the production account as a trusted entity is incorrect because the design team needs to assume a role in the production account to access the S3 bucket, not create a role in the development account.\n\nF. Creating a group in the development account that contains all the IAM users of the design team and attaching a policy that allows the sts:AssumeRole action on the role in the development account is incorrect because the design team needs to assume a role in the production account to access the S3 bucket, not the development account."
  },
  "104": {
    "question": "A company developed a pilot application by using AWS Elastic Beanstalk and Java. To save costs during development, the company's development team deployed the application into a single-instance environment. Recent tests indicate that the application consumes more CPU than expected. CPU utilization is regularly greater than 85%, which causes some performance bottlenecks.A solutions architect must mitigate the performance issues before the company launches the application to production.Which solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      "A. Create a new Elastic Beanstalk application. Select a load-balanced environment type. Select all Availability Zones. Add a scale-out rule that will run if the maximum CPU utilization is over 85% for 5 minutes.",
      "B. Create a second Elastic Beanstalk environment. Apply the traffic-splitting deployment policy. Specify a percentage of incoming traffic to direct to the new environment in the average CPU utilization is over 85% for 5 minutes.",
      "C. Modify the existing environment\u2019s capacity configuration to use a load-balanced environment type. Select all Availability Zones. Add a scale-out rule that will run if the average CPU utilization is over 85% for 5 minutes.",
      "D. Select the Rebuild environment action with the load balancing option. Select an Availability Zones. Add a scale-out rule that will run if the sum CPU utilization is over 85% for 5 minutes."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C because it directly addresses the performance issue and provides the least operational overhead. By modifying the existing environment's capacity configuration to use a load-balanced environment type, the solution will automatically scale out the application when the CPU utilization exceeds 85% for 5 minutes. This allows the application to handle increased traffic and load without the need to create a new Elastic Beanstalk application or environment, which would require additional configuration and maintenance overhead.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Create a new Elastic Beanstalk application: This is not necessary, as the problem can be solved by modifying the existing environment. Creating a new application would introduce unnecessary complexity and operational overhead.\n\nB. Create a second Elastic Beanstalk environment: This solution would not directly address the performance issue. Traffic splitting is used to deploy a new version of the application, not to scale out the existing environment.\n\nD. Select the Rebuild environment action with the load balancing option: Rebuilding the environment does not allow for changing the environment configuration, such as switching to a load-balanced environment type. The rebuild action is typically used to update the application version or configuration, not to scale the environment.\n\nIn summary, the correct answer (C) is the most efficient and straightforward solution to the performance issue, as it allows the existing environment to be modified to use a load-balanced configuration with automatic scaling, which addresses the problem with the least operational overhead."
  },
  "105": {
    "question": "A finance company is running its business-critical application on current-generation Linux EC2 instances. The application includes a self-managed MySQL database performing heavy I/O operations. The application is working fine to handle a moderate amount of traffic during the month. However, it slows down during the final three days of each month due to month-end reporting, even though the company is using Elastic Load Balancers and Auto Scaling within its infrastructure to meet the increased demand.Which of the following actions would allow the database to handle the month-end load with the LEAST impact on performance?",
    "choices": [
      "A. Pre-warming Elastic Load Balancers, using a bigger instance type, changing all Amazon EBS volumes to GP2 volumes.",
      "B. Performing a one-time migration of the database cluster to Amazon RDS, and creating several additional read replicas to handle the load during end of month.",
      "C. Using Amazon CloudWatch with AWS Lambda to change the type, size, or IOPS of Amazon EBS volumes in the cluster based on a specific CloudWatch metric.",
      "D. Replacing all existing Amazon EBS volumes with new PIOPS volumes that have the maximum available storage size and I/O per second by taking snapshots before the end of the month and reverting back afterwards."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B, performing a one-time migration of the database cluster to Amazon RDS and creating several additional read replicas to handle the load during the end of the month.\n\nThis is the most optimal solution for the given scenario because:\n\n- Migrating the database to Amazon RDS will offload the management and maintenance of the underlying database infrastructure to AWS, reducing the operational overhead for the finance company.\n- Creating additional read replicas on RDS will allow the system to handle the increased read traffic during the end-of-month reporting period. Read replicas can scale out to serve the increased read load without impacting the performance of the primary database.\n- RDS also provides other benefits such as automatic backups, software patching, and monitoring, further reducing the administrative burden on the finance company.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Pre-warming Elastic Load Balancers, using a bigger instance type, changing all Amazon EBS volumes to GP2 volumes.\n   - This solution may help, but it is not the most optimal. Increasing the instance size and switching to GP2 volumes may not be sufficient to handle the heavy I/O load during the end-of-month reporting period.\n\nC. Using Amazon CloudWatch with AWS Lambda to change the type, size, or IOPS of Amazon EBS volumes in the cluster based on a specific CloudWatch metric.\n   - This solution is not the most efficient as it requires additional setup and management of the CloudWatch and Lambda components. It also may not provide the necessary scaling capabilities to handle the sudden increase in load.\n\nD. Replacing all existing Amazon EBS volumes with new PIOPS volumes that have the maximum available storage size and I/O per second by taking snapshots before the end of the month and reverting back afterwards.\n   - This solution is overly complex and may not provide the necessary scaling capabilities. Reverting the changes after the end-of-month reporting period may also lead to unnecessary downtime or data migration.\n\nIn summary, the correct answer (B) is the most optimal solution as it leverages the scalability and management features of Amazon RDS to handle the increased load during the end-of-month reporting period."
  },
  "106": {
    "question": "A company runs a Java application that has complex dependencies on VMs that are in the company's data center. The application is stable. but the company wants to modernize the technology stack. The company wants to migrate the application to AWS and minimize the administrative overhead to maintain the servers.Which solution will meet these requirements with the LEAST code changes?",
    "choices": [
      "A. Migrate the application to Amazon Elastic Container Service (Amazon ECS) on AWS Fargate by using AWS App2Container. Store container images in Amazon Elastic Container Registry (Amazon ECR). Grant the ECS task execution role permission 10 access the ECR image repository. Configure Amazon ECS to use an Application Load Balancer (ALB). Use the ALB to interact with the application.",
      "B. Migrate the application code to a container that runs in AWS Lambda. Build an Amazon API Gateway REST API with Lambda integration. Use API Gateway to interact with the application.",
      "C. Migrate the application to Amazon Elastic Kubernetes Service (Amazon EKS) on EKS managed node groups by using AWS App2Container. Store container images in Amazon Elastic Container Registry (Amazon ECR). Give the EKS nodes permission to access the ECR image repository. Use Amazon API Gateway to interact with the application.",
      "D. Migrate the application code to a container that runs in AWS Lambda. Configure Lambda to use an Application Load Balancer (ALB). Use the ALB to interact with the application."
    ],
    "answer": "A",
    "explanation": "1. Explanation of why the correct answer (A) is right:\n\nThe correct answer is A because it provides the solution that minimizes the code changes required to migrate the existing Java application to AWS while also reducing the administrative overhead to maintain the servers. \n\nBy migrating the application to Amazon Elastic Container Service (Amazon ECS) on AWS Fargate, the company can leverage the benefits of containers without having to manage the underlying infrastructure. AWS Fargate is a serverless compute engine for containers that takes care of provisioning, scaling, and managing the compute resources, which aligns with the requirement to minimize administrative overhead.\n\nUsing AWS App2Container to containerize the application helps automate the migration process, further reducing the need for code changes. Storing the container images in Amazon Elastic Container Registry (Amazon ECR) allows the application to continue using the same dependencies and configurations it currently relies on.\n\nConfiguring the Application Load Balancer (ALB) to interact with the application provides a familiar interface for the users, as the ALB can be set up to use the same endpoints as the existing application.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. This option requires migrating the application code to a container that runs in AWS Lambda, which would require more code changes to the application than the correct answer (A). Additionally, using AWS Lambda and Amazon API Gateway to interact with the application would be a more significant architectural change compared to the ECS on Fargate solution.\n\nC. This option requires migrating the application to Amazon Elastic Kubernetes Service (Amazon EKS), which would introduce more administrative overhead than the ECS on Fargate solution. While EKS can provide more flexibility and control, it also requires managing the Kubernetes cluster and its underlying infrastructure, which goes against the requirement to minimize administrative overhead.\n\nD. This option suggests configuring Lambda to use an Application Load Balancer (ALB), which is not a native feature of Lambda. Instead, Lambda functions are typically accessed through Amazon API Gateway or other event-driven triggers, which would require more code changes than the ECS on Fargate solution."
  },
  "107": {
    "question": "A company has an asynchronous HTTP application that is hosted as an AWS Lambda function. A public Amazon API Gateway endpoint invokes the Lambda function. The Lambda function and the API Gateway endpoint reside in the us-east-1 Region. A solutions architect needs to redesign the application to support failover to another AWS Region.Which solution will meet these requirements?",
    "choices": [
      "A. Create an API Gateway endpoint in the us-west-2 Region to direct traffic to the Lambda function in us-east-1. Configure Amazon Route 53 to use a failover routing policy to route traffic for the two API Gateway endpoints.",
      "B. Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure API Gateway to direct traffic to the SQS queue instead of to the Lambda function. Configure the Lambda function to pull messages from the queue for processing.",
      "C. Deploy the Lambda function to the us-west-2 Region. Create an API Gateway endpoint in us-west-2 10 direct traffic to the Lambda function in us-west-2. Configure AWS Global Accelerator and an Application Load Balancer to manage traffic across the two API Gateway endpoints.",
      "D. Deploy the Lambda function and an API Gateway endpoint to the us-west-2 Region. Configure Amazon Route 53 to use a failover routing policy to route traffic for the two API Gateway endpoints."
    ],
    "answer": "D",
    "explanation": "1. Clear explanation of why the correct answer (D) is right:\n\nThe correct answer is D because it meets the requirement of supporting failover to another AWS Region. By deploying the Lambda function and an API Gateway endpoint to the us-west-2 Region, in addition to the existing setup in us-east-1 Region, the application now has a redundant infrastructure in a different Region. \n\nBy configuring Amazon Route 53 to use a failover routing policy, the traffic will be automatically routed to the healthy endpoint in the event of a failure in one of the Regions. This ensures that the application can continue to serve requests even if one of the Regions experiences an outage or other issues.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nA. This option only creates an additional API Gateway endpoint in us-west-2 and relies on Route 53's failover routing policy to direct traffic. However, it does not deploy the Lambda function to the new Region, making the failover incomplete.\n\nB. This option uses an Amazon SQS queue as a buffer between the API Gateway and the Lambda function. While this can provide some level of decoupling, it does not provide failover to another Region. Additionally, the use of the SQS queue would increase the latency of the system.\n\nC. This option deploys the Lambda function and an API Gateway endpoint to the us-west-2 Region, but it uses AWS Global Accelerator and an Application Load Balancer to manage traffic across the two API Gateway endpoints. This is not a failover solution, as both Regions will be active and serving traffic simultaneously."
  },
  "108": {
    "question": "A retail company has structured its AWS accounts to be part of an organization in AWS Organizations. The company has set up consolidated billing and has mapped its departments to the following OUs: Finance, Sales, Human Resources (HR), Marketing, and Operations. Each OU has multiple AWS accounts, one for each environment within a department. These environments are development, test, pre-production, and production.The HR department is releasing a new system that will launch in 3 months. In preparation, the HR department has purchased several Reserved Instances (RIs) in its production AWS account. The HR department will install the new application on this account. The HR department wants to make sure that other departments cannot share the RI discounts.Which solution will meet these requirements?",
    "choices": [
      "A. In the AWS Billing and Cost Management console for the HR department's production account turn off RI sharing.",
      "B. Remove the HR department's production AWS account from the organization. Add the account 10 the consolidating billing configuration only.",
      "C. In the AWS Billing and Cost Management console. use the organization\u2019s management account 10 turn off RI Sharing for the HR departments production AWS account.",
      "D. Create an SCP in the organization to restrict access to the RIs. Apply the SCP to the OUs of the other departments."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C because AWS Organizations allows you to manage RI discount sharing at the organization level. The organization's management account can control the RI discount sharing settings for all member accounts within the organization. By turning off RI sharing for the HR department's production account in the Billing and Cost Management console of the organization's management account, you can ensure that the RI discounts are not shared with other departments, which meets the given requirement.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Incorrect - Turning off RI sharing in the Billing and Cost Management console of the HR department's production account will not work, as RI discount sharing is controlled at the organization level, not the individual account level.\n\nB. Incorrect - Removing the HR department's production account from the organization and adding it to the consolidated billing configuration only will not prevent other departments from sharing the RI discounts. The RI discount sharing is still managed at the organization level, and the HR department's account would still be part of the consolidated billing.\n\nD. Incorrect - Creating an SCP (Service Control Policy) to restrict access to the RIs and applying it to the OUs of other departments will not work, as the RI discount sharing is managed at the Billing and Cost Management console, not through SCPs. SCPs are used to control the actions that can be performed in the member accounts, but they do not control RI discount sharing."
  },
  "109": {
    "question": "A large company is running a popular web application. The application runs on several Amazon EC2 Linux instances in an Auto Scaling group in a private subnet. An Application Load Balancer is targeting the instances in the Auto Scaling group in the private subnet. AWS Systems Manager Session Manager is configured, and AWS Systems Manager Agent is running on all the EC2 instances.The company recently released a new version of the application. Some EC2 instances are now being marked as unhealthy and are being terminated. As a result, the application is running at reduced capacity. A solutions architect tries to determine the root cause by analyzing Amazon CloudWatch logs that are collected from the application, but the logs are inconclusive.How should the solutions architect gain access to an EC2 instance to troubleshoot the issue?",
    "choices": [
      "A. Suspend the Auto Scaling group\u2019s HealthCheck scaling process. Use Session Manager to log in to an instance that is marked as unhealthy.",
      "B. Enable EC2 instance termination protection. Use Session Manager to log in to an instance that is marked as unhealthy.",
      "C. Set the termination policy to OldestInstance on the Auto Scaling group. Use Session Manager to log in to an instance that is marked an unhealthy.",
      "D. Suspend the Auto Scaling group\u2019s Terminate process. Use Session Manager to log in to an instance that is marked as unhealthy."
    ],
    "answer": "D",
    "explanation": "Explanation:\n\n1. Why the correct answer (D) is right:\nThe correct answer is D. Suspend the Auto Scaling group's Terminate process. Use Session Manager to log in to an instance that is marked as unhealthy.\n\nThis is the right approach because:\n- Suspending the Terminate process in the Auto Scaling group will prevent the unhealthy instances from being terminated automatically. This will allow the solutions architect to access the instances and troubleshoot the issue.\n- By using AWS Systems Manager Session Manager, the solutions architect can securely log in to the EC2 instances that are marked as unhealthy, without needing to manage any SSH keys or open up ports on the instances.\n\n2. Why the incorrect choices are wrong:\n\nA. Suspend the Auto Scaling group's HealthCheck scaling process. Use Session Manager to log in to an instance that is marked as unhealthy.\nThis is not the correct approach because suspending the HealthCheck scaling process would prevent the Auto Scaling group from detecting and terminating unhealthy instances. This would not allow the solutions architect to access the specific instances that are causing the issue.\n\nB. Enable EC2 instance termination protection. Use Session Manager to log in to an instance that is marked as unhealthy.\nEnabling termination protection on the EC2 instances would not prevent the Auto Scaling group from terminating the unhealthy instances. The Auto Scaling group's Terminate process would still terminate the instances marked as unhealthy, even with termination protection enabled.\n\nC. Set the termination policy to OldestInstance on the Auto Scaling group. Use Session Manager to log in to an instance that is marked an unhealthy.\nSetting the termination policy to OldestInstance would not prevent the Auto Scaling group from terminating the unhealthy instances. The instances would still be terminated, and the solutions architect would not be able to access the specific instances causing the issue."
  },
  "110": {
    "question": "A company wants to deploy an AWS WAF solution to manage AWS WAF rules across multiple AWS accounts. The accounts are managed under different OUs in AWS Organizations.Administrators must be able to add or remove accounts or OUs from managed AWS WAF rule sets as needed. Administrators also must have the ability to automatically update and remediate noncompliant AWS WAF rules in all accounts.Which solution meets these requirements with the LEAST amount of operational overhead?",
    "choices": [
      "A. Use AWS Firewall Manager to manage AWS WAF rules across accounts in the organization. Use an AWS Systems Manager Parameter Store parameter to store account numbers and OUs to manage. Update the parameter as needed to add or remove accounts or OUs. Use an Amazon EventBridge rule to identify any changes to the parameter and to invoke an AWS Lambda function to update the security policy in the Firewall Manager administrative account.",
      "B. Deploy an organization-wide AWS Config rule that requires all resources in the selected OUs to associate the AWS WAF rules. Deploy automated remediation actions by using AWS Lambda to fix noncompliant resources. Deploy AWS WAF rules by using an AWS CloudFormation stack set to target the same OUs where the AWS Config rule is applied.",
      "C. Create AWS WAF rules in the management account of the organization. Use AWS Lambda environment variables to store account numbers and OUs to manage. Update environment variables as needed to add or remove accounts or OUs. Create cross-account IAM roles in member accounts. Assume the roles by using AWS Security Token Service (AWS STS) in the Lambda function to create and update AWS WAF rules in the member accounts.",
      "D. Use AWS Control Tower to manage AWS WAF rules across accounts in the organization. Use AWS Key Management Service (AWS KMS) to store account numbers and OUs to manage. Update AWS KMS as needed to add or remove accounts or OUs. Create IAM users in member accounts. Allow AWS Control Tower in the management account to use the access key and secret access key to create and update AWS WAF rules in the member accounts."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\n\nThe correct answer is A because it meets all the requirements of the problem statement with the least amount of operational overhead.\n\nHere's why option A is the best solution:\n\n- It uses AWS Firewall Manager to centrally manage AWS WAF rules across multiple AWS accounts. Firewall Manager provides a single pane of glass to manage and enforce WAF rules consistently across the organization.\n\n- It uses an AWS Systems Manager Parameter Store parameter to store the list of accounts and OUs to manage. This allows easy updating of the managed accounts/OUs as needed, with the changes automatically reflected in the Firewall Manager security policy.\n\n- It uses an Amazon EventBridge rule to detect changes to the parameter store and trigger an AWS Lambda function to update the Firewall Manager security policy accordingly. This provides an automated way to keep the security policy up-to-date without manual intervention.\n\n- This solution minimizes operational overhead by automating the process of updating the managed accounts/OUs and remediating noncompliant WAF rules across all accounts.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. This option uses AWS Config to manage the WAF rules, but it does not provide a way to easily add or remove accounts/OUs from the managed scope. The AWS CloudFormation stack set approach also introduces more operational overhead compared to the Firewall Manager solution.\n\nC. This option requires manual setup of cross-account IAM roles and assumes-role calls in the Lambda function, which increases the operational overhead compared to the Firewall Manager solution.\n\nD. This option uses AWS Control Tower, which is a higher-level service that may not provide the same level of granular control and automation as the Firewall Manager solution."
  },
  "111": {
    "question": "A solutions architect is auditing the security setup or an AWS Lambda function for a company. The Lambda function retrieves, the latest changes from an Amazon Aurora database. The Lambda function and the database run in the same VPC. Lambda environment variables are providing the database credentials to the Lambda function.The Lambda function aggregates data and makes the data available in an Amazon S3 bucket that is configured for server-side encryption with AWS KMS managed encryption keys (SSE-KMS). The data must not travel across the Internet. If any database credentials become compromised, the company needs a solution that minimizes the impact of the compromise.What should the solutions architect recommend to meet these requirements?",
    "choices": [
      "A. Enable IAM database authentication on the Aurora DB cluster. Change the IAM role for the Lambda function to allow the function to access the database by using IAM database authentication. Deploy a gateway VPC endpoint for Amazon S3 in the VPC.",
      "B. Enable IAM database authentication on the Aurora DB cluster. Change the IAM role for the Lambda function to allow the function to access the database by using IAM database authentication. Enforce HTTPS on the connection to Amazon S3 during data transfers.",
      "C. Save the database credentials in AWS Systems Manager Parameter Store. Set up password rotation on the credentials in Parameter Store. Change the IAM role for the Lambda function to allow the function to access Parameter Store. Modify the Lambda function to retrieve the credentials from Parameter Store. Deploy a gateway VPC endpoint for Amazon S3 in the VPC.",
      "D. Save the database credentials in AWS Secrets Manager. Set up password rotation on the credentials in Secrets Manager. Change the IAM role for the Lambda function to allow the function to access Secrets Manager. Modify the Lambda function to retrieve the credentials from Secrets Manager. Enforce HTTPS on the connection to Amazon S3 during data transfers."
    ],
    "answer": "A",
    "explanation": "1. Explanation of why the correct answer (A) is right:\n\nThe correct answer (A) is the best solution to meet the given requirements:\n\n- It enables IAM database authentication on the Aurora DB cluster, which allows the Lambda function to access the database using IAM credentials instead of storing the database credentials in the Lambda environment variables. This minimizes the impact if the database credentials become compromised, as the IAM credentials can be revoked or changed without disrupting the Lambda function.\n- It changes the IAM role for the Lambda function to allow it to access the database using IAM database authentication, ensuring secure access to the database.\n- It deploys a gateway VPC endpoint for Amazon S3 in the VPC, which allows the data to be transferred between the Lambda function and the S3 bucket without traveling across the internet, meeting the requirement that the data must not travel across the internet.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. This option also enables IAM database authentication and changes the IAM role for the Lambda function, but it does not deploy a gateway VPC endpoint for Amazon S3. This means that the data would still travel across the internet, which does not meet the requirement.\n\nC. This option saves the database credentials in AWS Systems Manager Parameter Store and sets up password rotation, which is a good practice. However, it does not deploy a gateway VPC endpoint for Amazon S3, which means the data would still travel across the internet, not meeting the requirement.\n\nD. This option saves the database credentials in AWS Secrets Manager and sets up password rotation, which is also a good practice. However, it does not deploy a gateway VPC endpoint for Amazon S3, which means the data would still travel across the internet, not meeting the requirement."
  },
  "112": {
    "question": "A large mobile gaming company has successfully migrated all of its on-premises infrastructure to the AWS Cloud. A solutions architect is reviewing the environment to ensure that it was built according to the design and that it is running in alignment with the Well-Architected Framework.While reviewing previous monthly costs in Cost Explorer, the solutions architect notices that the creation and subsequent termination of several large instance types account for a high proportion of the costs. The solutions architect finds out that the company\u2019s developers are launching new Amazon EC2 instances as part of their testing and that the developers are not using the appropriate instance types.The solutions architect must implement a control mechanism to limit the instance types that only the developers can launch.Which solution will meet these requirements?",
    "choices": [
      "A. Create a desired-instance-type managed rule in AWS Config. Configure the rule with the instance types that are allowed. Attach the rule to an event to run each time a new EC2 instance is launched.",
      "B. In the EC2 console, create a launch template that specifies the instance types that are allowed. Assign the launch template to the developers\u2019 IAM accounts.",
      "C. Create a new IAM policy. Specify the instance types that are allowed. Attach the policy to an IAM group that contains the IAM accounts for the developers",
      "D. Use EC2 Image Builder to create an image pipeline for the developers and assist them in the creation of a golden image."
    ],
    "answer": "C",
    "explanation": "1. Clear explanation of why the correct answer (C) is right:\n\nThe correct answer is C because creating a new IAM policy that specifies the allowed instance types and attaching it to an IAM group containing the developers' IAM accounts is the most direct and effective way to limit the instance types that the developers can launch. This approach ensures that the developers can only launch the approved instance types, preventing them from launching large and expensive instances that are driving up costs. The IAM policy will be enforced across all AWS services and interfaces, including the AWS SDK, AWS CLI, and the EC2 console, providing a comprehensive control mechanism.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nA. Creating a desired-instance-type managed rule in AWS Config is not sufficient because it only identifies when an instance is launched with an unauthorized type, but does not prevent it. The developers would still be able to launch the unapproved instances.\n\nB. Creating a launch template that specifies the allowed instance types is not a comprehensive solution because it only limits the instances that can be launched through the EC2 console, but does not prevent the developers from launching instances using the AWS SDK, AWS CLI, or other AWS services.\n\nD. Using EC2 Image Builder to create an image pipeline and a golden image for the developers is not a direct solution to the problem of limiting the instance types. While it can help standardize the developers' environment, it does not provide the necessary control mechanism to restrict the instance types that can be launched."
  },
  "113": {
    "question": "A company is developing and hosting several projects in the AWS Cloud. The projects are developed across multiple AWS accounts under the same organization in AWS Organizations. The company requires the cost for cloud infrastructure to be allocated to the owning project. The team responsible for all of the AWS accounts has discovered that several Amazon EC2 instances are lacking the Project tag used for cost allocation.Which actions should a solutions architect lake to resolve the problem and prevent it from happening in the future? (Choose three.)",
    "choices": [
      "A. Create an AWS Config rule in each account to find resources with missing tags.",
      "B. Create an SCP in the organization with a deny action for ec2:RunInstances if the Project tag is missing.",
      "C. Use Amazon Inspector in the organization to find resources with missing tags.",
      "D. Create an IAM policy in each account with a deny action for ec2:RunInstances if the Project tag is missing.",
      "E. Create an AWS Config aggregator for the organization to collect a list of EC2 instances with the missing Project tag.",
      "F. Use AWS Security Hub to aggregate a list of EC2 instances with the missing Project tag."
    ],
    "answer": "ABE",
    "explanation": "1. Explanation of the correct answer:\n\nA. Create an AWS Config rule in each account to find resources with missing tags.\nThis is correct because creating an AWS Config rule in each account allows you to continuously monitor and detect resources that are missing the required Project tag. The Config rule can be configured to check for the presence of the Project tag and trigger actions when the tag is missing.\n\nB. Create an SCP in the organization with a deny action for ec2:RunInstances if the Project tag is missing.\nThis is also correct because creating an SCP (Service Control Policy) in the AWS Organization can be used to prevent the creation of EC2 instances without the required Project tag. The SCP can be configured to deny the ec2:RunInstances action if the Project tag is not present, effectively enforcing the tagging requirement across the organization.\n\nE. Create an AWS Config aggregator for the organization to collect a list of EC2 instances with the missing Project tag.\nThis is correct because the AWS Config aggregator allows you to consolidate and view the findings from the Config rules across multiple accounts within the organization. By creating an aggregator, you can get a centralized view of all EC2 instances missing the Project tag, making it easier to identify and address the issue.\n\n2. Explanations of why the incorrect choices are wrong:\n\nC. Use Amazon Inspector in the organization to find resources with missing tags.\nAmazon Inspector is primarily used for security and vulnerability assessments, not for tag compliance monitoring. It would not be the most appropriate tool to find resources with missing tags.\n\nD. Create an IAM policy in each account with a deny action for ec2:RunInstances if the Project tag is missing.\nWhile an IAM policy could be used to enforce the tagging requirement, it would need to be applied to each individual account, which can be more cumbersome to manage compared to using an SCP (choice B) at the organization level.\n\nF. Use AWS Security Hub to aggregate a list of EC2 instances with the missing Project tag.\nAWS Security Hub is a security and compliance monitoring service, and it is not primarily designed for tag compliance monitoring. While it could potentially be used to aggregate security findings, it would not be the most appropriate tool for the specific problem of finding EC2 instances with missing tags."
  },
  "114": {
    "question": "A company has an on-premises monitoring solution using a PostgreSQL database for persistence of events. The database is unable to scale due to heavy ingestion and it frequently runs out of storage.The company wants to create a hybrid solution and has already set up a VPN connection between its network and AWS. The solution should include the following attributes:\u2022\tManaged AWS services to minimize operational complexity.\u2022\tA buffer that automatically scales to match the throughput of data and requires no ongoing administration.\u2022\tA visualization tool to create dashboards to observe events in near-real time.\u2022\tSupport for semi-structured JSON data and dynamic schemas.Which combination of components will enable the company to create a monitoring solution that will satisfy these requirements? (Choose two.)",
    "choices": [
      "A. Use Amazon Kinesis Data Firehose to buffer events. Create an AWS Lambda function to process and transform events.",
      "B. Create an Amazon Kinesis data stream to buffer events. Create an AWS Lambda function to process and transform events.",
      "C. Configure an Amazon Aurora PostgreSQL DB cluster to receive events. Use Amazon QuickSight to read from the database and create near-real-time visualizations and dashboards.",
      "D. Configure Amazon Elasticsearch Service (Amazon ES) to receive events. Use the Kibana endpoint deployed with Amazon ES to create near-real-time visualizations and dashboards.",
      "E. Configure an Amazon Neptune DB instance to receive events. Use Amazon QuickSight to read from the database and create near-real-time visualizations and dashboards."
    ],
    "answer": "AD",
    "explanation": "1. Explanation of the correct answer (BD):\n\nThe correct answer is B (Create an Amazon Kinesis data stream to buffer events. Create an AWS Lambda function to process and transform events.) and D (Configure Amazon Elasticsearch Service (Amazon ES) to receive events. Use the Kibana endpoint deployed with Amazon ES to create near-real-time visualizations and dashboards.).\n\nThis combination of services satisfies the given requirements:\n\n- Amazon Kinesis Data Streams (B) provide a scalable and durable buffer to handle the high throughput of data, without the need for ongoing administration.\n- AWS Lambda (B) can be used to process and transform the semi-structured JSON data with dynamic schemas, before sending it to the next component.\n- Amazon Elasticsearch Service (D) can ingest and store the transformed events, supporting the semi-structured JSON data and dynamic schemas.\n- Kibana (D), the visualization tool deployed with Amazon ES, can be used to create near-real-time dashboards and visualizations.\n- All these services are managed AWS services, minimizing the operational complexity for the company.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Using Amazon Kinesis Data Firehose instead of Kinesis Data Streams is not the best choice, as Firehose has a limit on the maximum timeout for AWS Lambda functions, which may not be suitable for complex data transformations.\n\nC. Using Amazon Aurora PostgreSQL DB cluster is not the best choice, as it does not provide the same level of scalability and flexibility in handling semi-structured JSON data and dynamic schemas, compared to Amazon ES.\n\nE. Using Amazon Neptune DB instance is not the best choice, as it is primarily designed for graph-based data models, and may not be the most suitable option for the company's monitoring use case, which requires flexibility in handling semi-structured data."
  },
  "115": {
    "question": "A team collects and routes behavioral data for an entire company. The company runs a Multi-AZ VPC environment with public subnets, private subnets, and in internet gateway. Each public subnet also contains a NAT gateway. Most of the company\u2019s applications read from and write to Amazon Kinesis Data Streams. Most of the workloads run in private subnets.A solutions architect must review the infrastructure. The solution architect needs to reduce costs and maintain the function of the applications. The solutions architect uses Cost Explorer and notices that the cost in the EC2-Other category is consistently high. A further review shows that NatGateway-Bytes charges are increasing the cost in the EC2-Other category.What should the solutions architect do to meet these requirements?",
    "choices": [
      "A. Enable VPC Flow Logs. Use Amazon Athena to analyze the logs for traffic that can be removed. Ensure that security groups are blocking traffic that is responsible for high costs.",
      "B. Add an interface VPC endpoint for Kinesis Data Streams to the VPC. Ensure that applications have the correct IAM permissions to use the interface VPC endpoint.",
      "C. Enable VPC Flow Logs and Amazon Detective. Review Detective findings for traffic that is not related to Kinesis Data Streams. Configure security groups to block that traffic.",
      "D. Add an interface VPC endpoint for Kinesis Data Streams to the VPC. Ensure that the VPC endpoint policy allows traffic from the applications."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (Option D):\n\nThe correct answer is Option D: \"Add an interface VPC endpoint for Kinesis Data Streams to the VPC. Ensure that the VPC endpoint policy allows traffic from the applications.\"\n\nThis is the right solution for the following reasons:\n\n- The key issue identified is the high cost in the EC2-Other category due to NAT Gateway-Bytes charges. By adding an interface VPC endpoint for Kinesis Data Streams, the applications running in the private subnets can access the Kinesis service directly through the VPC endpoint, without going through the NAT gateways. This eliminates the data transfer costs associated with the NAT gateways.\n\n- To ensure the applications can properly access the Kinesis service through the VPC endpoint, the VPC endpoint policy needs to be configured correctly. The policy can specify the allowed traffic (e.g., HTTPS) and the resources (Kinesis Data Streams) that can be accessed through the endpoint.\n\n- This solution maintains the functionality of the applications, as they can still access Kinesis Data Streams, but at a reduced cost by avoiding the NAT gateway data transfer charges.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. \"Enable VPC Flow Logs. Use Amazon Athena to analyze the logs for traffic that can be removed. Ensure that security groups are blocking traffic that is responsible for high costs.\"\n- Analyzing VPC Flow Logs and adjusting security groups may help identify and block unnecessary traffic, but it does not address the root cause of the high NAT Gateway-Bytes charges.\n\nB. \"Add an interface VPC endpoint for Kinesis Data Streams to the VPC. Ensure that applications have the correct IAM permissions to use the interface VPC endpoint.\"\n- This is similar to the correct answer, but it focuses on IAM permissions rather than the VPC endpoint policy. IAM permissions are likely already in place, as the applications are already interacting with Kinesis. The key aspect is configuring the VPC endpoint policy to allow the desired traffic.\n\nC. \"Enable VPC Flow Logs and Amazon Detective. Review Detective findings for traffic that is not related to Kinesis Data Streams. Configure security groups to block that traffic.\"\n-"
  },
  "116": {
    "question": "A retail company has an on-premises data center in Europe. The company also has a multi-Region AWS presence that includes the eu-west-1 and us-east-1 Regions. The company wants to be able to route network traffic from its on-premises infrastructure into VPCs in either of those Regions. The company also needs to support traffic that is routed directly between VPCs in those Regions. No single points of failure can exist on the network.The company already has created two 1 Gbps AWS Direct Connect connections from its on-premises data center. Each connection goes into a separate Direct Connect location in Europe for high availability. These two locations are named DX-A and DX-B, respectively. Each Region has a single AWS Transit Gateway that is configured to route all inter-VPC traffic within that Region.Which solution will meet these requirements?",
    "choices": [
      "A. Create a private VIF from the DX-A connection into a Direct Connect gateway. Create a private VIF from the DX-B connection into the same Direct Connect gateway for high availability. Associate both the eu-west-1 and us-east-1 transit gateways with the Direct Connect gateway. Peer the transit gateways with each other to support cross-Region routing.",
      "B. Create a transit VIF from the DX-A connection into a Direct Connect gateway. Associate the eu-west-1 transit gateway with this Direct Connect gateway. Create a transit VIF from the DX-8 connection into a separate Direct Connect gateway. Associate the us-east-1 transit gateway with this separate Direct Connect gateway. Peer the Direct Connect gateways with each other to support high availability and cross-Region routing.",
      "C. Create a transit VIF from the DX-A connection into a Direct Connect gateway. Create a transit VIF from the DX-B connection into the same Direct Connect gateway for high availability. Associate both the eu-west-1 and us-east-1 transit gateways with this Direct Connect gateway. Configure the Direct Connect gateway to route traffic between the transit gateways.",
      "D. Create a transit VIF from the DX-A connection into a Direct Connect gateway. Create a transit VIF from the DX-B connection into the same Direct Connect gateway for high availability. Associate both the eu-west-1 and us-east-1 transit gateways with this Direct Connect gateway. Peer the transit gateways with each other to support cross-Region routing."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (D):\nThe correct answer (D) is the best solution because it addresses all the requirements mentioned in the question:\n\n- It creates a transit VIF from both DX-A and DX-B connections into a single Direct Connect gateway for high availability.\n- It associates both the eu-west-1 and us-east-1 transit gateways with the Direct Connect gateway, allowing the on-premises infrastructure to route traffic to VPCs in either Region.\n- It also allows for traffic to be routed directly between VPCs in the two Regions by peering the transit gateways with each other.\n- This solution avoids any single points of failure by leveraging the high availability provided by the two Direct Connect connections.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This solution is incorrect because it uses private VIFs instead of transit VIFs, which would not allow for cross-Region routing. Additionally, peering the transit gateways is not necessary when using a Direct Connect gateway, as the Direct Connect gateway can handle the routing between the associated transit gateways.\n\nB. This solution is incorrect because it creates two separate Direct Connect gateways, which adds unnecessary complexity and introduces a potential single point of failure. Peering the Direct Connect gateways is also not necessary when the transit gateways are associated with the Direct Connect gateway.\n\nC. This solution is incorrect because it suggests configuring the Direct Connect gateway to route traffic between the transit gateways. This is not necessary, as the transit gateways can be directly peered to handle the cross-Region routing."
  },
  "117": {
    "question": "A company is running an application in the AWS Cloud. The company's security team must approve the creation of all new IAM users. When a new IAM user is created, all access for the user must be removed automatically. The security team must then receive a notification to approve the user. The company has a multi-Region AWS CloudTrail trail in the AWS account.Which combination of steps will meet these requirements? (Choose three.)",
    "choices": [
      "A. Create an Amazon EventBridge (Amazon CloudWatch Events) rule. Define a pattern with the detail-type value set to AWS API Call via CloudTrail and an eventName of CreateUser.",
      "B. Configure CloudTrail to send a notification for the CreateUser event to an Amazon Simple Notification Service (Amazon SNS) topic.",
      "C. Invoke a container that runs in Amazon Elastic Container Service (Amazon ECS) with AWS Fargate technology to remove access.",
      "D. Invoke an AWS Step Functions state machine to remove access.",
      "E. Use Amazon Simple Notification Service (Amazon SNS) to notify the security team.",
      "F. Use Amazon Pinpoint to notify the security team."
    ],
    "answer": "ADE",
    "explanation": "1. Explanation of the correct answer:\n\nA. This option is correct because it sets up an Amazon EventBridge (formerly CloudWatch Events) rule that listens for the \"CreateUser\" event that is logged by AWS CloudTrail. This allows the system to detect when a new IAM user is created.\n\nD. This option is correct because it invokes an AWS Step Functions state machine to remove access for the new IAM user. Step Functions can orchestrate a series of actions, such as invoking a Lambda function to revoke the user's permissions.\n\nE. This option is correct because it uses Amazon SNS to notify the security team after the new IAM user's access has been removed. This ensures that the security team is informed and can approve the user.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. This option is incorrect because it only configures CloudTrail to send a notification, but does not actually remove the user's access or notify the security team.\n\nC. This option is incorrect because it invokes a container in Amazon ECS to remove access, but does not mention the use of EventBridge to detect the \"CreateUser\" event or the use of SNS to notify the security team.\n\nF. This option is incorrect because Amazon Pinpoint is a service for creating and managing SMS, email, and push notifications, which is not the requirement stated in the question. The question specifically asks for notifying the security team, which is better suited for Amazon SNS.\n\nIn summary, the correct answer (ADE) sets up a comprehensive solution that detects the creation of a new IAM user, automatically removes the user's access, and then notifies the security team using Amazon SNS, which meets all the requirements stated in the question."
  },
  "118": {
    "question": "A company wants to migrate to AWS. The company wants to use a multi-account structure with centrally managed access to all accounts and applications. The company also wants to keep the traffic on a private network. Multi-factor authentication (MFA) is required at login, and specific roles are assigned to user groups.The company must create separate accounts for development. staging, production, and shared network. The production account and the shared network account must have connectivity to all accounts. The development account and the staging account must have access only to each other.Which combination of steps should a solutions architect take 10 meet these requirements? (Choose three.)",
    "choices": [
      "A. Deploy a landing zone environment by using AWS Control Tower. Enroll accounts and invite existing accounts into the resulting organization in AWS Organizations.",
      "B. Enable AWS Security Hub in all accounts to manage cross-account access. Collect findings through AWS CloudTrail to force MFA login.",
      "C. Create transit gateways and transit gateway VPC attachments in each account. Configure appropriate route tables.",
      "D. Set up and enable AWS IAM Identity Center (AWS Single Sign-On). Create appropriate permission sets with required MFA for existing accounts.",
      "E. Enable AWS Control Tower in all accounts to manage routing between accounts. Collect findings through AWS CloudTrail to force MFA login.",
      "F. Create IAM users and groups. Configure MFA for all users. Set up Amazon Cognoto user pools and Identity pools to manage access to accounts and between accounts."
    ],
    "answer": "ACD",
    "explanation": "1. Explanation of the correct answer:\n\nA. Deploying a landing zone environment using AWS Control Tower and enrolling accounts in an organization in AWS Organizations is the right choice because it allows for a centralized management of access to all accounts and applications. This satisfies the requirement of having a multi-account structure with centrally managed access.\n\nC. Creating transit gateways and transit gateway VPC attachments in each account, and configuring appropriate route tables, is the right choice because it ensures that the traffic is kept on a private network. The transit gateway is created in the shared network account, and VPC attachments are made in each account. This allows for private connectivity between all accounts.\n\nD. Setting up and enabling AWS IAM Identity Center (AWS Single Sign-On) and creating appropriate permission sets with required MFA for existing accounts is the right choice because it allows for multi-factor authentication at login and specific roles to be assigned to user groups. This satisfies the requirement of having MFA at login and specific roles assigned to user groups.\n\n2. Explanation of why the incorrect choices are wrong:\n\nB. Enabling AWS Security Hub in all accounts to manage cross-account access and collecting findings through AWS CloudTrail to force MFA login is not enough to meet the requirement of creating separate accounts for development, staging, production, and shared network. It can be used in addition to the other steps, but not as a standalone solution.\n\nE. Enabling AWS Control Tower in all accounts to manage routing between accounts and collecting findings through AWS CloudTrail to force MFA login is not enough to meet the requirement of creating separate accounts for development, staging, production, and shared network. It can be used in addition to the other steps, but not as a standalone solution.\n\nF. Creating IAM users and groups and configuring MFA for all users and setting up Amazon Cognito user pools and Identity pools to manage access to accounts and between accounts does not address the requirement of creating separate accounts for development, staging, production, and shared network. Additionally, it does not address the requirement of keeping the traffic on a private network."
  },
  "119": {
    "question": "A company runs its application in the eu-west-1 Region and has one account for each of its environments: development, testing, and production. All the environments are running 24 hours a day, 7 days a week by using stateful Amazon EC2 instances and Amazon RDS for MySQL databases. The databases are between 500 GB and 800 GB in size.The development team and testing team work on business days during business hours, but the production environment operates 24 hours a day, 7 days a week. The company wants to reduce costs. All resources are tagged with an environment tag with either development, testing, or production as the key.What should a solutions architect do to reduce costs with the LEAST operational effort?",
    "choices": [
      "A. Create an Amazon EventBridge rule that runs once every day. Configure the rule to invoke one AWS Lambda function that starts or slops instances based on me tag, day, and time.",
      "B. Create an Amazon EventBridge rule that runs every business day in the evening. Configure the rule to invoke an AWS Lambda function that stops instances based on the tag. Create a second EventBridge rule that runs every business day in the morning. Configure the second rule lo invoke another Lambda function that starts instances based on the tag.",
      "C. Create an Amazon EventBridge rule that runs every business day in the evening, Configure the rule to invoke an AWS Lambda function that terminates, instances based on the lag. Create a second EventBridge rule that runs every business day in the morning. Configure the second rule lo invoke another Lambda function that restores the instances from their last backup based on the tag.",
      "D. Create an Amazon EventBridge rule that runs every hour. Configure the rule to invoke one AWS Lambda function that terminates or restores instances from their last backup based on the tag. day, and time."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B because it provides the least operational effort to reduce costs while considering the specific requirements of the company's environments.\n\nBy creating two Amazon EventBridge rules - one to stop instances in the evening and another to start instances in the morning on business days - the solution automatically manages the lifecycle of the instances based on the environment tag. This approach ensures that the development and testing environments are only running during business hours, while the production environment continues to run 24/7. This allows the company to reduce costs associated with running instances that are not in use, without the need for manual intervention or managing backups and restores.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option requires a single Lambda function to start or stop instances based on the tag, day, and time. This approach may not accurately capture the different operational requirements of the development, testing, and production environments, as it runs the rule once per day, which could lead to instances being stopped while in use or not being stopped when not in use.\n\nC. This option terminates instances during non-business hours and restores them in the morning. Terminating instances instead of stopping them can lead to data loss and longer startup times, which is not desirable, especially for the production environment that needs to be available 24/7.\n\nD. This option runs the EventBridge rule and Lambda function every hour, which is an excessive and unnecessary operational overhead. Terminating or restoring instances every hour could also lead to data loss and longer startup times, as well as increased costs due to the frequent instance lifecycle changes."
  },
  "120": {
    "question": "A company is planning to migrate its on-premises VMware cluster of 120 VMs to AWS. The VMs have many different operating systems and many custom software packages installed. The company also has an on-premises NFS server that is 10 TB in size. The company has set up a 10 Gbps AWS Direct Connect connection to AWS for the migration.Which solution will complete the migration to AWS in the LEAST amount of time?",
    "choices": [
      "A. Export the on-premises VMs and copy them to an Amazon S3 bucket. Use VM Import/Export to create AMIs from the VM images that are stored in Amazon S3. Order an AWS Snowball Edge device. Copy the NFS server data to the device. Restore the NFS server data to an Amazon EC2 instance that has NFS configured.",
      "B. Configure AWS Application Migration Service with a connection to the VMware cluster. Create a replication job for the VMS. Create an Amazon Elastic File System (Amazon EFS) file system. Configure AWS DataSync to copy the NFS server data to the EFS file system over the Direct Connect connection.",
      "C. Recreate the VMs on AWS as Amazon EC2 instances. Install all the required software packages. Create an Amazon FSx for Lustre file system. Configure AWS DataSync to copy the NFS server data to the FSx for Lustre file system over the Direct Connect connection.",
      "D. Order two AWS Snowball Edge devices. Copy the VMs and the NFS server data to the devices. Run VM Import/Export after the data from the devices is loaded to an Amazon S3 bucket. Create an Amazon Elastic File System (Amazon EFS) file system. Copy the NFS server data from Amazon S3 to the EFS file system."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B because it utilizes the most appropriate AWS services and migration techniques to complete the migration in the least amount of time.\n\nKey points:\n- AWS Application Migration Service (AWS MGN) is designed for high-speed and low-latency data transfer between on-premises infrastructure and AWS. It can replicate the VMs with all their custom software packages installed, ensuring a complete migration with no additional configuration required.\n- Creating an Amazon Elastic File System (Amazon EFS) file system to store the NFS server data is a suitable choice, as EFS is a highly available and scalable file storage service that can be easily scaled up or down as needed.\n- Using AWS DataSync to copy the NFS server data from the on-premises infrastructure to the EFS file system over the Direct Connect connection is an efficient way to migrate the large 10 TB NFS data.\n\n2. Explanations of why the other choices are incorrect:\n\nA. This option involves exporting the on-premises VMs, copying them to Amazon S3, and then using VM Import/Export to create AMIs. It also involves using an AWS Snowball Edge device to copy the NFS server data. This approach would take longer than the AWS Application Migration Service and data transfer over the Direct Connect connection.\n\nC. This option involves recreating the VMs on AWS as Amazon EC2 instances and manually installing all the required software packages. It also involves creating an Amazon FSx for Lustre file system and using AWS DataSync to copy the NFS server data. This manual process would take significantly longer than the automated migration tools provided by AWS Application Migration Service.\n\nD. This option involves using two AWS Snowball Edge devices to copy the VMs and NFS server data, and then running VM Import/Export after the data is loaded to an Amazon S3 bucket. It also involves creating an Amazon Elastic File System (Amazon EFS) file system and copying the NFS server data from Amazon S3 to the EFS file system. This approach would take longer than the direct data transfer over the Direct Connect connection."
  },
  "121": {
    "question": "A financial company is planning to migrate its web application from on premises to AWS. The company uses a third-party security tool to monitor the inbound traffic to the application. The company has used the security tool for the last 15 years, and the tool has no cloud solutions available from its vendor. The company's security team is concerned about how to integrate the security tool with AWS technology.The company plans to deploy the application migration to AWS on Amazon EC2 instances. The EC2 instances will run in an Auto Scaling group in a dedicated VPC. The company needs to use the security tool to inspect all packets that come in and out of the VPC. This inspection must occur in real time and must not affect the application's performance. A solutions architect must design a target architecture on AWS that is highly available within an AWS Region.Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
    "choices": [
      "A. Deploy the security tool on EC2 instances m a new Auto Scaling group in the existing VPC",
      "B. Deploy the web application behind a Network Load Balancer",
      "C. Deploy an Application Load Balancer in front of the security tool instances",
      "D. Provision a Gateway Load Balancer for each Availability Zone to redirect the traffic to the security tool",
      "E. Provision a transit gateway to facilitate communication between VPCs."
    ],
    "answer": "AD",
    "explanation": "Correct Answer Explanation:\n\n1. The correct answer is AD - Deploy the security tool on EC2 instances in a new Auto Scaling group in the existing VPC, and provision a Gateway Load Balancer (GWLB) for each Availability Zone to redirect the traffic to the security tool.\n\nThis combination of steps meets the requirements:\n\nA. Deploying the security tool on EC2 instances in an Auto Scaling group in the existing VPC allows the company to leverage the existing on-premises security tool, which has no cloud-native solution available. The Auto Scaling group ensures high availability and resiliency of the security tool instances.\n\nD. Provisioning a Gateway Load Balancer (GWLB) for each Availability Zone allows the architect to redirect all incoming and outgoing traffic from the VPC to the security tool instances for real-time packet inspection. The GWLB is designed specifically for this use case, enabling transparent traffic inspection without affecting the application's performance.\n\nIncorrect Answer Explanations:\n\nB. Deploying the web application behind a Network Load Balancer is not relevant to integrating the third-party security tool with AWS technology.\n\nC. Deploying an Application Load Balancer in front of the security tool instances is not necessary, as the Gateway Load Balancer (GWLB) is already handling the traffic redirection to the security tool.\n\nE. Provisioning a transit gateway to facilitate communication between VPCs is not relevant to the specific requirement of integrating the third-party security tool with AWS technology or inspecting packets in and out of the VPC."
  },
  "122": {
    "question": "A company has purchased appliances from different vendors. The appliances all have IoT sensors. The sensors send status information in the vendors' proprietary formats to a legacy application that parses the information into JSON. The parsing is simple, but each vendor has a unique format. Once daily, the application parses all the JSON records and stores the records in a relational database for analysis.The company needs to design a new data analysis solution that can deliver faster and optimize costs.Which solution will meet these requirements?",
    "choices": [
      "A. Connect the IoT sensors to AWS IoT Core. Set a rule to invoke an AWS Lambda function to parse the information and save a .csv file to Amazon. S3 Use AWS Glue to catalog the files. Use Amazon Athena and Amazon QuickSight for analysis.",
      "B. Migrate the application server to AWS Fargate, which will receive the information from IoT sensors and parse the information into a relational format. Save the parsed information to Amazon Redshlft for analysis.",
      "C. Create an AWS Transfer for SFTP server. Update the IoT sensor code to send the information as a .csv file through SFTP to the server. Use AWS Glue to catalog the files. Use Amazon Athena for analysis.",
      "D. Use AWS Snowball Edge to collect data from the IoT sensors directly to perform local analysis. Periodically collect the data into Amazon Redshift to perform global analysis."
    ],
    "answer": "A",
    "explanation": "1. Explanation of why the correct answer (A) is right:\n\nThe correct answer (A) is the most suitable solution for the given requirements. Here's why:\n\na. AWS IoT Core: Connecting the IoT sensors to AWS IoT Core allows the system to receive data in real-time, regardless of the proprietary formats used by the vendors. IoT Core supports various communication protocols (MQTT, HTTPS, etc.), making it a flexible and scalable solution.\n\nb. AWS Lambda: By setting a rule in IoT Core to invoke an AWS Lambda function, the system can efficiently parse the data in the vendor-specific formats and convert it into a standard format (e.g., CSV) for storage.\n\nc. Amazon S3: Saving the parsed data as CSV files to Amazon S3 provides a cost-effective and scalable storage solution, allowing the data to be accessed and analyzed as needed.\n\nd. AWS Glue: Using AWS Glue to catalog the CSV files in S3 enables the system to easily discover, extract, transform, and load the data into a format suitable for analysis.\n\ne. Amazon Athena and Amazon QuickSight: These AWS services provide a fast and cost-effective way to analyze the data stored in S3, without the need to manage a dedicated data warehouse or BI infrastructure.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. Migrate the application server to AWS Fargate, which will receive the information from IoT sensors and parse the information into a relational format. Save the parsed information to Amazon Redshift for analysis.\n- This solution is more expensive than the correct answer, as it involves the use of AWS Fargate and Amazon Redshift, which may not be necessary for the given requirements.\n\nC. Create an AWS Transfer for SFTP server. Update the IoT sensor code to send the information as a .csv file through SFTP to the server. Use AWS Glue to catalog the files. Use Amazon Athena for analysis.\n- This solution does not leverage the real-time data collection capabilities of AWS IoT Core, which is a more efficient and scalable approach.\n\nD. Use AWS Snowball Edge to collect data from the IoT sensors directly to perform local analysis. Periodically collect the data into Amazon"
  },
  "123": {
    "question": "A company is providing weather data over a REST-based API to several customers. The API is hosted by Amazon API Gateway and is integrated with different AWS Lambda functions for each API operation. The company uses Amazon Route 53 for DNS and has created a resource record of weather.example.com. The company stores data for the API in Amazon DynamoDB tables. The company needs a solution that will give the API the ability to fail over to a different AWS Region.Which solution will meet these requirements?",
    "choices": [
      "A. Deploy a new set of Lambda functions in a new Region. Update the API Gateway API to use an edge-optimized API endpoint with Lambda functions from both Regions as targets. Convert the DynamoDB tables to global tables.",
      "B. Deploy a new API Gateway API and Lambda functions in another Region. Change the Route 53 DNS record to a multivalue answer. Add both API Gateway APIs to the answer. Enable target health monitoring. Convert the DynamoDB tables to global tables.",
      "C. Deploy a new API Gateway API and Lambda functions in another Region. Change the Route 53 DNS record to a failover record. Enable target health monitoring. Convert the DynamoDB tables to global tables.",
      "D. Deploy a new API Gateway API in a new Region. Change the Lambda functions to global functions. Change the Route 53 DNS record to a multivalue answer. Add both API Gateway APIs to the answer. Enable target health monitoring. Convert the DynamoDB tables to global tables."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C because it provides the most comprehensive and appropriate solution to the given requirements.\n\nThe key aspects of the solution in choice C are:\n\n- Deploy a new API Gateway API and Lambda functions in another AWS Region. This ensures redundancy and high availability of the API infrastructure across Regions.\n- Change the Route 53 DNS record to a failover record. This allows the DNS to automatically route traffic to the secondary Region if the primary Region becomes unavailable, providing seamless failover.\n- Enable target health monitoring. This ensures that the failover to the secondary Region is triggered only when the primary Region's resources are unhealthy, maintaining reliability.\n- Convert the DynamoDB tables to global tables. This enables cross-Region data replication, ensuring data consistency and availability in the event of a failover.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This solution is incomplete as it does not address the failover mechanism. While it sets up redundant Lambda functions in another Region and converts DynamoDB tables to global tables, it does not configure the necessary Route 53 failover mechanism to automatically direct traffic to the secondary Region.\n\nB. This solution uses a multivalue answer for the Route 53 DNS record, which would not provide automatic failover. The multivalue answer simply returns multiple API Gateway endpoints, but does not guarantee failover in case of a Region outage. Additionally, target health monitoring is not sufficient for automatic failover in this case.\n\nD. This solution is also incomplete, as it does not deploy redundant API Gateway APIs in a second Region. Instead, it relies on global Lambda functions, which may not provide the required failover capabilities. The multivalue answer approach also does not ensure automatic failover.\n\nIn summary, choice C is the correct answer because it provides the most comprehensive and appropriate solution to the given requirements, including the deployment of redundant API infrastructure, the use of Route 53 failover records, target health monitoring, and the conversion of DynamoDB tables to global tables."
  },
  "124": {
    "question": "A company has hundreds of AWS accounts. The company recently implemented a centralized internal process for purchasing new Reserved Instances and modifying existing Reserved Instances. This process requires all business units that want to purchase or modify Reserved Instances to submit requests to a dedicated team for procurement. Previously, business units directly purchased or modified Reserved Instances in their own respective AWS accounts autonomously.A solutions architect needs to enforce the new process in the most secure way possible.Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
    "choices": [
      "A. Ensure that all AWS accounts are part of an organization in AWS Organizations with all features enabled.",
      "B. Use AWS Config to report on the attachment of an IAM policy that denies access to the ec2:PurchaseReservedInstancesOffering action and the ec2:ModifyReservedInstances action.",
      "C. In each AWS account, create an IAM policy that denies the ec2:PurchaseReservedInstancesOffering action and the ec2:ModifyReservedInstances action.",
      "D. Create an SCP that denies the ec2:PurchaseReservedInstancesOffering action and the ec2:ModifyReservedInstances action. Attach the SCP to each OU of the organization.",
      "E. Ensure that all AWS accounts are part of an organization in AWS Organizations that uses the consolidated billing feature."
    ],
    "answer": "AD",
    "explanation": "1. Explanation of the correct answer (A and D):\n\nA. Ensuring all AWS accounts are part of an organization in AWS Organizations with all features enabled allows for centralized management and control of the accounts. This is crucial for enforcing the new centralized process for purchasing and modifying Reserved Instances across the organization.\n\nD. Creating an SCP (Service Control Policy) that denies the ec2:PurchaseReservedInstancesOffering and ec2:ModifyReservedInstances actions, and attaching it to each OU (Organizational Unit) within the organization, directly enforces the new centralized purchasing process. This ensures that all business units are adhering to the new policy, while still allowing the dedicated team to perform these actions from the management account.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. Using AWS Config to report on the attachment of an IAM policy that denies access to the relevant actions is not sufficient to enforce the new process. AWS Config is a monitoring and auditing service, and does not actively enforce policies.\n\nC. Creating an IAM policy that denies the relevant actions in each AWS account is not the most secure approach, as it requires managing the policy across multiple accounts. It also does not provide centralized control and enforcement of the new process.\n\nE. Ensuring that all AWS accounts are part of an organization in AWS Organizations that uses the consolidated billing feature is not relevant to enforcing the new process for purchasing and modifying Reserved Instances. Consolidated billing is related to cost management, not access control."
  },
  "125": {
    "question": "A software as a service (SaaS) company provides a media software solution to customers. The solution is hosted on 50 VPCs across various AWS Regions and AWS accounts. One of the VPCs is designated as a management VPC. The compute resources in the VPCs work independently.The company has developed a new feature that requires all 50 VPCs to be able to communicate with each other. The new feature also requires one-way access from each customer's VPC to the company's management VPC. The management VPC hosts a compute resource that validates licenses for the media software solution.The number of VPCs that the company will use to host the solution will continue to increase as the solution grows.Which combination of steps will provide the required VPC connectivity with the LEAST operational overhead? (Choose two.)",
    "choices": [
      "A. Create a transit gateway. Attach all the company's VPCs and relevant subnets to the transit gateway.",
      "B. Create VPC peering connections between all the company's VPCs.",
      "C. Create a Network Load Balancer (NLB) that points to the compute resource for license validation. Create an AWS PrivateLink endpoint service that is available to each customer's VPAssociate the endpoint service with the NLB.",
      "D. Create a VPN appliance in each customer's VPC. Connect the company's management VPC to each customer's VPC by using AWS Site-to-Site VPN.",
      "E. Create a VPC peering connection between the company's management VPC and each customer's VPC."
    ],
    "answer": "AC",
    "explanation": "1. Clear explanation of why the correct answer is right:\n\nA. Creating a transit gateway is the best choice to establish connectivity between the company's 50 VPCs. A transit gateway acts as a central hub, allowing all the VPCs to connect to it, eliminating the need for multiple VPC peering connections. This approach is more scalable as the number of VPCs increases, reducing the operational overhead of managing individual VPC peerings.\n\nC. Creating an AWS PrivateLink endpoint service associated with an NLB in the management VPC provides a secure, one-way access from each customer's VPC to the license validation compute resource. This approach avoids the need for VPN connections or internet/NAT gateways, simplifying the network configuration and reducing operational overhead.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nB. VPC peering connections between all the company's VPCs would not be a scalable solution as the number of VPCs increases. It would require managing a large number of peering connections, leading to higher operational overhead.\n\nD. Creating VPN appliances in each customer's VPC and connecting them to the management VPC using AWS Site-to-Site VPN would be more complex to manage, especially as the number of VPCs grows. It would also require configuring and maintaining the VPN appliances in each customer's environment.\n\nE. Creating VPC peering connections between the management VPC and each customer's VPC would not provide the required connectivity between the company's 50 VPCs. It would still require managing a large number of peering connections."
  },
  "126": {
    "question": "An AWS partner company is building a service in AWS Organizations using its organization named org1. This service requires the partner company to have access to AWS resources in a customer account, which is in a separate organization named org2. The company must establish least privilege security access using an API or command line tool to the customer account.What is the MOST secure way to allow org1 to access resources in org2?",
    "choices": [
      "A. The customer should provide the partner company with their AWS account access keys to log in and perform the required tasks.",
      "B. The customer should create an IAM user and assign the required permissions to the IAM user. The customer should then provide the credentials to the partner company to log in and perform the required tasks.",
      "C. The customer should create an IAM role and assign the required permissions to the IAM role. The partner company should then use the IAM role\u2019s Amazon Resource Name (ARN) when requesting access to perform the required tasks.",
      "D. The customer should create an IAM role and assign the required permissions to the IAM role. The partner company should then use the IAM role\u2019s Amazon Resource Name (ARN), including the external ID in the IAM role\u2019s trust policy, when requesting access to perform the required tasks."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (D):\n\nThe correct answer is D because it addresses the \"confused deputy\" problem, which is a security risk where a trusted entity (the partner company in this case) can be tricked into performing an unintended action on behalf of a malicious entity. \n\nBy using an IAM role with an external ID in the trust policy, the customer account (org2) can ensure that the partner company (org1) is the intended principal that should be assuming the role. The external ID acts as a shared secret between the customer and the partner, which helps prevent the role from being used by an unintended party.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Providing the customer's AWS account access keys to the partner company is highly insecure, as it gives the partner company full, unrestricted access to the customer's AWS resources. This violates the principle of least privilege and increases the risk of unauthorized access or misuse.\n\nB. Creating an IAM user and providing the credentials to the partner company is better than option A, but it still does not address the confused deputy problem. The partner company would have direct access to the customer's resources, which is not the most secure approach.\n\nC. Creating an IAM role and providing the ARN to the partner company is a more secure approach than options A and B, as it allows the application of specific permissions. However, it still does not address the confused deputy problem, as the partner company could potentially use the role to perform unintended actions on behalf of a malicious entity."
  },
  "127": {
    "question": "A delivery company needs to migrate its third-party route planning application to AWS. The third party supplies a supported Docker image from a public registry. The image can run in as many containers as required to generate the route map.The company has divided the delivery area into sections with supply hubs so that delivery drivers travel the shortest distance possible from the hubs to the customers. To reduce the time necessary to generate route maps, each section uses its own set of Docker containers with a custom configuration that processes orders only in the section's area.The company needs the ability to allocate resources cost-effectively based on the number of running containers.Which solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      "A. Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster on Amazon EC2. Use the Amazon EKS CLI to launch the planning application in pods by using the --tags option to assign a custom tag to the pod.",
      "B. Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster on AWS Fargate. Use the Amazon EKS CLI to launch the planning application. Use the AWS CLI tag-resource API call to assign a custom tag to the pod.",
      "C. Create an Amazon Elastic Container Service (Amazon ECS) cluster on Amazon EC2. Use the AWS CLI with run-tasks set to true to launch the planning application by using the --tags option to assign a custom tag to the task.",
      "D. Create an Amazon Elastic Container Service (Amazon ECS) cluster on AWS Fargate. Use the AWS CLI run-task command and set enableECSManagedTags to true to launch the planning application. Use the --tags option to assign a custom tag to the task."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (D):\n\nThe correct answer is D, which recommends using Amazon Elastic Container Service (Amazon ECS) on AWS Fargate. This solution meets the requirements with the least operational overhead for the following reasons:\n\n- The delivery company needs to run the third-party route planning application in containers, and the question states that the application is provided as a supported Docker image. ECS is well-suited for running containerized applications, and Fargate eliminates the need to manage the underlying infrastructure, reducing operational overhead.\n- The company has divided the delivery area into sections, and each section requires its own set of Docker containers with a custom configuration. ECS allows you to launch tasks (containers) with custom configurations, and you can assign custom tags to the tasks to identify the specific sections.\n- The company needs the ability to allocate resources cost-effectively based on the number of running containers. Fargate automatically scales the resources required to run the tasks, allowing the company to pay only for the compute resources used, which aligns with the requirement for cost-effective resource allocation.\n- The question states that the company needs the \"least operational overhead\" solution, and ECS on Fargate provides a fully managed container orchestration service, further reducing the operational burden compared to other options like managing an Amazon EKS cluster.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Amazon EKS on Amazon EC2:\n- EKS is a more complex Kubernetes-based service that requires more operational overhead to manage, such as maintaining the underlying EC2 infrastructure and the Kubernetes control plane.\n- The question does not indicate a need for the advanced features and flexibility of Kubernetes, and ECS with Fargate can meet the requirements more simply and with less operational overhead.\n\nB. Amazon EKS on AWS Fargate:\n- While EKS on Fargate removes the need to manage the underlying EC2 infrastructure, it still requires more operational overhead compared to ECS on Fargate, as Kubernetes has a steeper learning curve and more complex concepts to manage.\n- The question does not mention a need for the advanced features of Kubernetes, and ECS on Fargate can meet the requirements more simply.\n- Tagging Kubernetes pods using the AWS CLI is not a straightforwar"
  },
  "128": {
    "question": "A company runs an IoT platform on AWS. IoT sensors in various locations send data to the company\u2019s Node.js API servers on Amazon EC2 instances running behind an Application Load Balancer. The data is stored in an Amazon RDS MySQL DB instance that uses a 4 TB General Purpose SSD volume.The number of sensors the company has deployed in the field has increased over time, and is expected to grow significantly. The API servers are consistently overloaded and RDS metrics show high write latency.Which of the following steps together will resolve the issues permanently and enable growth as new sensors are provisioned, while keeping this platform cost-efficient? (Choose two.)",
    "choices": [
      "A. Resize the MySQL General Purpose SSD storage to 6 TB to improve the volume\u2019s IOPS.",
      "B. Re-architect the database tier to use Amazon Aurora instead of an RDS MySQL DB instance and add read replicas.",
      "C. Leverage Amazon Kinesis Data Streams and AWS Lambda to ingest and process the raw data.",
      "D. Use AWS X-Ray to analyze and debug application issues and add more API servers to match the load.",
      "E. Re-architect the database tier to use Amazon DynamoDB instead of an RDS MySQL DB instance."
    ],
    "answer": "CE",
    "explanation": "1. Explanation of the correct answer:\n\nThe correct answer is C and B.\n\nC. Leverage Amazon Kinesis Data Streams and AWS Lambda to ingest and process the raw data.\nThis is the correct choice because it addresses the issue of API servers being consistently overloaded. Kinesis Data Streams can be used to ingest the data from the IoT sensors in real-time, and AWS Lambda can be used to process the data without putting additional load on the API servers. This can help to improve the overall performance and scalability of the platform.\n\nB. Re-architect the database tier to use Amazon Aurora instead of an RDS MySQL DB instance and add read replicas.\nThis is the correct choice because it addresses the issue of high write latency in the RDS MySQL DB instance. Amazon Aurora is a MySQL-compatible database service that is designed for high performance and scalability. By moving to Aurora, the platform can benefit from Aurora's distributed architecture and improved write performance. Additionally, adding read replicas can help to offload read traffic from the primary database and further improve performance.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Resize the MySQL General Purpose SSD storage to 6 TB to improve the volume's IOPS.\nThis is incorrect because simply increasing the storage size of the RDS MySQL DB instance will not address the underlying issue of high write latency. General Purpose SSD volumes have a fixed IOPS to storage ratio, so increasing the storage size may not significantly improve the overall IOPS performance.\n\nD. Use AWS X-Ray to analyze and debug application issues and add more API servers to match the load.\nThis is incorrect because adding more API servers without addressing the underlying issues with the database tier will not provide a long-term solution. While X-Ray can help to analyze and debug application issues, it does not address the scalability and performance challenges of the database tier.\n\nE. Re-architect the database tier to use Amazon DynamoDB instead of an RDS MySQL DB instance.\nThis is incorrect because DynamoDB is a NoSQL database, while the current system is using a MySQL database. Migrating from a relational database to a NoSQL database would require significant changes to the application and data model, which may not be feasible or desirable in this case."
  },
  "129": {
    "question": "An external audit of a company\u2019s serverless application reveals IAM policies that grant too many permissions. These policies are attached to the company's AWS Lambda execution roles. Hundreds of the company's Lambda functions have broad access permissions such as full access to Amazon S3 buckets and Amazon DynamoDB tables. The company wants each function to have only the minimum permissions that the function needs to complete its task.A solutions architect must determine which permissions each Lambda function needs.What should the solutions architect do to meet this requirement with the LEAST amount of effort?",
    "choices": [
      "A. Set up Amazon CodeGuru to profile the Lambda functions and search for AWS API calls. Create an inventory of the required API calls and resources for each Lambda function. Create new IAM access policies for each Lambda function. Review the new policies to ensure that they meet the company's business requirements.",
      "B. Turn on AWS CloudTrail logging for the AWS account. Use AWS Identity and Access Management Access Analyzer to generate IAM access policies based on the activity recorded in the CloudTrail log. Review the generated policies to ensure that they meet the company's business requirements.",
      "C. Turn on AWS CloudTrail logging for the AWS account. Create a script to parse the CloudTrail log, search for AWS API calls by Lambda execution role, and create a summary report. Review the report. Create IAM access policies that provide more restrictive permissions for each Lambda function.",
      "D. Turn on AWS CloudTrail logging for the AWS account. Export the CloudTrail logs to Amazon S3. Use Amazon EMR to process the CloudTrail logs in Amazon S3 and produce a report of API calls and resources used by each execution role. Create a new IAM access policy for each role. Export the generated roles to an S3 bucket. Review the generated policies to ensure that they meet the company\u2019s business requirements."
    ],
    "answer": "B",
    "explanation": "1. Explanation of why the correct answer (B) is right:\nThe correct answer is B because using AWS Identity and Access Management (IAM) Access Analyzer is the least amount of effort to determine the minimum permissions required for each Lambda function. IAM Access Analyzer analyzes the CloudTrail logs to generate IAM access policies based on the actual activity recorded for each execution role. This eliminates the need to manually review the logs, identify API calls, and create custom policies, which would require more effort.\n\n2. Explanations of why the incorrect choices are wrong:\nA. This option involves using Amazon CodeGuru, which is a tool for profiling and identifying issues in application code. However, it does not directly provide the functionality to generate IAM access policies based on the observed API calls, which is the key requirement in the question.\nC. This option involves manually parsing the CloudTrail logs to identify API calls and create a summary report. While this can be done, it requires more effort compared to using the automated IAM Access Analyzer tool.\nD. This option involves using Amazon EMR to process the CloudTrail logs and generate the IAM access policies. While this is a viable approach, it requires more setup and configuration effort compared to using the built-in IAM Access Analyzer functionality.\n\nIn summary, the correct answer (B) is the least amount of effort because it leverages the AWS IAM Access Analyzer, which can automatically generate the required IAM access policies based on the observed API calls in the CloudTrail logs, without the need for manual analysis or additional tools."
  },
  "130": {
    "question": "A solutions architect must analyze a company\u2019s Amazon EC2 instances and Amazon Elastic Block Store (Amazon EBS) volumes to determine whether the company is using resources efficiently. The company is running several large, high-memory EC2 instances to host database clusters that are deployed in active/passive configurations. The utilization of these EC2 instances varies by the applications that use the databases, and the company has not identified a pattern.The solutions architect must analyze the environment and take action based on the findings.Which solution meets these requirements MOST cost-effectively?",
    "choices": [
      "A. Create a dashboard by using AWS Systems Manager OpsCenter. Configure visualizations for Amazon CloudWatch metrics that are associated with the EC2 instances and their EBS volumes. Review the dashboard periodically, and identify usage patterns. Rightsize the EC2 instances based on the peaks in the metrics.",
      "B. Turn on Amazon CloudWatch detailed monitoring for the EC2 instances and their EBS volumes. Create and review a dashboard that is based on the metrics. Identify usage patterns. Rightsize the EC2 instances based on the peaks in the metrics.",
      "C. Install the Amazon CloudWatch agent on each of the EC2 instances. Turn on AWS Compute Optimizer, and let it run for at least 12 hours. Review the recommendations from Compute Optimizer, and rightsize the EC2 instances as directed.",
      "D. Sign up for the AWS Enterprise Support plan. Turn on AWS Trusted Advisor. Wait 12 hours. Review the recommendations from Trusted Advisor, and rightsize the EC2 instances as directed."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C because it provides the most cost-effective solution to analyze the company's EC2 instances and EBS volumes efficiently.\n\nThe key reasons why C is the best solution:\n\n- Installing the Amazon CloudWatch agent on each EC2 instance allows collecting detailed metrics, including memory utilization, which is crucial for analyzing the high-memory EC2 instances mentioned in the question.\n- Enabling AWS Compute Optimizer will analyze the usage patterns of the EC2 instances and provide recommendations on more cost-effective instance types and purchasing options (e.g., Reserved Instances, Savings Plans) based on the collected metrics.\n- Compute Optimizer can provide these recommendations after running for at least 12 hours, which is a relatively short time compared to the other options.\n- This solution is more cost-effective than the other options, as it does not require setting up a custom dashboard (A) or upgrading to a more expensive AWS Support plan (D).\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option is not the most cost-effective because it requires manual analysis of the dashboard and metrics to identify usage patterns and rightsize the instances. It also does not provide specific recommendations like Compute Optimizer.\n\nB. This option is less cost-effective than C because it requires enabling detailed monitoring, which incurs additional charges, and still requires manual analysis of the dashboard to identify usage patterns and rightsize the instances.\n\nD. This option is the most costly because it requires upgrading to the AWS Enterprise Support plan, which is the most expensive support tier. Additionally, the recommendations from AWS Trusted Advisor may not be as comprehensive as the machine learning-based recommendations from Compute Optimizer."
  },
  "131": {
    "question": "A company uses AWS Organizations for a multi-account setup in the AWS Cloud. The company uses AWS Control Tower for governance and uses AWS Transit Gateway for VPC connectivity across accounts.In an AWS application account, the company\u2019s application team has deployed a web application that uses AWS Lambda and Amazon RDS. The company's database administrators have a separate DBA account and use the account to centrally manage all the databases across the organization. The database administrators use an Amazon EC2 instance that is deployed in the DBA account to access an RDS database that is deployed m the application account.The application team has stored the database credentials as secrets in AWS Secrets Manager in the application account. The application team is manually sharing the secrets with the database administrators. The secrets are encrypted by the default AWS managed key for Secrets Manager in the application account. A solutions architect needs to implement a solution that gives the database administrators access to the database and eliminates the need to manually share the secrets.Which solution will meet these requirements?",
    "choices": [
      "A. Use AWS Resource Access Manager (AWS RAM) to share the secrets from the application account with the DBA account. In the DBA account, create an IAM role that is named DBA-Admin. Grant the role the required permissions to access the shared secrets. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets.",
      "B. In the application account, create an IAM role that is named DBA-Secret. Grant the role the required permissions to access the secrets. In the DBA account, create an IAM role that is named DBA-Admin. Grant the DBA-Admin role the required permissions to assume the DBA-Secret role in the application account. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets",
      "C. In the DBA account create an IAM role that is named DBA-Admin. Grant the role the required permissions to access the secrets and the default AWS managed key in the application account. In the application account, attach resource-based policies to the key to allow access from the DBA account. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets.",
      "D. In the DBA account, create an IAM role that is named DBA-Admin. Grant the role the required permissions to access the secrets in the application account. Attach an SCP to the application account to allow access to the secrets from the DBA account. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets."
    ],
    "answer": "B",
    "explanation": "1. Clear explanation of why the correct answer (B) is right:\n\nThe correct answer is B because it provides the most secure and appropriate solution to the given scenario. In this solution:\n\n- In the application account, an IAM role named \"DBA-Secret\" is created and granted the required permissions to access the secrets in Secrets Manager.\n- In the DBA account, an IAM role named \"DBA-Admin\" is created and granted the required permissions to assume the \"DBA-Secret\" role in the application account.\n- The \"DBA-Admin\" role is then attached to the EC2 instance in the DBA account, allowing the database administrators to access the cross-account secrets without the need to manually share them.\n\nThis approach leverages the principle of least privilege, where the database administrators are only granted the necessary permissions to access the secrets, and the access is managed through the IAM roles and cross-account role assumption. This eliminates the need for manual secret sharing and ensures a more secure and scalable solution.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Incorrect because AWS Resource Access Manager (RAM) is used to share resources, not Secrets Manager secrets. Secrets Manager secrets are not supported for sharing through RAM.\n\nC. Incorrect because the default AWS-managed KMS key used to encrypt the secrets in the application account cannot be accessed from the DBA account. Granting the DBA-Admin role the required permissions to the default KMS key would not be possible, as the key policy is managed by AWS.\n\nD. Incorrect because Service Control Policies (SCPs) can only be used to remove permissions, not grant them. Even with an SCP in place, the database administrators would still need the necessary IAM permissions and resource-based policies to access the secrets in the application account."
  },
  "132": {
    "question": "A company manages multiple AWS accounts by using AWS Organizations. Under the root OU, the company has two OUs: Research and DataOps.Because of regulatory requirements, all resources that the company deploys in the organization must reside in the ap-northeast-1 Region. Additionally, EC2 instances that the company deploys in the DataOps OU must use a predefined list of instance types.A solutions architect must implement a solution that applies these restrictions. The solution must maximize operational efficiency and must minimize ongoing maintenance.Which combination of steps will meet these requirements? (Choose two.)",
    "choices": [
      "A. Create an IAM role in one account under the DataOps OU. Use the ec2:InstanceType condition key in an inline policy on the role to restrict access to specific instance type.",
      "B. Create an IAM user in all accounts under the root OU. Use the aws:RequestedRegion condition key in an inline policy on each user to restrict access to all AWS Regions except ap-northeast-1.",
      "C. Create an SCP. Use the aws:RequestedRegion condition key to restrict access to all AWS Regions except ap-northeast-1. Apply the SCP to the root OU.",
      "D. Create an SCP. Use the ec2:Region condition key to restrict access to all AWS Regions except ap-northeast-1. Apply the SCP to the root OU, the DataOps OU, and the Research OU.",
      "E. Create an SCP. Use the ec2:InstanceType condition key to restrict access to specific instance types. Apply the SCP to the DataOps OU."
    ],
    "answer": "CE",
    "explanation": "1. Explanation of the correct answer (C and E):\n\nThe correct answer is a combination of steps C and E.\n\nC. Create an SCP (Service Control Policy) that uses the `aws:RequestedRegion` condition key to restrict access to all AWS Regions except `ap-northeast-1`. Apply this SCP to the root OU.\nThis ensures that all resources deployed in the organization, regardless of the OU, are limited to the `ap-northeast-1` Region, as per the regulatory requirements.\n\nE. Create an SCP that uses the `ec2:InstanceType` condition key to restrict access to specific instance types. Apply this SCP to the DataOps OU.\nThis ensures that EC2 instances deployed in the DataOps OU can only use the predefined list of instance types, as per the requirements.\n\nBy using SCPs, the solution maximizes operational efficiency and minimizes ongoing maintenance, as the policies are applied at the OU level and automatically enforce the required restrictions across all accounts within the organization.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option uses an IAM role with an inline policy, which would require manual management and maintenance for each account. It also does not address the requirement to restrict the AWS Region.\n\nB. This option uses an IAM user with an inline policy, which is less efficient than using an SCP, as it would require manual management and maintenance for each account. Additionally, it does not address the requirement to restrict the instance types in the DataOps OU.\n\nD. This option suggests using the `ec2:Region` condition key, which is not a valid condition key for EC2 actions. Instead, the `aws:RequestedRegion` condition key should be used to restrict access to specific AWS Regions. Additionally, applying the SCP to multiple OUs is unnecessary, as applying it to the root OU alone will ensure that the restriction applies to all accounts in the organization."
  },
  "133": {
    "question": "A company runs a serverless application in a single AWS Region. The application accesses external URLs and extracts metadata from those sites. The company uses an Amazon Simple Notification Service (Amazon SNS) topic to publish URLs to an Amazon Simple Queue Service (Amazon SQS) queue. An AWS Lambda function uses the queue as an event source and processes the URLs from the queue. Results are saved to an Amazon S3 bucket.The company wants to process each URL in other Regions to compare possible differences in site localization. URLs must be published from the existing Region. Results must be written to the existing S3 bucket in the current Region.Which combination of changes will produce multi-Region deployment that meets these requirements? (Choose two.)",
    "choices": [
      "A. Deploy the SQS queue with the Lambda function to other Regions.",
      "B. Subscribe the SNS topic in each Region to the SQS queue.",
      "C. Subscribe the SQS queue in each Region to the SNS topic.",
      "D. Configure the SQS queue to publish URLs to SNS topics in each Region.",
      "E. Deploy the SNS topic and the Lambda function to other Regions."
    ],
    "answer": "AC",
    "explanation": "1. Explanation of the correct answer (AC):\n\nA. Deploy the SQS queue with the Lambda function to other Regions.\nThis is correct because to process the URLs in multiple Regions, you need to have the SQS queue and the Lambda function deployed in those Regions. This allows the Lambda function to process the URLs from the SQS queue in each Region independently.\n\nC. Subscribe the SQS queue in each Region to the SNS topic.\nThis is correct because the SNS topic is publishing the URLs, and the SQS queue in each Region needs to be subscribed to the SNS topic to receive the URLs. This ensures that the URLs are distributed to the SQS queues in all the Regions.\n\n2. Explanations of the incorrect choices:\n\nB. Subscribe the SNS topic in each Region to the SQS queue.\nThis is incorrect because the SNS topic is the publisher, and the SQS queue is the subscriber. Subscribing the SNS topic in each Region to the SQS queue would not work, as the SNS topic cannot be a subscriber to the SQS queue.\n\nD. Configure the SQS queue to publish URLs to SNS topics in each Region.\nThis is incorrect because the SQS queue is not responsible for publishing the URLs to the SNS topics. The SNS topic is the publisher, and the SQS queue is the subscriber. Configuring the SQS queue to publish to SNS topics would not meet the requirements.\n\nE. Deploy the SNS topic and the Lambda function to other Regions.\nThis is incorrect because the question states that the URLs must be published from the existing Region, and the results must be written to the existing S3 bucket in the current Region. Deploying the SNS topic and the Lambda function to other Regions would not meet these requirements."
  },
  "134": {
    "question": "A company runs a proprietary stateless ETL application on an Amazon EC2 Linux instances. The application is a Linux binary, and the source code cannot be modified. The application is single-threaded, uses 2 GB of RAM, and is highly CPU intensive. The application is scheduled to run every 4 hours and runs for up to 20 minutes. A solutions architect wants to revise the architecture for the solution.Which strategy should the solutions architect use?",
    "choices": [
      "A. Use AWS Lambda to run the application. Use Amazon CloudWatch Logs to invoke the Lambda function every 4 hours.",
      "B. Use AWS Batch to run the application. Use an AWS Step Functions state machine to invoke the AWS Batch job every 4 hours.",
      "C. Use AWS Fargate to run the application. Use Amazon EventBridge (Amazon CloudWatch Events) to invoke the Fargate task every 4 hours.",
      "D. Use Amazon EC2 Spot Instances to run the application. Use AWS CodeDeploy to deploy and run the application every 4 hours."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C. Use AWS Fargate to run the application, and use Amazon EventBridge (Amazon CloudWatch Events) to invoke the Fargate task every 4 hours.\n\nThis is the best solution for the given requirements because:\n\n- AWS Fargate is a serverless compute engine for containers that allows you to run containers without managing the underlying EC2 instances. Since the application is a single-threaded, Linux binary that cannot be modified, Fargate provides a straightforward way to run it without worrying about the infrastructure.\n- Amazon EventBridge (formerly CloudWatch Events) can be used to schedule the Fargate task to run every 4 hours, which aligns with the requirement to run the application on a schedule.\n- Fargate is a good fit for the application's CPU-intensive and short-duration (up to 20 minutes) nature, as it can automatically scale the resources as needed.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Use AWS Lambda to run the application. Use Amazon CloudWatch Logs to invoke the Lambda function every 4 hours.\n- This is not the best solution because Lambda functions have a time limit of 15 minutes, while the application runs for up to 20 minutes. The application's CPU-intensive nature also makes it a better fit for a container-based solution like Fargate.\n\nB. Use AWS Batch to run the application. Use an AWS Step Functions state machine to invoke the AWS Batch job every 4 hours.\n- While AWS Batch is a good fit for running CPU-intensive batch workloads, the requirement to schedule the task every 4 hours is not as straightforward as using EventBridge. The Step Functions state machine adds unnecessary complexity for the given use case.\n\nD. Use Amazon EC2 Spot Instances to run the application. Use AWS CodeDeploy to deploy and run the application every 4 hours.\n- This solution requires managing the underlying EC2 infrastructure, which goes against the requirement of using a serverless compute solution. Additionally, the use of CodeDeploy to deploy and run the application every 4 hours is not as efficient as using a scheduled event-driven solution like EventBridge."
  },
  "135": {
    "question": "A company is creating a sequel for a popular online game. A large number of users from all over the world will play the game within the first week after launch. Currently, the game consists of the following components deployed in a single AWS Region:\u2022\tAmazon S3 bucket that stores game assets\u2022\tAmazon DynamoDB table that stores player scoresA solutions architect needs to design a multi-Region solution that will reduce latency, improve reliability, and require the least effort to implement.What should the solutions architect do to meet these requirements?",
    "choices": [
      "A. Create an Amazon CloudFront distribution to serve assets from the S3 bucket. Configure S3 Cross-Region Replication. Create a new DynamoDB table in a new Region. Use the new table as a replica target for DynamoDB global tables.",
      "B. Create an Amazon CloudFront distribution to serve assets from the S3 bucket. Configure S3 Same-Region Replication. Create a new DynamoDB table in a new Region. Configure asynchronous replication between the DynamoDB tables by using AWS Database Migration Service (AWS DMS) with change data capture (CDC).",
      "C. Create another S3 bucket in a new Region, and configure S3 Cross-Region Replication between the buckets. Create an Amazon CloudFront distribution and configure origin failover with two origins accessing the S3 buckets in each Region. Configure DynamoDB global tables by enabling Amazon DynamoDB Streams, and add a replica table in a new Region.",
      "D. Create another S3 bucket in the sine Region, and configure S3 Same-Region Replication between the buckets. Create an Amazon CloudFront distribution and configure origin failover with two origins accessing the S3 buckets. Create a new DynamoDB table in a new Region. Use the new table as a replica target for DynamoDB global tables."
    ],
    "answer": "C",
    "explanation": "Here's a detailed explanation of the correct answer and why the other choices are incorrect:\n\n1. Explanation of the correct answer (Option C):\n   - Option C is the correct answer because it addresses all the requirements of reducing latency, improving reliability, and requiring the least effort to implement.\n   - By creating another S3 bucket in a new Region and configuring S3 Cross-Region Replication, the game assets are replicated to the new Region, reducing latency for users accessing the assets from that region.\n   - Setting up an Amazon CloudFront distribution and configuring origin failover with two origins accessing the S3 buckets in each Region ensures high availability and failover in case one of the regions becomes unavailable.\n   - Configuring DynamoDB global tables by enabling Amazon DynamoDB Streams and adding a replica table in a new Region improves reliability by replicating the player scores across multiple regions, ensuring they are available even in the event of a regional failure.\n   - This solution requires minimal effort to implement compared to the other options, as it leverages the built-in features of AWS services like S3 Cross-Region Replication, CloudFront origin failover, and DynamoDB global tables.\n\n2. Explanations of why the incorrect choices are wrong:\n   - Option A is incorrect because using a new DynamoDB table as a replica target for DynamoDB global tables will not improve reliability compared to the global tables approach in Option C. It also requires more effort to set up the replication compared to the global tables solution.\n   - Option B is incorrect because configuring asynchronous replication between the DynamoDB tables using AWS DMS with CDC is not the best solution for this use case. It would require additional configuration and management effort compared to the global tables approach in Option C.\n   - Option D is incorrect because using S3 Same-Region Replication will not reduce latency for users in other regions, as the assets will only be replicated within the same region. The CloudFront origin failover setup is also not as robust as the one in Option C."
  },
  "136": {
    "question": "A company has an on-premises website application that provides real estate information for potential renters and buyers. The website uses a Java backend and a NoSQL MongoDB database to store subscriber data.The company needs to migrate the entire application to AWS with a similar structure. The application must be deployed for high availability, and the company cannot make changes to the application.Which solution will meet these requirements?",
    "choices": [
      "A. Use an Amazon Aurora DB cluster as the database for the subscriber data. Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones for the Java backend application.",
      "B. Use MongoDB on Amazon EC2 instances as the database for the subscriber data. Deploy EC2 instances in an Auto Scaling group in a single Availability Zone for the Java backend application.",
      "C. Configure Amazon DocumentDB (with MongoDB compatibility) with appropriately sized instances in multiple Availability Zones as the database for the subscriber data. Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones for the Java backend application.",
      "D. Configure Amazon DocumentDB (with MongoDB compatibility) in on-demand capacity mode in multiple Availability Zones as the database for the subscriber data. Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones for the Java backend application."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C. Configure Amazon DocumentDB (with MongoDB compatibility) with appropriately sized instances in multiple Availability Zones as the database for the subscriber data. Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones for the Java backend application.\n\nThis solution is correct because:\n\n- Amazon DocumentDB is a fully managed database service that is compatible with MongoDB, which aligns with the requirement to use a NoSQL MongoDB database.\n- Configuring DocumentDB with instances in multiple Availability Zones provides high availability for the database, as required.\n- Deploying the Java backend application on Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones also ensures high availability for the application.\n- Since the company cannot make changes to the application, using a managed service like DocumentDB is a better choice than managing the MongoDB database on EC2 instances.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option is incorrect because Amazon Aurora is a relational database service that is not compatible with MongoDB. The requirement is to use a NoSQL MongoDB database, so Aurora is not a suitable choice.\n\nB. This option is incorrect because managing the MongoDB database on EC2 instances in a single Availability Zone does not provide the required high availability. While this could work, using a managed service like DocumentDB is a better choice.\n\nD. This option is incorrect because there is no \"on-demand capacity mode\" for Amazon DocumentDB. DocumentDB only offers on-demand instances, not the on-demand capacity mode mentioned in the choice."
  },
  "137": {
    "question": "A digital marketing company has multiple AWS accounts that belong to various teams. The creative team uses an Amazon S3 bucket in its AWS account to securely store images and media files that are used as content for the company\u2019s marketing campaigns. The creative team wants to share the S3 bucket with the strategy team so that the strategy team can view the objects.A solutions architect has created an IAM role that is named strategy_reviewer in the Strategy account. The solutions architect also has set up a custom AWS Key Management Service (AWS KMS) key in the Creative account and has associated the key with the S3 bucket. However, when users from the Strategy account assume the IAM role and try to access objects in the S3 bucket, they receive an Access Denied error.The solutions architect must ensure that users in the Strategy account can access the S3 bucket. The solution must provide these users with only the minimum permissions that they need.Which combination of steps should the solutions architect take to meet these requirements? (Choose three.)",
    "choices": [
      "A. Create a bucket policy that includes read permissions for the S3 bucket. Set the principal of the bucket policy to the account ID of the Strategy account.",
      "B. Update the strategy_reviewer IAM role to grant full permissions for the S3 bucket and to grant decrypt permissions for the custom KMS key.",
      "C. Update the custom KMS key policy in the Creative account to grant decrypt permissions to the strategy_reviewer IAM role.",
      "D. Create a bucket policy that includes read permissions for the S3 bucket. Set the principal of the bucket policy to an anonymous user.",
      "E. Update the custom KMS key policy in the Creative account to grant encrypt permissions to the strategy_reviewer IAM role.",
      "F. Update the strategy_reviewer IAM role to grant read permissions for the S3 bucket and to grant decrypt permissions for the custom KMS key."
    ],
    "answer": "ACF",
    "explanation": "1. Explanation of the correct answer (A, C, F):\n\nA. This creates a bucket policy that grants read permissions to the S3 bucket for the Strategy account. This allows users from the Strategy account to access the objects in the bucket.\n\nC. This updates the custom KMS key policy in the Creative account to grant decrypt permissions to the strategy_reviewer IAM role. This is necessary because the S3 bucket is encrypted using the custom KMS key, and users from the Strategy account need to be able to decrypt the objects.\n\nF. This updates the strategy_reviewer IAM role to grant read permissions for the S3 bucket and decrypt permissions for the custom KMS key. This provides the minimum permissions required for users in the Strategy account to access the objects in the S3 bucket.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. Granting full permissions to the IAM role is not the minimum required to meet the requirements.\nD. Granting access to anonymous users is not a secure practice and does not meet the requirement of only allowing users from the Strategy account to access the bucket.\nE. Granting encrypt permissions to the IAM role is not necessary, as the users in the Strategy account only need to be able to decrypt the objects, not encrypt them."
  },
  "138": {
    "question": "A company built an ecommerce website on AWS using a three-tier web architecture. The application is Java-based and composed of an Amazon CloudFront distribution, an Apache web server layer of Amazon EC2 instances in an Auto Scaling group, and a backend Amazon Aurora MySQL database.Last month, during a promotional sales event, users reported errors and timeouts while adding items to their shopping carts. The operations team recovered the logs created by the web servers and reviewed Aurora DB cluster performance metrics. Some of the web servers were terminated before logs could be collected and the Aurora metrics were not sufficient for query performance analysis.Which combination of steps must the solutions architect take to improve application performance visibility during peak traffic events? (Choose three.)",
    "choices": [
      "A. Configure the Aurora MySQL DB cluster to publish slow query and error logs to Amazon CloudWatch Logs.",
      "B. Implement the AWS X-Ray SDK to trace incoming HTTP requests on the EC2 instances and implement tracing of SQL queries with the X-Ray SDK for Java.",
      "C. Configure the Aurora MySQL DB cluster to stream slow query and error logs to Amazon Kinesis.",
      "D. Install and configure an Amazon CloudWatch Logs agent on the EC2 instances to send the Apache logs to CloudWatch Logs.",
      "E. Enable and configure AWS CloudTrail to collect and analyze application activity from Amazon EC2 and Aurora",
      "F. Enable Aurora MySQL DB cluster performance benchmarking and publish the stream to AWS X-Ray."
    ],
    "answer": "ABD",
    "explanation": "1. Explanation of the correct answer (ABD):\n\nA. Configure the Aurora MySQL DB cluster to publish slow query and error logs to Amazon CloudWatch Logs:\nThis option is correct because publishing the slow query and error logs from the Aurora MySQL DB cluster to CloudWatch Logs will provide better visibility into the database performance. It will help identify slow-running queries or errors that might be contributing to the application's performance issues during the peak traffic event.\n\nB. Implement the AWS X-Ray SDK to trace incoming HTTP requests on the EC2 instances and implement tracing of SQL queries with the X-Ray SDK for Java:\nThis option is correct because integrating the AWS X-Ray SDK into the application will enable tracing of incoming HTTP requests on the EC2 instances. Additionally, tracing SQL queries with the X-Ray SDK for Java will provide insights into how the database queries are impacting the application's performance. This will help identify performance bottlenecks at both the service and database levels.\n\nD. Install and configure an Amazon CloudWatch Logs agent on the EC2 instances to send the Apache logs to CloudWatch Logs:\nThis option is correct because it will provide visibility into the web server logs, which can help identify performance issues with the web server layer, such as high latency or timeouts.\n\n2. Explanations of why the incorrect choices are wrong:\n\nC. Configure the Aurora MySQL DB cluster to stream slow query and error logs to Amazon Kinesis:\nThis option is incorrect because streaming the logs to Amazon Kinesis would not provide the same level of visibility and analysis capabilities as publishing the logs to CloudWatch Logs. CloudWatch Logs offers better integration with other AWS services and more advanced querying and analytics capabilities.\n\nE. Enable and configure AWS CloudTrail to collect and analyze application activity from Amazon EC2 and Aurora:\nThis option is incorrect because CloudTrail is primarily used for monitoring and auditing user and service activity, rather than performance analysis. It would not provide the level of detail and visibility required to diagnose the performance issues during the peak traffic event.\n\nF. Enable Aurora MySQL DB cluster performance benchmarking and publish the stream to AWS X-Ray:\nThis option is incorrect because enabling performance benchmarking and publishing the stream to AWS X-Ray would not provide the same level of visibility and analysis as integrating the"
  },
  "139": {
    "question": "A company runs a content management application on a single Windows Amazon EC2 instance in a development environment. The application reads and writes static content to a 2 TB Amazon Elastic Block Store (Amazon EBS) volume that is attached to the instance as the root device. The company plans to deploy this application in production as a highly available and fault-tolerant solution that runs on at least three EC2 instances across multiple Availability Zones.A solutions architect must design a solution that joins all the instances that run the application to an Active Directory domain. The solution also must implement Windows ACLs to control access to file contents. The application always must maintain exactly the same content on all running instances at any given point in time.Which solution will meet these requirements with the LEAST management overhead?",
    "choices": [
      "A. Create an Amazon Elastic File System (Amazon EFS) file share. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three instances. Implement a user data script to install the application, join the instance to the AD domain, and mount the EFS file share.",
      "B. Create a new AMI from the current EC2 Instance that is running. Create an Amazon FSx for Lustre file system. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three instances. Implement a user data script to join the instance to the AD domain and mount the FSx for Lustre file system.",
      "C. Create an Amazon FSx for Windows File Server file system. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three instances. Implement a user data script to install the application and mount the FSx for Windows File Server file system. Perform a seamless domain join to join the instance to the AD domain.",
      "D. Create a new AMI from the current EC2 instance that is running. Create an Amazon Elastic File System (Amazon EFS) file system. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three Instances. Perform a seamless domain join to join the instance to the AD domain."
    ],
    "answer": "C",
    "explanation": "Explanation:\n\n1. Correct Answer: Option C\n\nThe correct answer is Option C because it meets all the requirements of the problem statement:\n\n- It uses Amazon FSx for Windows File Server, which is a fully managed native Microsoft Windows file system. This allows the application to use Windows ACLs to control access to the file contents.\n- By creating an Auto Scaling group that extends across three Availability Zones, it ensures high availability and fault tolerance for the application.\n- The user data script can be used to install the application, mount the FSx for Windows File Server file system, and perform a seamless domain join to the Active Directory domain.\n\nThis solution has the least management overhead as the Amazon FSx for Windows File Server is a fully managed service, and the Auto Scaling group handles the deployment and scaling of the instances.\n\n2. Explanations for Incorrect Choices:\n\nA. This option uses Amazon EFS, which is a file system designed for Linux/POSIX-based workloads. It does not support the Windows-specific features required for the application, such as Windows ACLs.\n\nB. This option uses Amazon FSx for Lustre, which is a file system designed for high-performance computing (HPC) workloads. It is not suitable for a Windows-based application that requires Windows ACLs.\n\nD. This option uses Amazon EFS, which is a Linux/POSIX-based file system, and it does not mention the use of a Windows-specific file system like Amazon FSx for Windows File Server. Therefore, it does not meet the requirements for the Windows-based application."
  },
  "140": {
    "question": "A software as a service (SaaS) based company provides a case management solution to customers A3 part of the solution. The company uses a standalone Simple Mail Transfer Protocol (SMTP) server to send email messages from an application. The application also stores an email template for acknowledgement email messages that populate customer data before the application sends the email message to the customer.The company plans to migrate this messaging functionality to the AWS Cloud and needs to minimize operational overhead.Which solution will meet these requirements MOST cost-effectively?",
    "choices": [
      "A. Set up an SMTP server on Amazon EC2 instances by using an AMI from the AWS Marketplace. Store the email template in an Amazon S3 bucket. Create an AWS Lambda function to retrieve the template from the S3 bucket and to merge the customer data from the application with the template. Use an SDK in the Lambda function to send the email message.",
      "B. Set up Amazon Simple Email Service (Amazon SES) to send email messages. Store the email template in an Amazon S3 bucket. Create an AWS Lambda function to retrieve the template from the S3 bucket and to merge the customer data from the application with the template. Use an SDK in the Lambda function to send the email message.",
      "C. Set up an SMTP server on Amazon EC2 instances by using an AMI from the AWS Marketplace. Store the email template in Amazon Simple Email Service (Amazon SES) with parameters for the customer data. Create an AWS Lambda function to call the SES template and to pass customer data to replace the parameters. Use the AWS Marketplace SMTP server to send the email message.",
      "D. Set up Amazon Simple Email Service (Amazon SES) to send email messages. Store the email template on Amazon SES with parameters for the customer data. Create an AWS Lambda function to call the SendTemplatedEmail API operation and to pass customer data to replace the parameters and the email destination."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (Option D):\n\nThe correct answer is Option D:\n\"Set up Amazon Simple Email Service (Amazon SES) to send email messages. Store the email template on Amazon SES with parameters for the customer data. Create an AWS Lambda function to call the SendTemplatedEmail API operation and to pass customer data to replace the parameters and the email destination.\"\n\nThis is the most cost-effective solution that meets the requirements:\n- Amazon SES is a fully managed email service, which minimizes operational overhead for the company.\n- Storing the email template on Amazon SES with parameters allows the company to easily merge the customer data with the template.\n- Using an AWS Lambda function to call the SendTemplatedEmail API operation simplifies the process of sending the email, as the Lambda function can handle the logic of retrieving the template, replacing the parameters, and sending the email.\n\n2. Explanations of why the incorrect choices are wrong:\n\nOption A:\n\"Set up an SMTP server on Amazon EC2 instances by using an AMI from the AWS Marketplace. Store the email template in an Amazon S3 bucket. Create an AWS Lambda function to retrieve the template from the S3 bucket and to merge the customer data from the application with the template. Use an SDK in the Lambda function to send the email message.\"\nThis is not the most cost-effective solution because it requires setting up and managing an SMTP server on EC2 instances, which adds operational overhead and complexity.\n\nOption B:\n\"Set up Amazon Simple Email Service (Amazon SES) to send email messages. Store the email template in an Amazon S3 bucket. Create an AWS Lambda function to retrieve the template from the S3 bucket and to merge the customer data from the application with the template. Use an SDK in the Lambda function to send the email message.\"\nThis solution is not as cost-effective as Option D because it still requires the company to manage the email template storage and the integration with the application data, which adds more complexity.\n\nOption C:\n\"Set up an SMTP server on Amazon EC2 instances by using an AMI from the AWS Marketplace. Store the email template in Amazon Simple Email Service (Amazon SES) with parameters for the customer data. Create an AWS Lambda function to call the SES template and to pass customer data to"
  },
  "141": {
    "question": "A company is processing videos in the AWS Cloud by Using Amazon EC2 instances in an Auto Scaling group. It takes 30 minutes to process a video Several EC2 instances scale in and out depending on the number of videos in an Amazon Simple Queue Service (Amazon SQS) queue.The company has configured the SQS queue with a redrive policy that specifies a target dead-letter queue and a maxReceiveCount of 1. The company has set the visibility timeout for the SQS queue to 1 hour. The company has set up an Amazon CloudWatch alarm to notify the development team when there are messages in the dead-letter queue.Several times during the day. the development team receives notification that messages are in the dead-letter queue and that videos have not been processed property. An investigation finds no errors m the application logs.How can the company solve this problem?",
    "choices": [
      "A. Turn on termination protection tor the EC2 Instances",
      "B. Update the visibility timeout for the SQS queue to 3 hours",
      "C. Configure scale-in protection for the instances during processing",
      "D. Update the redrive policy and set maxReceiveCount to 0."
    ],
    "answer": "C",
    "explanation": "Here is a concise, technical explanation of the correct answer and why the incorrect answers are wrong:\n\n1. Correct answer: C. Configure scale-in protection for the instances during processing\n\nExplanation:\nThe issue described in the question is that messages are ending up in the dead-letter queue, even though there are no errors in the application logs. This suggests that the instances are being terminated before they can complete the video processing, causing the messages to be returned to the queue and eventually moved to the dead-letter queue.\n\nConfiguring scale-in protection for the instances during processing (option C) is the correct solution because it will prevent the instances from being terminated while they are actively processing videos. This ensures that the messages can be fully processed without being returned to the queue prematurely.\n\n2. Incorrect answers:\n\nA. Turn on termination protection for the EC2 instances\nExplanation: Turning on termination protection would prevent the instances from being scaled in and out as needed, which is a key requirement for the auto-scaling setup described in the question.\n\nB. Update the visibility timeout for the SQS queue to 3 hours\nExplanation: Increasing the visibility timeout to 3 hours is unnecessary, as the video processing takes only 30 minutes. A longer visibility timeout could delay the reprocessing of failed messages, but does not address the root cause of the issue, which is the premature termination of instances.\n\nD. Update the redrive policy and set maxReceiveCount to 0\nExplanation: The redrive policy and maxReceiveCount are designed to handle message failures, but in this case, the issue is not with the messages themselves, but with the premature termination of instances. Setting maxReceiveCount to 0 would prevent messages from being moved to the dead-letter queue, but would not solve the underlying problem."
  },
  "142": {
    "question": "A company has developed APIs that use Amazon API Gateway with Regional endpoints. The APIs call AWS Lambda functions that use API Gateway authentication mechanisms. After a design review, a solutions architect identifies a set of APIs that do not require public access.The solutions architect must design a solution to make the set of APIs accessible only from a VPC. All APIs need to be called with an authenticated userWhich solution will meet these requirements with the LEAST amount of effort?",
    "choices": [
      "A. Create an internal Application Load Balancer (ALB). Create a target group. Select the Lambda function to call. Use the ALB DNS name to call the API from the VPC.",
      "B. Remove the DNS entry that is associated with the API in API Gateway. Create a hosted zone in Amazon Route 53. Create a CNAME record in the hosted zone. Update the API in API Gateway with the CNAME record. Use the CNAME record to call the API from the VPC.",
      "C. Update the API endpoint from Regional to private in API Gateway. Create an interface VPC endpoint in the VPCreate a resource policy, and attach it to the API. Use the VPC endpoint to call the API from the VPC.",
      "D. Deploy the Lambda functions inside the VPC Provision an EC2 instance, and install an Apache server. From the Apache server, call the Lambda functions. Use the internal CNAME record of the EC2 instance to call the API from the VPC."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C because it addresses the key requirements of the problem statement:\n\n1. Making the set of APIs accessible only from a VPC:\n   - The solution involves updating the API endpoint from Regional to private in API Gateway, which restricts access to the API to only VPC endpoints.\n   - This ensures that the set of APIs are accessible only from within the VPC, and not from the public internet.\n\n2. Authenticating users for all API calls:\n   - The question states that \"All APIs need to be called with an authenticated user\", which is not explicitly mentioned in the solution choices.\n   - However, the use of API Gateway's authentication mechanisms, which is mentioned in the problem statement, can be leveraged to ensure that all API calls are authenticated, regardless of whether the APIs are accessed from the VPC or the public internet.\n\n2. Explanations of why the other choices are incorrect:\n\nA. Create an internal Application Load Balancer (ALB):\n   - This solution does not restrict access to the APIs to only the VPC. The APIs would still be publicly accessible, even though they are called through the internal ALB.\n\nB. Remove the DNS entry and use a CNAME record:\n   - Removing the DNS entry and using a CNAME record does not make the APIs accessible only from the VPC. The APIs would still be publicly accessible, just through a different endpoint.\n\nD. Deploy the Lambda functions inside the VPC and use an EC2 instance:\n   - Using an EC2 instance as a proxy to call the Lambda functions is an overly complex solution and does not directly address the requirement of making the APIs accessible only from the VPC.\n   - This solution also introduces an additional component (the EC2 instance) that needs to be managed and secured, which increases the overall operational complexity.\n\nIn summary, the correct answer (C) is the most straightforward and effective solution to meet the given requirements, as it leverages the VPC endpoint feature of API Gateway to restrict access to the set of APIs to only the VPC, while still allowing for authenticated user access."
  },
  "143": {
    "question": "A weather service provides high-resolution weather maps from a web application hosted on AWS in the eu-west-1 Region. The weather maps are updated frequently and stored in Amazon S3 along with static HTML content. The web application is fronted by Amazon CloudFront.The company recently expanded to serve users in the us-east-1 Region, and these new users report that viewing their respective weather maps is slow from time to time.Which combination of steps will resolve the us-east-1 performance issues? (Choose two.)",
    "choices": [
      "A. Configure the AWS Global Accelerator endpoint for the S3 bucket in eu-west-1. Configure endpoint groups for TCP ports 80 and 443 in us-east-1.",
      "B. Create a new S3 bucket in us-east-1. Configure S3 cross-Region replication to synchronize from the S3 bucket in eu-west-1.",
      "C. Use Lambda@Edge to modify requests from North America to use the S3 Transfer Acceleration endpoint in us-east-1.",
      "D. Use Lambda@Edge to modify requests from North America to use the S3 bucket in us-east-1.",
      "E. Configure the AWS Global Accelerator endpoint for us-east-1 as an origin on the CloudFront distribution. Use Lambda@Edge to modify requests from North America to use the new origin."
    ],
    "answer": "BD",
    "explanation": "1. Explanation of the correct answer (BD):\n\nB. Create a new S3 bucket in us-east-1. Configure S3 cross-Region replication to synchronize from the S3 bucket in eu-west-1.\nThis is the correct choice because creating a new S3 bucket in the us-east-1 Region and configuring cross-Region replication from the existing S3 bucket in eu-west-1 will allow users in the us-east-1 Region to access the weather maps from a closer location, improving performance. Cross-Region replication ensures that the data is kept in sync between the two S3 buckets.\n\nD. Use Lambda@Edge to modify requests from North America to use the S3 bucket in us-east-1.\nThis is also a correct choice because using Lambda@Edge to modify requests from North America to use the S3 bucket in us-east-1 will achieve the same goal of allowing users in the us-east-1 Region to access the weather maps from a closer location, improving performance.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Configure the AWS Global Accelerator endpoint for the S3 bucket in eu-west-1. Configure endpoint groups for TCP ports 80 and 443 in us-east-1.\nThis is incorrect because AWS Global Accelerator cannot have an S3 bucket as an endpoint. Global Accelerator is designed to work with IP addresses or DNS names, not S3 buckets.\n\nC. Use Lambda@Edge to modify requests from North America to use the S3 Transfer Acceleration endpoint in us-east-1.\nThis is incorrect because S3 Transfer Acceleration is used to accelerate PUT requests to an S3 bucket, not GET requests for retrieving the weather maps. Additionally, the S3 Transfer Acceleration endpoint is a global endpoint, not a regional one, so it would not provide any performance benefit for users in the us-east-1 Region.\n\nE. Configure the AWS Global Accelerator endpoint for us-east-1 as an origin on the CloudFront distribution. Use Lambda@Edge to modify requests from North America to use the new origin.\nThis is incorrect because using Global Accelerator as an origin for Clou"
  },
  "144": {
    "question": "A solutions architect is investigating an issue in which a company cannot establish new sessions in Amazon Workspaces. An initial analysis indicates that the issue involves user profiles. The Amazon Workspaces environment is configured to use Amazon FSx for Windows File Server as the profile share storage. The FSx for Windows File Server file system is configured with 10 TB of storage.The solutions architect discovers that the file system has reached Its maximum capacity. The solutions architect must ensure that users can regain access. The solution also must prevent the problem from occurring again.Which solution will meet these requirements?",
    "choices": [
      "A. Remove old user profiles to create space. Migrate the user profiles to an Amazon FSx for Lustre file system.",
      "B. Increase capacity by using the update-file-system command. Implement an Amazon CloudWatch metric that monitors free space. Use Amazon EventBridge to invoke an AWS Lambda function to increase capacity as required.",
      "C. Monitor the file system by using the FreeStorageCapacity metric in Amazon CloudWatch. Use AWS Step Functions to increase the capacity as required.",
      "D. Remove old user profiles to create space. Create an additional FSx for Windows File Server file system. Update the user profile redirection for 50% of the users to use the new file system."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is option B. This solution effectively addresses the current capacity issue and prevents the problem from occurring again.\n\nHere's the explanation:\n\n- The key steps in this solution are:\n  1. Increase the capacity of the existing FSx for Windows File Server file system using the `update-file-system` command.\n  2. Implement an Amazon CloudWatch metric that monitors the free storage capacity of the file system.\n  3. Use Amazon EventBridge to trigger an AWS Lambda function when the free storage capacity drops below a specified threshold, which can then automatically increase the file system capacity as required.\n\n- This approach solves the immediate issue by increasing the file system capacity, and it also proactively monitors the available storage and scales the file system as needed. This prevents the system from reaching its maximum capacity again, ensuring that users can regain access to their profiles.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Incorrect: Migrating the user profiles to an Amazon FSx for Lustre file system is not an appropriate solution, as Amazon FSx for Lustre is optimized for high-performance computing workloads, not user profiles. Additionally, manually removing old user profiles may not be a reliable long-term solution, as the issue could recur.\n\nC. Incorrect: While using AWS Step Functions to increase the file system capacity is a valid approach, the question specifically states that the \"solutions architect must ensure that users can regain access\" and \"the solution also must prevent the problem from occurring again.\" The use of Step Functions alone does not address the immediate capacity issue or provide a proactive monitoring and scaling solution to prevent the problem from happening again.\n\nD. Incorrect: Creating an additional FSx for Windows File Server file system and updating user profile redirection for a portion of the users may temporarily alleviate the issue, but it does not provide a long-term solution to prevent the problem from occurring again. This approach may also be more complex to implement and maintain compared to the solution in option B."
  },
  "145": {
    "question": "An international delivery company hosts a delivery management system on AWS. Drivers use the system to upload confirmation of delivery. Confirmation includes the recipient\u2019s signature or a photo of the package with the recipient. The driver\u2019s handheld device uploads signatures and photos through FTP to a single Amazon EC2 instance. Each handheld device saves a file in a directory based on the signed-in user, and the file name matches the delivery number. The EC2 instance then adds metadata to the file after querying a central database to pull delivery information. The file is then placed in Amazon S3 for archiving.As the company expands, drivers report that the system is rejecting connections. The FTP server is having problems because of dropped connections and memory issues in response to these problems, a system engineer schedules a cron task to reboot the EC2 instance every 30 minutes. The billing team reports that files are not always in the archive and that the central system is not always updated.A solutions architect needs to design a solution that maximizes scalability to ensure that the archive always receives the files and that systems are always updated. The handheld devices cannot be modified, so the company cannot deploy a new application.Which solution will meet these requirements?",
    "choices": [
      "A. Create an AMI of the existing EC2 instance. Create an Auto Scaling group of EC2 instances behind an Application Load Balancer. Configure the Auto Scaling group to have a minimum of three instances.",
      "B. Use AWS Transfer Family to create an FTP server that places the files in Amazon Elastic File System (Amazon EFS). Mount the EFS volume to the existing EC2 instance. Point the EC2 instance to the new path for file processing.",
      "C. Use AWS Transfer Family to create an FTP server that places the files in Amazon S3. Use an S3 event notification through Amazon Simple Notification Service (Amazon SNS) to invoke an AWS Lambda function. Configure the Lambda function to add the metadata and update the delivery system.",
      "D. Update the handheld devices to place the files directly in Amazon S3. Use an S3 event notification through Amazon Simple Queue Service (Amazon SQS) to invoke an AWS Lambda function. Configure the Lambda function to add the metadata and update the delivery system."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C because it addresses the key requirements of the problem statement:\n\n- Maximizing scalability: Using AWS Transfer Family to create an FTP server that places the files directly in Amazon S3 eliminates the need for a single EC2 instance to handle all the FTP connections. This allows for better scalability and resilience.\n\n- Ensuring the archive always receives the files: By placing the files directly in Amazon S3, the delivery confirmations are stored in a scalable and highly available storage solution, ensuring the files are always archived.\n\n- Updating the central system: The solution uses an S3 event notification through Amazon SNS to trigger an AWS Lambda function. This Lambda function can then add the necessary metadata and update the central delivery system, ensuring the system is always updated.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option still relies on a single EC2 instance, even though it uses an Auto Scaling group. The root cause of the issue (the FTP server on the EC2 instance) is not addressed, and the system may still experience scalability and reliability issues.\n\nB. This option uses AWS Transfer Family to create an FTP server, but it still stores the files on an Amazon EFS volume, which is mounted to the existing EC2 instance. This does not address the scalability and reliability issues with the FTP server on the EC2 instance, and the central delivery system may still not be updated consistently.\n\nD. This option requires modifying the handheld devices, which is not possible according to the problem statement. Additionally, using Amazon SQS and Lambda to process the files may not be as efficient as the approach in C, which uses S3 event notifications directly."
  },
  "146": {
    "question": "A company is running an application in the AWS Cloud. The application runs on containers m an Amazon Elastic Container Service (Amazon ECS) cluster. The ECS tasks use the Fargate launch type. The application's data is relational and is stored in Amazon Aurora MySQL. To meet regulatory requirements, the application must be able to recover to a separate AWS Region in the event of an application failure. In case of a failure, no data can be lost.Which solution will meet these requirements with the LEAST amount of operational overhead?",
    "choices": [
      "A. Provision an Aurora Replica in a different Region.",
      "B. Set up AWS DataSync for continuous replication of the data to a different Region.",
      "C. Set up AWS Database Migration Service (AWS DMS) to perform a continuous replication of the data to a different Region.",
      "D. Use Amazon Data Lifecycle Manager (Amazon DLM) to schedule a snapshot every 5 minutes."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\nThe correct answer is A, \"Provision an Aurora Replica in a different Region.\" This solution meets the requirements with the least amount of operational overhead because:\n\n- Aurora provides built-in, automatic replication of data between the primary database and the replica in a different Region. This ensures that in the event of an application failure, the data can be recovered from the replica with no data loss.\n- Aurora's replication is a fully managed service, requiring minimal operational overhead. The replication happens automatically, and you don't need to set up and manage a separate replication process.\n- Promoting the Aurora Replica to become the new primary database in case of a failure is a straightforward process, further reducing the operational overhead compared to other solutions.\n\n2. Explanations of the incorrect choices:\n\nB. \"Set up AWS DataSync for continuous replication of the data to a different Region\":\n- AWS DataSync is not a fully managed service and requires more configuration and management overhead compared to Aurora's built-in replication.\n- Setting up and managing a separate data replication process using DataSync adds additional operational complexity.\n\nC. \"Set up AWS Database Migration Service (AWS DMS) to perform a continuous replication of the data to a different Region\":\n- AWS DMS is a powerful tool for migrating data between databases, but it is not designed for continuous real-time replication like Aurora's built-in replication.\n- Setting up and managing a separate data replication process using DMS adds additional operational complexity.\n\nD. \"Use Amazon Data Lifecycle Manager (Amazon DLM) to schedule a snapshot every 5 minutes\":\n- Amazon DLM can be used for scheduling snapshots, but it does not provide real-time replication and may not meet the requirement of no data loss in case of a failure.\n- Relying on scheduled snapshots alone does not provide the same level of data protection and recovery capabilities as Aurora's built-in replication."
  },
  "147": {
    "question": "A financial services company receives a regular data feed from its credit card servicing partner. Approximately 5,000 records are sent every 15 minutes in plaintext, delivered over HTTPS directly into an Amazon S3 bucket with server-side encryption. This feed contains sensitive credit card primary account number (PAN) data. The company needs to automatically mask the PAN before sending the data to another S3 bucket for additional internal processing. The company also needs to remove and merge specific fields, and then transform the record into JSON format. Additionally, extra feeds are likely to be added in the future, so any design needs to be easily expandable.Which solutions will meet these requirements?",
    "choices": [
      "A. Invoke an AWS Lambda function on file delivery that extracts each record and writes it to an Amazon SQS queue. Invoke another Lambda function when new messages arrive in the SQS queue to process the records, writing the results to a temporary location in Amazon S3. Invoke a final Lambda function once the SQS queue is empty to transform the records into JSON format and send the results to another S3 bucket for internal processing.",
      "B. Invoke an AWS Lambda function on file delivery that extracts each record and writes it to an Amazon SQS queue. Configure an AWS Fargate container application to automatically scale to a single instance when the SQS queue contains messages. Have the application process each record, and transform the record into JSON format. When the queue is empty, send the results to another S3 bucket for internal processing and scale down the AWS Fargate instance.",
      "C. Create an AWS Glue crawler and custom classifier based on the data feed formats and build a table definition to match. Invoke an AWS Lambda function on file delivery to start an AWS Glue ETL job to transform the entire record according to the processing and transformation requirements. Define the output format as JSON. Once complete, have the ETL job send the results to another S3 bucket for internal processing.",
      "D. Create an AWS Glue crawler and custom classifier based upon the data feed formats and build a table definition to match. Perform an Amazon Athena query on file delivery to start an Amazon EMR ETL job to transform the entire record according to the processing and transformation requirements. Define the output format as JSON. Once complete, send the results to another S3 bucket for internal processing and scale down the EMR cluster."
    ],
    "answer": "C",
    "explanation": "1. Explanation of why the correct answer (C) is right:\n\nThe correct answer is C because it leverages AWS Glue, which is the most appropriate service for the given requirements. Here's why:\n\n- AWS Glue can handle the required ETL (Extract, Transform, Load) tasks, including masking sensitive data (PAN), merging and removing specific fields, and transforming the data into JSON format.\n- AWS Glue's Crawler and Custom Classifier feature allows it to easily adapt to new data feed formats that may be added in the future, making the solution easily expandable.\n- By using an AWS Glue ETL job triggered by an AWS Lambda function, the solution can efficiently process the data in a batch manner, without the need for complex orchestration using multiple Lambda functions and queues (as in option A) or the potential overhead of a Fargate container (as in option B).\n- Compared to using Athena and Amazon EMR (option D), AWS Glue is better suited for this type of ETL workload, as it is designed specifically for data preparation and transformation tasks.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option uses multiple Lambda functions, an SQS queue, and a temporary S3 location, which adds unnecessary operational overhead and complexity. The use of Lambda and SQS is not the most efficient approach for the given data processing requirements.\n\nB. Using AWS Fargate may not be the most cost-effective solution, as it is more suitable for long-running applications. Additionally, it may not be the best choice for handling large amounts of data, as it can be more difficult to scale and manage than a dedicated data processing service like AWS Glue.\n\nD. While Athena and Amazon EMR are powerful tools, they are more suitable for large-scale, complex data processing tasks. For the given requirements, which involve relatively small data volumes (5,000 records every 15 minutes) and straightforward ETL operations, these services may be overkill and less efficient than the AWS Glue-based solution in option C."
  },
  "148": {
    "question": "A company wants to use AWS to create a business continuity solution in case the company's main on-premises application fails. The application runs on physical servers that also run other applications. The on-premises application that the company is planning to migrate uses a MySQL database as a data store. All the company's on-premises applications use operating systems that are compatible with Amazon EC2.Which solution will achieve the company's goal with the LEAST operational overhead?",
    "choices": [
      "A. Install the AWS Replication Agent on the source servers, including the MySQL servers. Set up replication for all servers. Launch test instances for regular drills. Cut over to the test instances to fail over the workload in the case of a failure event.",
      "B. Install the AWS Replication Agent on the source servers, including the MySQL servers. Initialize AWS Elastic Disaster Recovery in the target AWS Region. Define the launch settings. Frequently perform failover and fallback from the most recent point in time.",
      "C. Create AWS Database Migration Service (AWS DMS) replication servers and a target Amazon Aurora MySQL DB cluster to host the database. Create a DMS replication task to copy the existing data to the target DB cluster. Create a local AWS Schema Conversion Tool (AWS SCT) change data capture (CDC) task to keep the data synchronized. Install the rest of the software on EC2 instances by starting with a compatible base AMI.",
      "D. Deploy an AWS Storage Gateway Volume Gateway on premises. Mount volumes on all on-premises servers. Install the application and the MySQL database on the new volumes. Take regular snapshots. Install all the software on EC2 Instances by starting with a compatible base AMI. Launch a Volume Gateway on an EC2 instance. Restore the volumes from the latest snapshot. Mount the new volumes on the EC2 instances in the case of a failure event."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B because it aligns best with the given requirements and has the least operational overhead compared to the other options.\n\nOption B describes using the AWS Elastic Disaster Recovery (AWS DRS) service, which allows you to replicate your on-premises servers, including the MySQL database, to the AWS cloud. This approach has the following benefits:\n\n- It uses the AWS Replication Agent, which is easy to install and configure on the source servers, minimizing operational overhead.\n- It allows you to define the launch settings for the replicated instances in AWS, streamlining the failover process.\n- It enables you to perform frequent failover and fallback tests from the most recent point in time, ensuring a reliable and well-tested disaster recovery solution.\n\nThis approach meets the company's goal of creating a business continuity solution in case the on-premises application fails, with the least operational overhead compared to the other options.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option involves setting up replication for all servers, including the MySQL servers, which adds more operational complexity compared to option B. The requirement is to create a business continuity solution, not a full-fledged replication setup.\n\nC. This option involves using AWS Database Migration Service (AWS DMS) and AWS Schema Conversion Tool (AWS SCT), which are more complex and tailored for full application migrations. The requirement is for a disaster recovery solution, not a complete migration.\n\nD. This option involves deploying an AWS Storage Gateway Volume Gateway on-premises, which adds additional complexity and operational overhead. It also requires restoring volumes from snapshots in the event of a failure, which may not be as efficient as the failover process provided by AWS DRS."
  },
  "149": {
    "question": "A company is subject to regulatory audits of its financial information. External auditors who use a single AWS account need access to the company's AWS account. A solutions architect must provide the auditors with secure, read-only access to the company's AWS account. The solution must comply with AWS security best practices.Which solution will meet these requirements?",
    "choices": [
      "A. In the company's AWS account, create resource policies for all resources in the account to grant access to the auditors' AWS account. Assign a unique external ID to the resource policy.",
      "B. In the company's AWS account, create an IAM role that trusts the auditors' AWS account. Create an IAM policy that has the required permissions. Attach the policy to the role. Assign a unique external ID to the role's trust policy.",
      "C. In the company's AWS account, create an IAM user. Attach the required IAM policies to the IAM user. Create API access keys for the IAM user. Share the access keys with the auditors.",
      "D. In the company's AWS account, create an IAM group that has the required permissions. Create an IAM user in the company's account for each auditor. Add the IAM users to the IAM group."
    ],
    "answer": "B",
    "explanation": "Here is a clear explanation of why the correct answer (B) is right, and brief explanations of why each incorrect choice is wrong:\n\n1. Explanation of the correct answer (B):\nThe correct answer is B, which recommends creating an IAM role that trusts the auditors' AWS account and attaching the required IAM policies to the role. This is the best solution for the following reasons:\n\n- It follows AWS security best practices by granting the auditors read-only access to the company's AWS account through an IAM role, rather than providing direct access through IAM users or access keys.\n- The IAM role's trust policy is configured to trust the auditors' AWS account, ensuring that only the authorized auditors can assume the role and access the company's resources.\n- Assigning a unique external ID to the role's trust policy adds an extra layer of security, making it more difficult for unauthorized parties to assume the role.\n- This solution ensures that the auditors have the necessary permissions to perform their audits while minimizing the risk of unauthorized access or data exposure.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This solution of creating resource policies for all resources in the account is not recommended, as it would be cumbersome to manage and maintain. Additionally, using a resource policy to grant access to an external AWS account is not a common or recommended practice.\n\nC. Providing the auditors with IAM user access keys is not a secure solution, as it could lead to the keys being misused or shared with unauthorized parties. IAM users should be used for internal users and employees, not external third-party auditors.\n\nD. Creating IAM users for each auditor and adding them to a group with the required permissions is not the best solution, as it would be difficult to manage and revoke access for a potentially large number of auditors. It also does not follow the principle of least privilege, as the auditors would have unnecessary permissions beyond just read-only access."
  },
  "150": {
    "question": "A company has a latency-sensitive trading platform that uses Amazon DynamoDB as a storage backend. The company configured the DynamoDB table to use on-demand capacity mode. A solutions architect needs to design a solution to improve the performance of the trading platform. The new solution must ensure high availability for the trading platform.Which solution will meet these requirements with the LEAST latency?",
    "choices": [
      "A. Create a two-node DynamoDB Accelerator (DAX) cluster. Configure an application to read and write data by using DAX.",
      "B. Create a three-node DynamoDB Accelerator (DAX) cluster. Configure an application to read data by using DAX and to write data directly to the DynamoDB table.",
      "C. Create a three-node DynamoDB Accelerator (DAX) cluster. Configure an application to read data directly from the DynamoDB table and to write data by using DAX.",
      "D. Create a single-node DynamoDB Accelerator (DAX) cluster. Configure an application to read data by using DAX and to write data directly to the DynamoDB table."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B, as it provides the solution that meets the requirements of high availability and the least latency for the latency-sensitive trading platform.\n\nExplanation:\n- A three-node DynamoDB Accelerator (DAX) cluster is required to achieve high availability, as a DAX cluster needs at least three nodes to be fault-tolerant.\n- Configuring the application to read data using DAX and write data directly to the DynamoDB table ensures low latency for both read and write operations.\n- Writing directly to the DynamoDB table ensures that the data is immediately available, without the potential for stale data that could occur with caching in DAX.\n- Reading from the DAX cluster provides low-latency responses for the latency-sensitive trading platform.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Create a two-node DynamoDB Accelerator (DAX) cluster. Configure an application to read and write data by using DAX.\n- A two-node DAX cluster is not fault-tolerant and does not meet the high availability requirement.\n\nC. Create a three-node DynamoDB Accelerator (DAX) cluster. Configure an application to read data directly from the DynamoDB table and to write data by using DAX.\n- Writing data using DAX introduces additional latency, as the data needs to be written to both the DynamoDB table and the DAX cluster. This does not meet the requirement of the least latency.\n\nD. Create a single-node DynamoDB Accelerator (DAX) cluster. Configure an application to read data by using DAX and to write data directly to the DynamoDB table.\n- A single-node DAX cluster is not fault-tolerant and does not meet the high availability requirement."
  },
  "151": {
    "question": "A company has migrated an application from on premises to AWS. The application frontend is a static website that runs on two Amazon EC2 instances behind an Application Load Balancer (ALB). The application backend is a Python application that runs on three EC2 instances behind another ALB. The EC2 instances are large, general purpose On-Demand Instances that were sized to meet the on-premises specifications for peak usage of the application.The application averages hundreds of thousands of requests each month. However, the application is used mainly during lunchtime and receives minimal traffic during the rest of the day.A solutions architect needs to optimize the infrastructure cost of the application without negatively affecting the application availability.Which combination of steps will meet these requirements? (Choose two.)",
    "choices": [
      "A. Change all the EC2 instances to compute optimized instances that have the same number of cores as the existing EC2 instances.",
      "B. Move the application frontend to a static website that is hosted on Amazon S3.",
      "C. Deploy the application frontend by using AWS Elastic Beanstalk. Use the same instance type for the nodes.",
      "D. Change all the backend EC2 instances to Spot Instances.",
      "E. Deploy the backend Python application to general purpose burstable EC2 instances that have the same number of cores as the existing EC2 instances."
    ],
    "answer": "BE",
    "explanation": "1. Explanation of the correct answer:\n\nB. Move the application frontend to a static website that is hosted on Amazon S3.\n- This is a good optimization because S3 is a highly scalable and cost-effective storage service, which is well-suited for hosting static website content. By moving the frontend to S3, you can reduce the infrastructure costs associated with the frontend EC2 instances.\n\nE. Deploy the backend Python application to general purpose burstable EC2 instances that have the same number of cores as the existing EC2 instances.\n- Burstable instances (such as T2 or T3 instances) are a cost-effective option for workloads with variable CPU utilization patterns, which fits the given scenario where the application experiences peak traffic during lunchtime and minimal traffic during the rest of the day. Burstable instances can provide the necessary CPU performance during peak periods while costing less during off-peak periods.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Change all the EC2 instances to compute optimized instances that have the same number of cores as the existing EC2 instances.\n- This is not the most cost-effective solution because compute-optimized instances are designed for workloads that require sustained high CPU performance, which is not the case in the given scenario where the application has variable CPU utilization.\n\nC. Deploy the application frontend by using AWS Elastic Beanstalk. Use the same instance type for the nodes.\n- Elastic Beanstalk is a higher-level service that provides automatic scaling and management of the underlying infrastructure. While it can be a good option in some cases, it may not be the most cost-effective solution in this scenario, as the manual optimization of instance types and scaling would likely provide better cost savings.\n\nD. Change all the backend EC2 instances to Spot Instances.\n- Spot Instances can provide significant cost savings, but they are subject to interruption and may not be suitable for mission-critical components of the application, such as the backend. The question specifically states that the solution should not negatively affect the application availability, and Spot Instances may not meet this requirement."
  },
  "152": {
    "question": "A company is running an event ticketing platform on AWS and wants to optimize the platform's cost-effectiveness. The platform is deployed on Amazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 and is backed by an Amazon RDS for MySQL DB instance. The company is developing new application features to run on Amazon EKS with AWS Fargate.The platform experiences infrequent high peaks in demand. The surges in demand depend on event dates.Which solution will provide the MOST cost-effective setup for the platform?",
    "choices": [
      "A. Purchase Standard Reserved Instances for the EC2 instances that the EKS cluster uses in its baseline load. Scale the cluster with Spot Instances to handle peaks. Purchase 1-year All Upfront Reserved Instances for the database to meet predicted peak load for the year.",
      "B. Purchase Compute Savings Plans for the predicted medium load of the EKS cluster. Scale the cluster with On-Demand Capacity Reservations based on event dates for peaks. Purchase 1-year No Upfront Reserved Instances for the database to meet the predicted base load. Temporarily scale out database read replicas during peaks.",
      "C. Purchase EC2 Instance Savings Plans for the predicted base load of the EKS cluster. Scale the cluster with Spot Instances to handle peaks. Purchase 1-year All Upfront Reserved Instances for the database to meet the predicted base load. Temporarily scale up the DB instance manually during peaks.",
      "D. Purchase Compute Savings Plans for the predicted base load of the EKS cluster. Scale the cluster with Spot Instances to handle peaks. Purchase 1-year All Upfront Reserved Instances for the database to meet the predicted base load. Temporarily scale up the DB instance manually during peaks."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B because it provides the most cost-effective setup for the event ticketing platform:\n\n- Compute Savings Plans for the predicted medium load of the EKS cluster: This allows the company to save on compute costs for the baseline load of the EKS cluster running on Fargate.\n- On-Demand Capacity Reservations based on event dates for peaks: This allows the company to scale the EKS cluster during high-demand periods without paying for unused capacity.\n- 1-year No Upfront Reserved Instances for the database to meet the predicted base load: This provides cost savings for the database's base load.\n- Temporarily scale out database read replicas during peaks: This allows the company to handle increased database load during high-demand periods without manually scaling up the DB instance.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option is not optimal because:\n- Using Reserved Instances for the EC2 instances in the EKS cluster is not the best approach, as the company is moving to Fargate, which does not support Reserved Instances.\n- Scaling the cluster with Spot Instances to handle peaks is not recommended for a production system, as Spot Instances can be interrupted.\n- Purchasing 1-year All Upfront Reserved Instances for the database to meet the predicted peak load is not cost-effective, as the company can temporarily scale out read replicas during peaks.\n\nC. This option is not optimal because:\n- Purchasing EC2 Instance Savings Plans for the predicted base load of the EKS cluster is not as cost-effective as Compute Savings Plans, which can cover both EC2 and Fargate.\n- Manually scaling up the DB instance during peaks is not as efficient as temporarily scaling out database read replicas.\n\nD. This option is not optimal because:\n- Manually scaling up the DB instance during peaks is not as efficient as temporarily scaling out database read replicas.\n\nIn summary, the correct answer (B) provides the most cost-effective setup by leveraging Compute Savings Plans, On-Demand Capacity Reservations, and temporary database scaling, which aligns with the company's move to Fargate and the predicted load patterns"
  },
  "153": {
    "question": "A company has deployed an application on AWS Elastic Beanstalk. The application uses Amazon Aurora for the database layer. An Amazon CloudFront distribution serves web requests and includes the Elastic Beanstalk domain name as the origin server. The distribution is configured with an alternate domain name that visitors use when they access the application.Each week, the company takes the application out of service for routine maintenance. During the time that the application is unavailable, the company wants visitors to receive an informational message instead of a CloudFront error message.A solutions architect creates an Amazon S3 bucket as the first step in the process.Which combination of steps should the solutions architect take next to meet the requirements? (Choose three.)",
    "choices": [
      "A. Upload static informational content to the S3 bucket.",
      "B. Create a new CloudFront distribution. Set the S3 bucket as the origin.",
      "C. Set the S3 bucket as a second origin in the original CloudFront distribution. Configure the distribution and the S3 bucket to use an origin access identity (OAI).",
      "D. During the weekly maintenance, edit the default cache behavior to use the S3 origin. Revert the change when the maintenance is complete.",
      "E. During the weekly maintenance, create a cache behavior for the S3 origin on the new distribution. Set the path pattern to \\ Set the precedence to 0. Delete the cache behavior when the maintenance is complete.",
      "F. During the weekly maintenance, configure Elastic Beanstalk to serve traffic from the S3 bucket."
    ],
    "answer": "ACD",
    "explanation": "1. Clear explanation of why the correct answer is right:\n\nThe correct answer is A, C, and D because:\n\nA. Uploading static informational content to the S3 bucket is the first step to provide a custom message to users during the weekly maintenance. This content will be displayed instead of the default CloudFront error message.\n\nC. Setting the S3 bucket as a second origin in the original CloudFront distribution, and configuring the distribution and the S3 bucket to use an Origin Access Identity (OAI), ensures that only CloudFront has access to the S3 bucket. This secures the content in the S3 bucket.\n\nD. Editing the default cache behavior to use the S3 origin during the weekly maintenance, and then reverting the change when the maintenance is complete, allows CloudFront to seamlessly switch between the Elastic Beanstalk origin and the S3 origin. This ensures that the static informational content is displayed to users when the application is unavailable.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nB. Creating a new CloudFront distribution and setting the S3 bucket as the origin is unnecessary. The existing CloudFront distribution can be configured to use the S3 bucket as a second origin.\n\nE. Creating a cache behavior for the S3 origin on a new distribution during the weekly maintenance is more complex and prone to human error. It's better to use the existing CloudFront distribution and edit the default cache behavior.\n\nF. Configuring Elastic Beanstalk to serve traffic from the S3 bucket is not necessary, as CloudFront is already being used as the web request server. The solution should focus on configuring CloudFront to display the static informational content from the S3 bucket during maintenance."
  },
  "154": {
    "question": "A company gives users the ability to upload images from a custom application. The upload process invokes an AWS Lambda function that processes and stores the image in an Amazon S3 bucket. The application invokes the Lambda function by using a specific function version ARN.The Lambda function accepts image processing parameters by using environment variables. The company often adjusts the environment variables of the Lambda function to achieve optimal image processing output. The company tests different parameters and publishes a new function version with the updated environment variables after validating results. This update process also requires frequent changes to the custom application to invoke the new function version ARN. These changes cause interruptions for users.A solutions architect needs to simplify this process to minimize disruption to users.Which solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      "A. Directly modify the environment variables of the published Lambda function version. Use the SLATEST version to test image processing parameters.",
      "B. Create an Amazon DynamoDB table to store the image processing parameters. Modify the Lambda function to retrieve the image processing parameters from the DynamoDB table.",
      "C. Directly code the image processing parameters within the Lambda function and remove the environment variables. Publish a new function version when the company updates the parameters.",
      "D. Create a Lambda function alias. Modify the client application to use the function alias ARN. Reconfigure the Lambda alias to point to new versions of the function when the company finishes testing."
    ],
    "answer": "D",
    "explanation": "1. Clear explanation of why the correct answer (D) is right:\n\nThe correct answer is D because using a Lambda function alias simplifies the process of updating the image processing parameters with the least operational overhead. By modifying the client application to use the function alias ARN, the custom application can continue to invoke the latest version of the Lambda function without the need to update the application code every time the company updates the image processing parameters. This reduces the risk of causing interruptions for users. The solutions architect can test different parameters by using different versions of the function and reconfigure the alias to point to the new version after validating the results, without affecting the client application.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nA. Directly modifying the environment variables of the published Lambda function version would cause all clients to use the updated environment variables immediately, without the ability to test different parameters.\n\nB. Using DynamoDB to store image processing parameters increases complexity and operational overhead, and it would not eliminate the need for updating the custom application.\n\nC. Directly coding the image processing parameters within the Lambda function and publishing new versions would not eliminate the need for updating the custom application."
  },
  "155": {
    "question": "A global media company is planning a multi-Region deployment of an application. Amazon DynamoDB global tables will back the deployment to keep the user experience consistent across the two continents where users are concentrated. Each deployment will have a public Application Load Balancer (ALB). The company manages public DNS internally. The company wants to make the application available through an apex domain.Which solution will meet these requirements with the LEAST effort?",
    "choices": [
      "A. Migrate public DNS to Amazon Route 53. Create CNAME records for the apex domain to point to the ALB. Use a geolocation routing policy to route traffic based on user location.",
      "B. Place a Network Load Balancer (NLB) in front of the ALMigrate public DNS to Amazon Route 53. Create a CNAME record for the apex domain to point to the NLB\u2019s static IP address. Use a geolocation routing policy to route traffic based on user location.",
      "C. Create an AWS Global Accelerator accelerator with multiple endpoint groups that target endpoints in appropriate AWS Regions. Use the accelerator\u2019s static IP address to create a record in public DNS for the apex domain.",
      "D. Create an Amazon API Gateway API that is backed by AWS Lambda in one of the AWS Regions. Configure a Lambda function to route traffic to application deployments by using the round robin method. Create CNAME records for the apex domain to point to the API's URL."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\nThe correct answer is C because it uses AWS Global Accelerator, which is the most appropriate solution to meet the requirements with the least effort. Global Accelerator provides a static IP address that can be used to create a DNS record for the apex domain, which solves the issue of not being able to use a CNAME record for an apex domain. Global Accelerator also automatically routes traffic to the appropriate application deployment based on user location, which satisfies the requirement for a consistent user experience across the two continents.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This solution is incorrect because, as mentioned in the community discussions, DNS standards do not allow the use of CNAME records for apex domains. Instead, A or AAAA records must be used.\n\nB. This solution is incorrect because, similar to option A, it involves using a CNAME record for the apex domain, which is not allowed.\n\nD. This solution is incorrect because it involves creating an Amazon API Gateway API, which is not the most suitable option for the given requirements. The question specifically mentions the need for a consistent user experience across the two continents, and routing traffic through an API Gateway may not be the most efficient approach to achieve this."
  },
  "156": {
    "question": "A company is developing a new serverless API by using Amazon API Gateway and AWS Lambda. The company integrated the Lambda functions with API Gateway to use several shared libraries and custom classes.A solutions architect needs to simplify the deployment of the solution and optimize for code reuse.Which solution will meet these requirements?",
    "choices": [
      "A. Deploy the shared libraries and custom classes into a Docker image. Store the image in an S3 bucket. Create a Lambda layer that uses the Docker image as the source. Deploy the API's Lambda functions as Zip packages. Configure the packages to use the Lambda layer.",
      "B. Deploy the shared libraries and custom classes to a Docker image. Upload the image to Amazon Elastic Container Registry (Amazon ECR). Create a Lambda layer that uses the Docker image as the source. Deploy the API's Lambda functions as Zip packages. Configure the packages to use the Lambda layer.",
      "C. Deploy the shared libraries and custom classes to a Docker container in Amazon Elastic Container Service (Amazon ECS) by using the AWS Fargate launch type. Deploy the API's Lambda functions as Zip packages. Configure the packages to use the deployed container as a Lambda layer.",
      "D. Deploy the shared libraries, custom classes, and code for the API's Lambda functions to a Docker image. Upload the image to Amazon Elastic Container Registry (Amazon ECR). Configure the API's Lambda functions to use the Docker image as the deployment package."
    ],
    "answer": "D",
    "explanation": "Here's a clear explanation of the correct answer and brief explanations of why the incorrect choices are wrong:\n\n1. Explanation of the correct answer (D):\n   - The correct answer is D: Deploy the shared libraries, custom classes, and code for the API's Lambda functions to a Docker image. Upload the image to Amazon Elastic Container Registry (Amazon ECR). Configure the API's Lambda functions to use the Docker image as the deployment package.\n   - This is the correct solution because it simplifies the deployment of the serverless API and optimizes for code reuse.\n   - By packaging the shared libraries, custom classes, and Lambda function code into a Docker image and storing it in Amazon ECR, the solution can be easily deployed and scaled across multiple Lambda functions.\n   - Using a Docker image as the deployment package eliminates the need for managing separate Lambda layers or Zip packages, simplifying the deployment process.\n   - The Docker image approach also allows for efficient code reuse, as the shared libraries and custom classes can be easily shared across multiple Lambda functions.\n\n2. Explanations of why the incorrect choices are wrong:\n   - A is incorrect because a Docker image cannot be used as the source for a Lambda layer. Lambda layers only support .zip or .jar files as the source.\n   - B is incorrect because, similar to A, a Docker image cannot be used as the source for a Lambda layer.\n   - C is incorrect because it suggests deploying the shared libraries and custom classes to a Docker container in Amazon Elastic Container Service (Amazon ECS) using the AWS Fargate launch type. This approach does not directly integrate with API Gateway and Lambda, and it would require additional configuration and management overhead compared to the correct solution.\n\nIn summary, the correct answer (D) is the best solution because it simplifies the deployment of the serverless API and optimizes for code reuse by using a Docker image as the deployment package, which can be easily integrated with API Gateway and Lambda."
  },
  "157": {
    "question": "A manufacturing company is building an inspection solution for its factory. The company has IP cameras at the end of each assembly line. The company has used Amazon SageMaker to train a machine learning (ML) model to identify common defects from still images.The company wants to provide local feedback to factory workers when a defect is detected. The company must be able to provide this feedback even if the factory\u2019s internet connectivity is down. The company has a local Linux server that hosts an API that provides local feedback to the workers.How should the company deploy the ML model to meet these requirements?",
    "choices": [
      "A. Set up an Amazon Kinesis video stream from each IP camera to AWS. Use Amazon EC2 instances to take still images of the streams. Upload the images to an Amazon S3 bucket. Deploy a SageMaker endpoint with the ML model. Invoke an AWS Lambda function to call the inference endpoint when new images are uploaded. Configure the Lambda function to call the local API when a defect is detected.",
      "B. Deploy AWS IoT Greengrass on the local server. Deploy the ML model to the Greengrass server. Create a Greengrass component to take still images from the cameras and run inference. Configure the component to call the local API when a defect is detected.",
      "C. Order an AWS Snowball device. Deploy a SageMaker endpoint the ML model and an Amazon EC2 instance on the Snowball device. Take still images from the cameras. Run inference from the EC2 instance. Configure the instance to call the local API when a defect is detected.",
      "D. Deploy Amazon Monitron devices on each IP camera. Deploy an Amazon Monitron Gateway on premises. Deploy the ML model to the Amazon Monitron devices. Use Amazon Monitron health state alarms to call the local API from an AWS Lambda function when a defect is detected."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (Option B):\n\nThe correct answer is Option B: Deploy AWS IoT Greengrass on the local server. This is the most suitable solution because it allows the company to deploy the ML model on the local server and run inference locally, even when the factory's internet connectivity is down.\n\nAWS IoT Greengrass is a software that extends cloud capabilities to local devices, enabling them to collect and analyze data closer to the source of information, without requiring constant connectivity to the cloud. By deploying the Greengrass software on the local server, the company can run the ML model directly on the server, process the images from the IP cameras, and provide local feedback to the factory workers when a defect is detected, even during internet outages.\n\n2. Explanations of why the other options are incorrect:\n\nA. This option involves using Amazon Kinesis video streams, Amazon S3, and AWS Lambda to process the images and call the local API. However, this approach requires constant internet connectivity to upload the images and invoke the Lambda function, which does not meet the requirement of providing local feedback during internet outages.\n\nC. This option involves using an AWS Snowball device to deploy the ML model and inference infrastructure. While this approach can work offline, it is not the most suitable solution for this scenario, as it requires the deployment of additional hardware (the Snowball device) and may not be as scalable or flexible as the Greengrass-based solution.\n\nD. This option involves using Amazon Monitron, which is a dedicated hardware and software solution for industrial equipment monitoring. While it supports offline operation, it may not be the most suitable choice for this scenario, as it is a specialized solution and may not integrate as seamlessly with the existing infrastructure and requirements as the Greengrass-based solution."
  },
  "158": {
    "question": "A solutions architect must create a business case for migration of a company's on-premises data center to the AWS Cloud. The solutions architect will use a configuration management database (CMDB) export of all the company's servers to create the case.Which solution will meet these requirements MOST cost-effectively?",
    "choices": [
      "A. Use AWS Well-Architected Tool to import the CMDB data to perform an analysis and generate recommendations.",
      "B. Use Migration Evaluator to perform an analysis. Use the data import template to upload the data from the CMDB export.",
      "C. Implement resource matching rules. Use the CMDB export and the AWS Price List Bulk API to query CMDB data against AWS services in bulk.",
      "D. Use AWS Application Discovery Service to import the CMDB data to perform an analysis."
    ],
    "answer": "B",
    "explanation": "1. Clear explanation of why the correct answer (B) is right:\n\nThe correct answer is B, \"Use Migration Evaluator to perform an analysis. Use the data import template to upload the data from the CMDB export.\"\n\nThe Migration Evaluator (formerly known as TSO Logic) is the most cost-effective solution to meet the requirements of this question. It allows you to:\n\n- Import data from the CMDB export directly into the tool, without the need for any additional setup or configuration.\n- Perform a comprehensive analysis of the on-premises environment and provide recommendations for migrating to AWS.\n- Generate a detailed cost analysis that estimates the total cost of running the applications in the AWS cloud, which is crucial for building a business case.\n\nThe Migration Evaluator is designed specifically for this type of migration assessment and business case development, making it the most suitable and cost-effective solution among the choices provided.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nA. Use AWS Well-Architected Tool to import the CMDB data to perform an analysis and generate recommendations.\n- The Well-Architected Tool is primarily used to assess the architecture of existing AWS workloads, not for migration assessments or business case development.\n- It does not have the capability to directly import data from a CMDB export.\n\nC. Implement resource matching rules. Use the CMDB export and the AWS Price List Bulk API to query CMDB data against AWS services in bulk.\n- This approach would require significant custom development and integration work, which is not the most cost-effective solution.\n- It does not provide the same level of analysis and business case generation capabilities as the Migration Evaluator.\n\nD. Use AWS Application Discovery Service to import the CMDB data to perform an analysis.\n- Application Discovery Service can import CMDB data, but it is primarily focused on discovering and mapping the on-premises environment, not generating a comprehensive business case for migration.\n- It does not provide the same level of cost analysis and recommendations as the Migration Evaluator."
  },
  "159": {
    "question": "A company has a website that runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances are in an Auto Scaling group. The ALB is associated with an AWS WAF web ACL.The website often encounters attacks in the application layer. The attacks produce sudden and significant increases in traffic on the application server. The access logs show that each attack originates from different IP addresses. A solutions architect needs to implement a solution to mitigate these attacks.Which solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      "A. Create an Amazon CloudWatch alarm that monitors server access. Set a threshold based on access by IP address. Configure an alarm action that adds the IP address to the web ACL\u2019s deny list.",
      "B. Deploy AWS Shield Advanced in addition to AWS WAF. Add the ALB as a protected resource.",
      "C. Create an Amazon CloudWatch alarm that monitors user IP addresses. Set a threshold based on access by IP address. Configure the alarm to invoke an AWS Lambda function to add a deny rule in the application server\u2019s subnet route table for any IP addresses that activate the alarm.",
      "D. Inspect access logs to find a pattern of IP addresses that launched the attacks. Use an Amazon Route 53 geolocation routing policy to deny traffic from the countries that host those IP addresses."
    ],
    "answer": "B",
    "explanation": "Explanation:\n\n1. Correct Answer: B. Deploy AWS Shield Advanced in addition to AWS WAF. Add the ALB as a protected resource.\n\nThis is the correct answer because:\n\n- The question mentions the website is encountering \"attacks in the application layer\" that produce \"sudden and significant increases in traffic on the application server\". This indicates a Distributed Denial of Service (DDoS) attack, which is what AWS Shield Advanced is designed to protect against.\n- AWS Shield Advanced provides comprehensive DDoS protection, including automatic attack detection and mitigation, for your AWS resources. By deploying AWS Shield Advanced in addition to AWS WAF, you can get the best protection against both DDoS attacks and application-layer attacks.\n- Adding the Application Load Balancer (ALB) as a protected resource ensures that the DDoS protection extends to the entry point of your application, providing a more comprehensive defense.\n\n2. Incorrect Answers:\n\nA. Create an Amazon CloudWatch alarm that monitors server access. Set a threshold based on access by IP address. Configure an alarm action that adds the IP address to the web ACL's deny list.\n- This approach may not be effective against DDoS attacks, as the attackers are likely using multiple, constantly changing IP addresses.\n- Manually adding IP addresses to the web ACL's deny list can lead to operational overhead and potential false positives, affecting legitimate users.\n\nC. Create an Amazon CloudWatch alarm that monitors user IP addresses. Set a threshold based on access by IP address. Configure the alarm to invoke an AWS Lambda function to add a deny rule in the application server's subnet route table for any IP addresses that activate the alarm.\n- This approach is similar to option A and may not be effective against DDoS attacks with constantly changing IP addresses.\n- Adding deny rules to the subnet route table could lead to potential issues with legitimate traffic and increased operational overhead.\n\nD. Inspect access logs to find a pattern of IP addresses that launched the attacks. Use an Amazon Route 53 geolocation routing policy to deny traffic from the countries that host those IP addresses.\n- Relying on historical attack patterns and country-based blocking may not be effective against sophisticated, distributed DDoS attacks that use IP addresses from various locations.\n- Geolocation-based blocking can also lead to false positives an"
  },
  "160": {
    "question": "A telecommunications company is running an application on AWS. The company has set up an AWS Direct Connect connection between the company's on-premises data center and AWS. The company deployed the application on Amazon EC2 instances in multiple Availability Zones behind an internal Application Load Balancer (ALB). The company's clients connect from the on-premises network by using HTTPS. The TLS terminates in the ALB. The company has multiple target groups and uses path-based routing to forward requests based on the URL path.The company is planning to deploy an on-premises firewall appliance with an allow list that is based on IP address. A solutions architect must develop a solution to allow traffic flow to AWS from the on-premises network so that the clients can continue to access the application.Which solution will meet these requirements?",
    "choices": [
      "A. Configure the existing ALB to use static IP addresses. Assign IP addresses in multiple Availability Zones to the ALB. Add the ALB IP addresses to the firewall appliance.",
      "B. Create a Network Load Balancer (NLB). Associate the NLB with one static IP addresses in multiple Availability Zones. Create an ALB-type target group for the NLB and add the existing ALAdd the NLB IP addresses to the firewall appliance. Update the clients to connect to the NLB.",
      "C. Create a Network Load Balancer (NLB). Associate the LNB with one static IP addresses in multiple Availability Zones. Add the existing target groups to the NLB. Update the clients to connect to the NLB. Delete the ALB Add the NLB IP addresses to the firewall appliance.",
      "D. Create a Gateway Load Balancer (GWLB). Assign static IP addresses to the GWLB in multiple Availability Zones. Create an ALB-type target group for the GWLB and add the existing ALB. Add the GWLB IP addresses to the firewall appliance. Update the clients to connect to the GWLB."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B because it provides the best solution to meet the given requirements:\n\n- It creates a Network Load Balancer (NLB) and associates it with a static IP address in multiple Availability Zones.\n- It then creates an ALB-type target group for the NLB and adds the existing ALB to it.\n- This allows the NLB to provide a static IP endpoint for the on-premises firewall appliance, while still leveraging the existing ALB's features, such as path-based routing.\n- The clients can continue to connect to the application by using the NLB's static IP address, and the traffic will be forwarded to the ALB behind the scenes.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Cannot configure the existing ALB to use static IP addresses, as ALBs do not support static IP addresses.\n\nC. Cannot create an NLB and directly add the existing target groups to it, as the path-based routing feature of the ALB would not be preserved.\n\nD. The Gateway Load Balancer (GWLB) is designed for a different use case, where it acts as a proxy for network traffic. It does not support ALB-type target groups, and the clients would need to be updated to connect to the GWLB instead of the ALB."
  },
  "161": {
    "question": "A telecommunications company is running an application on AWS. The company has set up an AWS Direct Connect connection between the company's on-premises data center and AWS. The company deployed the application on Amazon EC2 instances in multiple Availability Zones behind an internal Application Load Balancer (ALB). The company's clients connect from the on-premises network by using HTTPS. The TLS terminates in the ALB. The company has multiple target groups and uses path-based routing to forward requests based on the URL path.The company is planning to deploy an on-premises firewall appliance with an allow list that is based on IP address. A solutions architect must develop a solution to allow traffic flow to AWS from the on-premises network so that the clients can continue to access the application.Which solution will meet these requirements?",
    "choices": [
      "A. Configure the existing ALB to use static IP addresses. Assign IP addresses in multiple Availability Zones to the ALB. Add the ALB IP addresses to the firewall appliance.",
      "B. Create a Network Load Balancer (NLB). Associate the NLB with one static IP addresses in multiple Availability Zones. Create an ALB-type target group for the NLB and add the existing ALAdd the NLB IP addresses to the firewall appliance. Update the clients to connect to the NLB.",
      "C. Create a Network Load Balancer (NLB). Associate the LNB with one static IP addresses in multiple Availability Zones. Add the existing target groups to the NLB. Update the clients to connect to the NLB. Delete the ALB Add the NLB IP addresses to the firewall appliance.",
      "D. Create a Gateway Load Balancer (GWLB). Assign static IP addresses to the GWLB in multiple Availability Zones. Create an ALB-type target group for the GWLB and add the existing ALB. Add the GWLB IP addresses to the firewall appliance. Update the clients to connect to the GWLB."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B because it provides a solution that meets the requirements. By creating a Network Load Balancer (NLB) and associating it with static IP addresses in multiple Availability Zones, the company can provide a static IP endpoint for the on-premises firewall appliance. The NLB can then be configured with an ALB-type target group, allowing it to forward the traffic to the existing Application Load Balancer (ALB) that is handling the application's load balancing and path-based routing. This approach preserves the ALB functionality while providing a static IP endpoint for the on-premises firewall.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Configure the existing ALB to use static IP addresses: This is not a valid solution because ALBs cannot be assigned static IP addresses. ALBs are designed to have dynamic IP addresses, which is not compatible with the on-premises firewall's requirement for a static IP address.\n\nC. Create a Network Load Balancer (NLB) and add the existing target groups: This solution is not correct because NLBs do not support path-based routing, which is a key requirement for the application. Path-based routing is a feature of ALBs, and it cannot be directly implemented with an NLB.\n\nD. Create a Gateway Load Balancer (GWLB): The Gateway Load Balancer (GWLB) is not the appropriate choice here, as it is designed for a different use case, specifically for network traffic inspection and monitoring. The GWLB requires EC2 instances or IP addresses as targets, which does not align with the requirement of integrating with the existing ALB that is using path-based routing."
  },
  "162": {
    "question": "A company runs an application on a fleet of Amazon EC2 instances that are in private subnets behind an internet-facing Application Load Balancer (ALB). The ALB is the origin for an Amazon CloudFront distribution. An AWS WAF web ACL that contains various AWS managed rules is associated with the CloudFront distribution.The company needs a solution that will prevent internet traffic from directly accessing the ALB.Which solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      "A. Create a new web ACL that contains the same rules that the existing web ACL contains. Associate the new web ACL with the ALB.",
      "B. Associate the existing web ACL with the ALB.",
      "C. Add a security group rule to the ALB to allow traffic from the AWS managed prefix list for CloudFront only.",
      "D. Add a security group rule to the ALB to allow only the various CloudFront IP address ranges."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C, \"Add a security group rule to the ALB to allow traffic from the AWS managed prefix list for CloudFront only.\" This is the best solution with the least operational overhead because:\n\n- It allows only the CloudFront IP address ranges to access the ALB, which effectively prevents direct internet access to the ALB.\n- Using the AWS managed prefix list for CloudFront is more efficient and easier to manage than manually maintaining a list of CloudFront IP address ranges, which can change over time.\n- This approach is more secure than associating the existing AWS WAF web ACL directly with the ALB, as it adds an additional layer of protection by limiting access to the ALB at the network level.\n\n2. Explanations of the incorrect answers:\n\nA. Create a new web ACL that contains the same rules that the existing web ACL contains. Associate the new web ACL with the ALB.\n   - This solution has more operational overhead than using the AWS managed prefix list, as it requires creating and managing a new web ACL.\n   - It does not directly prevent direct internet access to the ALB, as the web ACL would still be associated with the ALB.\n\nB. Associate the existing web ACL with the ALB.\n   - This solution does not prevent direct internet access to the ALB, as the web ACL would be associated with the CloudFront distribution, not the ALB.\n   - It does not add an additional layer of protection at the network level, which is required to meet the stated requirements.\n\nD. Add a security group rule to the ALB to allow only the various CloudFront IP address ranges.\n   - This solution is more operationally complex than using the AWS managed prefix list, as it requires manually maintaining a list of CloudFront IP address ranges, which can change over time.\n   - It does not provide the same level of efficiency and ease of management as using the AWS managed prefix list."
  },
  "163": {
    "question": "A company is running an application that uses an Amazon ElastiCache for Redis cluster as a caching layer. A recent security audit revealed that the company has configured encryption at rest for ElastiCache. However, the company did not configure ElastiCache to use encryption in transit. Additionally, users can access the cache without authentication.A solutions architect must make changes to require user authentication and to ensure that the company is using end-to-end encryption.Which solution will meet these requirements?",
    "choices": [
      "A. Create an AUTH token. Store the token in AWS System Manager Parameter Store, as an encrypted parameter. Create a new cluster with AUTH, and configure encryption in transit. Update the application to retrieve the AUTH token from Parameter Store when necessary and to use the AUTH token for authentication.",
      "B. Create an AUTH token. Store the token in AWS Secrets Manager. Configure the existing cluster to use the AUTH token, and configure encryption in transit. Update the application to retrieve the AUTH token from Secrets Manager when necessary and to use the AUTH token for authentication.",
      "C. Create an SSL certificate. Store the certificate in AWS Secrets Manager. Create a new cluster, and configure encryption in transit. Update the application to retrieve the SSL certificate from Secrets Manager when necessary and to use the certificate for authentication.",
      "D. Create an SSL certificate. Store the certificate in AWS Systems Manager Parameter Store, as an encrypted advanced parameter. Update the existing cluster to configure encryption in transit. Update the application to retrieve the SSL certificate from Parameter Store when necessary and to use the certificate for authentication."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (A):\n\nThe correct answer is A because it meets the requirements of the problem statement:\n\n- It creates an AUTH token, which provides user authentication for accessing the ElastiCache Redis cluster.\n- It stores the AUTH token securely in AWS Systems Manager Parameter Store as an encrypted parameter, ensuring the token is protected.\n- It creates a new ElastiCache Redis cluster with the AUTH feature enabled, which requires users to authenticate before accessing the cache.\n- It also configures the new cluster to use encryption in transit, ensuring end-to-end encryption for the data.\n- The application is updated to retrieve the AUTH token from Parameter Store when necessary and use it for authentication, completing the solution.\n\n2. Explanations of why the other choices are incorrect:\n\nB. This option is incorrect because it suggests updating the existing ElastiCache cluster to use the AUTH token and enable encryption in transit. However, as mentioned in the community discussions, once an ElastiCache cluster is created without encryption in transit, it cannot be modified to enable it. A new cluster must be created to enable encryption in transit.\n\nC. This option is incorrect because it suggests using an SSL certificate for authentication, which is not the correct approach for Redis authentication. Redis uses the AUTH command for user authentication, not SSL/TLS certificates.\n\nD. This option is incorrect because it suggests storing the SSL certificate in Systems Manager Parameter Store and updating the existing cluster to enable encryption in transit. As mentioned earlier, once an ElastiCache cluster is created without encryption in transit, it cannot be modified to enable it. A new cluster must be created to enable encryption in transit."
  },
  "164": {
    "question": "A company is running a compute workload by using Amazon EC2 Spot Instances that are in an Auto Scaling group. The launch template uses two placement groups and a single instance type.Recently, a monitoring system reported Auto Scaling instance launch failures that correlated with longer wait times for system users. The company needs to improve the overall reliability of the workload.Which solution will meet this requirement?",
    "choices": [
      "A. Replace the launch template with a launch configuration to use an Auto Scaling group that uses attribute-based instance type selection.",
      "B. Create a new launch template version that uses attribute-based instance type selection. Configure the Auto Scaling group to use the new launch template version.",
      "C. Update the launch template Auto Scaling group to increase the number of placement groups.",
      "D. Update the launch template to use a larger instance type."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B, which is to \"Create a new launch template version that uses attribute-based instance type selection. Configure the Auto Scaling group to use the new launch template version.\"\n\nThis is the right solution because:\n- Updating the existing launch template with attribute-based instance type selection allows the Auto Scaling group to automatically select the most suitable instance types based on the specified attributes, rather than being limited to a fixed instance type.\n- Configuring the Auto Scaling group to use the new launch template version ensures that the updated configuration is applied to all new instances launched by the group.\n- Attribute-based instance type selection can improve the reliability of the workload by allowing the Auto Scaling group to leverage a broader range of instance types that meet the specified requirements, reducing the likelihood of launch failures due to instance type unavailability.\n\n2. Explanations of why the other choices are incorrect:\n\nA. Replace the launch template with a launch configuration to use an Auto Scaling group that uses attribute-based instance type selection.\n- This is incorrect because launch configurations have been deprecated in favor of launch templates, and should not be used for new deployments.\n\nC. Update the launch template Auto Scaling group to increase the number of placement groups.\n- This is incorrect because the launch template itself cannot be updated. Creating a new version of the launch template is the appropriate way to make changes.\n\nD. Update the launch template to use a larger instance type.\n- This is incorrect because updating the instance type in the launch template is not the best solution to improve the overall reliability of the workload. Attribute-based instance type selection is a more flexible and scalable approach."
  },
  "165": {
    "question": "A delivery company is running a serverless solution in the AWS Cloud. The solution manages user data, delivery information, and past purchase details. The solution consists of several microservices. The central user service stores sensitive data in an Amazon DynamoDB table. Several of the other microservices store a copy of parts of the sensitive data in different storage services.The company needs the ability to delete user information upon request. As soon as the central user service deletes a user, every other microservice must also delete its copy of the data immediately.Which solution will meet these requirements?",
    "choices": [
      "A. Activate DynamoDB Streams on the DynamoDB table. Create an AWS Lambda trigger for the DynamoDB stream that will post events about user deletion in an Amazon Simple Queue Service (Amazon SQS) queue. Configure each microservice to poll the queue and delete the user from the DynamoDB table.",
      "B. Set up DynamoDB event notifications on the DynamoDB table. Create an Amazon Simple Notification Service (Amazon SNS) topic as a target for the DynamoDB event notification. Configure each microservice to subscribe to the SNS topic and to delete the user from the DynamoDB table.",
      "C. Configure the central user service to post an event on a custom Amazon EventBridge event bus when the company deletes a user. Create an EventBridge rule for each microservice to match the user deletion event pattern and invoke logic in the microservice to delete the user from the DynamoDB table.",
      "D. Configure the central user service to post a message on an Amazon Simple Queue Service (Amazon SQS) queue when the company deletes a user. Configure each microservice to create an event filter on the SQS queue and to delete the user from the DynamoDB table."
    ],
    "answer": "C",
    "explanation": "1. Clear explanation of why the correct answer (C) is right:\n\nThe correct answer is C because it best addresses the key requirements:\n\n- The central user service needs to delete user information upon request.\n- As soon as the central user service deletes a user, every other microservice must also delete its copy of the data immediately.\n\nOption C uses Amazon EventBridge, which allows the central user service to post a user deletion event on a custom event bus. Each microservice can then create an EventBridge rule to match this event pattern and invoke logic to delete the user data from their respective stores. This ensures immediate synchronization across all microservices when a user is deleted from the central user service.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nA. DynamoDB Streams and a Lambda trigger can achieve the same effect as Option C, but it introduces an additional component (SQS queue) that is not necessary. Polling the SQS queue may also introduce latency, whereas the EventBridge solution is more direct.\n\nB. There is no feature in AWS called \"DynamoDB event notifications.\" This option is not a valid solution.\n\nD. Using a single SQS queue for all microservices to poll and delete user data is not a scalable or efficient approach, as it introduces additional complexity and potential bottlenecks. The EventBridge solution in Option C is more appropriate for this use case."
  },
  "166": {
    "question": "A delivery company is running a serverless solution in the AWS Cloud. The solution manages user data, delivery information, and past purchase details. The solution consists of several microservices. The central user service stores sensitive data in an Amazon DynamoDB table. Several of the other microservices store a copy of parts of the sensitive data in different storage services.The company needs the ability to delete user information upon request. As soon as the central user service deletes a user, every other microservice must also delete its copy of the data immediately.Which solution will meet these requirements?",
    "choices": [
      "A. Activate DynamoDB Streams on the DynamoDB table. Create an AWS Lambda trigger for the DynamoDB stream that will post events about user deletion in an Amazon Simple Queue Service (Amazon SQS) queue. Configure each microservice to poll the queue and delete the user from the DynamoDB table.",
      "B. Set up DynamoDB event notifications on the DynamoDB table. Create an Amazon Simple Notification Service (Amazon SNS) topic as a target for the DynamoDB event notification. Configure each microservice to subscribe to the SNS topic and to delete the user from the DynamoDB table.",
      "C. Configure the central user service to post an event on a custom Amazon EventBridge event bus when the company deletes a user. Create an EventBridge rule for each microservice to match the user deletion event pattern and invoke logic in the microservice to delete the user from the DynamoDB table.",
      "D. Configure the central user service to post a message on an Amazon Simple Queue Service (Amazon SQS) queue when the company deletes a user. Configure each microservice to create an event filter on the SQS queue and to delete the user from the DynamoDB table."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C because it best meets the requirements of the scenario. When the central user service deletes a user in the DynamoDB table, it should publish an event to a central event bus (Amazon EventBridge) so that all the other microservices can be notified and delete their copies of the user data. This ensures that the deletion of user data is propagated across all the microservices, as required. Using EventBridge allows for a decoupled, event-driven architecture where the microservices can react to the user deletion event asynchronously.\n\n2. Explanations of the incorrect choices:\n\nA. This solution using DynamoDB Streams and SQS is not optimal because it requires each microservice to poll the SQS queue to receive the user deletion event. This introduces latency and complexity, as each microservice would need to manage its own queue polling and deletion logic.\n\nB. This solution using DynamoDB event notifications and Amazon SNS is incorrect because there is no such feature as \"DynamoDB event notifications\" in AWS. DynamoDB Streams is the correct feature to use for capturing changes to the DynamoDB table.\n\nD. This solution using SQS is similar to the one in A, but it is less efficient because it requires each microservice to create its own event filter on the SQS queue. This can lead to increased complexity and potential issues with message delivery and processing.\n\nIn summary, the correct answer (C) using Amazon EventBridge provides a more scalable, decoupled, and efficient solution for propagating user deletion events across the microservices, as compared to the other options."
  },
  "167": {
    "question": "A company is running a web application in a VPC. The web application runs on a group of Amazon EC2 instances behind an Application Load Balancer (ALB). The ALB is using AWS WAF.An external customer needs to connect to the web application. The company must provide IP addresses to all external customers.Which solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      "A. Replace the ALB with a Network Load Balancer (NLB). Assign an Elastic IP address to the NLB.",
      "B. Allocate an Elastic IP address. Assign the Elastic IP address to the ALProvide the Elastic IP address to the customer.",
      "C. Create an AWS Global Accelerator standard accelerator. Specify the ALB as the accelerator's endpoint. Provide the accelerator's IP addresses to the customer.",
      "D. Configure an Amazon CloudFront distribution. Set the ALB as the origin. Ping the distribution's DNS name to determine the distribution's public IP address. Provide the IP address to the customer."
    ],
    "answer": "C",
    "explanation": "1. Clear explanation of why the correct answer (C) is right:\n\nThe correct answer is C, which is to create an AWS Global Accelerator standard accelerator and specify the Application Load Balancer (ALB) as the accelerator's endpoint. This solution meets the requirements with the least operational overhead.\n\nThe key reasons why this is the best solution:\n\n- AWS Global Accelerator provides static, anycast IP addresses that act as a fixed entry point to your application, which can be directly provided to the external customer.\n- By using Global Accelerator with the existing ALB, you can leverage the ALB's integration with AWS WAF, maintaining the security benefits without additional operational overhead.\n- Global Accelerator automatically scales to handle traffic spikes and provides high availability, reducing the operational burden on the customer-facing solution.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nA. Replace the ALB with a Network Load Balancer (NLB) and assign an Elastic IP address to the NLB:\n   - This is not a suitable solution because AWS WAF does not support integration with NLBs. The question states that the company must use AWS WAF, which is only compatible with Application Load Balancers.\n\nB. Allocate an Elastic IP address and assign it to the ALB:\n   - This is not a viable solution because Application Load Balancers do not support the use of Elastic IP addresses. ALBs have a dynamic DNS name instead of a static IP address.\n\nD. Configure an Amazon CloudFront distribution with the ALB as the origin:\n   - While this could provide a static IP address to the customer, the operational overhead would be higher compared to using AWS Global Accelerator. CloudFront distributions typically use a prefix list of IP addresses, which can be more complex for the customer to configure."
  },
  "168": {
    "question": "A company has a few AWS accounts for development and wants to move its production application to AWS. The company needs to enforce Amazon Elastic Block Store (Amazon EBS) encryption at rest current production accounts and future production accounts only. The company needs a solution that includes built-in blueprints and guardrails.Which combination of steps will meet these requirements? (Choose three.)",
    "choices": [
      "A. Use AWS CloudFormation StackSets to deploy AWS Config rules on production accounts.",
      "B. Create a new AWS Control Tower landing zone in an existing developer account. Create OUs for accounts. Add production and development accounts to production and development OUs, respectively.",
      "C. Create a new AWS Control Tower landing zone in the company\u2019s management account. Add production and development accounts to production and development OUs. respectively.",
      "D. Invite existing accounts to join the organization in AWS Organizations. Create SCPs to ensure compliance.",
      "E. Create a guardrail from the management account to detect EBS encryption.",
      "F. Create a guardrail for the production OU to detect EBS encryption."
    ],
    "answer": "CDF",
    "explanation": "1. Clear explanation of why the correct answer is right:\n\nC. Create a new AWS Control Tower landing zone in the company's management account. Add production and development accounts to production and development OUs, respectively.\nThis is the correct choice because it aligns with the requirements of the question. By creating a new AWS Control Tower landing zone in the management account, the company can establish the necessary organizational structure and controls to enforce EBS encryption at rest for the production accounts. Specifically, by adding the production accounts to a dedicated production OU, the company can apply the required encryption control to that OU.\n\nD. Invite existing accounts to join the organization in AWS Organizations. Create SCPs to ensure compliance.\nThis is also a correct choice. By inviting the existing accounts to join the organization in AWS Organizations, the company can then create Service Control Policies (SCPs) to enforce the EBS encryption requirement on the production accounts.\n\nF. Create a guardrail for the production OU to detect EBS encryption.\nThis is the third correct choice. By creating a guardrail (now known as a control) for the production OU, the company can ensure that EBS encryption is enforced for all production accounts under that OU.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nA. Use AWS CloudFormation StackSets to deploy AWS Config rules on production accounts.\nThis is incorrect because it does not address the requirement of establishing a centralized control mechanism across the organization. CloudFormation StackSets and AWS Config rules would only apply to individual accounts, not the entire organization.\n\nB. Create a new AWS Control Tower landing zone in an existing developer account. Create OUs for accounts. Add production and development accounts to production and development OUs, respectively.\nThis is incorrect because the question specifies that the landing zone should be created in the company's management account, not an existing developer account. Creating the landing zone in the management account allows for better centralized control and management of the organization's accounts.\n\nE. Create a guardrail from the management account to detect EBS encryption.\nThis choice is incorrect because it only addresses the detection of EBS encryption, not the enforcement of it. The question requires a solution that includes both built-in blueprints and guardrails (controls) to enforce the encryption requirement."
  },
  "169": {
    "question": "A company is running a critical stateful web application on two Linux Amazon EC2 instances behind an Application Load Balancer (ALB) with an Amazon RDS for MySQL database. The company hosts the DNS records for the application in Amazon Route 53. A solutions architect must recommend a solution to improve the resiliency of the application.The solution must meet the following objectives:\u2022\tApplication tier: RPO of 2 minutes. RTO of 30 minutes\u2022\tDatabase tier: RPO of 5 minutes. RTO of 30 minutesThe company does not want to make significant changes to the existing application architecture. The company must ensure optimal latency after a failover.Which solution will meet these requirements?",
    "choices": [
      "A. Configure the EC2 instances to use AWS Elastic Disaster Recovery. Create a cross-Region read replica for the RDS DB instance. Create an ALB in a second AWS Region. Create an AWS Global Accelerator endpoint, and associate the endpoint with the ALBs. Update DNS records to point to the Global Accelerator endpoint.",
      "B. Configure the EC2 instances to use Amazon Data Lifecycle Manager (Amazon DLM) to take snapshots of the EBS volumes. Configure RDS automated backups. Configure backup replication to a second AWS Region. Create an ALB in the second Region. Create an AWS Global Accelerator endpoint, and associate the endpoint with the ALBs. Update DNS records to point to the Global Accelerator endpoint.",
      "C. Create a backup plan in AWS Backup for the EC2 instances and RDS DB instance. Configure backup replication to a second AWS Region. Create an ALB in the second Region. Configure an Amazon CloudFront distribution in front of the ALB. Update DNS records to point to CloudFront.",
      "D. Configure the EC2 instances to use Amazon Data Lifecycle Manager (Amazon DLM) to take snapshots of the EBS volumes. Create a cross-Region read replica for the RDS DB instance. Create an ALB in a second AWS Region. Create an AWS Global Accelerator endpoint, and associate the endpoint with the ALBs."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\n\nThe correct answer is A because it best meets the requirements specified in the question.\n\n- Application tier: RPO of 2 minutes, RTO of 30 minutes\n  - AWS Elastic Disaster Recovery (DRS) can provide this by continuously replicating the EC2 instances to another AWS Region, allowing for a quick failover with a 2-minute RPO.\n\n- Database tier: RPO of 5 minutes, RTO of 30 minutes\n  - Creating a cross-Region read replica for the RDS DB instance can provide the 5-minute RPO and 30-minute RTO for the database tier.\n\n- The solution also includes creating an ALB in a second AWS Region and an AWS Global Accelerator endpoint, which can help ensure optimal latency after a failover by directing traffic to the nearest healthy resources.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. This solution uses Amazon Data Lifecycle Manager (DLM) and RDS automated backups, which may not meet the strict RPO and RTO requirements specified in the question. Backup and restore operations can take longer than the specified RTO.\n\nC. This solution uses AWS Backup, which is a general-purpose backup service and may not provide the level of granularity and control required to meet the specific RPO and RTO requirements for the application and database tiers. Additionally, using CloudFront in front of the ALB may not be the optimal solution for ensuring low latency after a failover.\n\nD. This solution is similar to the correct answer, but it does not include the AWS Global Accelerator component, which is crucial for ensuring optimal latency after a failover by directing traffic to the nearest healthy resources."
  },
  "170": {
    "question": "A solutions architect wants to cost-optimize and appropriately size Amazon EC2 instances in a single AWS account. The solutions architect wants to ensure that the instances are optimized based on CPU, memory, and network metrics.Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
    "choices": [
      "A. Purchase AWS Business Support or AWS Enterprise Support for the account.",
      "B. Turn on AWS Trusted Advisor and review any \u201cLow Utilization Amazon EC2 Instances\u201d recommendations.",
      "C. Install the Amazon CloudWatch agent and configure memory metric collection on the EC2 instances.",
      "D. Configure AWS Compute Optimizer in the AWS account to receive findings and optimization recommendations.",
      "E. Create an EC2 Instance Savings Plan for the AWS Regions, instance families, and operating systems of interest."
    ],
    "answer": "CD",
    "explanation": "Here's a clear explanation of the correct answer and brief explanations of why each incorrect choice is wrong:\n\n1. Explanation of the correct answer (CD):\n\nC. Install the Amazon CloudWatch agent and configure memory metric collection on the EC2 instances:\nThis is correct because the CloudWatch agent allows you to collect custom metrics, including memory utilization, from your EC2 instances. By configuring the agent to collect memory metrics, the solutions architect can get a detailed view of the memory usage of the instances, which is crucial for optimizing instance sizes.\n\nD. Configure AWS Compute Optimizer in the AWS account to receive findings and optimization recommendations:\nThis is also correct because the AWS Compute Optimizer service analyzes the metrics (CPU, memory, and network) of your EC2 instances and provides recommendations on the most appropriate instance types and sizes based on your workload. By enabling Compute Optimizer, the solutions architect can get actionable insights to optimize the instances.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Purchase AWS Business Support or AWS Enterprise Support for the account:\nThis is incorrect because the question does not mention the need for any specific AWS support plan. The required actions can be performed using the default AWS services and tools, without the need for a paid support plan.\n\nB. Turn on AWS Trusted Advisor and review any \"Low Utilization Amazon EC2 Instances\" recommendations:\nThis is incorrect because Trusted Advisor is primarily focused on identifying underutilized instances based on CPU utilization, but it does not provide comprehensive recommendations that consider memory and network metrics. The question requires optimizing instances based on all three metrics (CPU, memory, and network).\n\nE. Create an EC2 Instance Savings Plan for the AWS Regions, instance families, and operating systems of interest:\nThis is incorrect because Savings Plans are a way to save money on EC2 instances by committing to a certain level of usage, but they do not directly address the optimization of instance sizes based on the workload metrics. While Savings Plans can be a part of the overall cost optimization strategy, they do not fulfill the specific requirements mentioned in the question."
  },
  "171": {
    "question": "A company uses an AWS CodeCommit repository. The company must store a backup copy of the data that is in the repository in a second AWS Region.Which solution will meet these requirements?",
    "choices": [
      "A. Configure AWS Elastic Disaster Recovery to replicate the CodeCommit repository data to the second Region.",
      "B. Use AWS Backup to back up the CodeCommit repository on an hourly schedule. Create a cross-Region copy in the second Region.",
      "C. Create an Amazon EventBridge rule to invoke AWS CodeBuild when the company pushes code to the repository. Use CodeBuild to clone the repository. Create a .zip file of the content. Copy the file to an S3 bucket in the second Region.",
      "D. Create an AWS Step Functions workflow on an hourly schedule to take a snapshot of the CodeCommit repository. Configure the workflow to copy the snapshot to an S3 bucket in the second Region"
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C. This solution uses AWS CodeBuild to clone the CodeCommit repository and create a .zip file of the content, which is then copied to an S3 bucket in a secondary AWS Region. This approach meets the requirement of storing a backup copy of the CodeCommit repository data in a second AWS Region.\n\n2. Explanations of the incorrect choices:\n\nA. Incorrect. AWS Elastic Disaster Recovery is designed for replicating and recovering EC2 instances, not CodeCommit repositories.\n\nB. Incorrect. AWS Backup does not support CodeCommit repositories directly. The question specifies that the company must store a backup copy of the data in the CodeCommit repository, not a backup of the CodeCommit service itself.\n\nD. Incorrect. While creating an AWS Step Functions workflow to take snapshots of the CodeCommit repository and copy them to an S3 bucket in a secondary Region is a valid approach, it is more complex than the solution in C. The question does not indicate a need for the additional features provided by Step Functions."
  },
  "172": {
    "question": "A company has multiple business units that each have separate accounts on AWS. Each business unit manages its own network with several VPCs that have CIDR ranges that overlap. The company\u2019s marketing team has created a new internal application and wants to make the application accessible to all the other business units. The solution must use private IP addresses only.Which solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      "A. Instruct each business unit to add a unique secondary CIDR range to the business unit's VPC. Peer the VPCs and use a private NAT gateway in the secondary range to route traffic to the marketing team.",
      "B. Create an Amazon EC2 instance to serve as a virtual appliance in the marketing account's VPC. Create an AWS Site-to-Site VPN connection between the marketing team and each business unit's VPC. Perform NAT where necessary.",
      "C. Create an AWS PrivateLink endpoint service to share the marketing application. Grant permission to specific AWS accounts to connect to the service. Create interface VPC endpoints in other accounts to access the application by using private IP addresses.",
      "D. Create a Network Load Balancer (NLB) in front of the marketing application in a private subnet. Create an API Gateway API. Use the Amazon API Gateway private integration to connect the API to the NLB. Activate IAM authorization for the API. Grant access to the accounts of the other business units."
    ],
    "answer": "C",
    "explanation": "1. Explanation of why the correct answer (C) is right:\n\nThe correct answer is C, which is to use AWS PrivateLink to share the marketing application across the different business units.\n\nThe key requirements here are:\n- The solution must use private IP addresses only\n- The solution must have the least operational overhead\n\nAWS PrivateLink meets these requirements best:\n- It allows the marketing team to create an endpoint service that can be accessed by the other business units using private IP addresses, without the need for VPC peering or NAT gateways.\n- This approach does not require any changes to the existing VPC configurations of the other business units, which minimizes operational overhead.\n- The other business units can create interface VPC endpoints to access the marketing application, again using only private IP addresses.\n\nThis is the most straightforward and least operationally complex solution compared to the other choices.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This approach requires the business units to add secondary CIDR ranges to their VPCs and then peer the VPCs. This adds complexity and operational overhead, as the business units would need to coordinate and manage the VPC peering connections.\n\nB. Creating a virtual appliance EC2 instance and setting up VPN connections between the marketing account and each business unit's VPC adds more complexity and operational overhead. The need to perform NAT where necessary also increases the complexity.\n\nD. This approach uses a Network Load Balancer (NLB) and an API Gateway, which adds more components to the overall solution. The need to activate IAM authorization and grant access to the other business unit accounts also increases the operational overhead compared to the PrivateLink solution.\n\nIn summary, the PrivateLink solution (C) is the most straightforward and least operationally complex way to share the marketing application across the different business units while using only private IP addresses."
  },
  "173": {
    "question": "A company needs to audit the security posture of a newly acquired AWS account. The company\u2019s data security team requires a notification only when an Amazon S3 bucket becomes publicly exposed. The company has already established an Amazon Simple Notification Service (Amazon SNS) topic that has the data security team's email address subscribed.Which solution will meet these requirements?",
    "choices": [
      "A. Create an S3 event notification on all S3 buckets for the isPublic event. Select the SNS topic as the target for the event notifications.",
      "B. Create an analyzer in AWS Identity and Access Management Access Analyzer. Create an Amazon EventBridge rule for the event type \u201cAccess Analyzer Finding\u201d with a filter for \u201cisPublic: true.\u201d Select the SNS topic as the EventBridge rule target.",
      "C. Create an Amazon EventBridge rule for the event type \u201cBucket-Level API Call via CloudTrail\u201d with a filter for \u201cPutBucketPolicy.\u201d Select the SNS topic as the EventBridge rule target.",
      "D. Activate AWS Config and add the cloudtrail-s3-dataevents-enabled rule. Create an Amazon EventBridge rule for the event type \u201cConfig Rules Re-evaluation Status\u201d with a filter for \u201cNON_COMPLIANT.\u201d Select the SNS topic as the EventBridge rule target."
    ],
    "answer": "B",
    "explanation": "1. Explanation for the correct answer (Option B):\n\nThe correct answer is Option B because it directly addresses the requirement to receive a notification only when an Amazon S3 bucket becomes publicly exposed. The solution involves the following steps:\n\n- Create an AWS Identity and Access Management (IAM) Access Analyzer to continuously monitor the S3 buckets for any public access.\n- Create an Amazon EventBridge rule that triggers on the \"Access Analyzer Finding\" event type and filters for findings where the \"isPublic\" field is set to \"true\".\n- Select the existing Amazon SNS topic as the target for the EventBridge rule, which will send a notification to the data security team's email address.\n\nThis approach is the most appropriate solution because it directly addresses the specific requirement to receive notifications only when an S3 bucket becomes publicly exposed, without the need to monitor all S3 events or rely on CloudTrail data.\n\n2. Explanations for the incorrect choices:\n\nA. Incorrect because Amazon S3 does not currently provide event notifications for the \"isPublic\" event. The available events are related to object-level operations, not bucket-level permissions changes.\n\nC. Incorrect because monitoring the \"PutBucketPolicy\" event would notify you of any changes to the S3 bucket policy, but not specifically when a bucket becomes publicly exposed. This approach is too broad and does not meet the requirement to receive notifications only for public exposure.\n\nD. Incorrect because the \"cloudtrail-s3-dataevents-enabled\" rule in AWS Config checks if CloudTrail is logging data events for all S3 buckets, which is a different requirement than monitoring for public exposure of S3 buckets. Additionally, this approach would trigger a notification for any S3 bucket that becomes non-compliant, not just those that become publicly exposed."
  },
  "174": {
    "question": "A solutions architect needs to assess a newly acquired company\u2019s portfolio of applications and databases. The solutions architect must create a business case to migrate the portfolio to AWS. The newly acquired company runs applications in an on-premises data center. The data center is not well documented. The solutions architect cannot immediately determine how many applications and databases exist. Traffic for the applications is variable. Some applications are batch processes that run at the end of each month.The solutions architect must gain a better understanding of the portfolio before a migration to AWS can begin.Which solution will meet these requirements?",
    "choices": [
      "A. Use AWS Server Migration Service (AWS SMS) and AWS Database Migration Service (AWS DMS) to evaluate migration. Use AWS Service Catalog to understand application and database dependencies.",
      "B. Use AWS Application Migration Service. Run agents on the on-premises infrastructure. Manage the agents by using AWS Migration Hub. Use AWS Storage Gateway to assess local storage needs and database dependencies.",
      "C. Use Migration Evaluator to generate a list of servers. Build a report for a business case. Use AWS Migration Hub to view the portfolio. Use AWS Application Discovery Service to gain an understanding of application dependencies.",
      "D. Use AWS Control Tower in the destination account to generate an application portfolio. Use AWS Server Migration Service (AWS SMS) to generate deeper reports and a business case. Use a landing zone for core accounts and resources."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C because it provides the best solution to meet the given requirements:\n\n- Use Migration Evaluator to generate a list of servers: This tool can help the solutions architect gain a better understanding of the portfolio by providing a list of the servers and their configurations in the on-premises data center, which is not well documented.\n- Build a report for a business case: The list of servers and their details generated by Migration Evaluator can be used to build a comprehensive business case for migrating the portfolio to AWS.\n- Use AWS Migration Hub to view the portfolio: AWS Migration Hub provides a centralized view of the migration process, allowing the solutions architect to track the progress and status of the migration.\n- Use AWS Application Discovery Service to gain an understanding of application dependencies: This service can help the solutions architect understand the dependencies between the various applications, which is important for planning the migration process.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This choice is incorrect because it does not utilize the Migration Evaluator tool to first understand the existing portfolio, which is a crucial first step. Additionally, it does not mention the use of AWS Application Discovery Service to gain an understanding of application dependencies.\n\nB. This choice is incorrect because it uses the AWS Application Migration Service, which is more suitable for lift-and-shift migrations, rather than first evaluating the portfolio. It also does not mention the use of Migration Evaluator or AWS Application Discovery Service.\n\nD. This choice is incorrect because it focuses on using AWS Control Tower and AWS Server Migration Service (SMS) to generate reports, rather than first understanding the portfolio using tools like Migration Evaluator and AWS Application Discovery Service."
  },
  "175": {
    "question": "A company has an application that runs as a ReplicaSet of multiple pods in an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The EKS cluster has nodes in multiple Availability Zones. The application generates many small files that must be accessible across all running instances of the application. The company needs to back up the files and retain the backups for 1 year.Which solution will meet these requirements while providing the FASTEST storage performance?",
    "choices": [
      "A. Create an Amazon Elastic File System (Amazon EFS) file system and a mount target for each subnet that contains nodes in the EKS cluster. Configure the ReplicaSet to mount the file system. Direct the application to store files in the file system. Configure AWS Backup to back up and retain copies of the data for 1 year.",
      "B. Create an Amazon Elastic Block Store (Amazon EBS) volume. Enable the EBS Multi-Attach feature. Configure the ReplicaSet to mount the EBS volume. Direct the application to store files in the EBS volume. Configure AWS Backup to back up and retain copies of the data for 1 year.",
      "C. Create an Amazon S3 bucket. Configure the ReplicaSet to mount the S3 bucket. Direct the application to store files in the S3 bucket. Configure S3 Versioning to retain copies of the data. Configure an S3 Lifecycle policy to delete objects after 1 year.",
      "D. Configure the ReplicaSet to use the storage available on each of the running application pods to store the files locally. Use a third-party tool to back up the EKS cluster for 1 year."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\nThe correct answer is A because Amazon EFS provides the fastest storage performance among the options presented. EFS is a scalable, elastic, and durable file storage service that allows multiple instances to access the same file system concurrently, making it well-suited for the given scenario.\n\nBy creating an Amazon EFS file system and mounting it on each subnet that contains nodes in the EKS cluster, the application can access the shared file storage across all running instances. This ensures that the small files generated by the application are accessible from any pod in the ReplicaSet. Additionally, configuring AWS Backup to back up and retain the data for 1 year provides the necessary data protection and long-term retention.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. Amazon EBS (with Multi-Attach feature) is not the best choice, as it is primarily designed for block-level storage and may not provide the same level of performance and concurrency as EFS for shared file access across multiple instances.\n\nC. Amazon S3 is an object storage service, which is not optimized for the low-latency, high-throughput requirements of the application. While S3 Versioning and Lifecycle policies can be used to retain the data, S3 is generally slower than file storage services like EFS for this type of workload.\n\nD. Using local storage on each pod and relying on a third-party backup tool does not provide the same level of performance, availability, and data protection as using a shared file system like EFS with AWS Backup. The local storage approach may also introduce challenges in terms of data consistency and access across the ReplicaSet instances."
  },
  "176": {
    "question": "A company runs a customer service center that accepts calls and automatically sends all customers a managed, interactive, two-way experience survey by text message. The applications that support the customer service center run on machines that the company hosts in an on-premises data center. The hardware that the company uses is old, and the company is experiencing downtime with the system. The company wants to migrate the system to AWS to improve reliability.Which solution will meet these requirements with the LEAST ongoing operational overhead?",
    "choices": [
      "A. Use Amazon Connect to replace the old call center hardware. Use Amazon Pinpoint to send text message surveys to customers.",
      "B. Use Amazon Connect to replace the old call center hardware. Use Amazon Simple Notification Service (Amazon SNS) to send text message surveys to customers.",
      "C. Migrate the call center software to Amazon EC2 instances that are in an Auto Scaling group. Use the EC2 instances to send text message surveys to customers.",
      "D. Use Amazon Pinpoint to replace the old call center hardware and to send text message surveys to customers."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\n\nThe correct answer is option A: \"Use Amazon Connect to replace the old call center hardware. Use Amazon Pinpoint to send text message surveys to customers.\"\n\nThis solution provides the least ongoing operational overhead for the following reasons:\n\n- Amazon Connect is a fully managed contact center service, which means AWS handles the provisioning, scaling, and maintenance of the underlying infrastructure. This reduces the operational overhead for the company compared to managing an on-premises call center system.\n- Amazon Pinpoint is a fully managed service for sending SMS messages, which also reduces the operational overhead for the company compared to managing the SMS messaging infrastructure themselves.\n- By using these two integrated AWS services, the company can migrate the entire customer service and survey system to the cloud, simplifying the overall architecture and reducing the need for ongoing maintenance and management of the underlying infrastructure.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. Use Amazon Connect to replace the old call center hardware. Use Amazon Simple Notification Service (Amazon SNS) to send text message surveys to customers.\n- This solution is suboptimal because Amazon SNS is a general-purpose messaging service, whereas Amazon Pinpoint is specifically designed for marketing and customer engagement use cases, including SMS messaging. Pinpoint provides more features and capabilities for managing and tracking customer surveys compared to SNS.\n\nC. Migrate the call center software to Amazon EC2 instances that are in an Auto Scaling group. Use the EC2 instances to send text message surveys to customers.\n- This solution requires more operational overhead compared to the correct answer (A). The company would need to manage the EC2 instances, including provisioning, scaling, and maintaining the underlying infrastructure. Additionally, the company would need to set up the SMS messaging functionality on their own, which adds more operational complexity compared to using a fully managed service like Amazon Pinpoint.\n\nD. Use Amazon Pinpoint to replace the old call center hardware and to send text message surveys to customers.\n- This solution is incomplete because it does not address the call center functionality. While Amazon Pinpoint can handle the SMS survey sending, the company still needs to replace the old call center hardware, which is not addressed in this solution. The correct answer (A) provides a more comprehensive solution by using both Amazon Connect for the call center and Amazon Pin"
  },
  "177": {
    "question": "A company is building a call center by using Amazon Connect. The company\u2019s operations team is defining a disaster recovery (DR) strategy across AWS Regions. The contact center has dozens of contact flows, hundreds of users, and dozens of claimed phone numbers.Which solution will provide DR with the LOWEST RTO?",
    "choices": [
      "A. Create an AWS Lambda function to check the availability of the Amazon Connect instance and to send a notification to the operations team in case of unavailability. Create an Amazon EventBridge rule to invoke the Lambda function every 5 minutes. After notification, instruct the operations team to use the AWS Management Console to provision a new Amazon Connect instance in a second Region. Deploy the contact flows, users, and claimed phone numbers by using an AWS CloudFormation template.",
      "B. Provision a new Amazon Connect instance with all existing users in a second Region. Create an AWS Lambda function to check the availability of the Amazon Connect instance. Create an Amazon EventBridge rule to invoke the Lambda function every 5 minutes. In the event of an issue, configure the Lambda function to deploy an AWS CloudFormation template that provisions contact flows and claimed numbers in the second Region.",
      "C. Provision a new Amazon Connect instance with all existing contact flows and claimed phone numbers in a second Region. Create an Amazon Route 53 health check for the URL of the Amazon Connect instance. Create an Amazon CloudWatch alarm for failed health checks. Create an AWS Lambda function to deploy an AWS CloudFormation template that provisions all users. Configure the alarm to invoke the Lambda function.",
      "D. Provision a new Amazon Connect instance with all existing users and contact flows in a second Region. Create an Amazon Route 53 health check for the URL of the Amazon Connect instance. Create an Amazon CloudWatch alarm for failed health checks. Create an AWS Lambda function to deploy an AWS CloudFormation template that provisions claimed phone numbers. Configure the alarm to invoke the Lambda function."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (D):\n\nThe correct answer is option D because it provides the lowest Recovery Time Objective (RTO) in the event of a disaster. By pre-provisioning a new Amazon Connect instance in a second AWS Region with all the existing users and contact flows already in place, the only remaining task during the disaster recovery process is to deploy the claimed phone numbers. This can be done quickly and efficiently using an AWS Lambda function triggered by a CloudWatch alarm when the primary instance becomes unavailable, as determined by an Amazon Route 53 health check. This approach minimizes the time needed to restore the full functionality of the contact center, resulting in the lowest RTO.\n\n2. Explanations of the incorrect choices:\n\nA. This option requires manual intervention to provision a new Amazon Connect instance and deploy the contact flows, users, and phone numbers using an AWS CloudFormation template. This manual process would take more time than the automated approach in option D, resulting in a higher RTO.\n\nB. This option also requires deploying the contact flows and phone numbers using an AWS CloudFormation template, which would take more time than the pre-provisioned approach in option D.\n\nC. This option pre-provisions the contact flows and phone numbers, but still requires deploying the users using an AWS CloudFormation template. The time required to deploy the users would be longer than the approach in option D, which has the users already pre-provisioned in the secondary Region."
  },
  "178": {
    "question": "A company runs an application on AWS. The company curates data from several different sources. The company uses proprietary algorithms to perform data transformations and aggregations. After the company performs ETL processes, the company stores the results in Amazon Redshift tables. The company sells this data to other companies. The company downloads the data as files from the Amazon Redshift tables and transmits the files to several data customers by using FTP. The number of data customers has grown significantly. Management of the data customers has become difficult.The company will use AWS Data Exchange to create a data product that the company can use to share data with customers. The company wants to confirm the identities of the customers before the company shares data. The customers also need access to the most recent data when the company publishes the data.Which solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      "A. Use AWS Data Exchange for APIs to share data with customers. Configure subscription verification. In the AWS account of the company that produces the data, create an Amazon API Gateway Data API service integration with Amazon Redshift. Require the data customers to subscribe to the data product.",
      "B. In the AWS account of the company that produces the data, create an AWS Data Exchange datashare by connecting AWS Data Exchange to the Redshift cluster. Configure subscription verification. Require the data customers to subscribe to the data product.",
      "C. Download the data from the Amazon Redshift tables to an Amazon S3 bucket periodically. Use AWS Data Exchange for S3 to share data with customers. Configure subscription verification. Require the data customers to subscribe to the data product.",
      "D. Publish the Amazon Redshift data to an Open Data on AWS Data Exchange. Require the customers to subscribe to the data product in AWS Data Exchange. In the AWS account of the company that produces the data, attach IAM resource-based policies to the Amazon Redshift tables to allow access only to verified AWS accounts."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B because it meets the requirements with the least operational overhead. By creating an AWS Data Exchange datashare by connecting AWS Data Exchange to the Redshift cluster, the company can easily share data with customers while confirming their identities through subscription verification. This approach allows customers to directly query the data in Redshift, ensuring they have access to the most recent data without the need for additional data extraction, transformation, or loading processes. The use of a datashare minimizes operational overhead compared to the other options.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option requires creating an Amazon API Gateway Data API service integration with Amazon Redshift, which adds more complexity and operational overhead compared to using a datashare.\n\nC. This option requires downloading the data from Amazon Redshift to an Amazon S3 bucket periodically, adding an extra step and potential data latency compared to using a datashare.\n\nD. This option publishes the data to an Open Data on AWS Data Exchange, which does not allow for subscription verification. This means that the company cannot confirm the identities of the customers before sharing the data, which does not meet the requirement.\n\nIn summary, the correct answer (B) is the most suitable solution as it provides the desired functionalities with the least operational overhead by leveraging the AWS Data Exchange datashare feature."
  },
  "179": {
    "question": "A company runs a processing engine in the AWS Cloud. The engine processes environmental data from logistics centers to calculate a sustainability index. The company has millions of devices in logistics centers that are spread across Europe. The devices send information to the processing engine through a RESTful API.The API experiences unpredictable bursts of traffic. The company must implement a solution to process all data that the devices send to the processing engine. Data loss is unacceptable.Which solution will meet these requirements?",
    "choices": [
      "A. Create an Application Load Balancer (ALB) for the RESTful API. Create an Amazon Simple Queue Service (Amazon SQS) queue. Create a listener and a target group for the ALB Add the SQS queue as the target. Use a container that runs in Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type to process messages in the queue.",
      "B. Create an Amazon API Gateway HTTP API that implements the RESTful API. Create an Amazon Simple Queue Service (Amazon SQS) queue. Create an API Gateway service integration with the SQS queue. Create an AWS Lambda function to process messages in the SQS queue.",
      "C. Create an Amazon API Gateway REST API that implements the RESTful API. Create a fleet of Amazon EC2 instances in an Auto Scaling group. Create an API Gateway Auto Scaling group proxy integration. Use the EC2 instances to process incoming data.",
      "D. Create an Amazon CloudFront distribution for the RESTful API. Create a data stream in Amazon Kinesis Data Streams. Set the data stream as the origin for the distribution. Create an AWS Lambda function to consume and process data in the data stream."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B because it provides a solution that meets the requirements of the scenario:\n\n- Amazon API Gateway HTTP API is used to implement the RESTful API, which can handle the unpredictable bursts of traffic.\n- Amazon SQS queue is used to ensure that all data sent by the devices is processed without any loss, as SQS can handle high volumes of messages and provide reliable message delivery.\n- AWS Lambda function is used to process the messages in the SQS queue, which can scale automatically to handle the incoming data.\n\nThis solution ensures that all data is processed without any loss, and the use of serverless services (Amazon API Gateway and AWS Lambda) provides a scalable and cost-effective solution to handle the unpredictable traffic.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option is incorrect because the Application Load Balancer (ALB) cannot directly target an Amazon SQS queue. The ALB is designed to distribute traffic across EC2 instances, not SQS queues.\n\nC. This option is incorrect because while Amazon API Gateway and EC2 Auto Scaling can handle high loads, they don't provide a built-in mechanism to ensure that all messages are processed without loss. The EC2 instances may not be able to keep up with the unpredictable bursts of traffic.\n\nD. This option is incorrect because Amazon CloudFront is a content delivery network (CDN), and it is not typically used to handle incoming API requests. It is primarily used to cache and deliver content to users. Additionally, using Kinesis Data Streams may not be the most appropriate choice for handling the unpredictable bursts of traffic, as it is designed for continuous data streams rather than intermittent bursts."
  },
  "180": {
    "question": "A company runs a processing engine in the AWS Cloud. The engine processes environmental data from logistics centers to calculate a sustainability index. The company has millions of devices in logistics centers that are spread across Europe. The devices send information to the processing engine through a RESTful API.The API experiences unpredictable bursts of traffic. The company must implement a solution to process all data that the devices send to the processing engine. Data loss is unacceptable.Which solution will meet these requirements?",
    "choices": [
      "A. Create an Application Load Balancer (ALB) for the RESTful API. Create an Amazon Simple Queue Service (Amazon SQS) queue. Create a listener and a target group for the ALB Add the SQS queue as the target. Use a container that runs in Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type to process messages in the queue.",
      "B. Create an Amazon API Gateway HTTP API that implements the RESTful API. Create an Amazon Simple Queue Service (Amazon SQS) queue. Create an API Gateway service integration with the SQS queue. Create an AWS Lambda function to process messages in the SQS queue.",
      "C. Create an Amazon API Gateway REST API that implements the RESTful API. Create a fleet of Amazon EC2 instances in an Auto Scaling group. Create an API Gateway Auto Scaling group proxy integration. Use the EC2 instances to process incoming data.",
      "D. Create an Amazon CloudFront distribution for the RESTful API. Create a data stream in Amazon Kinesis Data Streams. Set the data stream as the origin for the distribution. Create an AWS Lambda function to consume and process data in the data stream."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (Option B):\n\nOption B is the correct answer because it aligns well with the requirements of the scenario:\n\n- It uses Amazon API Gateway HTTP API to implement the RESTful API, which can handle the unpredictable bursts of traffic.\n- It integrates the API Gateway with Amazon SQS queue to ensure that all data sent by the devices is captured and processed without any data loss.\n- It uses an AWS Lambda function to process the messages in the SQS queue, which provides a scalable and serverless solution to handle the processing workload.\n\nThis solution ensures that all data is captured and processed without any data loss, which is a critical requirement in the scenario.\n\n2. Explanations of why the incorrect choices are wrong:\n\nOption A is incorrect because Application Load Balancer (ALB) cannot directly target an Amazon SQS queue. ALB is designed to load balance HTTP/HTTPS traffic, not to integrate with messaging services like SQS.\n\nOption C is incorrect because while Amazon API Gateway and EC2 Auto Scaling can handle high loads, they don't provide a built-in mechanism to ensure that all messages are processed without loss. The EC2 instances in the Auto Scaling group may not be able to keep up with the unpredictable bursts of traffic, leading to potential data loss.\n\nOption D is incorrect because Amazon CloudFront is a content delivery network (CDN) and is not typically used to handle incoming API requests. It is primarily designed to cache and deliver content to users, not to process real-time data streams.\n\nIn summary, Option B is the correct answer because it uses the appropriate AWS services (API Gateway, SQS, and Lambda) to ensure that all data is captured and processed without any data loss, which is the critical requirement in the scenario."
  },
  "181": {
    "question": "A company is designing its network configuration in the AWS Cloud. The company uses AWS Organizations to manage a multi-account setup. The company has three OUs. Each OU contains more than 100 AWS accounts. Each account has a single VPC, and all the VPCs in each OU are in the same AWS Region.The CIDR ranges for all the AWS accounts do not overlap. The company needs to implement a solution in which VPCs in the same OU can communicate with each other but cannot communicate with VPCs in other OUs.Which solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      "A. Create an AWS CloudFormation stack set that establishes VPC peering between accounts in each OU. Provision the stack set in each OU.",
      "B. In each OU, create a dedicated networking account that has a single VPC. Share this VPC with all the other accounts in the OU by using AWS Resource Access Manager (AWS RAM). Create a VPC peering connection between the networking account and each account in the OU.",
      "C. Provision a transit gateway in an account in each OU. Share the transit gateway across the organization by using AWS Resource Access Manager (AWS RAM). Create transit gateway VPC attachments for each VPC.",
      "D. In each OU, create a dedicated networking account that has a single VPC. Establish a VPN connection between the networking account and the other accounts in the OU. Use third-party routing software to route transitive traffic between the VPCs."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is option C, which suggests using a transit gateway to enable communication between VPCs within the same OU while preventing communication between VPCs in different OUs.\n\nThis solution is the least operationally complex because it leverages the capabilities of AWS Transit Gateway, a network transit hub that allows you to connect multiple VPCs and on-premises networks through a single gateway. By provisioning a transit gateway in one account and sharing it across the organization using AWS Resource Access Manager (RAM), the company can easily establish VPC attachments to the transit gateway for all the VPCs in the same OU, enabling seamless communication. The use of a shared transit gateway also eliminates the need for manual VPC peering or VPN connections, reducing the operational overhead.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. VPC peering between accounts in each OU would require manually creating and managing multiple VPC peering connections, which can become operationally complex as the number of accounts and VPCs grows.\n\nB. Creating a dedicated networking account with a single VPC and sharing it with other accounts in the OU using AWS RAM would still require manual VPC peering or VPN connections between the networking account and each account in the OU, leading to higher operational overhead.\n\nD. Establishing VPN connections between a dedicated networking account and the other accounts in the OU, along with the need for third-party routing software to manage transitive traffic, would result in a more complex and less scalable solution compared to the transit gateway approach.\n\nIn summary, the correct answer (C) leverages the capabilities of AWS Transit Gateway, which provides a centralized and scalable approach to connecting VPCs within the same OU while preventing communication between VPCs in different OUs, resulting in the least operational overhead."
  },
  "182": {
    "question": "A company is migrating an application to AWS. It wants to use fully managed services as much as possible during the migration. The company needs to store large important documents within the application with the following requirements:1. The data must be highly durable and available2. The data must always be encrypted at rest and in transit3. The encryption key must be managed by the company and rotated periodicallyWhich of the following solutions should the solutions architect recommend?",
    "choices": [
      "A. Deploy the storage gateway to AWS in file gateway mode. Use Amazon EBS volume encryption using an AWS KMS key to encrypt the storage gateway volumes.",
      "B. Use Amazon S3 with a bucket policy to enforce HTTPS for connections to the bucket and to enforce server-side encryption and AWS KMS for object encryption.",
      "C. Use Amazon DynamoDB with SSL to connect to DynamoDB. Use an AWS KMS key to encrypt DynamoDB objects at rest.",
      "D. Deploy instances with Amazon EBS volumes attached to store this data. Use EBS volume encryption using an AWS KMS key to encrypt the data."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B, which recommends using Amazon S3 with a bucket policy to enforce HTTPS for connections to the bucket and to enforce server-side encryption and AWS KMS for object encryption.\n\nThis solution meets all the given requirements:\n\n1. **Highly durable and available**: Amazon S3 provides 11 9's of durability and high availability, making it an excellent choice for storing large, important documents.\n\n2. **Encryption at rest and in transit**: Amazon S3 supports server-side encryption, which ensures that data is automatically encrypted at rest. By using a bucket policy to enforce HTTPS, the data is also encrypted in transit.\n\n3. **Encryption key management**: The solution uses an AWS KMS (Key Management Service) key to encrypt the data, which allows the company to manage and rotate the encryption key as required.\n\n2. Explanations of why the other choices are incorrect:\n\nA. Deploy the storage gateway to AWS in file gateway mode. Use Amazon EBS volume encryption using an AWS KMS key to encrypt the storage gateway volumes.\n- This solution does not meet the requirement for high durability and availability, as Amazon EBS volumes are not as durable and highly available as Amazon S3.\n\nC. Use Amazon DynamoDB with SSL to connect to DynamoDB. Use an AWS KMS key to encrypt DynamoDB objects at rest.\n- DynamoDB is a NoSQL database, which is not the best choice for storing large documents. It is more suitable for structured data with high-speed access requirements.\n\nD. Deploy instances with Amazon EBS volumes attached to store this data. Use EBS volume encryption using an AWS KMS key to encrypt the data.\n- This solution also does not meet the requirement for high durability and availability, as Amazon EBS volumes are not as durable and highly available as Amazon S3.\n\nIn summary, the correct answer (B) is the best solution because it meets all the given requirements, including high durability and availability, encryption at rest and in transit, and the ability to manage and rotate the encryption key using AWS KMS."
  },
  "183": {
    "question": "A company\u2019s public API runs as tasks on Amazon Elastic Container Service (Amazon ECS). The tasks run on AWS Fargate behind an Application Load Balancer (ALB) and are configured with Service Auto Scaling for the tasks based on CPU utilization. This service has been running well for several months.Recently, API performance slowed down and made the application unusable. The company discovered that a significant number of SQL injection attacks had occurred against the API and that the API service had scaled to its maximum amount.A solutions architect needs to implement a solution that prevents SQL injection attacks from reaching the ECS API service. The solution must allow legitimate traffic through and must maximize operational efficiency.Which solution meets these requirements?",
    "choices": [
      "A. Create a new AWS WAF web ACL to monitor the HTTP requests and HTTPS requests that are forwarded to the ALB in front of the ECS tasks.",
      "B. Create a new AWS WAF Bot Control implementation. Add a rule in the AWS WAF Bot Control managed rule group to monitor traffic and allow only legitimate traffic to the ALB in front of the ECS tasks.",
      "C. Create a new AWS WAF web ACL. Add a new rule that blocks requests that match the SQL database rule group. Set the web ACL to allow all other traffic that does not match those rules. Attach the web ACL to the ALB in front of the ECS tasks.",
      "D. Create a new AWS WAF web ACL. Create a new empty IP set in AWS WAF. Add a new rule to the web ACL to block requests that originate from IP addresses in the new IP set. Create an AWS Lambda function that scrapes the API logs for IP addresses that send SQL injection attacks, and add those IP addresses to the IP set. Attach the web ACL to the ALB in front of the ECS tasks."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C because it provides the most effective solution to prevent SQL injection attacks from reaching the ECS API service. \n\nBy creating a new AWS WAF web ACL and adding a new rule that blocks requests that match the SQL database rule group, the solution can effectively filter out malicious SQL injection attempts before they reach the ECS API service. The SQL database rule group contains rules specifically designed to detect and block common SQL injection attack patterns, making it an appropriate choice for this use case.\n\nAdditionally, the solution is configured to allow all other traffic that does not match the SQL injection rules, ensuring that legitimate traffic can still access the API service. This approach helps maximize the operational efficiency of the system by only filtering out the malicious traffic while allowing legitimate requests to pass through.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This solution does not specifically target SQL injection attacks, as it only monitors the HTTP and HTTPS requests forwarded to the ALB. It does not provide the specialized SQL injection protection that the SQL database rule group in the correct answer offers.\n\nB. AWS WAF Bot Control is designed to monitor and control bot traffic, such as scrapers, scanners, and crawlers. It does not specifically target SQL injection attacks, which are a different type of security threat.\n\nD. This solution is reactive, as it requires an additional AWS Lambda function to scrape the API logs and add IP addresses associated with SQL injection attacks to the IP set. This approach may not be as efficient or proactive as the SQL database rule group in the correct answer, which can directly detect and block the SQL injection attack patterns."
  },
  "184": {
    "question": "An environmental company is deploying sensors in major cities throughout a country to measure air quality. The sensors connect to AWS IoT Core to ingest timeseries data readings. The company stores the data in Amazon DynamoDB.For business continuity, the company must have the ability to ingest and store data in two AWS Regions.Which solution will meet these requirements?",
    "choices": [
      "A. Create an Amazon Route 53 alias failover routing policy with values for AWS IoT Core data endpoints in both Regions Migrate data to Amazon Aurora global tables.",
      "B. Create a domain configuration for AWS IoT Core in each Region. Create an Amazon Route 53 latency-based routing policy. Use AWS IoT Core data endpoints in both Regions as values. Migrate the data to Amazon MemoryDB for Redis and configure cross-Region replication.",
      "C. Create a domain configuration for AWS IoT Core in each Region. Create an Amazon Route 53 health check that evaluates domain configuration health. Create a failover routing policy with values for the domain name from the AWS IoT Core domain configurations. Update the DynamoDB table to a global table.",
      "D. Create an Amazon Route 53 latency-based routing policy. Use AWS IoT Core data endpoints in both Regions as values. Configure DynamoDB streams and cross-Region data replication."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C because it meets the requirements of the problem statement:\n\n- It creates a domain configuration for AWS IoT Core in each Region, ensuring that the sensors can connect to the appropriate AWS IoT Core endpoints in each Region.\n- It creates an Amazon Route 53 health check that evaluates the domain configuration health, allowing for automatic failover between the Regions in case of a failure.\n- It creates a failover routing policy with values for the domain name from the AWS IoT Core domain configurations, ensuring that the sensors can automatically connect to the active Region.\n- It updates the DynamoDB table to a global table, which allows for data replication across Regions, providing the necessary business continuity for data ingestion and storage.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option is incorrect because:\n   - It uses an alias failover routing policy, which is not suitable for automatically failing over between Regions.\n   - Migrating the data to Amazon Aurora global tables is not necessary, as DynamoDB global tables can already provide the required data replication and failover capabilities.\n\nB. This option is incorrect because:\n   - It uses a latency-based routing policy, which does not provide the necessary failover capabilities.\n   - Migrating the data to Amazon MemoryDB for Redis is not necessary, as DynamoDB global tables can already provide the required data replication and failover capabilities.\n\nD. This option is incorrect because:\n   - It uses a latency-based routing policy, which does not provide the necessary failover capabilities.\n   - Configuring DynamoDB streams and cross-Region data replication is not necessary, as DynamoDB global tables can already provide the required data replication and failover capabilities.\n\nThe key difference between the correct answer (C) and the incorrect answers is that C uses a failover routing policy, which is specifically designed to provide automatic failover between Regions, while the other options use less suitable routing policies. Additionally, C leverages the built-in capabilities of DynamoDB global tables, which eliminates the need to migrate the data to other database solutions."
  },
  "185": {
    "question": "A company uses AWS Organizations for a multi-account setup in the AWS Cloud. The company's finance team has a data processing application that uses AWS Lambda and Amazon DynamoDB. The company's marketing team wants to access the data that is stored in the DynamoDB table.The DynamoDB table contains confidential data. The marketing team can have access to only specific attributes of data in the DynamoDB table. The finance team and the marketing team have separate AWS accounts.What should a solutions architect do to provide the marketing team with the appropriate access to the DynamoDB table?",
    "choices": [
      "A. Create an SCP to grant the marketing team's AWS account access to the specific attributes of the DynamoDB table. Attach the SCP to the OU of the finance team.",
      "B. Create an IAM role in the finance team's account by using IAM policy conditions for specific DynamoDB attributes (fine-grained access control). Establish trust with the marketing team's account. In the marketing team's account, create an IAM role that has permissions to assume the IAM role in the finance team's account.",
      "C. Create a resource-based IAM policy that includes conditions for specific DynamoDB attributes (fine-grained access control). Attach the policy to the DynamoDB table. In the marketing team's account, create an IAM role that has permissions to access the DynamoDB table in the finance team's account.",
      "D. Create an IAM role in the finance team's account to access the DynamoDB table. Use an IAM permissions boundary to limit the access to the specific attributes. In the marketing team's account, create an IAM role that has permissions to assume the IAM role in the finance team's account."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B. This is the best solution because it allows the marketing team to access the specific attributes of the DynamoDB table in the finance team's account, while maintaining separation of concerns and fine-grained access control.\n\nThe key steps are:\n\n1. In the finance team's account, create an IAM role with an IAM policy that grants access to the specific DynamoDB attributes that the marketing team needs. This ensures fine-grained access control.\n2. Establish trust between the finance team's account and the marketing team's account, allowing the marketing team to assume the IAM role in the finance team's account.\n3. In the marketing team's account, create an IAM role that has permissions to assume the IAM role in the finance team's account.\n\nThis approach keeps the confidential data in the finance team's account, while allowing the marketing team to access the specific data they need, without compromising the overall security of the DynamoDB table.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This is incorrect because using an SCP (Service Control Policy) to grant access to the marketing team's account is not the appropriate solution. SCPs are used to set service-level permissions and constraints at the organizational unit (OU) level, not to grant fine-grained access to specific data attributes.\n\nC. This is incorrect because, while creating a resource-based IAM policy on the DynamoDB table is a valid approach, it does not address the need for cross-account access. The marketing team would still need an IAM role in their account to access the DynamoDB table in the finance team's account.\n\nD. This is incorrect because using an IAM permissions boundary is not the best solution for this use case. Permissions boundaries are used to limit the maximum permissions that an IAM entity (user or role) can have, but they do not provide a mechanism for cross-account access."
  },
  "186": {
    "question": "A solutions architect is creating an application that stores objects in an Amazon S3 bucket. The solutions architect must deploy the application in two AWS Regions that will be used simultaneously. The objects in the two S3 buckets must remain synchronized with each other.Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose three.)",
    "choices": [
      "A. Create an S3 Multi-Region Access Point Change the application to refer to the Multi-Region Access Point",
      "B. Configure two-way S3 Cross-Region Replication (CRR) between the two S3 buckets",
      "C. Modify the application to store objects in each S3 bucket",
      "D. Create an S3 Lifecycle rule for each S3 bucket to copy objects from one S3 bucket to the other S3 bucket",
      "E. Enable S3 Versioning for each S3 bucket",
      "F. Configure an event notification for each S3 bucket to invoke an AWS Lambda function to copy objects from one S3 bucket to the other S3 bucket"
    ],
    "answer": "ABE",
    "explanation": "1. Explanation of the correct answer (ABE):\n\nA. Create an S3 Multi-Region Access Point and change the application to refer to the Multi-Region Access Point. This is the correct choice because a Multi-Region Access Point allows the application to access the nearest S3 bucket, providing low latency access to the data, regardless of the AWS Region.\n\nB. Configure two-way S3 Cross-Region Replication (CRR) between the two S3 buckets. This is the correct choice because CRR ensures that the objects in the two S3 buckets remain synchronized across the two AWS Regions.\n\nE. Enable S3 Versioning for each S3 bucket. This is the correct choice because S3 Versioning is a requirement for CRR to function properly, as it allows the replication of object versions between the two S3 buckets.\n\n2. Explanations of why the incorrect choices are wrong:\n\nC. Modify the application to store objects in each S3 bucket. This would result in higher operational overhead, as the application would need to manage storing and updating objects in both S3 buckets independently.\n\nD. Create an S3 Lifecycle rule for each S3 bucket to copy objects from one S3 bucket to the other S3 bucket. This approach would not provide automatic synchronization between the two S3 buckets, and the Lifecycle rule would need to be configured and maintained separately for each bucket.\n\nF. Configure an event notification for each S3 bucket to invoke an AWS Lambda function to copy objects from one S3 bucket to the other S3 bucket. This approach would also result in higher operational overhead, as it would require managing the Lambda function and the event notifications for both S3 buckets separately."
  },
  "187": {
    "question": "A company has an IoT platform that runs in an on-premises environment. The platform consists of a server that connects to IoT devices by using the MQTT protocol. The platform collects telemetry data from the devices at least once every 5 minutes. The platform also stores device metadata in a MongoDB cluster.An application that is installed on an on-premises machine runs periodic jobs to aggregate and transform the telemetry and device metadata. The application creates reports that users view by using another web application that runs on the same on-premises machine. The periodic jobs take 120-600 seconds to run. However, the web application is always running.The company is moving the platform to AWS and must reduce the operational overhead of the stack.Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose three.)",
    "choices": [
      "A. Use AWS Lambda functions to connect to the IoT devices",
      "B. Configure the IoT devices to publish to AWS IoT Core",
      "C. Write the metadata to a self-managed MongoDB database on an Amazon EC2 instance",
      "D. Write the metadata to Amazon DocumentDB (with MongoDB compatibility)",
      "E. Use AWS Step Functions state machines with AWS Lambda tasks to prepare the reports and to write the reports to Amazon S3. Use Amazon CloudFront with an S3 origin to serve the reports",
      "F. Use an Amazon Elastic Kubernetes Service (Amazon EKS) cluster with Amazon EC2 instances to prepare the reports. Use an ingress controller in the EKS cluster to serve the reports"
    ],
    "answer": "BDE",
    "explanation": "1. Explanation of the correct answer (BDE):\n\nThe correct answer is BDE because this combination of steps would meet the requirements with the least operational overhead.\n\nB. Configure the IoT devices to publish to AWS IoT Core:\nThis is the right approach as it offloads the responsibility of managing the MQTT connection and device communication to AWS IoT Core, reducing the operational overhead on the company's side.\n\nD. Write the metadata to Amazon DocumentDB (with MongoDB compatibility):\nUsing Amazon DocumentDB eliminates the need for the company to manage and maintain a MongoDB cluster, reducing the operational overhead.\n\nE. Use AWS Step Functions state machines with AWS Lambda tasks to prepare the reports and to write the reports to Amazon S3. Use Amazon CloudFront with an S3 origin to serve the reports:\nThis approach leverages serverless technologies (AWS Lambda and AWS Step Functions) to automate the report generation process, reducing the operational overhead. Additionally, serving the reports using Amazon CloudFront and S3 is a highly scalable and low-maintenance solution.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Use AWS Lambda functions to connect to the IoT devices:\nThis is not the right approach as it would add more operational overhead to the company. Connecting directly to the IoT devices using AWS Lambda functions would be more complex and less efficient than using AWS IoT Core.\n\nC. Write the metadata to a self-managed MongoDB database on an Amazon EC2 instance:\nMaintaining and managing a MongoDB cluster on an Amazon EC2 instance would increase the operational overhead for the company, which is the opposite of what the question is asking for.\n\nF. Use an Amazon Elastic Kubernetes Service (Amazon EKS) cluster with Amazon EC2 instances to prepare the reports. Use an ingress controller in the EKS cluster to serve the reports:\nWhile this approach is also valid, it would have a higher operational overhead compared to the correct answer (BDE). Managing and maintaining an EKS cluster would require more effort than the serverless approach provided in the correct answer."
  },
  "188": {
    "question": "A global manufacturing company plans to migrate the majority of its applications to AWS. However, the company is concerned about applications that need to remain within a specific country or in the company's central on-premises data center because of data regulatory requirements or requirements for latency of single-digit milliseconds. The company also is concerned about the applications that it hosts in some of its factory sites, where limited network infrastructure exists.The company wants a consistent developer experience so that its developers can build applications once and deploy on premises, in the cloud, or in a hybrid architecture. The developers must be able to use the same tools, APIs, and services that are familiar to them.Which solution will provide a consistent hybrid experience to meet these requirements?",
    "choices": [
      "A. Migrate all applications to the closest AWS Region that is compliant. Set up an AWS Direct Connect connection between the central on-premises data center and AWS. Deploy a Direct Connect gateway.",
      "B. Use AWS Snowball Edge Storage Optimized devices for the applications that have data regulatory requirements or requirements for latency of single-digit milliseconds. Retain the devices on premises. Deploy AWS Wavelength to host the workloads in the factory sites.",
      "C. Install AWS Outposts for the applications that have data regulatory requirements or requirements for latency of single-digit milliseconds. Use AWS Snowball Edge Compute Optimized devices to host the workloads in the factory sites.",
      "D. Migrate the applications that have data regulatory requirements or requirements for latency of single-digit milliseconds to an AWS Local Zone. Deploy AWS Wavelength to host the workloads in the factory sites."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C. Install AWS Outposts for the applications that have data regulatory requirements or requirements for latency of single-digit milliseconds. Use AWS Snowball Edge Compute Optimized devices to host the workloads in the factory sites.\n\nThis option best addresses the key requirements:\n- Data regulatory requirements: AWS Outposts allow you to run compute and storage on-premises, within the specific country, to meet data sovereignty requirements.\n- Single-digit milliseconds latency: AWS Outposts provide low-latency access to AWS services, meeting the requirement for sub-10ms latency.\n- Consistent developer experience: Developers can use the same tools, APIs, and services they are familiar with when deploying to on-premises Outposts or to the cloud.\n- Factory site workloads: AWS Snowball Edge Compute Optimized devices can be deployed to the factory sites with limited network infrastructure, providing a local compute solution.\n\n2. Explanations of why the other choices are incorrect:\n\nA. Migrate all applications to the closest AWS Region that is compliant. Set up an AWS Direct Connect connection between the central on-premises data center and AWS. Deploy a Direct Connect gateway.\n- This option does not address the data regulatory requirements or single-digit milliseconds latency, as the applications would be hosted in an AWS Region, potentially outside the specific country.\n\nB. Use AWS Snowball Edge Storage Optimized devices for the applications that have data regulatory requirements or requirements for latency of single-digit milliseconds. Retain the devices on premises. Deploy AWS Wavelength to host the workloads in the factory sites.\n- Snowball Edge devices are a good fit for the factory sites, but Wavelength is designed for mobile networks and may not provide the required low latency for the on-premises applications.\n\nD. Migrate the applications that have data regulatory requirements or requirements for latency of single-digit milliseconds to an AWS Local Zone. Deploy AWS Wavelength to host the workloads in the factory sites.\n- AWS Local Zones can provide low latency, but they may not be available in all the required countries. Wavelength, again, is focused on mobile networks and may not be the best fit for"
  },
  "189": {
    "question": "A company is updating an application that customers use to make online orders. The number of attacks on the application by bad actors has increased recently.The company will host the updated application on an Amazon Elastic Container Service (Amazon ECS) cluster. The company will use Amazon DynamoDB to store application data. A public Application Load Balancer (ALB) will provide end users with access to the application. The company must prevent attacks and ensure business continuity with minimal service interruptions during an ongoing attack.Which combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
    "choices": [
      "A. Create an Amazon CloudFront distribution with the ALB as the origin. Add a custom header and random value on the CloudFront domain. Configure the ALB to conditionally forward traffic if the header and value match.",
      "B. Deploy the application in two AWS Regions. Configure Amazon Route 53 to route to both Regions with equal weight.",
      "C. Configure auto scaling for Amazon ECS tasks Create a DynamoDB Accelerator (DAX) cluster.",
      "D. Configure Amazon ElastiCache to reduce overhead on DynamoDB.",
      "E. Deploy an AWS WAF web ACL that includes an appropriate rule group. Associate the web ACL with the Amazon CloudFront distribution."
    ],
    "answer": "AE",
    "explanation": "1. Explanation of the correct answer (A and E):\n\nA. Create an Amazon CloudFront distribution with the ALB as the origin. Add a custom header and random value on the CloudFront domain. Configure the ALB to conditionally forward traffic if the header and value match.\n- This approach adds an additional layer of security by validating requests at the CloudFront level before they reach the Application Load Balancer (ALB). The custom header and random value act as a simple form of request validation, helping to filter out potentially malicious requests.\n\nE. Deploy an AWS WAF web ACL that includes an appropriate rule group. Associate the web ACL with the Amazon CloudFront distribution.\n- AWS WAF (Web Application Firewall) provides a set of customizable security rules to protect web applications from common web-based attacks, such as SQL injection and cross-site scripting (XSS). By associating the WAF web ACL with the CloudFront distribution, you can apply these rules to filter out malicious traffic before it reaches the application.\n\nThe combination of A and E provides both request validation and comprehensive web application firewall protection, which helps to prevent attacks and ensure business continuity with minimal service interruptions during ongoing attacks. This is a cost-effective solution as it leverages existing AWS services without the need for additional infrastructure or complex configurations.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. Deploy the application in two AWS Regions. Configure Amazon Route 53 to route to both Regions with equal weight.\n- While this approach provides high availability and business continuity by distributing the application across multiple Regions, it does not directly address the issue of preventing attacks on the application. It only ensures that the application remains available in the event of an attack, but does not protect the application itself.\n\nC. Configure auto scaling for Amazon ECS tasks Create a DynamoDB Accelerator (DAX) cluster.\n- This choice focuses on scaling and performance optimization for the application infrastructure, but it does not specifically address the requirement of preventing attacks and ensuring business continuity during ongoing attacks.\n\nD. Configure Amazon ElastiCache to reduce overhead on DynamoDB.\n- This choice is also focused on performance optimization for the DynamoDB database, but does not address the security and business continuity requirements stated in the"
  },
  "190": {
    "question": "A company runs a web application on AWS. The web application delivers static content from an Amazon S3 bucket that is behind an Amazon CloudFront distribution. The application serves dynamic content by using an Application Load Balancer (ALB) that distributes requests to a fleet of Amazon EC2 instances in Auto Scaling groups. The application uses a domain name setup in Amazon Route 53.Some users reported occasional issues when the users attempted to access the website during peak hours. An operations team found that the ALB sometimes returned HTTP 503 Service Unavailable errors. The company wants to display a custom error message page when these errors occur. The page should be displayed immediately for this error code.Which solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      "A. Set up a Route 53 failover routing policy. Configure a health check to determine the status of the ALB endpoint and to fail over to the failover S3 bucket endpoint.",
      "B. Create a second CloudFront distribution and an S3 static website to host the custom error page. Set up a Route 53 failover routing policy. Use an active-passive configuration between the two distributions.",
      "C. Create a CloudFront origin group that has two origins. Set the ALB endpoint as the primary origin. For the secondary origin, set an S3 bucket that is configured to host a static website Set up origin failover for the CloudFront distribution. Update the S3 static website to incorporate the custom error page.",
      "D. Create a CloudFront function that validates each HTTP response code that the ALB returns. Create an S3 static website in an S3 bucket. Upload the custom error page to the S3 bucket as a failover. Update the function to read the S3 bucket and to serve the error page to the end users."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C because it uses the CloudFront origin group feature to provide a seamless failover mechanism for serving the custom error page. \n\nThe key points are:\n\n- The CloudFront origin group has two origins: the primary origin is the Application Load Balancer (ALB), and the secondary origin is an S3 bucket configured as a static website.\n- When the ALB returns HTTP 503 Service Unavailable errors, CloudFront will automatically failover to the secondary origin (the S3 bucket) and serve the custom error page.\n- Updating the S3 static website to incorporate the custom error page ensures that the error page is readily available and can be served to users without any additional processing or delays.\n\nThis solution meets the requirements of displaying a custom error message page immediately for the HTTP 503 error code with the least operational overhead, as it leverages the built-in failover capabilities of CloudFront origin groups.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This solution uses a Route 53 failover routing policy, which is not the most appropriate for the immediate display of a custom error page. The failover process may introduce additional delays, and the custom error page would need to be hosted on a separate S3 bucket.\n\nB. This solution involves creating a second CloudFront distribution and an S3 static website to host the custom error page, which adds unnecessary complexity and operational overhead. It also uses a Route 53 failover routing policy, which is not the most appropriate for the immediate display of a custom error page.\n\nD. This solution involves creating a CloudFront function to validate the HTTP response codes and serve the custom error page from an S3 bucket. However, CloudFront functions cannot perform network access, so they cannot read the content of the S3 bucket and serve the error page directly. This solution would not meet the requirement of immediate display of the custom error page."
  },
  "191": {
    "question": "A company is planning to migrate an application to AWS. The application runs as a Docker container and uses an NFS version 4 file share.A solutions architect must design a secure and scalable containerized solution that does not require provisioning or management of the underlying infrastructure.Which solution will meet these requirements?",
    "choices": [
      "A. Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type. Use Amazon Elastic File System (Amazon EFS) for shared storage. Reference the EFS file system ID, container mount point, and EFS authorization IAM role in the ECS task definition.",
      "B. Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type. Use Amazon FSx for Lustre for shared storage. Reference the FSx for Lustre file system ID, container mount point, and FSx for Lustre authorization IAM role in the ECS task definition.",
      "C. Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Amazon EC2 launch type and auto scaling turned on. Use Amazon Elastic File System (Amazon EFS) for shared storage. Mount the EFS file system on the ECS container instances. Add the EFS authorization IAM role to the EC2 instance profile.",
      "D. Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Amazon EC2 launch type and auto scaling turned on. Use Amazon Elastic Block Store (Amazon EBS) volumes with Multi-Attach enabled for shared storage. Attach the EBS volumes to ECS container instances. Add the EBS authorization IAM role to an EC2 instance profile."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\nThe correct answer is A because it meets all the requirements of the question:\n\n- The application runs as a Docker container, and Amazon ECS with the Fargate launch type allows you to deploy and run containerized applications without having to manage the underlying infrastructure.\n- The application uses an NFS version 4 file share, and Amazon EFS provides a managed NFS file system that can be used as shared storage for the containers.\n- By referencing the EFS file system ID, container mount point, and EFS authorization IAM role in the ECS task definition, the solution ensures a secure and scalable containerized solution without the need to provision or manage the underlying infrastructure.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. This option is incorrect because Amazon FSx for Lustre is not compatible with the Fargate launch type of Amazon ECS. Fargate only supports specific storage options, and FSx for Lustre is not one of them.\n\nC. This option is incorrect because it requires the management of the underlying EC2 infrastructure, which goes against the requirement of not needing to provision or manage the underlying infrastructure.\n\nD. This option is incorrect because it requires the management of the underlying EC2 infrastructure, which goes against the requirement of not needing to provision or manage the underlying infrastructure. Additionally, using Amazon EBS volumes with Multi-Attach enabled may not be the best solution for a shared file system, as it is primarily designed for block-level storage."
  },
  "192": {
    "question": "A company is running an application in the AWS Cloud. The core business logic is running on a set of Amazon EC2 instances in an Auto Scaling group. An Application Load Balancer (ALB) distributes traffic to the EC2 instances. Amazon Route 53 record api.example.com is pointing to the ALB.The company's development team makes major updates to the business logic. The company has a rule that when changes are deployed, only 10% of customers can receive the new logic during a testing window. A customer must use the same version of the business logic during the testing window.How should the company deploy the updates to meet these requirements?",
    "choices": [
      "A. Create a second ALB, and deploy the new logic to a set of EC2 instances in a new Auto Scaling group. Configure the ALB to distribute traffic to the EC2 instances. Update the Route 53 record to use weighted routing, and point the record to both of the ALBs.",
      "B. Create a second target group that is referenced by the ALDeploy the new logic to EC2 instances in this new target group. Update the ALB listener rule to use weighted target groups. Configure ALB target group stickiness.",
      "C. Create a new launch configuration for the Auto Scaling group. Specify the launch configuration to use the AutoScalingRollingUpdate policy, and set the MaxBatchSize option to 10. Replace the launch configuration on the Auto Scaling group. Deploy the changes.",
      "D. Create a second Auto Scaling group that is referenced by the ALB. Deploy the new logic on a set of EC2 instances in this new Auto Scaling group. Change the ALB routing algorithm to least outstanding requests (LOR). Configure ALB session stickiness."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B because it addresses the requirement that a customer must use the same version of the business logic during the testing window. To achieve this, the solution suggests creating a second target group that is referenced by the Application Load Balancer (ALB) and deploying the new logic to EC2 instances in this new target group. This allows the ALB to distribute traffic to the two target groups (the original and the new one) using weighted routing, while also enabling ALB target group stickiness. The stickiness ensures that a customer is consistently routed to the same target group (and therefore the same version of the business logic) during the testing window.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option is incorrect because it involves creating a second ALB and updating the Route 53 record to use weighted routing between the two ALBs. While this approach would allow for traffic distribution, it does not provide the necessary session stickiness to ensure that a customer uses the same version of the business logic during the testing window.\n\nC. This option is incorrect because it involves using the AutoScalingRollingUpdate policy to replace the launch configuration on the existing Auto Scaling group. This approach does not provide the ability to route a specific percentage of traffic to the new business logic, nor does it ensure that a customer uses the same version of the business logic during the testing window.\n\nD. This option is incorrect because it involves creating a second Auto Scaling group referenced by the ALB and changing the ALB routing algorithm to least outstanding requests (LOR). While this would allow for traffic distribution, it does not provide the necessary session stickiness to ensure that a customer uses the same version of the business logic during the testing window."
  },
  "193": {
    "question": "A large education company recently introduced Amazon Workspaces to provide access to internal applications across multiple universities. The company is storing user profiles on an Amazon FSx for Windows File Server file system. The file system is configured with a DNS alias and is connected to a self-managed Active Directory. As more users begin to use the Workspaces, login time increases to unacceptable levels.An investigation reveals a degradation in performance of the file system. The company created the file system on HDD storage with a throughput of 16 MBps. A solutions architect must improve the performance of the file system during a defined maintenance window.What should the solutions architect do to meet these requirements with the LEAST administrative effort?",
    "choices": [
      "A. Use AWS Backup to create a point-in-time backup of the file system. Restore the backup to a new FSx for Windows File Server file system. Select SSD as the storage type. Select 32 MBps as the throughput capacity. When the backup and restore process is completed, adjust the DNS alias accordingly. Delete the original file system.",
      "B. Disconnect users from the file system. In the Amazon FSx console, update the throughput capacity to 32 MBps. Update the storage type to SSD. Reconnect users to the file system.",
      "C. Deploy an AWS DataSync agent onto a new Amazon EC2 instance. Create a task. Configure the existing file system as the source location. Configure a new FSx for Windows File Server file system with SSD storage and 32 MBps of throughput as the target location. Schedule the task. When the task is completed, adjust the DNS alias accordingly. Delete the original file system.",
      "D. Enable shadow copies on the existing file system by using a Windows PowerShell command. Schedule the shadow copy job to create a point-in-time backup of the file system. Choose to restore previous versions. Create a new FSx for Windows File Server file system with SSD storage and 32 MBps of throughput. When the copy job is completed, adjust the DNS alias. Delete the original file system."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B because it is the most straightforward and least administratively intensive approach to improve the performance of the Amazon FSx for Windows File Server file system. The key steps are:\n\n- Disconnect users from the file system to ensure no ongoing access during the maintenance window.\n- In the Amazon FSx console, update the throughput capacity to 32 MBps, which will immediately improve the performance of the file system.\n- The storage type cannot be changed from HDD to SSD without creating a new file system, so this is not possible in this case.\n- Once the throughput capacity is updated, reconnect the users to the improved file system.\n\nThis approach requires the least administrative effort as it only involves modifying the throughput capacity of the existing file system, without the need to create a new file system or perform a backup and restore operation.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option involves creating a backup of the file system and restoring it to a new file system with SSD storage and higher throughput capacity. While this would improve performance, it requires additional steps and administrative effort compared to the correct answer.\n\nC. This option involves using AWS DataSync to migrate the data to a new file system with SSD storage and higher throughput capacity. While this would also improve performance, it is more complex and requires additional setup and management of the DataSync agent and task.\n\nD. This option involves enabling shadow copies on the existing file system and then creating a new file system with SSD storage and higher throughput capacity. While this would improve performance, it is more complex and requires additional steps compared to the correct answer."
  },
  "194": {
    "question": "A company hosts an application on AWS. The application reads and writes objects that are stored in a single Amazon S3 bucket. The company must modify the application to deploy the application in two AWS Regions.Which solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      "A. Set up an Amazon CloudFront distribution with the S3 bucket as an origin. Deploy the application to a second Region Modify the application to use the CloudFront distribution. Use AWS Global Accelerator to access the data in the S3 bucket.",
      "B. Create a new S3 bucket in a second Region. Set up bidirectional S3 Cross-Region Replication (CRR) between the original S3 bucket and the new S3 bucket. Configure an S3 Multi-Region Access Point that uses both S3 buckets. Deploy a modified application to both Regions.",
      "C. Create a new S3 bucket in a second Region Deploy the application in the second Region. Configure the application to use the new S3 bucket. Set up S3 Cross-Region Replication (CRR) from the original S3 bucket to the new S3 bucket.",
      "D. Set up an S3 gateway endpoint with the S3 bucket as an origin. Deploy the application to a second Region. Modify the application to use the new S3 gateway endpoint. Use S3 Intelligent-Tiering on the S3 bucket."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (Option B):\n\nThe correct answer is Option B, which involves the following steps:\n- Create a new S3 bucket in a second AWS Region.\n- Set up bidirectional S3 Cross-Region Replication (CRR) between the original S3 bucket and the new S3 bucket.\n- Configure an S3 Multi-Region Access Point that uses both S3 buckets.\n- Deploy the modified application to both Regions.\n\nThis solution meets the requirement to deploy the application in two AWS Regions while providing the least operational overhead. The use of bidirectional S3 CRR ensures that the data in both S3 buckets remains in sync, allowing the application to access the data from either Region. The S3 Multi-Region Access Point provides a single, global endpoint for the application, abstracting away the underlying bucket locations and simplifying the application's configuration.\n\n2. Explanations of the incorrect choices:\n\nA. This option introduces additional services (CloudFront and AWS Global Accelerator) that add complexity to the solution and may not be necessary for the given requirements. The application could directly access the S3 buckets without the need for these additional services.\n\nC. This option sets up unidirectional S3 CRR, which means the application in the second Region would only have access to the data replicated from the original S3 bucket. It does not provide the same level of data consistency and access as the bidirectional CRR in the correct answer.\n\nD. This option introduces an S3 gateway endpoint, which is a type of VPC endpoint that allows private access to S3 from within a VPC. However, the question does not mention any requirement for a VPC or the need for private access to S3. Additionally, the use of S3 Intelligent-Tiering is not directly relevant to the given requirements."
  },
  "195": {
    "question": "A company is running a large containerized workload in the AWS Cloud. The workload consists of approximately 100 different services. The company uses Amazon Elastic Container Service (Amazon ECS) to orchestrate the workload.Recently the company\u2019s development team started using AWS Fargate instead of Amazon EC2 instances in the ECS cluster. In the past, the workload has come close to running the maximum number of EC2 instances that are available in the account.The company is worried that the workload could reach the maximum number of ECS tasks that are allowed. A solutions architect must implement a solution that will notify the development team when Fargate reaches 80% of the maximum number of tasks.What should the solutions architect do to meet this requirement?",
    "choices": [
      "A. Use Amazon CloudWatch to monitor the Sample Count statistic for each service in the ECS cluster. Set an alarm for when the math expression sample count/SERVICE_QUOTA(service)*100 is greater than 80. Notify the development team by using Amazon Simple Notification Service (Amazon SNS).",
      "B. Use Amazon CloudWatch to monitor service quotas that are published under the AWS/Usage metric namespace. Set an alarm for when the math expression metric/SERVICE_QUOTA(metric)*100 is greater than 80. Notify the development team by using Amazon Simple Notification Service (Amazon SNS).",
      "C. Create an AWS Lambda function to poll detailed metrics from the ECS cluster. When the number of running Fargate tasks is greater than 80, invoke Amazon Simple Email Service (Amazon SES) to notify the development team.",
      "D. Create an AWS Config rule to evaluate whether the Fargate SERVICE_QUOTA is greater than 80. Use Amazon Simple Email Service (Amazon SES) to notify the development team when the AWS Config rule is not compliant."
    ],
    "answer": "B",
    "explanation": "1. Clear explanation of why the correct answer (B) is right:\n\nThe correct answer is B because it uses the most appropriate approach to monitor the ECS Fargate task usage and notify the development team when it reaches 80% of the maximum allowed tasks.\n\nSpecifically:\n\n- The AWS/Usage metric namespace provides detailed metrics for service usage and quotas, including the number of running ECS Fargate tasks. This is the right data source to monitor the Fargate task usage.\n- The math expression `metric/SERVICE_QUOTA(metric)*100` allows the solutions architect to calculate the percentage of the Fargate task quota being used, making it easy to set an alarm when usage reaches 80%.\n- Using Amazon CloudWatch to set an alarm and trigger notifications via Amazon SNS is a native and efficient way to monitor the service usage and notify the development team when the threshold is crossed.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nA. This choice is incorrect because it uses the \"Sample Count\" statistic, which is not the right metric to monitor the Fargate task usage. The question specifically asks to monitor the Fargate task usage, which is better captured by the metrics in the AWS/Usage namespace.\n\nC. This choice is incorrect because it uses AWS Lambda and Amazon SES, which are not the most efficient and native solutions for this use case. The question specifically asks to use Amazon CloudWatch and Amazon SNS for notifications.\n\nD. This choice is incorrect because it uses AWS Config to evaluate the Fargate task quota, which is not the most efficient approach. The question specifically asks to use Amazon CloudWatch to monitor the service usage and quotas."
  },
  "196": {
    "question": "A solutions architect is designing an application to accept timesheet entries from employees on their mobile devices. Timesheets will be submitted weekly, with most of the submissions occurring on Friday. The data must be stored in a format that allows payroll administrators to run monthly reports. The infrastructure must be highly available and scale to match the rate of incoming data and reporting requests.Which combination of steps meets these requirements while minimizing operational overhead? (Choose two.)",
    "choices": [
      "A. Deploy the application to Amazon EC2 On-Demand Instances with load balancing across multiple Availability Zones. Use scheduled Amazon EC2 Auto Scaling to add capacity before the high volume of submissions on Fridays.",
      "B. Deploy the application in a container using Amazon Elastic Container Service (Amazon ECS) with load balancing across multiple Availability Zones. Use scheduled ServiceAuto Scaling to add capacity before the high volume of submissions on Fridays.",
      "C. Deploy the application front end to an Amazon S3 bucket served by Amazon CloudFront. Deploy the application backend using Amazon API Gateway with an AWS Lambda proxy integration.",
      "D. Store the timesheet submission data in Amazon Redshift. Use Amazon QuickSight to generate the reports using Amazon Redshift as the data source.",
      "E. Store the timesheet submission data in Amazon S3. Use Amazon Athena and Amazon QuickSight to generate the reports using Amazon S3 as the data source."
    ],
    "answer": "CE",
    "explanation": "1. Explanation of the correct answer (CE):\n\nThe correct answer is CE, which combines the following steps:\n\nC. Deploy the application front end to an Amazon S3 bucket served by Amazon CloudFront. Deploy the application backend using Amazon API Gateway with an AWS Lambda proxy integration.\nThis approach aligns with the requirements by:\n- Hosting the static frontend on S3 and CloudFront, which is highly available and scalable.\n- Using API Gateway and Lambda for the dynamic backend, which also provides high availability and scalability without the need to manage EC2 instances.\n- This serverless architecture minimizes operational overhead compared to managing EC2 instances.\n\nE. Store the timesheet submission data in Amazon S3. Use Amazon Athena and Amazon QuickSight to generate the reports using Amazon S3 as the data source.\nThis approach addresses the requirement to store the data in a format that allows for monthly reporting. S3 provides a scalable and durable storage solution, and Athena and QuickSight can be used to easily generate reports from the data in S3.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Deploy the application to Amazon EC2 On-Demand Instances with load balancing across multiple Availability Zones. Use scheduled Amazon EC2 Auto Scaling to add capacity before the high volume of submissions on Fridays.\nThis choice is incorrect because managing EC2 instances would introduce more operational overhead, which goes against the requirement to minimize operational overhead.\n\nB. Deploy the application in a container using Amazon Elastic Container Service (Amazon ECS) with load balancing across multiple Availability Zones. Use scheduled ServiceAuto Scaling to add capacity before the high volume of submissions on Fridays.\nThis choice is incorrect because, while it provides scalability and high availability, it still requires more operational overhead compared to the serverless approach in the correct answer.\n\nD. Store the timesheet submission data in Amazon Redshift. Use Amazon QuickSight to generate the reports using Amazon Redshift as the data source.\nThis choice is incorrect because Redshift is a more complex and powerful data warehouse solution, which may be overkill for the given requirements. The simpler approach of using S3 and Athena/QuickSight is a"
  },
  "197": {
    "question": "A company is storing sensitive data in an Amazon S3 bucket. The company must log all activities for objects in the S3 bucket and must keep the logs for 5 years. The company's security team also must receive an email notification every time there is an attempt to delete data in the S3 bucket.Which combination of steps will meet these requirements MOST cost-effectively? (Choose three.)",
    "choices": [
      "A. Configure AWS CloudTrail to log S3 data events.",
      "B. Configure S3 server access logging for the S3 bucket.",
      "C. Configure Amazon S3 to send object deletion events to Amazon Simple Email Service (Amazon SES).",
      "D. Configure Amazon S3 to send object deletion events to an Amazon EventBridge event bus that publishes to an Amazon Simple Notification Service (Amazon SNS) topic.",
      "E. Configure Amazon S3 to send the logs to Amazon Timestream with data storage tiering.",
      "F. Configure a new S3 bucket to store the logs with an S3 Lifecycle policy."
    ],
    "answer": "ADF",
    "explanation": "1. Clear explanation of why the correct answer (ADF) is right:\n\nA. Configure AWS CloudTrail to log S3 data events.\nThis is the correct choice because CloudTrail provides detailed logging of all object-level operations, including deletions, which meets the requirement of logging all activities for objects in the S3 bucket.\n\nD. Configure Amazon S3 to send object deletion events to an Amazon EventBridge event bus that publishes to an Amazon Simple Notification Service (Amazon SNS) topic.\nThis is the correct choice because it sets up a notification system to alert the security team whenever there is an attempt to delete data in the S3 bucket, as required.\n\nF. Configure a new S3 bucket to store the logs with an S3 Lifecycle policy.\nThis is the correct choice because it ensures the logs are stored for the required 5-year duration, in a cost-effective manner using S3 Lifecycle policies.\n\n2. Explanations of why each incorrect choice is wrong:\n\nB. Configure S3 server access logging for the S3 bucket.\nThis is incorrect because S3 server access logging tracks requests at the bucket level, not object-level operations (e.g., deletions), which is the specific requirement in this case.\n\nC. Configure Amazon S3 to send object deletion events to Amazon Simple Email Service (Amazon SES).\nThis is incorrect because it does not meet the requirement of sending notifications to the security team. Using SES would only send email notifications, not to a specific team or service.\n\nE. Configure Amazon S3 to send the logs to Amazon Timestream with data storage tiering.\nThis is incorrect because it is a more expensive solution than the recommended approach of using S3 Lifecycle policies to manage the log storage. Timestream is a more specialized and costly service compared to the cost-effective S3 Lifecycle policy."
  },
  "198": {
    "question": "A company is building a hybrid environment that includes servers in an on-premises data center and in the AWS Cloud. The company has deployed Amazon EC2 instances in three VPCs. Each VPC is in a different AWS Region. The company has established an AWS Direct. Connect connection to the data center from the Region that is closest to the data center.The company needs the servers in the on-premises data center to have access to the EC2 instances in all three VPCs. The servers in the on-premises data center also must have access to AWS public services.Which combination of steps will meet these requirements with the LEAST cost? (Choose two.)",
    "choices": [
      "A. Create a Direct Connect gateway in the Region that is closest to the data center. Attach the Direct Connect connection to the Direct Connect gateway. Use the Direct Connect gateway to connect the VPCs in the other two Regions.",
      "B. Set up additional Direct Connect connections from the on-premises data center to the other two Regions.",
      "C. Create a private VIF. Establish an AWS Site-to-Site VPN connection over the private VIF to the VPCs in the other two Regions.",
      "D. Create a public VIF. Establish an AWS Site-to-Site VPN connection over the public VIF to the VPCs in the other two Regions.",
      "E. Use VPC peering to establish a connection between the VPCs across the Regions Create a private VIF with the existing Direct Connect connection to connect to the peered VPCs."
    ],
    "answer": "AD",
    "explanation": "1. Explanation of the correct answer (A and D):\n\nA. Create a Direct Connect gateway in the Region that is closest to the data center. Attach the Direct Connect connection to the Direct Connect gateway. Use the Direct Connect gateway to connect the VPCs in the other two Regions.\n\nThis is correct because a Direct Connect gateway is a global resource that allows you to connect your on-premises data center to multiple VPCs across different AWS Regions through a single Direct Connect connection. By creating the Direct Connect gateway in the Region closest to the data center, you can then use it to connect to the VPCs in the other two Regions, meeting the requirement of providing access from the on-premises data center to all three VPCs.\n\nD. Create a public VIF. Establish an AWS Site-to-Site VPN connection over the public VIF to connect to the VPCs in the other two Regions.\n\nThis is also correct because it allows the on-premises data center to access the AWS public services, which is one of the requirements. By creating a public VIF and establishing a Site-to-Site VPN connection over it, the on-premises servers can access the public AWS services, in addition to the EC2 instances in the three VPCs through the Direct Connect gateway (as per choice A).\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. Set up additional Direct Connect connections from the on-premises data center to the other two Regions.\nThis is incorrect because it is not the least-cost solution. Setting up multiple Direct Connect connections from the on-premises data center to each Region is more expensive than using a single Direct Connect connection and a Direct Connect gateway.\n\nC. Create a private VIF. Establish an AWS Site-to-Site VPN connection over the private VIF to the VPCs in the other two Regions.\nThis is incorrect because you cannot establish a Site-to-Site VPN connection over a private VIF. The private VIF is used to connect the on-premises data center to a specific VPC, not to connect to multiple VPCs across Regions.\n\nE. Use VPC peering to establish a connection between the VPCs across the Regions. Create a private VIF with the existing Direct Connect connection to connect to the peered VP"
  },
  "199": {
    "question": "A company is using an organization in AWS Organizations to manage hundreds of AWS accounts. A solutions architect is working on a solution to provide baseline protection for the Open Web Application Security Project (OWASP) top 10 web application vulnerabilities. The solutions architect is using AWS WAF for all existing and new Amazon CloudFront distributions that are deployed within the organization.Which combination of steps should the solutions architect take to provide the baseline protection? (Choose three.)",
    "choices": [
      "A. Enable AWS Config in all accounts",
      "B. Enable Amazon GuardDuty in all accounts",
      "C. Enable all features for the organization",
      "D. Use AWS Firewall Manager to deploy AWS WAF rules in all accounts for all CloudFront distributions",
      "E. Use AWS Shield Advanced to deploy AWS WAF rules in all accounts for all CloudFront distributions",
      "F. Use AWS Security Hub to deploy AWS WAF rules in all accounts for all CloudFront distributions"
    ],
    "answer": "ACD",
    "explanation": "1. Clear explanation of why the correct answer (ACD) is right:\n\nA. Enable AWS Config in all accounts\nThis is required because AWS Firewall Manager, which is used to deploy AWS WAF rules across the organization, needs AWS Config to detect newly created resources.\n\nC. Enable all features for the organization\nThis is required because AWS Firewall Manager has a prerequisite that the organization must be using AWS Organizations with all features enabled.\n\nD. Use AWS Firewall Manager to deploy AWS WAF rules in all accounts for all CloudFront distributions\nThis is the correct step to use AWS Firewall Manager to deploy AWS WAF rules across the organization, which provides the baseline protection for the OWASP top 10 web application vulnerabilities.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nB. Enable Amazon GuardDuty in all accounts\nWhile Amazon GuardDuty is a useful security service, it is not required for the specific use case of deploying AWS WAF rules across the organization using AWS Firewall Manager.\n\nE. Use AWS Shield Advanced to deploy AWS WAF rules in all accounts for all CloudFront distributions\nAWS Shield Advanced is a DDoS protection service, and it does not provide the functionality to deploy AWS WAF rules across the organization. The correct service to use is AWS Firewall Manager.\n\nF. Use AWS Security Hub to deploy AWS WAF rules in all accounts for all CloudFront distributions\nAWS Security Hub is a security monitoring and compliance service, but it does not have the capability to deploy AWS WAF rules across the organization. The correct service to use is AWS Firewall Manager."
  },
  "200": {
    "question": "A solutions architect has implemented a SAML 2.0 federated identity solution with their company's on-premises identity provider (IdP) to authenticate users' access to the AWS environment. When the solutions architect tests authentication through the federated identity web portal, access to the AWS environment is granted. However, when test users attempt to authenticate through the federated identity web portal, they are not able to access the AWS environment.Which items should the solutions architect check to ensure identity federation is properly configured? (Choose three.)",
    "choices": [
      "A. The IAM user's permissions policy has allowed the use of SAML federation for that user.",
      "B. The IAM roles created for the federated users' or federated groups' trust policy have set the SAML provider as the principal.B. Test users are not in the AWSFederatedUsers group in the company's IdP.",
      "C. The web portal calls the AWS STS AssumeRoleWithSAML API with the ARN of the SAML provider, the ARN of the IAM role, and the SAML assertion from IdP.",
      "D. The on-premises IdP's DNS hostname is reachable from the AWS environment VPCs.",
      "E. The company's IdP defines SAML assertions that properly map users or groups. In the company to IAM roles with appropriate permissions."
    ],
    "answer": "BCE",
    "explanation": "1. Clear explanation of the correct answer:\n\nThe correct answer is BCE.\n\nB) The IAM roles created for the federated users' or federated groups' trust policy have set the SAML provider as the principal.\nThis is crucial because the IAM roles must have a trust policy that allows the SAML provider (the on-premises IdP) to assume the role and grant access to the AWS environment.\n\nC) The web portal calls the AWS STS AssumeRoleWithSAML API with the ARN of the SAML provider, the ARN of the IAM role, and the SAML assertion from IdP.\nThe web portal must correctly use the AWS STS AssumeRoleWithSAML API to exchange the SAML assertion from the IdP for temporary AWS credentials that can be used to access the AWS environment.\n\nE) The company's IdP defines SAML assertions that properly map users or groups. In the company to IAM roles with appropriate permissions.\nThe SAML assertions from the IdP must correctly map the users or groups to the appropriate IAM roles in AWS, which have the necessary permissions to access the desired resources.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA) The IAM user's permissions policy has allowed the use of SAML federation for that user.\nThis is incorrect because the question is about federated access, not IAM user access. Federated access uses IAM roles, not IAM users.\n\nB) Test users are not in the AWSFederatedUsers group in the company's IdP.\nThis is incorrect because the question does not mention the AWSFederatedUsers group. The mapping of users or groups to IAM roles is handled by the SAML assertions, not by a specific group in the IdP.\n\nD) The on-premises IdP's DNS hostname is reachable from the AWS environment VPCs.\nThis is incorrect because the question does not mention any VPC or network connectivity issues. The solutions architect should check the IAM role trust policy and the SAML assertion mapping, not the network connectivity."
  },
  "201": {
    "question": "A solutions architect needs to improve an application that is hosted in the AWS Cloud. The application uses an Amazon Aurora MySQL DB instance that is experiencing overloaded connections. Most of the application\u2019s operations insert records into the database. The application currently stores credentials in a text-based configuration file.The solutions architect needs to implement a solution so that the application can handle the current connection load. The solution must keep the credentials secure and must provide the ability to rotate the credentials automatically on a regular basis.Which solution will meet these requirements?",
    "choices": [
      "A. Deploy an Amazon RDS Proxy layer. In front of the DB instance. Store the connection credentials as a secret in AWS Secrets Manager.",
      "B. Deploy an Amazon RDS Proxy layer in front of the DB instance. Store the connection credentials in AWS Systems Manager Parameter Store",
      "C. Create an Aurora Replica. Store the connection credentials as a secret in AWS Secrets Manager",
      "D. Create an Aurora Replica. Store the connection credentials in AWS Systems Manager Parameter Store."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\n\nThe correct answer is A. Deploy an Amazon RDS Proxy layer in front of the DB instance and store the connection credentials as a secret in AWS Secrets Manager.\n\nThis solution addresses the key requirements:\n\na) Handling the current connection load: RDS Proxy establishes a persistent database connection pool and reuses these connections, reducing the overhead of opening new connections for each application request. This helps handle the overloaded connections issue.\n\nb) Keeping the credentials secure: Storing the credentials as a secret in AWS Secrets Manager ensures they are securely stored and can be automatically rotated on a regular basis.\n\n2. Explanations of why the other choices are incorrect:\n\nB. Deploy an Amazon RDS Proxy layer in front of the DB instance and store the connection credentials in AWS Systems Manager Parameter Store:\n- This is similar to the correct answer, but AWS Secrets Manager is a better choice than Parameter Store for securely storing and rotating credentials.\n\nC. Create an Aurora Replica and store the connection credentials as a secret in AWS Secrets Manager:\n- Creating an Aurora Replica does not address the issue of overloaded connections, as the replica is used for read scaling, not write scaling.\n\nD. Create an Aurora Replica and store the connection credentials in AWS Systems Manager Parameter Store:\n- Same as choice C, the Aurora Replica does not solve the connection overload issue.\n- Additionally, storing the credentials in Parameter Store does not provide the ability to automatically rotate the credentials, as Secrets Manager does."
  },
  "202": {
    "question": "A company needs to build a disaster recovery (DR) solution for its ecommerce website. The web application is hosted on a fleet of t3.large Amazon EC2 instances and uses an Amazon RDS for MySQL DB instance. The EC2 instances are in an Auto Scaling group that extends across multiple Availability Zones.In the event of a disaster, the web application must fail over to the secondary environment with an RPO of 30 seconds and an RTO of 10 minutes.Which solution will meet these requirements MOST cost-effectively?",
    "choices": [
      "A. Use infrastructure as code (IaC) to provision the new infrastructure in the DR Region. Create a cross-Region read replica for the DB instance. Set up a backup plan in AWS Backup to create cross-Region backups for the EC2 instances and the DB instance. Create a cron expression to back up the EC2 instances and the DB instance every 30 seconds to the DR Region. Recover the EC2 instances from the latest EC2 backup. Use an Amazon Route 53 geolocation routing policy to automatically fail over to the DR Region in the event of a disaster.",
      "B. Use infrastructure as code (IaC) to provision the new infrastructure in the DR Region. Create a cross-Region read replica for the DB instance. Set up AWS Elastic Disaster Recovery to continuously replicate the EC2 instances to the DR Region. Run the EC2 instances at the minimum capacity in the DR Region. Use an Amazon Route 53 failover routing policy to automatically fail over to the DR Region in the event of a disaster. Increase the desired capacity of the Auto Scaling group.",
      "C. Set up a backup plan in AWS Backup to create cross-Region backups for the EC2 instances and the DB instance. Create a cron expression to back up the EC2 instances and the DB instance every 30 seconds to the DR Region. Use infrastructure as code (IaC) to provision the new infrastructure in the DR Region. Manually restore the backed-up data on new instances. Use an Amazon Route 53 simple routing policy to automatically fail over to the DR Region in the event of a disaster.",
      "D. Use infrastructure as code (IaC) to provision the new infrastructure in the DR Region. Create an Amazon Aurora global database. Set up AWS Elastic Disaster Recovery to continuously replicate the EC2 instances to the DR Region. Run the Auto Scaling group of EC2 instances at full capacity in the DR Region. Use an Amazon Route 53 failover routing policy to automatically fail over to the DR Region in the event of a disaster."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (Option B):\n\nOption B is the most cost-effective solution as it meets the given RPO and RTO requirements while minimizing the resources required in the disaster recovery (DR) environment.\n\nKey points:\n- Uses infrastructure as code (IaC) to provision the new infrastructure in the DR region, enabling automated and repeatable deployments.\n- Creates a cross-Region read replica for the Amazon RDS DB instance, ensuring the database is replicated and available in the DR region.\n- Leverages AWS Elastic Disaster Recovery to continuously replicate the EC2 instances from the primary region to the DR region, keeping the application up-to-date.\n- Runs the EC2 instances at the minimum capacity in the DR region, reducing costs by only utilizing resources when failover occurs.\n- Uses an Amazon Route 53 failover routing policy to automatically fail over to the DR region in the event of a disaster, minimizing downtime.\n- Increases the desired capacity of the Auto Scaling group in the DR region to ensure sufficient resources are available to handle the workload during failover.\n\n2. Explanation of why each incorrect choice is wrong:\n\na. Option A:\n- Creating cross-Region backups every 30 seconds for the EC2 instances and the DB instance is not necessary, as the application is stateless (handled by the Auto Scaling group) and the database can be replicated using a read replica.\n- Recovering the EC2 instances from the latest backup would take longer than the required 10-minute RTO.\n\nc. Option C:\n- Relying on manual restoration of backups for the EC2 instances and the DB instance would not meet the 10-minute RTO requirement.\n- Using a simple routing policy in Amazon Route 53 would not automatically fail over to the DR region in the event of a disaster.\n\nd. Option D:\n- Using an Amazon Aurora global database is not necessary, as a cross-Region read replica for the Amazon RDS DB instance is sufficient to meet the requirements.\n- Running the Auto Scaling group of EC2 instances at full capacity in the DR region would be more costly than the minimum capacity approach in Option B."
  },
  "203": {
    "question": "A company is planning a one-time migration of an on-premises MySQL database to Amazon Aurora MySQL in the us-east-1 Region. The company's current internet connection has limited bandwidth. The on-premises MySQL database is 60 TB in size. The company estimates that it will take a month to transfer the data to AWS over the current internet connection. The company needs a migration solution that will migrate the database more quickly.Which solution will migrate the database in the LEAST amount of time?",
    "choices": [
      "A. Request a 1 Gbps AWS Direct Connect connection between the on-premises data center and AWS. Use AWS Database Migration Service (AWS DMS) to migrate the on-premises MySQL database to Aurora MySQL.",
      "B. Use AWS DataSync with the current internet connection to accelerate the data transfer between the on-premises data center and AWS. Use AWS Application Migration Service to migrate the on-premises MySQL database to Aurora MySQL.",
      "C. Order an AWS Snowball Edge device. Load the data into an Amazon S3 bucket by using the S3 interface. Use AWS Database Migration Service (AWS DMS) to migrate the data from Amazon S3 to Aurora MySQL.",
      "D. Order an AWS Snowball device. Load the data into an Amazon S3 bucket by using the S3 Adapter for Snowball. Use AWS Application Migration Service to migrate the data from Amazon S3 to Aurora MySQL."
    ],
    "answer": "C",
    "explanation": "Explanation:\n\n1. Correct Answer: C\n\nThe correct answer is C because it provides the fastest migration solution for the given scenario.\n\nThe key factors are:\n- The database size is 60 TB, which is too large to migrate over the limited internet bandwidth in a reasonable timeframe (estimated to take 1 month).\n- Option C recommends using an AWS Snowball Edge device to first copy the data to Amazon S3, and then use AWS Database Migration Service (AWS DMS) to migrate the data from S3 to Amazon Aurora MySQL.\n\nThe Snowball Edge device can transfer data much faster than the limited internet connection, as it provides a high-speed data transfer mechanism. Once the data is in S3, AWS DMS can quickly migrate it to Aurora MySQL, resulting in the fastest overall migration time compared to the other options.\n\n2. Incorrect Choices:\n\nA. This option suggests using a 1 Gbps AWS Direct Connect connection to migrate the data. While this would be faster than the limited internet connection, it would still take a significant amount of time to transfer 60 TB of data over a 1 Gbps link (estimated to be around 2 months). This is slower than the Snowball Edge + DMS approach in option C.\n\nB. This option suggests using AWS DataSync and AWS Application Migration Service. While DataSync can help accelerate the data transfer, it is still limited by the available internet bandwidth. Application Migration Service is designed for migrating applications, not large database migrations. Therefore, this option is not the fastest solution for the given scenario.\n\nD. This option is similar to C, but it suggests using a standard AWS Snowball device instead of the Snowball Edge. The Snowball Edge has a higher data transfer capacity and is more suitable for large-scale database migrations like the one described in the question."
  },
  "204": {
    "question": "A company has an application in the AWS Cloud. The application runs on a fleet of 20 Amazon EC2 instances. The EC2 instances are persistent and store data on multiple attached Amazon Elastic Block Store (Amazon EBS) volumes.The company must maintain backups in a separate AWS Region. The company must be able to recover the EC2 instances and their configuration within 1 business day, with loss of no more than 1 day's worth of data. The company has limited staff and needs a backup solution that optimizes operational efficiency and cost. The company already has created an AWS CloudFormation template that can deploy the required network configuration in a secondary Region.Which solution will meet these requirements?",
    "choices": [
      "A. Create a second CloudFormation template that can recreate the EC2 instances in the secondary Region. Run daily multivolume snapshots by using AWS Systems Manager Automation runbooks. Copy the snapshots to the secondary Region. In the event of a failure launch the CloudFormation templates, restore the EBS volumes from snapshots, and transfer usage to the secondary Region.",
      "B. Use Amazon Data Lifecycle Manager (Amazon DLM) to create daily multivolume snapshots of the EBS volumes. In the event of a failure, launch the CloudFormation template and use Amazon DLM to restore the EBS volumes and transfer usage to the secondary Region.",
      "C. Use AWS Backup to create a scheduled daily backup plan for the EC2 instances. Configure the backup task to copy the backups to a vault in the secondary Region. In the event of a failure, launch the CloudFormation template, restore the instance volumes and configurations from the backup vault, and transfer usage to the secondary Region.",
      "D. Deploy EC2 instances of the same size and configuration to the secondary Region. Configure AWS DataSync daily to copy data from the primary Region to the secondary Region. In the event of a failure, launch the CloudFormation template and transfer usage to the secondary Region."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C. Using AWS Backup to create a scheduled daily backup plan for the EC2 instances is the best solution to meet the given requirements.\n\nAWS Backup is a centralized service that provides a simple, cost-effective way to back up data across AWS services, including EC2 instances and their attached EBS volumes. The key benefits of using AWS Backup in this scenario are:\n\n- It can back up both the EC2 instances and their attached EBS volumes, ensuring the complete configuration is captured.\n- The backups can be configured to be copied to a vault in the secondary Region, as required.\n- In the event of a failure, the EC2 instances and their configurations can be restored from the backup vault in the secondary Region, allowing for recovery within the 1 business day requirement.\n- AWS Backup provides a centralized management console for configuring and monitoring backups, optimizing operational efficiency.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This solution relies on creating a separate CloudFormation template to recreate the EC2 instances in the secondary Region. While this may work, it does not provide a centralized backup solution and may require more operational effort to manage.\n\nB. Using Amazon Data Lifecycle Manager (DLM) to create daily snapshots of the EBS volumes would only backup the data, not the complete EC2 instance configuration. This means that in the event of a failure, the instances would need to be manually recreated, which does not meet the 1 business day recovery requirement.\n\nD. Deploying EC2 instances to the secondary Region and using AWS DataSync to copy data daily would provide a secondary site, but it does not offer a robust backup solution. In the event of a failure, there would be no way to quickly recover the EC2 instance configurations, which is a key requirement."
  },
  "205": {
    "question": "A company is designing a new website that hosts static content. The website will give users the ability to upload and download large files. According to company requirements, all data must be encrypted in transit and at rest. A solutions architect is building the solution by using Amazon S3 and Amazon CloudFront.Which combination of steps will meet the encryption requirements? (Choose three.)",
    "choices": [
      "A. Turn on S3 server-side encryption for the S3 bucket that the web application uses.",
      "B. Add a policy attribute of \"aws:SecureTransport\": \"true\" for read and write operations in the S3 ACLs.",
      "C. Create a bucket policy that denies any unencrypted operations in the S3 bucket that the web application uses.",
      "D. Configure encryption at rest on CloudFront by using server-side encryption with AWS KMS keys (SSE-KMS).",
      "E. Configure redirection of HTTP requests to HTTPS requests in CloudFront.",
      "F. Use the RequireSSL option in the creation of presigned URLs for the S3 bucket that the web application uses."
    ],
    "answer": "ACE",
    "explanation": "1. Explanation of the correct answer (ACE):\n\nA. Turn on S3 server-side encryption for the S3 bucket that the web application uses.\nThis is the correct choice because S3 server-side encryption (SSE-S3) ensures that data is encrypted at rest in the S3 bucket, meeting the requirement for encryption at rest.\n\nC. Create a bucket policy that denies any unencrypted operations in the S3 bucket that the web application uses.\nThis is the correct choice because a bucket policy that denies unencrypted operations ensures that all data uploaded to the S3 bucket is encrypted, meeting the requirement for encryption in transit and at rest.\n\nE. Configure redirection of HTTP requests to HTTPS requests in CloudFront.\nThis is the correct choice because configuring CloudFront to redirect HTTP requests to HTTPS ensures that data is encrypted in transit, meeting the requirement for encryption in transit.\n\n2. Explanations of why incorrect choices are wrong:\n\nB. Add a policy attribute of \"aws:SecureTransport\": \"true\" for read and write operations in the S3 ACLs.\nThis is incorrect because S3 ACLs are used to control access to the bucket, not to enforce encryption. The \"aws:SecureTransport\" condition in the bucket policy is the correct way to enforce encryption in transit.\n\nD. Configure encryption at rest on CloudFront by using server-side encryption with AWS KMS keys (SSE-KMS).\nThis is incorrect because CloudFront does not provide encryption at rest. Encryption at rest should be configured at the S3 bucket level using SSE-S3.\n\nF. Use the RequireSSL option in the creation of presigned URLs for the S3 bucket that the web application uses.\nThis is incorrect because presigned URLs are used to grant temporary access to S3 objects, not to enforce encryption. Encryption in transit should be enforced at the CloudFront level."
  },
  "206": {
    "question": "A company is implementing a serverless architecture by using AWS Lambda functions that need to access a Microsoft SQL Server DB instance on Amazon RDS. The company has separate environments for development and production, including a clone of the database system.The company's developers are allowed to access the credentials for the development database. However, the credentials for the production database must be encrypted with a key that only members of the IT security team's IAM user group can access. This key must be rotated on a regular basis.What should a solutions architect do in the production environment to meet these requirements?",
    "choices": [
      "A. Store the database credentials in AWS Systems Manager Parameter Store by using a SecureString parameter that is encrypted by an AWS Key Management Service (AWS KMS) customer managed key. Attach a role to each Lambda function to provide access to the SecureString parameter. Restrict access to the SecureString parameter and the customer managed key so that only the IT security team can access the parameter and the key.",
      "B. Encrypt the database credentials by using the AWS Key Management Service (AWS KMS) default Lambda key. Store the credentials in the environment variables of each Lambda function. Load the credentials from the environment variables in the Lambda code. Restrict access to the KMS key so that only the IT security team can access the key.",
      "C. Store the database credentials in the environment variables of each Lambda function. Encrypt the environment variables by using an AWS Key Management Service (AWS KMS) customer managed key. Restrict access to the customer managed key so that only the IT security team can access the key.",
      "D. Store the database credentials in AWS Secrets Manager as a secret that is associated with an AWS Key Management Service (AWS KMS) customer managed key. Attach a role to each Lambda function to provide access to the secret. Restrict access to the secret and the customer managed key so that only the IT security team can access the secret and the key."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (D):\n\nThe correct answer is D because it best meets the requirements outlined in the question:\n\n- Storing the database credentials in AWS Secrets Manager as a secret ensures that the credentials are encrypted and protected. Secrets Manager allows you to easily rotate, manage, and retrieve database credentials, API keys, and other sensitive information.\n\n- Associating the secret with an AWS KMS customer-managed key allows you to control and restrict access to the encryption key. Only the IT security team's IAM user group can access the customer-managed key, ensuring that the production database credentials are properly secured.\n\n- Attaching a role to the Lambda functions provides them with the necessary permissions to access the secret stored in Secrets Manager, without the need to hardcode the credentials in the Lambda function code.\n\n- This solution supports the requirement for regular key rotation, as Secrets Manager makes it easy to rotate the encryption key used to protect the sensitive data.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option uses AWS Systems Manager Parameter Store, which is not the recommended approach for storing sensitive information like production database credentials. Parameter Store does not provide the same level of security and control over the encryption key as Secrets Manager.\n\nB. Storing the credentials in the Lambda function's environment variables, even if encrypted with the default KMS key, is not a secure practice. Environment variables can be easily accessed and are not the recommended way to store sensitive information.\n\nC. Similar to option B, storing the encrypted credentials in the Lambda function's environment variables is not a secure long-term solution. The environment variables can be accessed, and the rotation of the customer-managed KMS key would be more complex to manage."
  },
  "207": {
    "question": "An online retail company is migrating its legacy on-premises .NET application to AWS. The application runs on load-balanced frontend web servers, load-balanced application servers, and a Microsoft SQL Server database.The company wants to use AWS managed services where possible and does not want to rewrite the application. A solutions architect needs to implement a solution to resolve scaling issues and minimize licensing costs as the application scales.Which solution will meet these requirements MOST cost-effectively?",
    "choices": [
      "A. Deploy Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer for the web tier and for the application tier. Use Amazon Aurora PostgreSQL with Babelfish turned on to replatform the SQL Server database.",
      "B. Create images of all the servers by using AWS Database Migration Service (AWS DMS). Deploy Amazon EC2 instances that are based on the on-premises imports. Deploy the instances in an Auto Scaling group behind a Network Load Balancer for the web tier and for the application tier. Use Amazon DynamoDB as the database tier.",
      "C. Containerize the web frontend tier and the application tier. Provision an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. Create an Auto Scaling group behind a Network Load Balancer for the web tier and for the application tier. Use Amazon RDS for SQL Server to host the database.",
      "D. Separate the application functions into AWS Lambda functions. Use Amazon API Gateway for the web frontend tier and the application tier. Migrate the data to Amazon S3. Use Amazon Athena to query the data."
    ],
    "answer": "A",
    "explanation": "1. Explanation of why the correct answer (A) is right:\n\nThe correct answer is A, which proposes using Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer for the web and application tiers, and Amazon Aurora PostgreSQL with Babelfish turned on to replatform the SQL Server database.\n\nThis solution is the most cost-effective because it:\n- Leverages managed services (EC2, Auto Scaling, Application Load Balancer) to handle scaling and load balancing, reducing operational overhead.\n- Migrates the existing SQL Server database to Amazon Aurora PostgreSQL with Babelfish, which allows the application to continue using the same SQL Server APIs without the need for rewriting the application. This helps minimize licensing costs as the application scales.\n- Babelfish is a feature of Amazon Aurora PostgreSQL that enables it to understand SQL Server commands, allowing a seamless migration from the on-premises SQL Server without the need for extensive application changes.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. This option involves manually creating EC2 instances based on the on-premises server images and using Amazon DynamoDB as the database. This would require significant application changes to adapt to the DynamoDB data model, which goes against the requirement of \"not wanting to rewrite the application.\"\n\nC. Using Amazon EKS for containerization and Amazon RDS for SQL Server is a viable solution, but it does not address the licensing cost concerns as effectively as option A. The SQL Server licensing costs would still be present in this solution.\n\nD. Separating the application functions into AWS Lambda functions and using Amazon API Gateway, Amazon S3, and Amazon Athena would require a complete rewrite of the application, which is not desired based on the requirements."
  },
  "208": {
    "question": "A software-as-a-service (SaaS) provider exposes APIs through an Application Load Balancer (ALB). The ALB connects to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster that is deployed in the us-east-1 Region. The exposed APIs contain usage of a few non-standard REST methods: LINK, UNLINK, LOCK, and UNLOCK.Users outside the United States are reporting long and inconsistent response times for these APIs. A solutions architect needs to resolve this problem with a solution that minimizes operational overhead.Which solution meets these requirements?",
    "choices": [
      "A. Add an Amazon CloudFront distribution. Configure the ALB as the origin.",
      "B. Add an Amazon API Gateway edge-optimized API endpoint to expose the APIs. Configure the ALB as the target.",
      "C. Add an accelerator in AWS Global Accelerator. Configure the ALB as the origin.",
      "D. Deploy the APIs to two additional AWS Regions: eu-west-1 and ap-southeast-2. Add latency-based routing records in Amazon Route 53."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C. Adding an accelerator in AWS Global Accelerator and configuring the ALB as the origin is the best solution to address the problem of long and inconsistent response times for users outside the United States.\n\nAWS Global Accelerator is a network service that improves the availability and performance of your applications by routing user traffic through the AWS global network infrastructure, which is optimized for performance and availability. By using Global Accelerator, the traffic from users outside the US-east-1 region will be routed through the Global Accelerator network, which can help improve the response times and consistency, without the need to deploy the APIs to additional regions.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Add an Amazon CloudFront distribution. Configure the ALB as the origin:\n   - CloudFront is a content delivery network (CDN) service, which is optimized for delivering static content. It may not be the best solution for handling the non-standard REST methods (LINK, UNLINK, LOCK, and UNLOCK) used by the APIs.\n\nB. Add an Amazon API Gateway edge-optimized API endpoint to expose the APIs. Configure the ALB as the target:\n   - This solution would add an additional layer (API Gateway) between the users and the APIs, which may introduce additional latency and complexity. It does not directly address the issue of long and inconsistent response times for users outside the US-east-1 region.\n\nD. Deploy the APIs to two additional AWS Regions: eu-west-1 and ap-southeast-2. Add latency-based routing records in Amazon Route 53:\n   - This solution would require deploying the APIs to multiple regions, which increases operational overhead and complexity. It may also not be the most cost-effective solution, as it would require maintaining and managing the APIs in multiple regions."
  },
  "209": {
    "question": "A company runs an IoT application in the AWS Cloud. The company has millions of sensors that collect data from houses in the United States. The sensors use the MQTT protocol to connect and send data to a custom MQTT broker. The MQTT broker stores the data on a single Amazon EC2 instance. The sensors connect to the broker through the domain named iot.example.com. The company uses Amazon Route 53 as its DNS service. The company stores the data in Amazon DynamoDB.On several occasions, the amount of data has overloaded the MQTT broker and has resulted in lost sensor data. The company must improve the reliability of the solution.Which solution will meet these requirements?",
    "choices": [
      "A. Create an Application Load Balancer (ALB) and an Auto Scaling group for the MQTT broker. Use the Auto Scaling group as the target for the ALB. Update the DNS record in Route 53 to an alias record. Point the alias record to the ALB. Use the MQTT broker to store the data.",
      "B. Set up AWS IoT Core to receive the sensor data. Create and configure a custom domain to connect to AWS IoT Core. Update the DNS record in Route 53 to point to the AWS IoT Core Data-ATS endpoint. Configure an AWS IoT rule to store the data.",
      "C. Create a Network Load Balancer (NLB). Set the MQTT broker as the target. Create an AWS Global Accelerator accelerator. Set the NLB as the endpoint for the accelerator. Update the DNS record in Route 53 to a multivalue answer record. Set the Global Accelerator IP addresses as values. Use the MQTT broker to store the data.",
      "D. Set up AWS IoT Greengrass to receive the sensor data. Update the DNS record in Route 53 to point to the AWS IoT Greengrass endpoint. Configure an AWS IoT rule to invoke an AWS Lambda function to store the data."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (Option B):\n\nOption B is the correct answer because it utilizes AWS IoT Core, which is a fully managed service that can handle the ingestion and processing of IoT data at scale. AWS IoT Core supports the MQTT protocol, which is the protocol used by the sensors in the given scenario. By setting up AWS IoT Core to receive the sensor data, the company can offload the MQTT broker functionality and benefit from the scalability, reliability, and security features provided by the AWS IoT Core service. The solution also involves configuring a custom domain to connect to AWS IoT Core, which allows the sensors to continue using the familiar iot.example.com domain. Finally, the company can configure an AWS IoT rule to store the data in DynamoDB, as required.\n\n2. Explanations of why the other options are incorrect:\n\nA. This option involves setting up an Application Load Balancer (ALB) and an Auto Scaling group for the MQTT broker. While this can help improve the reliability and scalability of the MQTT broker, it still relies on a custom-managed MQTT broker, which may not be as scalable and reliable as a fully managed service like AWS IoT Core.\n\nC. This option uses a Network Load Balancer (NLB) and AWS Global Accelerator to improve the reliability and performance of the MQTT broker. While this approach can be effective, it still requires the company to manage the MQTT broker infrastructure, which may not be as scalable and reliable as a fully managed service like AWS IoT Core.\n\nD. This option sets up AWS IoT Greengrass to receive the sensor data. While Greengrass can be useful for edge computing scenarios, it may not be the most suitable solution for the given use case, which involves a large number of sensors and the need for a highly scalable and reliable data ingestion solution. AWS IoT Core is a more appropriate choice for this scenario."
  },
  "210": {
    "question": "A company has Linux-based Amazon EC2 instances. Users must access the instances by using SSH with EC2 SSH key pairs. Each machine requires a unique EC2 key pair.The company wants to implement a key rotation policy that will, upon request, automatically rotate all the EC2 key pairs and keep the keys in a securely encrypted place. The company will accept less than 1 minute of downtime during key rotation.Which solution will meet these requirements?",
    "choices": [
      "A. Store all the keys in AWS Secrets Manager. Define a Secrets Manager rotation schedule to invoke an AWS Lambda function to generate new key pairs. Replace public keys on EC2 instances. Update the private keys in Secrets Manager.",
      "B. Store all the keys in Parameter Store, a capability of AWS Systems Manager, as a string. Define a Systems Manager maintenance window to invoke an AWS Lambda function to generate new key pairs. Replace public keys on EC2 instances. Update the private keys in Parameter Store.",
      "C. Import the EC2 key pairs into AWS Key Management Service (AWS KMS). Configure automatic key rotation for these key pairs. Create an Amazon EventBridge scheduled rule to invoke an AWS Lambda function to initiate the key rotation in AWS KMS.",
      "D. Add all the EC2 instances to Fleet Manager, a capability of AWS Systems Manager. Define a Systems Manager maintenance window to issue a Systems Manager Run Command document to generate new key pairs and to rotate public keys to all the instances in Fleet Manager."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\n\nThe correct answer is A. This solution meets all the requirements stated in the question:\n\n- It stores all the EC2 SSH key pairs securely in AWS Secrets Manager, which provides automatic key rotation capabilities.\n- It defines a Secrets Manager rotation schedule to invoke an AWS Lambda function to generate new key pairs.\n- The Lambda function replaces the public keys on the EC2 instances and updates the private keys in Secrets Manager.\n- This approach can achieve less than 1 minute of downtime during the key rotation process, as the Secrets Manager rotation feature can automatically handle the key replacement without disrupting the instances.\n\n2. Explanations of why the other choices are incorrect:\n\nB. This solution using AWS Systems Manager Parameter Store is not ideal, as it does not provide automatic key rotation capabilities. The process of generating new key pairs, replacing public keys on instances, and updating the private keys in Parameter Store would need to be manually orchestrated, which increases the risk of errors and does not meet the requirement of less than 1 minute of downtime.\n\nC. Using AWS KMS for storing and rotating the EC2 SSH key pairs is not the best solution, as KMS is primarily designed for encrypting data, not for managing SSH keys. The process of creating an EventBridge scheduled rule to invoke a Lambda function to initiate the key rotation in KMS is more complex and may not achieve the desired level of automation and reliability.\n\nD. The Fleet Manager solution in AWS Systems Manager does not provide the level of automation and security required for the key rotation process. While it can execute Systems Manager Run Command documents to generate new key pairs and rotate them on the instances, it still requires manual intervention and does not offer the same level of integration and ease of use as the Secrets Manager solution in option A."
  },
  "211": {
    "question": "A company wants to migrate to AWS. The company is running thousands of VMs in a VMware ESXi environment. The company has no configuration management database and has little knowledge about the utilization of the VMware portfolio.A solutions architect must provide the company with an accurate inventory so that the company can plan for a cost-effective migration.Which solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      "A. Use AWS Systems Manager Patch Manager to deploy Migration Evaluator to each VM. Review the collected data in Amazon QuickSight. Identify servers that have high utilization. Remove the servers that have high utilization from the migration list. Import the data to AWS Migration Hub.",
      "B. Export the VMware portfolio to a .csv file. Check the disk utilization for each server. Remove servers that have high utilization. Export the data to AWS Application Migration Service. Use AWS Server Migration Service (AWS SMS) to migrate the remaining servers.",
      "C. Deploy the Migration Evaluator agentless collector to the ESXi hypervisor. Review the collected data in Migration Evaluator. Identify inactive servers. Remove the inactive servers from the migration list. Import the data to AWS Migration Hub.",
      "D. Deploy the AWS Application Migration Service Agent to each VM. When the data is collected, use Amazon Redshift to import and analyze the data. Use Amazon QuickSight for data visualization."
    ],
    "answer": "C",
    "explanation": "Explanation:\n\n1. Correct Answer: C\n   - The question states that the company has no configuration management database and little knowledge about the utilization of their VMware portfolio, so an accurate inventory is required before planning the migration.\n   - The Migration Evaluator agentless collector can be deployed to the ESXi hypervisor to collect the necessary data without installing any agents on the individual VMs, which minimizes the operational overhead.\n   - Reviewing the collected data in Migration Evaluator allows the solutions architect to identify inactive servers that can be removed from the migration list, ensuring a more cost-effective migration.\n   - Importing the data to AWS Migration Hub allows for further planning and execution of the migration.\n\n2. Incorrect Choices:\n   A. This option uses AWS Systems Manager Patch Manager to deploy Migration Evaluator, which adds unnecessary complexity and operational overhead compared to the agentless approach in the correct answer.\n   B. Exporting the VMware portfolio to a CSV file and manually checking the disk utilization is a labor-intensive and error-prone approach, especially for a large number of VMs. Additionally, removing high-utilization servers from the migration list may not be the best strategy, as these servers may still need to be migrated.\n   D. Deploying the AWS Application Migration Service Agent to each VM is a high-overhead approach for the initial inventory collection, as it requires installing an agent on every VM. Using Amazon Redshift and Amazon QuickSight for data analysis and visualization is also an unnecessary added complexity compared to the simpler approach in the correct answer."
  },
  "212": {
    "question": "A company runs a microservice as an AWS Lambda function. The microservice writes data to an on-premises SQL database that supports a limited number of concurrent connections. When the number of Lambda function invocations is too high, the database crashes and causes application downtime. The company has an AWS Direct Connect connection between the company's VPC and the on-premises data center. The company wants to protect the database from crashes.Which solution will meet these requirements?",
    "choices": [
      "A. Write the data to an Amazon Simple Queue Service (Amazon SQS) queue. Configure the Lambda function to read from the queue and write to the existing database. Set a reserved concurrency limit on the Lambda function that is less than the number of connections that the database supports.",
      "B. Create a new Amazon Aurora Serverless DB cluster. Use AWS DataSync to migrate the data from the existing database to Aurora Serverless. Reconfigure the Lambda function to write to Aurora.",
      "C. Create an Amazon RDS Proxy DB instance. Attach the RDS Proxy DB instance to the Amazon RDS DB instance. Reconfigure the Lambda function to write to the RDS Proxy DB instance.",
      "D. Write the data to an Amazon Simple Notification Service (Amazon SNS) topic. Invoke the Lambda function to write to the existing database when the topic receives new messages. Configure provisioned concurrency for the Lambda function to be equal to the number of connections that the database supports."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\nThe correct answer is A, which is to \"Write the data to an Amazon Simple Queue Service (Amazon SQS) queue. Configure the Lambda function to read from the queue and write to the existing database. Set a reserved concurrency limit on the Lambda function that is less than the number of connections that the database supports.\"\n\nThis solution is the best fit because it addresses the key requirement of protecting the on-premises database from crashing due to too many concurrent connections. By using an SQS queue, the Lambda function can decouple the data ingestion from the database write operations. The Lambda function can then read from the queue and write to the database at a controlled rate, determined by the reserved concurrency limit. This ensures that the database is not overwhelmed by too many concurrent connections, preventing it from crashing and causing application downtime.\n\n2. Explanations of why the other choices are incorrect:\n\nB. Create a new Amazon Aurora Serverless DB cluster and migrate the data: This solution is incorrect because the requirement states that the company wants to \"protect the existing database from crashes\", not migrate the data to a new database.\n\nC. Create an Amazon RDS Proxy DB instance: This solution is incorrect because the requirement states that the database is on-premises, not an Amazon RDS instance. The RDS Proxy feature is specific to Amazon RDS and cannot be used with an on-premises database.\n\nD. Write the data to an Amazon SNS topic and invoke the Lambda function: This solution is incorrect because it does not address the issue of the database being overwhelmed by too many concurrent connections. Using an SNS topic does not provide the same level of decoupling and rate control as using an SQS queue, which is the key requirement in this scenario."
  },
  "213": {
    "question": "A company uses a Grafana data visualization solution that runs on a single Amazon EC2 instance to monitor the health of the company's AWS workloads. The company has invested time and effort to create dashboards that the company wants to preserve. The dashboards need to be highly available and cannot be down for longer than 10 minutes. The company needs to minimize ongoing maintenance.Which solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      "A. Migrate to Amazon CloudWatch dashboards. Recreate the dashboards to match the existing Grafana dashboards. Use automatic dashboards where possible.",
      "B. Create an Amazon Managed Grafana workspace. Configure a new Amazon CloudWatch data source. Export dashboards from the existing Grafana instance. Import the dashboards into the new workspace.",
      "C. Create an AMI that has Grafana pre-installed. Store the existing dashboards in Amazon Elastic File System (Amazon EFS). Create an Auto Scaling group that uses the new AMI. Set the Auto Scaling group's minimum, desired, and maximum number of instances to one. Create an Application Load Balancer that serves at least two Availability Zones.",
      "D. Configure AWS Backup to back up the EC2 instance that runs Grafana once each hour. Restore the EC2 instance from the most recent snapshot in an alternate Availability Zone when required."
    ],
    "answer": "B",
    "explanation": "1. Clear explanation of why the correct answer (B) is right:\n\nOption B is the correct answer because it meets all the requirements stated in the question:\n\n- It allows the company to preserve their existing Grafana dashboards by migrating them to the new Amazon Managed Grafana workspace.\n- It ensures high availability of the dashboards as Amazon Managed Grafana is a fully managed service that takes care of the underlying infrastructure, including scalability and availability.\n- It minimizes ongoing maintenance as Amazon Managed Grafana is a managed service, reducing the operational overhead for the company.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nA. Migrate to Amazon CloudWatch dashboards:\n   - This option requires recreating the existing Grafana dashboards in CloudWatch, which means the company will lose their investment in creating the original dashboards.\n   - Using automatic dashboards may not fully replicate the customized dashboards the company has created.\n\nC. Create an AMI with pre-installed Grafana:\n   - This option still requires the company to manage the underlying Grafana infrastructure, including updates, scaling, and high availability, resulting in higher operational overhead.\n   - Using an Auto Scaling group with a single instance may not meet the high availability requirement, as a single instance failure could cause the dashboards to be unavailable for more than 10 minutes.\n\nD. Configure AWS Backup to back up the EC2 instance:\n   - This option does not address the high availability requirement, as restoring from a backup in an alternate Availability Zone may take longer than 10 minutes.\n   - It also does not minimize the operational overhead, as the company would still need to manage the Grafana infrastructure and restore from backups when necessary."
  },
  "214": {
    "question": "A company needs to migrate its customer transactions database from on premises to AWS. The database resides on an Oracle DB instance that runs on a Linux server. According to a new security requirement, the company must rotate the database password each year.Which solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      "A. Convert the database to Amazon DynamoDB by using the AWS Schema Conversion Tool (AWS SCT). Store the password in AWS Systems Manager Parameter Store. Create an Amazon CloudWatch alarm to invoke an AWS Lambda function for yearly passtard rotation.",
      "B. Migrate the database to Amazon RDS for Oracle. Store the password in AWS Secrets Manager. Turn on automatic rotation. Configure a yearly rotation schedule.",
      "C. Migrate the database to an Amazon EC2 instance. Use AWS Systems Manager Parameter Store to keep and rotate the connection string by using an AWS Lambda function on a yearly schedule.",
      "D. Migrate the database to Amazon Neptune by using the AWS Schema Conversion Tool (AWS SCT). Create an Amazon CloudWatch alarm to invoke an AWS Lambda function for yearly password rotation."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (Option B):\n\nOption B is the correct answer because it provides the least operational overhead for the given requirements:\n\n- Migrating the database to Amazon RDS for Oracle meets the requirement of moving the on-premises Oracle database to the cloud.\n- Storing the database password in AWS Secrets Manager and enabling the automatic rotation feature satisfies the requirement of rotating the password yearly. AWS Secrets Manager handles the password rotation automatically, reducing the operational overhead.\n- Configuring a yearly rotation schedule in AWS Secrets Manager is a straightforward and managed solution, requiring minimal manual intervention.\n\n2. Explanations of why the incorrect choices are wrong:\n\nOption A:\n- Converting the database to Amazon DynamoDB introduces significant changes to the database architecture, which may not be a desirable or feasible solution for the given requirements.\n- Using AWS Systems Manager Parameter Store and a custom Lambda function to rotate the password annually introduces more operational overhead compared to the automated solution in Option B.\n\nOption C:\n- Hosting the database on an Amazon EC2 instance requires more manual management and configuration compared to the fully managed Amazon RDS solution in Option B.\n- Using AWS Systems Manager Parameter Store and a custom Lambda function to rotate the password annually is less efficient than the automated rotation feature in AWS Secrets Manager.\n\nOption D:\n- Migrating the Oracle database to Amazon Neptune, a graph database, is not a suitable solution for the given requirements, as it would require significant changes to the application and data structure.\n- Using Amazon CloudWatch alarms and a custom Lambda function to rotate the password annually is less efficient than the automated rotation feature in AWS Secrets Manager.\n\nIn summary, Option B is the correct choice because it provides the most managed and least operationally intensive solution for migrating the Oracle database to the cloud and automatically rotating the database password yearly, as per the requirements."
  },
  "215": {
    "question": "A solutions architect is designing an AWS account structure for a company that consists of multiple teams. All the teams will work in the same AWS Region. The company needs a VPC that is connected to the on-premises network. The company expects less than 50 Mbps of total traffic to and from the on-premises network.Which combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
    "choices": [
      "A. Create an AWS CloudFormation template that provisions a VPC and the required subnets. Deploy the template to each AWS account.",
      "B. Create an AWS CloudFormation template that provisions a VPC and the required subnets. Deploy the template to a shared services account. Share the subnets by using AWS Resource Access Manager.",
      "C. Use AWS Transit Gateway along with an AWS Site-to-Site VPN for connectivity to the on-premises network. Share the transit gateway by using AWS Resource Access Manager.",
      "D. Use AWS Site-to-Site VPN for connectivity to the on-premises network.",
      "E. Use AWS Direct Connect for connectivity to the on-premises network."
    ],
    "answer": "BD",
    "explanation": "1. Explanation of the correct answer (BD):\n\nB. Create an AWS CloudFormation template that provisions a VPC and the required subnets. Deploy the template to a shared services account. Share the subnets by using AWS Resource Access Manager.\nThis is the most cost-effective approach because it provisions a single VPC that can be shared across multiple teams/accounts, rather than creating a separate VPC for each account. By using a shared services account and AWS Resource Access Manager (RAM) to share the subnets, the company can avoid the overhead of managing multiple VPCs.\n\nD. Use AWS Site-to-Site VPN for connectivity to the on-premises network.\nThis is the most cost-effective option for connecting the VPC to the on-premises network, as the company expects less than 50 Mbps of total traffic. AWS Site-to-Site VPN is a lower-cost option compared to AWS Direct Connect, which is more suitable for higher-bandwidth requirements.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Create an AWS CloudFormation template that provisions a VPC and the required subnets. Deploy the template to each AWS account.\nThis is not the most cost-effective approach, as it would require provisioning a separate VPC for each account, leading to higher management overhead and costs.\n\nC. Use AWS Transit Gateway along with an AWS Site-to-Site VPN for connectivity to the on-premises network. Share the transit gateway by using AWS Resource Access Manager.\nWhile this approach would work, it is not the most cost-effective for the given requirements. The use of a Transit Gateway is not necessary when the company only expects less than 50 Mbps of total traffic to and from the on-premises network, making the Site-to-Site VPN a more cost-effective solution.\n\nE. Use AWS Direct Connect for connectivity to the on-premises network.\nThis is not the most cost-effective option, as AWS Direct Connect is more suitable for higher-bandwidth requirements. Given the company's expected traffic of less than 50 Mbps, the AWS Site-to-Site VPN would be a more cost-effective solution."
  },
  "216": {
    "question": "A solutions architect at a large company needs to set up network security for outbound traffic to the internet from all AWS accounts within an organization in AWS Organizations. The organization has more than 100 AWS accounts, and the accounts route to each other by using a centralized AWS Transit Gateway. Each account has both an internet gateway and a NAT gateway for outbound traffic to the internet. The company deploys resources only into a single AWS Region.The company needs the ability to add centrally managed rule-based filtering on all outbound traffic to the internet for all AWS accounts in the organization. The peak load of outbound traffic will not exceed 25 Gbps in each Availability Zone.Which solution meets these requirements?",
    "choices": [
      "A. Create a new VPC for outbound traffic to the internet. Connect the existing transit gateway to the new VPC. Configure a new NAT gateway. Create an Auto Scaling group of Amazon EC2 instances that run an open-source internet proxy for rule-based filtering across all Availability Zones in the Region. Modify all default routes to point to the proxy's Auto Scaling group.",
      "B. Create a new VPC for outbound traffic to the internet. Connect the existing transit gateway to the new VPC. Configure a new NAT gateway. Use an AWS Network Firewall firewall for rule-based filtering. Create Network Firewall endpoints in each Availability Zone. Modify all default routes to point to the Network Firewall endpoints.",
      "C. Create an AWS Network Firewall firewall for rule-based filtering in each AWS account. Modify all default routes to point to the Network Firewall firewalls in each account.",
      "D. In each AWS account, create an Auto Scaling group of network-optimized Amazon EC2 instances that run an open-source internet proxy for rule-based filtering. Modify all default routes to point to the proxy's Auto Scaling group."
    ],
    "answer": "B",
    "explanation": "1. Explanation for the correct answer (B):\n\nThe correct answer is B because it addresses all the requirements mentioned in the question:\n\n- It creates a new VPC dedicated for outbound traffic to the internet, which helps isolate and manage the outbound traffic separately.\n- It connects the existing transit gateway to the new VPC, ensuring connectivity between the VPCs.\n- It configures a new NAT gateway within the new VPC, providing the necessary outbound connectivity to the internet.\n- It uses AWS Network Firewall, a managed firewall service, for rule-based filtering on the outbound traffic. This allows the company to define and enforce custom rules for traffic leaving the VPC.\n- It creates Network Firewall endpoints in each Availability Zone, which serve as the traffic inspection points where Network Firewall applies the filtering rules.\n- It modifies all default routes in the VPCs to point to the Network Firewall endpoints, ensuring that all outbound traffic flows through the firewall for rule-based filtering.\n\n2. Explanations for the incorrect choices:\n\nA. This choice is incorrect because it uses a third-party open-source internet proxy solution instead of a managed AWS service like Network Firewall. Additionally, the question mentions that the company deploys resources only into a single AWS Region, but this solution requires modifying default routes in all AWS accounts, which may not be practical at scale.\n\nC. This choice is incorrect because it creates Network Firewall firewalls in each AWS account, which goes against the requirement of having a centrally managed solution for rule-based filtering on outbound traffic.\n\nD. This choice is incorrect because it creates an Auto Scaling group of network-optimized EC2 instances running an open-source internet proxy in each AWS account, which also goes against the requirement of having a centrally managed solution."
  },
  "217": {
    "question": "A company uses a load balancer to distribute traffic to Amazon EC2 instances in a single Availability Zone. The company is concerned about security and wants a solutions architect to re-architect the solution to meet the following requirements:\u2022\tInbound requests must be filtered for common vulnerability attacks.\u2022\tRejected requests must be sent to a third-party auditing application.\u2022\tAll resources should be highly available.Which solution meets these requirements?",
    "choices": [
      "A. Configure a Multi-AZ Auto Scaling group using the application's AMI. Create an Application Load Balancer (ALB) and select the previously created Auto Scaling group as the target. Use Amazon Inspector to monitor traffic to the ALB and EC2 instances. Create a web ACL in WAF. Create an AWS WAF using the web ACL and ALB. Use an AWS Lambda function to frequently push the Amazon Inspector report to the third-party auditing application.",
      "B. Configure an Application Load Balancer (ALB) and add the EC2 instances as targets. Create a web ACL in WAF. Create an AWS WAF using the web ACL and ALB name and enable logging with Amazon CloudWatch Logs. Use an AWS Lambda function to frequently push the logs to the third-party auditing application.",
      "C. Configure an Application Load Balancer (ALB) along with a target group adding the EC2 instances as targets. Create an Amazon Kinesis Data Firehose with the destination of the third-party auditing application. Create a web ACL in WAF. Create an AWS WAF using the web ACL and ALB then enable logging by selecting the Kinesis Data Firehose as the destination. Subscribe to AWS Managed Rules in AWS Marketplace, choosing the WAF as the subscriber.",
      "D. Configure a Multi-AZ Auto Scaling group using the application's AMI. Create an Application Load Balancer (ALB) and select the previously created Auto Scaling group as the target. Create an Amazon Kinesis Data Firehose with a destination of the third-party auditing application. Create a web ACL in WAF. Create an AWS WAF using the WebACL and ALB then enable logging by selecting the Kinesis Data Firehose as the destination. Subscribe to AWS Managed Rules in AWS Marketplace, choosing the WAF as the subscriber."
    ],
    "answer": "D",
    "explanation": "Explanation of the correct answer (D):\n\n1. The correct answer is D because it fully meets all the requirements stated in the question.\n   - Inbound requests are filtered for common vulnerability attacks using AWS WAF, which can be configured with a web ACL to monitor traffic to the Application Load Balancer (ALB).\n   - Rejected requests are sent to a third-party auditing application using Amazon Kinesis Data Firehose, which can be configured as the destination for the WAF logs.\n   - All resources are highly available by using a Multi-AZ Auto Scaling group with the application's AMI, and an ALB is used to distribute traffic to the instances.\n\nExplanations of the incorrect choices:\n\nA. This option is incorrect because it uses Amazon Inspector, which is a vulnerability scanner and does not directly monitor traffic flow. Additionally, the use of an AWS Lambda function to push the Amazon Inspector report to the third-party auditing application is more complex than the Kinesis Data Firehose approach in the correct answer.\n\nB. This option is incorrect because it does not mention the use of a Multi-AZ Auto Scaling group, which is required for high availability. Additionally, it uses an AWS Lambda function to push the logs to the third-party auditing application, which is more complex than the Kinesis Data Firehose approach in the correct answer.\n\nC. This option is incorrect because it does not mention the use of a Multi-AZ Auto Scaling group, which is required for high availability. Additionally, the question does not specify the use of AWS Marketplace-managed rules, so this approach may not be necessary."
  },
  "218": {
    "question": "A company is migrating an application to AWS. It wants to use fully managed services as much as possible during the migration. The company needs to store large important documents within the application with the following requirements:1. The data must be highly durable and available2. The data must always be encrypted at rest and in transit3. The encryption key must be managed by the company and rotated periodicallyWhich of the following solutions should the solutions architect recommend?",
    "choices": [
      "A. Deploy the storage gateway to AWS in file gateway mode. Use Amazon EBS volume encryption using an AWS KMS key to encrypt the storage gateway volumes.",
      "B. Use Amazon S3 with a bucket policy to enforce HTTPS for connections to the bucket and to enforce server-side encryption and AWS KMS for object encryption.",
      "C. Use Amazon DynamoDB with SSL to connect to DynamoDB. Use an AWS KMS key to encrypt DynamoDB objects at rest.",
      "D. Deploy instances with Amazon EBS volumes attached to store this data. Use EBS volume encryption using an AWS KMS key to encrypt the data."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B, which recommends using Amazon S3 with a bucket policy to enforce HTTPS for connections to the bucket and to enforce server-side encryption with AWS KMS for object encryption.\n\nThis solution meets all the given requirements:\n\n- Amazon S3 provides highly durable and available storage for large, important documents.\n- The bucket policy can enforce HTTPS connections, ensuring the data is encrypted in transit.\n- Server-side encryption with AWS KMS allows the company to manage and rotate the encryption keys, meeting the requirement for customer-managed encryption keys.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Deploy the storage gateway to AWS in file gateway mode. Use Amazon EBS volume encryption using an AWS KMS key to encrypt the storage gateway volumes.\n- This solution uses Amazon EBS, which is not a fully managed service like S3. It also requires the deployment and management of a storage gateway, which adds complexity.\n- EBS volume encryption, while providing encryption at rest, does not meet the requirement for encryption in transit.\n\nC. Use Amazon DynamoDB with SSL to connect to DynamoDB. Use an AWS KMS key to encrypt DynamoDB objects at rest.\n- DynamoDB is a NoSQL database, not a storage service for large documents. It is not the most appropriate solution for the given requirements.\n- While DynamoDB supports encryption at rest with AWS KMS, it does not provide the high durability and availability required for large, important documents.\n\nD. Deploy instances with Amazon EBS volumes attached to store this data. Use EBS volume encryption using an AWS KMS key to encrypt the data.\n- This solution uses Amazon EC2 instances and EBS volumes, which are not fully managed services like S3.\n- EBS volume encryption, while providing encryption at rest, does not meet the requirement for encryption in transit.\n\nIn summary, the correct answer, B, is the best solution as it leverages the fully managed Amazon S3 service with built-in support for server-side encryption and customer-managed keys, meeting all the given requirements."
  },
  "219": {
    "question": "A company has set up its entire infrastructure on AWS. The company uses Amazon EC2 instances to host its ecommerce website and uses Amazon S3 to store static data. Three engineers at the company handle the cloud administration and development through one AWS account. Occasionally, an engineer alters an EC2 security group configuration of another engineer and causes noncompliance issues in the environment.A solutions architect must set up a system that tracks changes that the engineers make. The system must send alerts when the engineers make noncompliant changes to the security settings for the EC2 instances.What is the FASTEST way for the solutions architect to meet these requirements?",
    "choices": [
      "A. Set up AWS Organizations for the company. Apply SCPs to govern and track noncompliant security group changes that are made to the AWS account.",
      "B. Enable AWS CloudTrail to capture the changes to EC2 security groups. Enable Amazon CloudWatch rules to provide alerts when noncompliant security settings are detected.",
      "C. Enable SCPs on the AWS account to provide alerts when noncompliant security group changes are made to the environment.",
      "D. Enable AWS Config on the EC2 security groups to track any noncompliant changes. Send the changes as alerts through an Amazon Simple Notification Service (Amazon SNS) topic."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (D):\n\nThe correct answer is D. Enable AWS Config on the EC2 security groups to track any noncompliant changes, and send the changes as alerts through an Amazon Simple Notification Service (Amazon SNS) topic.\n\nThis is the fastest way to meet the requirements for the following reasons:\n\n- AWS Config can continuously monitor and record configuration changes to the EC2 security groups, including any noncompliant changes.\n- AWS Config can be configured to send notifications through an Amazon SNS topic whenever a noncompliant change is detected.\n- This setup provides near real-time detection and notification of noncompliant security group changes, which is faster than alternatives like CloudTrail and CloudWatch.\n- The integration between AWS Config and Amazon SNS eliminates the need to set up additional services and configurations, making it the fastest solution.\n\n2. Explanations of the incorrect choices:\n\nA. Set up AWS Organizations for the company and apply SCPs to govern and track noncompliant security group changes: This is not the fastest solution, as it requires setting up AWS Organizations, which is a more complex and time-consuming process. Additionally, SCPs primarily govern permissions and access, rather than providing real-time monitoring and alerting of specific configuration changes.\n\nB. Enable AWS CloudTrail to capture the changes to EC2 security groups, and enable Amazon CloudWatch rules to provide alerts when noncompliant security settings are detected: This is a valid solution, but it is not the fastest. The setup required for CloudTrail, CloudWatch Logs, CloudWatch Metric Filters, and CloudWatch Alarms is more complex and time-consuming than the direct integration between AWS Config and Amazon SNS.\n\nC. Enable SCPs on the AWS account to provide alerts when noncompliant security group changes are made to the environment: This is not a viable solution, as SCPs are designed for access control and governance, not for providing real-time alerts on configuration changes. SCPs do not have the capability to detect and alert on noncompliant security group changes."
  },
  "220": {
    "question": "A company has IoT sensors that monitor traffic patterns throughout a large city. The company wants to read and collect data from the sensors and perform aggregations on the data.A solutions architect designs a solution in which the IoT devices are streaming to Amazon Kinesis Data Streams. Several applications are reading from the stream. However, several consumers are experiencing throttling and are periodically encountering a ReadProvisionedThroughputExceeded error.Which actions should the solutions architect take to resolve this issue? (Choose three.)",
    "choices": [
      "A. Reshard the stream to increase the number of shards in the stream.",
      "B. Use the Kinesis Producer Library (KPL). Adjust the polling frequency.",
      "C. Use consumers with the enhanced fan-out feature.",
      "D. Reshard the stream to reduce the number of shards in the stream.",
      "E. Use an error retry and exponential backoff mechanism in the consumer logic.",
      "F. Configure the stream to use dynamic partitioning."
    ],
    "answer": "ACE",
    "explanation": "1. Explanation of the correct answer:\n\nThe correct answer is ACE.\n\nA. Reshard the stream to increase the number of shards in the stream:\n- Increasing the number of shards in the Kinesis Data Streams increases the overall throughput capacity of the stream.\n- More shards mean more concurrent reads can be performed without exceeding the provisioned throughput, which should help resolve the throttling and ReadProvisionedThroughputExceeded errors.\n\nC. Use consumers with the enhanced fan-out feature:\n- Enhanced fan-out allows multiple consumers to read from the same shard concurrently, without being limited by the read capacity of the shard.\n- This helps distribute the load across the consumers and reduces the chances of throttling.\n\nE. Use an error retry and exponential backoff mechanism in the consumer logic:\n- Implementing an error retry mechanism with exponential backoff in the consumer logic helps handle throttling errors gracefully.\n- When a ReadProvisionedThroughputExceeded error occurs, the consumer can retry the read operation after a certain delay, gradually increasing the delay between retries to avoid overwhelming the system.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. Use the Kinesis Producer Library (KPL) and adjust the polling frequency:\n- The KPL is primarily used for producing data into the Kinesis stream, not consuming it.\n- Adjusting the polling frequency may not directly address the throttling issue on the consumer side.\n\nD. Reshard the stream to reduce the number of shards in the stream:\n- Reducing the number of shards in the Kinesis Data Streams will decrease the overall throughput capacity of the stream.\n- This would likely exacerbate the throttling and ReadProvisionedThroughputExceeded errors, as there would be fewer concurrent reads that can be performed.\n\nF. Configure the stream to use dynamic partitioning:\n- Dynamic partitioning is more about balancing the data across shards and does not increase the overall read capacity of the stream.\n- While it can be beneficial for even distribution of data, it does not directly address the throttling issue."
  },
  "221": {
    "question": "A company uses AWS Organizations to manage its AWS accounts. The company needs a list of all its Amazon EC2 instances that have underutilized CPU or memory usage. The company also needs recommendations for how to downsize these underutilized instances.Which solution will meet these requirements with the LEAST effort?",
    "choices": [
      "A. Install a CPU and memory monitoring tool from AWS Marketplace on all the EC2 instances. Store the findings in Amazon S3. Implement a Python script to identify underutilized instances. Reference EC2 instance pricing information for recommendations about downsizing options.",
      "B. Install the Amazon CloudWatch agent on all the EC2 instances by using AWS Systems Manager. Retrieve the resource optimization recommendations from AWS Cost Explorer in the organization\u2019s management account. Use the recommendations to downsize underutilized instances in all accounts of the organization.",
      "C. Install the Amazon CloudWatch agent on all the EC2 instances by using AWS Systems Manager. Retrieve the resource optimization recommendations from AWS Cost Explorer in each account of the organization. Use the recommendations to downsize underutilized instances in all accounts of the organization.",
      "D. Install the Amazon CloudWatch agent on all the EC2 instances by using AWS Systems Manager. Create an AWS Lambda function to extract CPU and memory usage from all the EC2 instances. Store the findings as files in Amazon S3. Use Amazon Athena to find underutilized instances. Reference EC2 instance pricing information for recommendations about downsizing options."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the Correct Answer (Option B):\n\nOption B is the correct answer because it requires the least effort to meet the given requirements. Here's why:\n\n- It leverages existing AWS services - CloudWatch agent and Cost Explorer - which are already available to the company, eliminating the need to install any additional third-party tools.\n- The CloudWatch agent can be deployed on all EC2 instances using AWS Systems Manager, a native AWS service, streamlining the setup process.\n- The resource optimization recommendations, including recommendations for downsizing underutilized instances, can be retrieved directly from the AWS Cost Explorer in the organization's management account. This provides a centralized view of the resource utilization across all accounts, minimizing the effort required to gather the necessary information.\n- The downsizing recommendations from Cost Explorer can be directly used to right-size the underutilized instances, without the need for additional manual effort to reference EC2 instance pricing information.\n\n2. Explanations of the Incorrect Choices:\n\nA. Install a third-party monitoring tool and write custom Python script:\n- Requires manual installation of a third-party tool on all EC2 instances, increasing the overall effort.\n- Developing and maintaining a custom Python script to identify underutilized instances adds complexity and ongoing maintenance overhead.\n- The manual effort to reference EC2 instance pricing information for downsizing recommendations is more time-consuming compared to the centralized approach in Option B.\n\nC. Retrieve recommendations from Cost Explorer in each account:\n- This approach would require retrieving the recommendations separately for each account within the organization, increasing the administrative overhead compared to the centralized management approach in Option B.\n\nD. Use CloudWatch, Lambda, and Athena:\n- While this option leverages native AWS services, it involves creating and maintaining additional services (Lambda and Athena), which increases the overall complexity and effort compared to the straightforward combination of CloudWatch and Cost Explorer in Option B.\n\nIn summary, Option B is the correct answer as it requires the least effort by leveraging existing AWS services and providing a centralized view of the resource optimization recommendations across the organization's accounts."
  },
  "222": {
    "question": "A company wants to run a custom network analysis software package to inspect traffic as traffic leaves and enters a VPC. The company has deployed the solution by using AWS CloudFormation on three Amazon EC2 instances in an Auto Scaling group. All network routing has been established to direct traffic to the EC2 instances.Whenever the analysis software stops working, the Auto Scaling group replaces an instance. The network routes are not updated when the instance replacement occurs.Which combination of steps will resolve this issue? (Choose three.)",
    "choices": [
      "A. Create alarms based on EC2 status check metrics that will cause the Auto Scaling group to replace the failed instance.",
      "B. Update the CloudFormation template to install the Amazon CloudWatch agent on the EC2 instances. Configure the CloudWatch agent to send process metrics for the application.",
      "C. Update the CloudFormation template to install AWS Systems Manager Agent on the EC2 instances. Configure Systems Manager Agent to send process metrics for the application.",
      "D. Create an alarm for the custom metric in Amazon CloudWatch for the failure scenarios. Configure the alarm to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic.",
      "E. Create an AWS Lambda function that responds to the Amazon Simple Notification Service (Amazon SNS) message to take the instance out of service. Update the network routes to point to the replacement instance.",
      "F. In the CloudFormation template, write a condition that updates the network routes when a replacement instance is launched."
    ],
    "answer": "BDE",
    "explanation": "1. Explanation of the correct answer (BDE):\n\nB. This is correct because installing the Amazon CloudWatch agent on the EC2 instances and configuring it to send process metrics for the application will provide valuable monitoring data that can be used to detect when the analysis software stops working.\n\nD. This is correct because creating an alarm for a custom metric in Amazon CloudWatch for the failure scenarios and configuring the alarm to publish a message to an Amazon SNS topic will enable the system to be notified when the application fails.\n\nE. This is correct because creating an AWS Lambda function that responds to the Amazon SNS message to take the instance out of service and update the network routes to point to the replacement instance will address the issue of the network routes not being updated when the instance replacement occurs.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This is incorrect because creating alarms based on EC2 status check metrics will not address the issue of the network routes not being updated when the instance replacement occurs.\n\nC. This is incorrect because installing the AWS Systems Manager Agent and configuring it to send process metrics for the application is not necessary to solve the problem, as the CloudWatch agent in choice B serves the same purpose.\n\nF. This is incorrect because updating the network routes in the CloudFormation template will not automatically update the routes when a replacement instance is launched. The network routes need to be dynamically updated, as specified in the correct answer (choice E).\n\nIn summary, the correct answer (BDE) provides a comprehensive solution that involves monitoring the application, detecting failures, and dynamically updating the network routes when a replacement instance is launched, which addresses the issue described in the question."
  },
  "223": {
    "question": "A company is developing a new on-demand video application that is based on microservices. The application will have 5 million users at launch and will have 30 million users after 6 months. The company has deployed the application on Amazon Elastic Container Service (Amazon ECS) on AWS Fargate. The company developed the application by using ECS services that use the HTTPS protocol.A solutions architect needs to implement updates to the application by using blue/green deployments. The solution must distribute traffic to each ECS service through a load balancer. The application must automatically adjust the number of tasks in response to an Amazon CloudWatch alarm.Which solution will meet these requirements?",
    "choices": [
      "A. Configure the ECS services to use the blue/green deployment type and a Network Load Balancer. Request increases to the service quota for tasks per service to meet the demand.",
      "B. Configure the ECS services to use the blue/green deployment type and a Network Load Balancer. Implement Auto Scaling group for each ECS service by using the Cluster Autoscaler.",
      "C. Configure the ECS services to use the blue/green deployment type and an Application Load Balancer. Implement an Auto Scaling group for each ECS service by using the Cluster Autoscaler.",
      "D. Configure the ECS services to use the blue/green deployment type and an Application Load Balancer. Implement Service Auto Scaling for each ECS service."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (D):\n\nThe correct answer is D because it meets all the requirements stated in the question:\n\n- It configures the ECS services to use the blue/green deployment type and an Application Load Balancer (ALB). ALB is the appropriate choice here since the application uses HTTPS, and ALB supports HTTPS protocol at the load balancer level.\n- It implements Service Auto Scaling for each ECS service. Service Auto Scaling automatically adjusts the number of tasks in response to an Amazon CloudWatch alarm, which is a requirement specified in the question.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Incorrect because it uses a Network Load Balancer (NLB), which does not support HTTPS protocol at the load balancer level. The question states that the application uses HTTPS.\n\nB. Incorrect because it uses a Network Load Balancer (NLB), which does not support HTTPS protocol at the load balancer level. Also, it implements Auto Scaling group for each ECS service using the Cluster Autoscaler, which is not applicable for Fargate as there is no need to manage the underlying infrastructure.\n\nC. Incorrect because it implements an Auto Scaling group for each ECS service using the Cluster Autoscaler, which is not applicable for Fargate as there is no need to manage the underlying infrastructure. The correct approach is to use Service Auto Scaling, as mentioned in the correct answer."
  },
  "224": {
    "question": "A company is running a containerized application in the AWS Cloud. The application is running by using Amazon Elastic Container Service (Amazon ECS) on a set of Amazon EC2 instances. The EC2 instances run in an Auto Scaling group.The company uses Amazon Elastic Container Registry (Amazon ECR) to store its container images. When a new image version is uploaded, the new image version receives a unique tag.The company needs a solution that inspects new image versions for common vulnerabilities and exposures. The solution must automatically delete new image tags that have Critical or High severity findings. The solution also must notify the development team when such a deletion occurs.Which solution meets these requirements?",
    "choices": [
      "A. Configure scan on push on the repository. Use Amazon EventBridge to invoke an AWS Step Functions state machine when a scan is complete for images that have Critical or High severity findings. Use the Step Functions state machine to delete the image tag for those images and to notify the development team through Amazon Simple Notification Service (Amazon SNS).",
      "B. Configure scan on push on the repository. Configure scan results to be pushed to an Amazon Simple Queue Service (Amazon SQS) queue. Invoke an AWS Lambda function when a new message is added to the SQS queue. Use the Lambda function to delete the image tag for images that have Critical or High severity findings. Notify the development team by using Amazon Simple Email Service (Amazon SES).",
      "C. Schedule an AWS Lambda function to start a manual image scan every hour. Configure Amazon EventBridge to invoke another Lambda function when a scan is complete. Use the second Lambda function to delete the image tag for images that have Critical or High severity findings. Notify the development team by using Amazon Simple Notification Service (Amazon SNS).",
      "D. Configure periodic image scan on the repository. Configure scan results to be added to an Amazon Simple Queue Service (Amazon SQS) queue. Invoke an AWS Step Functions state machine when a new message is added to the SQS queue. Use the Step Functions state machine to delete the image tag for images that have Critical or High severity findings. Notify the development team by using Amazon Simple Email Service (Amazon SES)."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\n\nThe correct answer is A because it best meets the requirements outlined in the question. The key points are:\n\n- It configures \"scan on push\" on the ECR repository, which will scan new images as they are uploaded.\n- It uses Amazon EventBridge to trigger a Step Functions state machine when a scan is complete and finds critical or high severity vulnerabilities.\n- The Step Functions state machine is used to delete the affected image tag and notify the development team using Amazon SNS.\n\nThis approach satisfies all the requirements:\n- It automatically scans new images for vulnerabilities\n- It deletes the image tags with critical/high severity findings\n- It notifies the development team when such a deletion occurs\n\n2. Explanations of the incorrect choices:\n\nB. This option uses an SQS queue to receive the scan results, which is not the recommended approach. The question specifically states to use EventBridge to trigger the workflow, which is a more appropriate solution.\n\nC. This option schedules a periodic scan using a Lambda function, which is not as effective as scanning on push. It also uses EventBridge to trigger a second Lambda function, which is less efficient than using a Step Functions state machine.\n\nD. This option also uses an SQS queue to receive the scan results, which is not the recommended approach. It then triggers a Step Functions state machine, which is similar to the correct answer, but it uses Amazon SES for notification instead of the more appropriate Amazon SNS.\n\nIn summary, the correct answer (A) is the most efficient and appropriate solution to meet the given requirements, while the other options have various limitations or use less optimal approaches."
  },
  "225": {
    "question": "A company is deploying a new web-based application and needs a storage solution for the Linux application servers. The company wants to create a single location for updates to application data for all instances. The active dataset will be up to 100 GB in size. A solutions architect has determined that peak operations will occur for 3 hours daily and will require a total of 225 MiBps of read throughput.The solutions architect must design a Multi-AZ solution that makes a copy of the data available in another AWS Region for disaster recovery (DR). The DR copy has an RPO of less than 1 hour.Which solution will meet these requirements?",
    "choices": [
      "A. Deploy a new Amazon Elastic File System (Amazon EFS) Multi-AZ file system. Configure the file system for 75 MiBps of provisioned throughput. Implement replication to a file system in the DR Region.",
      "B. Deploy a new Amazon FSx for Lustre file system. Configure Bursting Throughput mode for the file system. Use AWS Backup to back up the file system to the DR Region.",
      "C. Deploy a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume with 225 MiBps of throughput. Enable Multi-Attach for the EBS volume. Use AWS Elastic Disaster Recovery to replicate the EBS volume to the DR Region.",
      "D. Deploy an Amazon FSx for OpenZFS file system in both the production Region and the DR Region. Create an AWS DataSync scheduled task to replicate the data from the production file system to the DR file system every 10 minutes."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\n\nThe correct answer is A, which is to deploy a new Amazon Elastic File System (Amazon EFS) Multi-AZ file system. This solution meets the requirements in the following ways:\n\n- Amazon EFS supports Multi-AZ deployment, which provides high availability and durability of the file system across multiple Availability Zones within a single AWS Region.\n- EFS can be configured to provide up to 75 MiBps of provisioned throughput, which meets the peak operations requirement of 225 MiBps of read throughput (75 MiBps is the minimum provisioned throughput to meet this requirement).\n- EFS supports cross-Region replication, which can be used to create a copy of the data in the Disaster Recovery (DR) Region, meeting the requirement for a DR copy with an RPO of less than 1 hour.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. Deploy a new Amazon FSx for Lustre file system:\n- There is no \"Bursting Throughput mode\" for Amazon FSx for Lustre. Throughput is provisioned based on the file system's storage capacity.\n- Using AWS Backup to back up the file system to the DR Region does not meet the RPO requirement of less than 1 hour.\n\nC. Deploy a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume:\n- The gp3 EBS volume cannot meet the 225 MiBps of throughput requirement, as the maximum throughput for gp3 volumes is 1000 MiBps.\n- EBS volumes cannot be shared across multiple instances, so the \"single location for updates to application data for all instances\" requirement cannot be met.\n\nD. Deploy an Amazon FSx for OpenZFS file system in both the production Region and the DR Region:\n- AWS DataSync tasks cannot be scheduled more frequently than hourly, so the RPO requirement of less than 1 hour cannot be met."
  },
  "226": {
    "question": "A company has hundreds of AWS accounts. The company uses an organization in AWS Organizations to manage all the accounts. The company has turned on all features.A finance team has allocated a daily budget for AWS costs. The finance team must receive an email notification if the organization's AWS costs exceed 80% of the allocated budget. A solutions architect needs to implement a solution to track the costs and deliver the notifications.Which solution will meet these requirements?",
    "choices": [
      "A. In the organization's management account, use AWS Budgets to create a budget that has a daily period. Add an alert threshold and set the value to 80%. Use Amazon Simple Notification Service (Amazon SNS) to notify the finance team.",
      "B. In the organization\u2019s management account, set up the organizational view feature for AWS Trusted Advisor. Create an organizational view report for cost optimization. Set an alert threshold of 80%. Configure notification preferences. Add the email addresses of the finance team.",
      "C. Register the organization with AWS Control Tower. Activate the optional cost control (guardrail). Set a control (guardrail) parameter of 80%. Configure control (guardrail) notification preferences. Use Amazon Simple Notification Service (Amazon SNS) to notify the finance team.",
      "D. Configure the member accounts to save a daily AWS Cost and Usage Report to an Amazon S3 bucket in the organization's management account. Use Amazon EventBridge to schedule a daily Amazon Athena query to calculate the organization\u2019s costs. Configure Athena to send an Amazon CloudWatch alert if the total costs are more than 80% of the allocated budget. Use Amazon Simple Notification Service (Amazon SNS) to notify the finance team."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\n\nThe correct answer is A because it directly addresses the requirements presented in the question:\n\n- The company has hundreds of AWS accounts managed through an AWS Organization, and the finance team needs to monitor the overall costs for the organization.\n- AWS Budgets can be configured at the organization level (in the management account) to track the daily AWS costs across all member accounts.\n- AWS Budgets allows setting an alert threshold at 80% of the allocated daily budget.\n- Amazon SNS can be used to send email notifications to the finance team when the 80% threshold is exceeded.\n\nThis solution is the most straightforward and efficient way to meet the given requirements without requiring additional setup or configuration across multiple services.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. Trusted Advisor is a service that provides recommendations for cost optimization, but it does not provide the specific daily budget tracking and alerting required in this scenario.\n\nC. AWS Control Tower is a service for setting up and governing a secure multi-account AWS environment, but it does not provide the specific daily budget tracking and alerting required in this scenario.\n\nD. This solution is overly complex and not necessary to meet the requirements. Configuring daily AWS Cost and Usage Reports, Amazon Athena, Amazon EventBridge, and Amazon CloudWatch alerts is not required when AWS Budgets can directly handle the daily budget tracking and alerting.\n\nIn summary, option A is the correct choice because it directly addresses the requirement of tracking daily AWS costs across the organization and providing email notifications to the finance team when the costs exceed 80% of the allocated budget, using the most appropriate and straightforward AWS service (AWS Budgets)."
  },
  "227": {
    "question": "A company provides auction services for artwork and has users across North America and Europe. The company hosts its application in Amazon EC2 instances in the us-east-1 Region. Artists upload photos of their work as large-size. high-resolution image files from their mobile phones to a centralized Amazon S3 bucket created in the us-east-1 Region. The users in Europe are reporting slow performance for their image uploads.How can a solutions architect improve the performance of the image upload process?",
    "choices": [
      "A. Redeploy the application to use S3 multipart uploads.",
      "B. Create an Amazon CloudFront distribution and point to the application as a custom origin.",
      "C. Configure the buckets to use S3 Transfer Acceleration.",
      "D. Create an Auto Scaling group for the EC2 instances and create a scaling policy."
    ],
    "answer": "C",
    "explanation": "1. Clear explanation of the correct answer:\n\nThe correct answer is C. Configure the buckets to use S3 Transfer Acceleration.\n\nThe key issue in the question is that users in Europe are experiencing slow performance for their image uploads. Since the S3 bucket is located in the us-east-1 Region, and the users are in Europe, the high network latency between the two locations is likely causing the performance issue.\n\nS3 Transfer Acceleration addresses this problem by using the Amazon CloudFront network to optimize the data transfer path between the user's location and the S3 bucket. This helps reduce the network latency and improve the upload performance for the European users.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Redeploy the application to use S3 multipart uploads.\nThis option would not directly address the issue of high network latency between Europe and the S3 bucket in the us-east-1 Region. Multipart uploads can improve upload performance in general, but they do not solve the problem of geographic distance.\n\nB. Create an Amazon CloudFront distribution and point to the application as a custom origin.\nCloudFront is primarily used for content delivery, not for optimizing upload performance. While CloudFront could potentially be used to cache the uploaded images, it would not directly address the issue of high network latency for the initial image uploads from Europe to the S3 bucket.\n\nD. Create an Auto Scaling group for the EC2 instances and create a scaling policy.\nScaling the EC2 instances would not improve the upload performance for the European users. The issue is not with the compute resources, but rather with the network latency between Europe and the S3 bucket location."
  },
  "228": {
    "question": "A company wants to containerize a multi-tier web application and move the application from an on-premises data center to AWS. The application includes web. application, and database tiers. The company needs to make the application fault tolerant and scalable. Some frequently accessed data must always be available across application servers. Frontend web servers need session persistence and must scale to meet increases in traffic.Which solution will meet these requirements with the LEAST ongoing operational overhead?",
    "choices": [
      "A. Run the application on Amazon Elastic Container Service (Amazon ECS) on AWS Fargate. Use Amazon Elastic File System (Amazon EFS) for data that is frequently accessed between the web and application tiers. Store the frontend web server session data in Amazon Simple Queue Service (Amazon SQS).",
      "B. Run the application on Amazon Elastic Container Service (Amazon ECS) on Amazon EC2. Use Amazon ElastiCache for Redis to cache frontend web server session data. Use Amazon Elastic Block Store (Amazon EBS) with Multi-Attach on EC2 instances that are distributed across multiple Availability Zones.",
      "C. Run the application on Amazon Elastic Kubernetes Service (Amazon EKS). Configure Amazon EKS to use managed node groups. Use ReplicaSets to run the web servers and applications. Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system across all EKS pods to store frontend web server session data.",
      "D. Deploy the application on Amazon Elastic Kubernetes Service (Amazon EKS). Configure Amazon EKS to use managed node groups. Run the web servers and application as Kubernetes deployments in the EKS cluster. Store the frontend web server session data in an Amazon DynamoDB table. Create an Amazon Elastic File System (Amazon EFS) volume that all applications will mount at the time of deployment."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (D):\n\nThe correct answer is D because it provides the least ongoing operational overhead for the given requirements.\n\nKey points:\n\n- Deploying the application on Amazon Elastic Kubernetes Service (Amazon EKS) allows for a highly scalable and fault-tolerant platform, with managed node groups handling the underlying infrastructure.\n- Storing the frontend web server session data in an Amazon DynamoDB table provides a fully managed, highly available, and scalable solution for session persistence without the need to manage additional components like caching.\n- Creating an Amazon Elastic File System (Amazon EFS) volume that all applications can mount at deployment time allows for shared access to frequently accessed data across the tiers, ensuring high availability and fault tolerance.\n\nOverall, this solution leverages fully managed AWS services (EKS, DynamoDB, EFS) to meet the requirements with minimal operational overhead, as the underlying infrastructure and scaling are handled by AWS.\n\n2. Explanations of the incorrect choices:\n\nA. This option uses Amazon Elastic Container Service (Amazon ECS) on AWS Fargate, which is a good choice for minimizing operational overhead. However, storing the frontend web server session data in Amazon Simple Queue Service (Amazon SQS) is not the optimal solution, as SQS is designed for asynchronous message queuing, not for storing session data.\n\nB. This option uses Amazon Elastic Container Service (Amazon ECS) on Amazon EC2, which requires more operational overhead than the correct answer (D) as it involves managing the underlying EC2 infrastructure. Additionally, using Amazon Elastic Block Store (Amazon EBS) with Multi-Attach across Availability Zones is not an optimal solution for the stated requirement of \"frequently accessed data must always be available across application servers,\" as EBS with Multi-Attach is limited to the same Availability Zone.\n\nC. This option uses Amazon Elastic Kubernetes Service (Amazon EKS), which is a good choice for the given requirements. However, using Amazon Elastic File System (Amazon EFS) to store the frontend web server session data is not the most optimal solution, as EFS is designed for shared file storage, not for storing session data. This would result in higher operational overhead compared to the correct answer (D), which uses DynamoDB for session storage."
  },
  "229": {
    "question": "A solutions architect is planning to migrate critical Microsoft SQL Server databases to AWS. Because the databases are legacy systems, the solutions architect will move the databases to a modern data architecture. The solutions architect must migrate the databases with near-zero downtime.Which solution will meet these requirements?",
    "choices": [
      "A. Use AWS Application Migration Service and the AWS Schema Conversion Tool (AWS SCT). Perform an in-place upgrade before the migration. Export the migrated data to Amazon Aurora Serverless after cutover. Repoint the applications to Amazon Aurora.",
      "B. Use AWS Database Migration Service (AWS DMS) to rehost the database. Set Amazon S3 as a target. Set up change data capture (CDC) replication. When the source and destination are fully synchronized, load the data from Amazon S3 into an Amazon RDS for Microsoft SQL Server DB instance.",
      "C. Use native database high availability tools. Connect the source system to an Amazon RDS for Microsoft SQL Server DB instance. Configure replication accordingly. When data replication is finished, transition the workload to an Amazon RDS for Microsoft SQL Server DB instance.",
      "D. Use AWS Application Migration Service. Rehost the database server on Amazon EC2. When data replication is finished, detach the database and move the database to an Amazon RDS for Microsoft SQL Server DB instance. Reattach the database and then cut over all networking."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\nThe correct answer is C because it leverages the native database high availability tools and replication methods provided by Microsoft SQL Server. This approach allows the solutions architect to perform a near-zero downtime migration by setting up replication between the on-premises SQL Server and the target Amazon RDS for Microsoft SQL Server DB instance. The data is directly replicated between the source and destination, without the need for intermediate storage like Amazon S3. This ensures a seamless and efficient migration process that meets the requirement of near-zero downtime.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option involves an in-place upgrade before the migration, which may not be feasible or desirable for a legacy system. Additionally, the use of Amazon Aurora Serverless as the target database introduces an extra step of exporting the data and repointing the applications, which can increase the overall complexity and downtime of the migration.\n\nB. While AWS DMS can be a suitable option for database migrations, the use of Amazon S3 as an intermediate storage target adds an unnecessary step that can increase the downtime during the final cutover. Native SQL Server replication tools can directly replicate the data between the source and the target RDS instance, providing a more efficient and downtime-minimizing approach.\n\nD. This option involves rehosting the database server on Amazon EC2 and then moving it to an Amazon RDS for Microsoft SQL Server DB instance. This two-step process can introduce additional complexity and potential downtime compared to the direct replication approach using native SQL Server tools, as described in the correct answer (C)."
  },
  "230": {
    "question": "A company's solutions architect is analyzing costs of a multi-application environment. The environment is deployed across multiple Availability Zones in a single AWS Region. After a recent acquisition, the company manages two organizations in AWS Organizations. The company has created multiple service provider applications as AWS PrivateLink-powered VPC endpoint services in one organization. The company has created multiple service consumer applications in the other organization.Data transfer charges are much higher than the company expected, and the solutions architect needs to reduce the costs. The solutions architect must recommend guidelines for developers to follow when they deploy services. These guidelines must minimize data transfer charges for the whole environment.Which guidelines meet these requirements? (Choose two.)",
    "choices": [
      "A. Use AWS Resource Access Manager to share the subnets that host the service provider applications with other accounts in the organization.",
      "B. Place the service provider applications and the service consumer applications in AWS accounts in the same organization.",
      "C. Turn off cross-zone load balancing for the Network Load Balancer in all service provider application deployments.",
      "D. Ensure that service consumer compute resources use the Availability Zone-specific endpoint service by using the endpoint's local DNS name.",
      "E. Create a Savings Plan that provides adequate coverage for the organization's planned inter-Availability Zone data transfer usage."
    ],
    "answer": "CD",
    "explanation": "1. Explanation of the correct answer:\n\nC. Turn off cross-zone load balancing for the Network Load Balancer in all service provider application deployments.\nD. Ensure that service consumer compute resources use the Availability Zone-specific endpoint service by using the endpoint's local DNS name.\n\nThese guidelines help minimize data transfer charges in the given multi-application environment:\n\nC. Turning off cross-zone load balancing for the Network Load Balancer (NLB) ensures that traffic between the service consumer and service provider applications stays within the same Availability Zone (AZ). This avoids cross-AZ data transfer charges, which can be significant ($0.02 per GB) for NLB.\n\nD. Using the Availability Zone-specific endpoint service's local DNS name ensures that the service consumer compute resources access the service provider applications within the same AZ. This eliminates the need for cross-AZ data transfer, further reducing data transfer charges.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Using AWS Resource Access Manager (RAM) to share the subnets that host the service provider applications with other accounts in the organization does not directly address the data transfer cost issue. While it allows the service consumer applications to be deployed in the same organization's accounts, it does not ensure that the traffic stays within the same AZ.\n\nB. Placing the service provider applications and the service consumer applications in AWS accounts in the same organization does not necessarily reduce data transfer charges. If the applications are deployed across different AZs, there will still be cross-AZ data transfer costs.\n\nE. Creating a Savings Plan for the organization's planned inter-Availability Zone data transfer usage does not directly reduce the data transfer charges. It only provides a discounted rate for the data transfer, but does not address the root cause of the high costs."
  },
  "231": {
    "question": "A company has an on-premises Microsoft SQL Server database that writes a nightly 200 GB export to a local drive. The company wants to move the backups to more robust cloud storage on Amazon S3. The company has set up a 10 Gbps AWS Direct Connect connection between the on-premises data center and AWS.Which solution meets these requirements MOST cost-effectively?",
    "choices": [
      "A. Create a new S3 bucket. Deploy an AWS Storage Gateway file gateway within the VPC that is connected to the Direct Connect connection. Create a new SMB file share. Write nightly database exports to the new SMB file share.",
      "B. Create an Amazon FSx for Windows File Server Single-AZ file system within the VPC that is connected to the Direct Connect connection. Create a new SMB file share. Write nightly database exports to an SMB file share on the Amazon FSx file system. Enable nightly backups.",
      "C. Create an Amazon FSx for Windows File Server Multi-AZ file system within the VPC that is connected to the Direct Connect connection. Create a new SMB file share. Write nightly database exports to an SMB file share on the Amazon FSx file system. Enable nightly backups.",
      "D. Create a new S3 bucket. Deploy an AWS Storage Gateway volume gateway within the VPC that is connected to the Direct Connect connection. Create a new SMB file share. Write nightly database exports to the new SMB file share on the volume gateway, and automate copies of this data to an S3 bucket."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\nThe correct answer is A, which suggests using the AWS Storage Gateway file gateway. This is the most cost-effective solution for the given requirements for the following reasons:\n\n- The file gateway allows you to create an SMB file share, which can be used to write the 200 GB nightly database exports. This provides a familiar interface for the on-premises system to access the cloud storage.\n- The file gateway automatically uploads the data from the SMB file share to the S3 bucket, providing a reliable and automated backup solution.\n- The 10 Gbps AWS Direct Connect connection ensures efficient and low-latency data transfer between the on-premises data center and the AWS cloud, making this a cost-effective solution.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. Amazon FSx for Windows File Server:\n- While Amazon FSx provides a fully managed Windows file system, it is generally more expensive than using the AWS Storage Gateway file gateway, especially for large volumes of data like the 200 GB nightly exports.\n- The single-AZ and multi-AZ options (B and C) are essentially the same in terms of cost-effectiveness for this use case, as the backup requirements do not necessitate the added resiliency of a multi-AZ deployment.\n\nC. Amazon FSx for Windows File Server Multi-AZ:\n- Same as the explanation for choice B, the multi-AZ option is not significantly more cost-effective than the single-AZ option for this use case.\n\nD. AWS Storage Gateway volume gateway:\n- The volume gateway uses iSCSI, which is not as suitable for this use case as the file gateway, which provides an SMB file share interface.\n- While the volume gateway can automate data transfers to S3, it would require an additional step of creating an SMB file share on top of the iSCSI volume, which adds complexity and potential cost compared to the file gateway solution."
  },
  "232": {
    "question": "A company needs to establish a connection from its on-premises data center to AWS. The company needs to connect all of its VPCs that are located in different AWS Regions with transitive routing capabilities between VPC networks. The company also must reduce network outbound traffic costs, increase bandwidth throughput, and provide a consistent network experience for end users.Which solution will meet these requirements?",
    "choices": [
      "A. Create an AWS Site-to-Site VPN connection between the on-premises data center and a new central VPC. Create VPC peering connections that initiate from the central VPC to all other VPCs.",
      "B. Create an AWS Direct Connect connection between the on-premises data center and AWS. Provision a transit VIF, and connect it to a Direct Connect gateway. Connect the Direct Connect gateway to all the other VPCs by using a transit gateway in each Region.",
      "C. Create an AWS Site-to-Site VPN connection between the on-premises data center and a new central VPUse a transit gateway with dynamic routing. Connect the transit gateway to all other VPCs.",
      "D. Create an AWS Direct Connect connection between the on-premises data center and AWS. Establish an AWS Site-to-Site VPN connection between all VPCs in each Region. Create VPC peering connections that initiate from the central VPC to all other VPCs."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B because it best meets the requirements stated in the question:\n\n- AWS Direct Connect connection between the on-premises data center and AWS provides higher bandwidth throughput and a more consistent network experience compared to a Site-to-Site VPN.\n- Provisioning a transit VIF and connecting it to a Direct Connect gateway allows for transitive routing capabilities between the VPC networks in different AWS Regions.\n- Using a Direct Connect gateway to connect the transit gateway to all the other VPCs helps reduce network outbound traffic costs by routing traffic directly between the on-premises data center and the VPCs.\n\n2. Explanations of the incorrect choices:\n\nA. This option uses VPC peering, which does not provide transitive routing capabilities between the VPCs. It also does not leverage the higher bandwidth and more consistent network experience of Direct Connect.\n\nC. This option uses a Site-to-Site VPN connection, which has lower bandwidth and less consistent network performance compared to Direct Connect. It also does not directly address the requirement to reduce network outbound traffic costs.\n\nD. This option combines Site-to-Site VPN and VPC peering, which does not provide the same level of performance, transitive routing, and cost optimization as the Direct Connect and transit gateway solution in option B.\n\nIn summary, option B is the best choice because it uses the higher-performance and more cost-effective Direct Connect solution, along with a transit gateway to enable transitive routing between the VPCs, thereby meeting all the requirements stated in the question."
  },
  "233": {
    "question": "A company is developing a new on-demand video application that is based on microservices. The application will have 5 million users at launch and will have 30 million users after 6 months. The company has deployed the application on Amazon Elastic Container Service (Amazon ECS) on AWS Fargate. The company developed the application by using ECS services that use the HTTPS protocol.A solutions architect needs to implement updates to the application by using blue/green deployments. The solution must distribute traffic to each ECS service through a load balancer. The application must automatically adjust the number of tasks in response to an Amazon CloudWatch alarm.Which solution will meet these requirements?",
    "choices": [
      "A. Configure the ECS services to use the blue/green deployment type and a Network Load Balancer. Request increases to the service quota for tasks per service to meet the demand.",
      "B. Configure the ECS services to use the blue/green deployment type and a Network Load Balancer. Implement Auto Scaling group for each ECS service by using the Cluster Autoscaler.",
      "C. Configure the ECS services to use the blue/green deployment type and an Application Load Balancer. Implement an Auto Scaling group for each ECS service by using the Cluster Autoscaler.",
      "D. Configure the ECS services to use the blue/green deployment type and an Application Load Balancer. Implement Service Auto Scaling for each ECS service."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (D):\n\nThe correct answer is D. Configure the ECS services to use the blue/green deployment type and an Application Load Balancer (ALB). Implement Service Auto Scaling for each ECS service.\n\nThis solution meets the requirements because:\n- The blue/green deployment type allows for safe and controlled application updates.\n- The Application Load Balancer can distribute traffic to the ECS services using HTTPS, which is required by the application.\n- Service Auto Scaling automatically adjusts the number of tasks in response to the Amazon CloudWatch alarm, ensuring the application can handle the increasing number of users.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option is incorrect because it uses a Network Load Balancer (NLB), which can only support TCP listeners and not HTTPS. The application requires HTTPS, so this option does not meet the requirements.\n\nB. This option is incorrect because it uses a Network Load Balancer, which does not support HTTPS. Additionally, it mentions implementing an Auto Scaling group using the Cluster Autoscaler, which is not applicable for Fargate, as Fargate does not use a cluster-based architecture.\n\nC. This option is incorrect because it mentions implementing an Auto Scaling group using the Cluster Autoscaler, which is not applicable for Fargate, as Fargate does not use a cluster-based architecture. The correct scaling mechanism for Fargate is Service Auto Scaling."
  },
  "234": {
    "question": "A company wants to use AWS for disaster recovery for an on-premises application. The company has hundreds of Windows-based servers that run the application. All the servers mount a common share.The company has an RTO of 15 minutes and an RPO of 5 minutes. The solution must support native failover and fallback capabilities.Which solution will meet these requirements MOST cost-effectively?",
    "choices": [
      "A. Create an AWS Storage Gateway File Gateway. Schedule daily Windows server backups. Save the data to Amazon S3. During a disaster, recover the on-premises servers from the backup. During tailback, run the on-premises servers on Amazon EC2 instances.",
      "B. Create a set of AWS CloudFormation templates to create infrastructure. Replicate all data to Amazon Elastic File System (Amazon EFS) by using AWS DataSync. During a disaster, use AWS CodePipeline to deploy the templates to restore the on-premises servers. Fail back the data by using DataSync.",
      "C. Create an AWS Cloud Development Kit (AWS CDK) pipeline to stand up a multi-site active-active environment on AWS. Replicate data into Amazon S3 by using the s3 sync command. During a disaster, swap DNS endpoints to point to AWS. Fail back the data by using the s3 sync command.",
      "D. Use AWS Elastic Disaster Recovery to replicate the on-premises servers. Replicate data to an Amazon FSx for Windows File Server file system by using AWS DataSync. Mount the file system to AWS servers. During a disaster, fail over the on-premises servers to AWS. Fail back to new or existing servers by using Elastic Disaster Recovery."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (Option D):\n\nOption D is the correct answer because it meets all the requirements specified in the question:\n\n- AWS Elastic Disaster Recovery (EDR) can be used to replicate the on-premises Windows-based servers to AWS, providing the required native failover and fallback capabilities.\n- The data can be replicated to an Amazon FSx for Windows File Server file system using AWS DataSync, which can meet the 5-minute RPO requirement.\n- During a disaster, the on-premises servers can be failed over to AWS, and when failing back, the data can be replicated back to new or existing on-premises servers using the Elastic Disaster Recovery service, meeting the 15-minute RTO requirement.\n- This solution is considered the most cost-effective as it utilizes managed services like FSx for Windows and EDR, reducing the operational overhead and complexity compared to other options.\n\n2. Explanations of why the incorrect choices are wrong:\n\nOption A:\n- Using a Storage Gateway File Gateway and daily backups to Amazon S3 may not meet the 5-minute RPO requirement, as the backup frequency is only daily.\n- The recovery process during a disaster and failback would be more complex, as it would involve recovering the on-premises servers from the backups, which may not meet the 15-minute RTO.\n\nOption B:\n- Creating a set of CloudFormation templates and replicating data to Amazon EFS may not provide the native failover and fallback capabilities required by the question.\n- The recovery and failback process using AWS CodePipeline may be more complex and may not meet the 15-minute RTO requirement.\n\nOption C:\n- Creating an active-active environment on AWS using AWS CDK and replicating data to Amazon S3 may not provide the native failover and fallback capabilities required by the question.\n- The failover and failback process using DNS endpoint swapping and S3 sync may be more complex and may not meet the 15-minute RTO requirement."
  },
  "235": {
    "question": "A company is running an application on Amazon EC2 instances in the AWS Cloud. The application is using a MongoDB database with a replica set as its data tier. The MongoDB database is installed on systems in the company\u2019s on-premises data center and is accessible through an AWS Direct Connect connection to the data center environment.A solutions architect must migrate the on-premises MongoDB database to Amazon DocumentDB (with MongoDB compatibility).Which strategy should the solutions architect choose to perform this migration?",
    "choices": [
      "A. Create a fleet of EC2 instances. Install MongoDB Community Edition on the EC2 instances, and create a database. Configure continuous synchronous replication with the database that is running in the on-premises data center.",
      "B. Create an AWS Database Migration Service (AWS DMS) replication instance. Create a source endpoint for the on-premises MongoDB database by using change data capture (CDC). Create a target endpoint for the Amazon DocumentDB database. Create and run a DMS migration task.",
      "C. Create a data migration pipeline by using AWS Data Pipeline. Define data nodes for the on-premises MongoDB database and the Amazon DocumentDB database. Create a scheduled task to run the data pipeline.",
      "D. Create a source endpoint for the on-premises MongoDB database by using AWS Glue crawlers. Configure continuous asynchronous replication between the MongoDB database and the Amazon DocumentDB database."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B, which recommends using AWS Database Migration Service (AWS DMS) to migrate the on-premises MongoDB database to Amazon DocumentDB.\n\nThis is the right strategy because AWS DMS is a managed service specifically designed for database migrations. It can create a source endpoint for the on-premises MongoDB database using change data capture (CDC), which allows it to capture changes made to the original database and replicate them to the target Amazon DocumentDB database in real-time. This approach ensures minimal downtime and data loss during the migration process.\n\n2. Explanations of why the other choices are incorrect:\n\nA. This choice involves installing MongoDB Community Edition on EC2 instances and configuring continuous synchronous replication with the on-premises database. However, this approach would not be a migration to Amazon DocumentDB, which is the goal. It would simply be replicating the on-premises MongoDB database to EC2 instances, which does not address the requirement to migrate to the Amazon DocumentDB service.\n\nC. This choice involves using AWS Data Pipeline to create a data migration pipeline between the on-premises MongoDB database and the Amazon DocumentDB database. While this could potentially work, AWS Data Pipeline is a more generic data orchestration service and may not be the most suitable option for a database migration scenario. AWS DMS is a more specialized and managed service for database migrations, making it a better choice.\n\nD. This choice involves using AWS Glue crawlers to create a source endpoint for the on-premises MongoDB database and then configuring continuous asynchronous replication to the Amazon DocumentDB database. While this approach could work, it does not utilize the change data capture (CDC) capabilities of AWS DMS, which are crucial for ensuring real-time, low-latency data replication during the migration process."
  },
  "236": {
    "question": "A company is designing an AWS Organizations structure. The company wants to standardize a process to apply tags across the entire organization. The company will require tags with specific values when a user creates a new resource. Each of the company's OUs will have unique tag values.Which solution will meet these requirements?",
    "choices": [
      "A. Use an SCP to deny the creation of resources that do not have the required tags. Create a tag policy that includes the tag values that the company has assigned to each OU. Attach the tag policies to the OUs.",
      "B. Use an SCP to deny the creation of resources that do not have the required tags. Create a tag policy that includes the tag values that the company has assigned to each OU. Attach the tag policies to the organization's management account.",
      "C. Use an SCP to allow the creation of resources only when the resources have the required tags. Create a tag policy that includes the tag values that the company has assigned to each OU. Attach the tag policies to the OUs.",
      "D. Use an SCP to deny the creation of resources that do not have the required tags. Define the list of tags. Attach the SCP to the OUs."
    ],
    "answer": "A",
    "explanation": "Explanation of the correct answer (A):\n\nThe correct answer is A because it aligns best with the requirements mentioned in the question:\n\n1. Standardize the tagging process across the entire organization:\n   - An SCP (Service Control Policy) can be used to deny the creation of resources that do not have the required tags, enforcing a standard tagging process.\n\n2. Require specific tag values when creating new resources:\n   - A tag policy can be created that includes the specific tag values assigned to each OU (Organizational Unit).\n   - By attaching the tag policy to the individual OUs, the tag values can be customized for each OU, meeting the requirement for unique tag values.\n\nThis solution allows for both organization-wide standardization of the tagging process and OU-specific customization of the tag values, which meets the requirements outlined in the question.\n\nExplanations of the incorrect choices:\n\nB. This option is incorrect because attaching the tag policy to the management account would not allow for OU-specific customization of the tag values. All OUs would have the same tag values.\n\nC. This option is incorrect because using an SCP to \"allow\" the creation of resources only with the required tags is not the correct approach. SCPs should be used to \"deny\" the creation of resources without the required tags, as described in the correct answer.\n\nD. This option is incorrect because it does not mention the use of a tag policy to define the specific tag values for each OU. Attaching the SCP to the OUs alone would not provide the OU-specific customization required by the question."
  },
  "237": {
    "question": "A company has more than 10,000 sensors that send data to an on-premises Apache Kafka server by using the Message Queuing Telemetry Transport (MQTT) protocol. The on-premises Kafka server transforms the data and then stores the results as objects in an Amazon S3 bucket.Recently, the Kafka server crashed. The company lost sensor data while the server was being restored. A solutions architect must create a new design on AWS that is highly available and scalable to prevent a similar occurrence.Which solution will meet these requirements?",
    "choices": [
      "A. Launch two Amazon EC2 instances to host the Kafka server in an active/standby configuration across two Availability Zones. Create a domain name in Amazon Route 53. Create a Route 53 failover policy. Route the sensors to send the data to the domain name.",
      "B. Migrate the on-premises Kafka server to Amazon Managed Streaming for Apache Kafka (Amazon MSK). Create a Network Load Balancer (NLB) that points to the Amazon MSK broker. Enable NLB health checks. Route the sensors to send the data to the NLB.",
      "C. Deploy AWS IoT Core, and connect it to an Amazon Kinesis Data Firehose delivery stream. Use an AWS Lambda function to handle data transformation. Route the sensors to send the data to AWS IoT Core.",
      "D. Deploy AWS IoT Core, and launch an Amazon EC2 instance to host the Kafka server. Configure AWS IoT Core to send the data to the EC2 instance. Route the sensors to send the data to AWS IoT Core."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\nThe correct answer is C because it aligns best with the requirements of the problem. The key aspects are:\n\n- It uses AWS IoT Core to receive the MQTT data from the 10,000+ sensors, which provides a highly available and scalable solution compared to the previous on-premises Kafka server.\n- It uses Amazon Kinesis Data Firehose to ingest the data from IoT Core, which is a managed service that can automatically scale to handle the high volume of data.\n- It uses an AWS Lambda function to handle the data transformation, which is a serverless compute service that can scale automatically without the need to manage infrastructure.\n- This design is highly available and scalable, and it prevents a similar occurrence of data loss in the event of a server crash, as the previous on-premises Kafka server experienced.\n\n2. Explanations of why the other choices are incorrect:\n\nA. This option uses an active/standby configuration of EC2 instances for the Kafka server, which still requires manual intervention and does not provide the same level of scalability and availability as a fully managed service like AWS IoT Core.\n\nB. This option uses Amazon MSK, which is a managed Kafka service, but it does not mention the data transformation capability provided by a Lambda function. Additionally, a single Network Load Balancer (NLB) may not be sufficient to handle the high volume of data from 10,000+ sensors, as it could become a bottleneck.\n\nD. This option uses AWS IoT Core to receive the data, but it then routes the data to an EC2 instance hosting the Kafka server, which is still a single point of failure and does not provide the same level of scalability and availability as the solution in option C."
  },
  "238": {
    "question": "A company recently started hosting new application workloads in the AWS Cloud. The company is using Amazon EC2 instances. Amazon Elastic File System (Amazon EFS) file systems, and Amazon RDS DB instances.To meet regulatory and business requirements, the company must make the following changes for data backups:\u2022\tBackups must be retained based on custom daily, weekly, and monthly requirements.\u2022\tBackups must be replicated to at least one other AWS Region immediately after capture.\u2022\tThe backup solution must provide a single source of backup status across the AWS environment.\u2022\tThe backup solution must send immediate notifications upon failure of any resource backup.Which combination of steps will meet these requirements with the LEAST amount of operational overhead? (Choose three.)",
    "choices": [
      "A. Create an AWS Backup plan with a backup rule for each of the retention requirements.",
      "B. Configure an AWS Backup plan to copy backups to another Region.",
      "C. Create an AWS Lambda function to replicate backups to another Region and send notification if a failure occurs.",
      "D. Add an Amazon Simple Notification Service (Amazon SNS) topic to the backup plan to send a notification for finished jobs that have any status except BACKUP_JOB_COMPLETED.",
      "E. Create an Amazon Data Lifecycle Manager (Amazon DLM) snapshot lifecycle policy for each of the retention requirements.",
      "F. Set up RDS snapshots on each database."
    ],
    "answer": "ABD",
    "explanation": "1. Clear explanation of why the correct answer (ABD) is right:\n\nA. Create an AWS Backup plan with a backup rule for each of the retention requirements.\nThis is correct because AWS Backup allows you to create backup plans with custom daily, weekly, and monthly backup rules to meet the specified retention requirements.\n\nB. Configure an AWS Backup plan to copy backups to another Region.\nThis is correct because AWS Backup supports cross-region backup, allowing you to automatically replicate backups to another AWS Region immediately after capture, as required.\n\nD. Add an Amazon Simple Notification Service (Amazon SNS) topic to the backup plan to send a notification for finished jobs that have any status except BACKUP_JOB_COMPLETED.\nThis is correct because AWS Backup allows you to integrate with Amazon SNS to send immediate notifications upon failure of any resource backup, meeting the requirement for notifications on backup failures.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nC. Create an AWS Lambda function to replicate backups to another Region and send notification if a failure occurs.\nThis is incorrect because AWS Backup already provides the functionality to automatically replicate backups to another Region and send notifications on failures, making a custom Lambda function unnecessary.\n\nE. Create an Amazon Data Lifecycle Manager (Amazon DLM) snapshot lifecycle policy for each of the retention requirements.\nThis is incorrect because Amazon Data Lifecycle Manager (DLM) is used for automating the creation, retention, and deletion of EBS snapshots and EBS-backed AMIs, not for backups of EC2 instances, EFS file systems, or RDS databases.\n\nF. Set up RDS snapshots on each database.\nThis is incorrect because it only addresses the backup requirement for the RDS databases, and does not cover the backup requirements for the EC2 instances and EFS file systems. Additionally, it does not provide the cross-region replication or centralized backup status and notification features required."
  },
  "239": {
    "question": "A company is developing a gene reporting device that will collect genomic information to assist researchers with collecting large samples of data from a diverse population. The device will push 8 KB of genomic data every second to a data platform that will need to process and analyze the data and provide information back to researchers. The data platform must meet the following requirements:\u2022\tProvide near-real-time analytics of the inbound genomic data\u2022\tEnsure the data is flexible, parallel, and durable\u2022\tDeliver results of processing to a data warehouseWhich strategy should a solutions architect use to meet these requirements?",
    "choices": [
      "A. Use Amazon Kinesis Data Firehose to collect the inbound sensor data, analyze the data with Kinesis clients, and save the results to an Amazon RDS instance.",
      "B. Use Amazon Kinesis Data Streams to collect the inbound sensor data, analyze the data with Kinesis clients, and save the results to an Amazon Redshift cluster using Amazon EMR.",
      "C. Use Amazon S3 to collect the inbound device data, analyze the data from Amazon SQS with Kinesis, and save the results to an Amazon Redshift cluster.",
      "D. Use an Amazon API Gateway to put requests into an Amazon SQS queue, analyze the data with an AWS Lambda function, and save the results to an Amazon Redshift cluster using Amazon EMR."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B. This option best meets the given requirements:\n\n- Kinesis Data Streams can collect the inbound genomic data in real-time and provide near-real-time analytics capabilities.\n- Kinesis Data Streams is a fully managed service that is flexible, parallel, and durable, allowing the data to be processed effectively.\n- The results of the data processing can be saved to an Amazon Redshift cluster using Amazon EMR, which satisfies the requirement of delivering the results to a data warehouse.\n\n2. Explanations of the incorrect choices:\n\nA. This option uses Kinesis Data Firehose instead of Kinesis Data Streams, which does not provide the same level of flexibility and parallelism required for the real-time processing of the genomic data. Additionally, it stores the results in an Amazon RDS instance, which is not a data warehouse solution.\n\nC. This option uses Amazon S3 and Amazon SQS, which is not the optimal solution for real-time data processing. The requirement states that the data platform must provide near-real-time analytics, which is better suited for a streaming service like Kinesis Data Streams.\n\nD. This option uses Amazon API Gateway and Amazon SQS, which is not the most efficient way to handle the real-time data processing requirements. The data should be ingested directly into a streaming service like Kinesis Data Streams, rather than going through an API Gateway and SQS queue."
  },
  "240": {
    "question": "A solutions architect needs to define a reference architecture for a solution for three-tier applications with web. application, and NoSQL data layers. The reference architecture must meet the following requirements:\u2022\tHigh availability within an AWS Region\u2022\tAble to fail over in 1 minute to another AWS Region for disaster recovery\u2022\tProvide the most efficient solution while minimizing the impact on the user experienceWhich combination of steps will meet these requirements? (Choose three.)",
    "choices": [
      "A. Use an Amazon Route 53 weighted routing policy set to 100/0 across the two selected Regions. Set Time to Live (TTL) to 1 hour.",
      "B. Use an Amazon Route 53 failover routing policy for failover from the primary Region to the disaster recovery Region. Set Time to Live (TTL) to 30 seconds.",
      "C. Use a global table within Amazon DynamoDB so data can be accessed in the two selected Regions.",
      "D. Back up data from an Amazon DynamoDB table in the primary Region every 60 minutes and then write the data to Amazon S3. Use S3 cross-Region replication to copy the data from the primary Region to the disaster recovery Region. Have a script import the data into DynamoDB in a disaster recovery scenario.",
      "E. Implement a hot standby model using Auto Scaling groups for the web and application layers across multiple Availability Zones in the Regions. Use zonal Reserved Instances for the minimum number of servers and On-Demand Instances for any additional resources.",
      "F. Use Auto Scaling groups for the web and application layers across multiple Availability Zones in the Regions. Use Spot Instances for the required resources."
    ],
    "answer": "BCE",
    "explanation": "Explanation:\n\n1. Correct Answer: BCE\n\nB. Use an Amazon Route 53 failover routing policy for failover from the primary Region to the disaster recovery Region. Set Time to Live (TTL) to 30 seconds.\nThis meets the requirement of being able to fail over in 1 minute to another AWS Region for disaster recovery. The Route 53 failover routing policy will automatically redirect traffic to the disaster recovery Region in the event of a failure in the primary Region.\n\nC. Use a global table within Amazon DynamoDB so data can be accessed in the two selected Regions.\nThis meets the requirement of high availability within an AWS Region. DynamoDB global tables provide automatic multi-master replication of data across Regions, ensuring low-latency access to data in multiple Regions.\n\nE. Implement a hot standby model using Auto Scaling groups for the web and application layers across multiple Availability Zones in the Regions. Use zonal Reserved Instances for the minimum number of servers and On-Demand Instances for any additional resources.\nThis meets the requirement of high availability within an AWS Region by using Auto Scaling groups across multiple Availability Zones. The use of Reserved Instances for the minimum resources and On-Demand Instances for additional resources provides the most efficient solution while minimizing the impact on the user experience.\n\n2. Incorrect Choices:\n\nA. Use an Amazon Route 53 weighted routing policy set to 100/0 across the two selected Regions. Set Time to Live (TTL) to 1 hour.\nThis does not meet the requirement of being able to fail over in 1 minute to another AWS Region for disaster recovery. The 1-hour TTL is too long for a failover scenario.\n\nD. Back up data from an Amazon DynamoDB table in the primary Region every 60 minutes and then write the data to Amazon S3. Use S3 cross-Region replication to copy the data from the primary Region to the disaster recovery Region. Have a script import the data into DynamoDB in a disaster recovery scenario.\nThis does not meet the requirement of being able to fail over in 1 minute to another AWS Region for disaster recovery. The process of importing data from S3 to DynamoDB in a disaster scenario would take longer than 1"
  },
  "241": {
    "question": "A company manufactures smart vehicles. The company uses a custom application to collect vehicle data. The vehicles use the MQTT protocol to connect to the application. The company processes the data in 5-minute intervals. The company then copies vehicle telematics data to on-premises storage. Custom applications analyze this data to detect anomalies.The number of vehicles that send data grows constantly. Newer vehicles generate high volumes of data. The on-premises storage solution is not able to scale for peak traffic, which results in data loss. The company must modernize the solution and migrate the solution to AWS to resolve the scaling challenges.Which solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      "A. Use AWS IoT Greengrass to send the vehicle data to Amazon Managed Streaming for Apache Kafka (Amazon MSK). Create an Apache Kafka application to store the data in Amazon S3. Use a pretrained model in Amazon SageMaker to detect anomalies.",
      "B. Use AWS IoT Core to receive the vehicle data. Configure rules to route data to an Amazon Kinesis Data Firehose delivery stream that stores the data in Amazon S3. Create an Amazon Kinesis Data Analytics application that reads from the delivery stream to detect anomalies.",
      "C. Use AWS IoT FleetWise to collect the vehicle data. Send the data to an Amazon Kinesis data stream. Use an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Use the built-in machine learning transforms in AWS Glue to detect anomalies.",
      "D. Use Amazon MQ for RabbitMQ to collect the vehicle data. Send the data to an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Use Amazon Lookout for Metrics to detect anomalies."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (Option B):\n\nOption B is the correct answer because it provides the least operational overhead solution to the given requirements.\n\nKey points:\n- AWS IoT Core can ingest the vehicle data directly from the MQTT protocol, without the need for additional edge components like AWS IoT Greengrass (used in Option A).\n- AWS IoT Core can be configured to route the data to an Amazon Kinesis Data Firehose delivery stream, which will then store the data in Amazon S3. This provides a scalable and managed data storage solution.\n- Amazon Kinesis Data Analytics can be used to build a streaming application that reads from the Kinesis Data Firehose delivery stream and performs anomaly detection using appropriate algorithms, such as Random Cut Forest.\n\nThis solution requires minimal operational overhead, as it leverages fully managed AWS services (IoT Core, Kinesis Data Firehose, Kinesis Data Analytics) to ingest, store, and analyze the vehicle data.\n\n2. Explanations of why the other options are incorrect:\n\nOption A:\n- Requires the use of AWS IoT Greengrass, which adds an additional edge component and increases operational overhead for the customer.\n- Using Apache Kafka (Amazon MSK) may be overkill for this use case and adds more complexity compared to the simpler Kinesis-based solution in Option B.\n\nOption C:\n- AWS IoT FleetWise requires additional setup and configuration compared to the more straightforward IoT Core solution in Option B.\n- Using AWS Glue for anomaly detection may be less efficient than the built-in machine learning capabilities in Kinesis Data Analytics.\n\nOption D:\n- Amazon MQ for RabbitMQ is a message broker service, which adds an additional layer of complexity compared to the direct integration with IoT Core in Option B.\n- Using Amazon Lookout for Metrics for anomaly detection may be overkill for this use case and adds more overhead compared to the Kinesis Data Analytics solution in Option B.\n\nIn summary, Option B provides the least operational overhead by leveraging fully managed AWS services for data ingestion, storage, and anomaly detection, making it the most suitable solution for the given requirements."
  },
  "242": {
    "question": "During an audit, a security team discovered that a development team was putting IAM user secret access keys in their code and then committing it to an AWS CodeCommit repository. The security team wants to automatically find and remediate instances of this security vulnerability.Which solution will ensure that the credentials are appropriately secured automatically?",
    "choices": [
      "A. Run a script nightly using AWS Systems Manager Run Command to search for credentials on the development instances. If found, use AWS Secrets Manager to rotate the credentials",
      "B. Use a scheduled AWS Lambda function to download and scan the application code from CodeCommit. If credentials are found, generate new credentials and store them in AWS KMS.",
      "C. Configure Amazon Macie to scan for credentials in CodeCommit repositories. If credentials are found, trigger an AWS Lambda function to disable the credentials and notify the user.",
      "D. Configure a CodeCommit trigger to invoke an AWS Lambda function to scan new code submissions for credentials. If credentials are found, disable them in AWS IAM and notify the user."
    ],
    "answer": "D",
    "explanation": "1. Explanation for the correct answer (D):\n\nThe correct answer is D, which involves configuring a CodeCommit trigger to invoke an AWS Lambda function to scan new code submissions for credentials. This is the most appropriate solution because it addresses the problem proactively, by scanning the code as soon as it is committed to the CodeCommit repository. This ensures that any sensitive credentials are detected and remediated immediately, before they can be used or exposed.\n\n2. Explanations for the incorrect choices:\n\nA. This solution is reactive, as it relies on a nightly script to search for credentials on the development instances. This approach is less effective because the credentials may have already been exposed or used before they are detected and rotated.\n\nB. Storing credentials in AWS KMS is not the correct approach, as KMS is used for encryption, not for securely storing secrets. Instead, AWS Secrets Manager should be used to store and manage sensitive credentials.\n\nC. Macie is a data security and data privacy service that primarily focuses on scanning Amazon S3 buckets for sensitive data. It does not have the capability to scan CodeCommit repositories for credentials, so this solution is not appropriate for the given scenario."
  },
  "243": {
    "question": "A company has a data lake in Amazon S3 that needs to be accessed by hundreds of applications across many AWS accounts. The company's information security policy states that the S3 bucket must not be accessed over the public internet and that each application should have the minimum permissions necessary to function.To meet these requirements, a solutions architect plans to use an S3 access point that is restricted to specific VPCs for each application.Which combination of steps should the solutions architect take to implement this solution? (Choose two.)",
    "choices": [
      "A. Create an S3 access point for each application in the AWS account that owns the S3 bucket. Configure each access point to be accessible only from the application\u2019s VPC. Update the bucket policy to require access from an access point.",
      "B. Create an interface endpoint for Amazon S3 in each application's VPC. Configure the endpoint policy to allow access to an S3 access point. Create a VPC gateway attachment for the S3 endpoint.",
      "C. Create a gateway endpoint for Amazon S3 in each application's VPConfigure the endpoint policy to allow access to an S3 access point. Specify the route table that is used to access the access point.",
      "D. Create an S3 access point for each application in each AWS account and attach the access points to the S3 bucket. Configure each access point to be accessible only from the application's VPC. Update the bucket policy to require access from an access point.",
      "E. Create a gateway endpoint for Amazon S3 in the data lake's VPC. Attach an endpoint policy to allow access to the S3 bucket. Specify the route table that is used to access the bucket."
    ],
    "answer": "AC",
    "explanation": "1. Clear explanation of why the correct answer is right:\n\nThe correct answer is A and C.\n\nA is correct because it aligns with the requirements outlined in the question. The solutions architect needs to create an S3 access point for each application in the AWS account that owns the S3 bucket, and configure each access point to be accessible only from the application's VPC. This ensures that the S3 bucket is not accessed over the public internet, and each application has the minimum permissions necessary to function.\n\nC is correct because it describes the necessary steps to create a gateway endpoint for Amazon S3 in each application's VPC, configure the endpoint policy to allow access to the S3 access point, and specify the route table that is used to access the access point. This ensures that the application can access the S3 bucket through the VPC, without exposing the bucket to the public internet.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nB is incorrect because it describes creating an interface endpoint for Amazon S3, which is not necessary in this scenario. The question states that the solution should use a gateway endpoint, not an interface endpoint.\n\nD is incorrect because it describes creating an S3 access point for each application in each AWS account, which is not necessary. The question states that the S3 bucket is owned by a single AWS account, and the solutions architect needs to create the access points in that account, not in each application's account.\n\nE is incorrect because it describes creating a gateway endpoint for Amazon S3 in the data lake's VPC, which is not necessary. The question states that the solutions architect needs to create the gateway endpoint in each application's VPC, not in the data lake's VPC."
  },
  "244": {
    "question": "A company has developed a hybrid solution between its data center and AWS. The company uses Amazon VPC and Amazon EC2 instances that send application logs to Amazon CloudWatch. The EC2 instances read data from multiple relational databases that are hosted on premises.The company wants to monitor which EC2 instances are connected to the databases in near-real time. The company already has a monitoring solution that uses Splunk on premises. A solutions architect needs to determine how to send networking traffic to Splunk.How should the solutions architect meet these requirements?",
    "choices": [
      "A. Enable VPC flows logs, and send them to CloudWatch. Create an AWS Lambda function to periodically export the CloudWatch logs to an Amazon S3 bucket by using the pre-defined export function. Generate ACCESS_KEY and SECRET_KEY AWS credentials. Configure Splunk to pull the logs from the S3 bucket by using those credentials.",
      "B. Create an Amazon Kinesis Data Firehose delivery stream with Splunk as the destination. Configure a pre-processing AWS Lambda function with a Kinesis Data Firehose stream processor that extracts individual log events from records sent by CloudWatch Logs subscription filters. Enable VPC flows logs, and send them to CloudWatch. Create a CloudWatch Logs subscription that sends log events to the Kinesis Data Firehose delivery stream.",
      "C. Ask the company to log every request that is made to the databases along with the EC2 instance IP address. Export the CloudWatch logs to an Amazon S3 bucket. Use Amazon Athena to query the logs grouped by database name. Export Athena results to another S3 bucket. Invoke an AWS Lambda function to automatically send any new file that is put in the S3 bucket to Splunk.",
      "D. Send the CloudWatch logs to an Amazon Kinesis data stream with Amazon Kinesis Data Analytics for SQL Applications. Configure a 1-minute sliding window to collect the events. Create a SQL query that uses the anomaly detection template to monitor any networking traffic anomalies in near-real time. Send the result to an Amazon Kinesis Data Firehose delivery stream with Splunk as the destination."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B because it provides the most appropriate solution to meet the near-real-time monitoring requirement using Amazon Kinesis Data Firehose.\n\nThe key aspects of the solution:\n\n- Enable VPC flow logs and send them to Amazon CloudWatch. This captures the network traffic information that needs to be monitored.\n- Create a Kinesis Data Firehose delivery stream with Splunk as the destination. This allows the networking traffic data to be forwarded to Splunk in near-real-time.\n- Configure a pre-processing AWS Lambda function with a Kinesis Data Firehose stream processor to extract individual log events from the records sent by the CloudWatch Logs subscription filters. This ensures the data is properly formatted for ingestion by Splunk.\n- Create a CloudWatch Logs subscription that sends the log events directly to the Kinesis Data Firehose delivery stream, enabling the near-real-time data transfer.\n\nThis solution meets the requirement of monitoring the networking traffic between the EC2 instances and the on-premises databases in near-real-time by leveraging the combination of VPC flow logs, CloudWatch, Kinesis Data Firehose, and the integration with Splunk.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This solution is not optimal for near-real-time monitoring, as it involves exporting CloudWatch logs to an S3 bucket and then configuring Splunk to pull the logs from the bucket. This introduces a delay that does not meet the \"near-real-time\" requirement.\n\nC. This solution is overly complex and does not leverage the available VPC flow logs. Querying CloudWatch logs using Athena and then sending the results to Splunk through an S3 bucket and Lambda function is not an efficient or near-real-time approach.\n\nD. While this solution uses Kinesis Data Analytics and Kinesis Data Firehose to enable near-real-time monitoring, it focuses on detecting anomalies in the networking traffic, which is not the primary requirement stated in the question. The requirement is to monitor which EC2 instances are connected to the databases, not necessarily detect anomalies."
  },
  "245": {
    "question": "A company has five development teams that have each created five AWS accounts to develop and host applications. To track spending, the development teams log in to each account every month, record the current cost from the AWS Billing and Cost Management console, and provide the information to the company's finance team.The company has strict compliance requirements and needs to ensure that resources are created only in AWS Regions in the United States. However, some resources have been created in other Regions.A solutions architect needs to implement a solution that gives the finance team the ability to track and consolidate expenditures for all the accounts. The solution also must ensure that the company can create resources only in Regions in the United States.Which combination of steps will meet these requirements in the MOST operationally efficient way? (Choose three.)",
    "choices": [
      "A. Create a new account to serve as a management account. Create an Amazon S3 bucket for the finance team. Use AWS Cost and Usage Reports to create monthly reports and to store the data in the finance team's S3 bucket.",
      "B. Create a new account to serve as a management account. Deploy an organization in AWS Organizations with all features enabled. Invite all the existing accounts to the organization. Ensure that each account accepts the invitation.",
      "C. Create an OU that includes all the development teams. Create an SCP that allows the creation of resources only in Regions that are in the United States. Apply the SCP to the OU.",
      "D. Create an OU that includes all the development teams. Create an SCP that denies the creation of resources in Regions that are outside the United States. Apply the SCP to the OU.",
      "E. Create an IAM role in the management account. Attach a policy that includes permissions to view the Billing and Cost Management console. Allow the finance team users to assume the role. Use AWS Cost Explorer and the Billing and Cost Management console to analyze cost.",
      "F. Create an IAM role in each AWS account. Attach a policy that includes permissions to view the Billing and Cost Management console. Allow the finance team users to assume the role."
    ],
    "answer": "BDE",
    "explanation": "1. Explanation of the correct answer (BDE):\n\nB - Creating a new account to serve as a management account and deploying an organization in AWS Organizations with all features enabled is the right approach. This allows the company to centrally manage all the existing accounts under the organization.\n\nD - Creating an OU that includes all the development teams and an SCP that denies the creation of resources in Regions outside the United States is the right way to ensure compliance with the company's requirement to create resources only in US Regions.\n\nE - Creating an IAM role in the management account and attaching a policy that includes permissions to view the Billing and Cost Management console is the right way to allow the finance team to analyze costs across all the accounts. Allowing the finance team users to assume the role provides them the necessary access.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA - Creating a new account to serve as a management account and using AWS Cost and Usage Reports to create monthly reports is not the most operationally efficient solution. AWS Organizations provides a more centralized way to manage accounts and policies.\n\nC - Creating an OU that includes all the development teams and an SCP that allows the creation of resources only in Regions that are in the United States is not the right approach. SCPs should be written in a deny-based manner to ensure compliance.\n\nF - Creating an IAM role in each AWS account and attaching a policy that includes permissions to view the Billing and Cost Management console is not the most operationally efficient solution. It would require managing the roles in multiple accounts, instead of centralizing the management in the management account."
  },
  "246": {
    "question": "A company needs to create and manage multiple AWS accounts for a number of departments from a central location. The security team requires read-only access to all accounts from its own AWS account. The company is using AWS Organizations and created an account for the security team.How should a solutions architect meet these requirements?",
    "choices": [
      "A. Use the OrganizationAccountAccessRole IAM role to create a new IAM policy with read-only access in each member account. Establish a trust relationship between the IAM policy in each member account and the security account. Ask the security team to use the IAM policy to gain access.",
      "B. Use the OrganizationAccountAccessRole IAM role to create a new IAM role with read-only access in each member account. Establish a trust relationship between the IAM role in each member account and the security account. Ask the security team to use the IAM role to gain access.",
      "C. Ask the security team to use AWS Security Token Service (AWS STS) to call the AssumeRole API for the OrganizationAccountAccessRole IAM role in the management account from the security account. Use the generated temporary credentials to gain access.",
      "D. Ask the security team to use AWS Security Token Service (AWS STS) to call the AssumeRole API for the OrganizationAccountAccessRole IAM role in the member account from the security account. Use the generated temporary credentials to gain access."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B because it provides the most appropriate solution to the given requirements.\n\nThe key aspects are:\n\n- Use the OrganizationAccountAccessRole IAM role in the member accounts.\n- Create a new IAM role with read-only access in each member account.\n- Establish a trust relationship between the new IAM role in each member account and the security account.\n- The security team can then use the IAM role to gain read-only access to the member accounts.\n\nThis approach meets the requirements by:\n- Leveraging the existing OrganizationAccountAccessRole to simplify setup.\n- Creating a dedicated read-only role in each member account, which provides the required level of access.\n- Establishing a trust relationship between the read-only role and the security account, which allows the security team to assume the role and gain the necessary access.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option suggests creating an IAM policy with read-only access, rather than an IAM role. However, you cannot establish a trust relationship between an IAM policy and an AWS account, as required by the question.\n\nC. This option suggests using the OrganizationAccountAccessRole in the management account, which has administrator-level access. This would provide the security team with more privileges than necessary, violating the requirement for read-only access.\n\nD. This option suggests using the OrganizationAccountAccessRole in the member accounts, which also has administrator-level access. Similar to option C, this would provide the security team with more privileges than necessary.\n\nIn summary, the correct answer (B) is the most appropriate solution as it leverages the existing OrganizationAccountAccessRole to create a dedicated read-only role in each member account, and then establishes a trust relationship between the read-only role and the security account, allowing the security team to assume the role and gain the required read-only access."
  },
  "247": {
    "question": "A large company runs workloads in VPCs that are deployed across hundreds of AWS accounts. Each VPC consists of public subnets and private subnets that span across multiple Availability Zones. NAT gateways are deployed in the public subnets and allow outbound connectivity to the internet from the private subnets.A solutions architect is working on a hub-and-spoke design. All private subnets in the spoke VPCs must route traffic to the internet through an egress VPC. The solutions architect already has deployed a NAT gateway in an egress VPC in a central AWS account.Which set of additional steps should the solutions architect take to meet these requirements?",
    "choices": [
      "A. Create peering connections between the egress VPC and the spoke VPCs. Configure the required routing to allow access to the internet.",
      "B. Create a transit gateway, and share it with the existing AWS accounts. Attach existing VPCs to the transit gateway. Configure the required routing to allow access to the internet.",
      "C. Create a transit gateway in every account. Attach the NAT gateway to the transit gateways. Configure the required routing to allow access to the internet.",
      "D. Create an AWS PrivateLink connection between the egress VPC and the spoke VPCs. Configure the required routing to allow access to the internet."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B. Create a transit gateway, and share it with the existing AWS accounts. Attach existing VPCs to the transit gateway. Configure the required routing to allow access to the internet.\n\nThe key reasons why this is the correct approach:\n\n- The question mentions that there are hundreds of VPCs deployed across multiple AWS accounts. Managing VPC peering connections between all these VPCs would be complex and difficult to scale.\n- A transit gateway provides a centralized hub to connect all the spoke VPCs, simplifying the network architecture and making it easier to manage.\n- By sharing the transit gateway across the existing AWS accounts using Resource Access Manager (RAM), the solutions architect can easily attach the spoke VPCs to the transit gateway without having to create and manage individual VPC peering connections.\n- The transit gateway can be configured to route traffic from the private subnets of the spoke VPCs to the egress VPC, allowing access to the internet through the centralized NAT gateway.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Create peering connections between the egress VPC and the spoke VPCs. Configure the required routing to allow access to the internet.\n- This approach would not scale well for hundreds of VPCs across multiple accounts, as it would require creating and managing numerous VPC peering connections.\n- VPC peering has limitations, such as the need for non-overlapping CIDR blocks, which could be challenging to manage at scale.\n\nC. Create a transit gateway in every account. Attach the NAT gateway to the transit gateways. Configure the required routing to allow access to the internet.\n- Creating a transit gateway in every account would be an inefficient and unnecessary duplication of resources.\n- A single transit gateway shared across the accounts, as mentioned in the correct answer, is a more scalable and cost-effective solution.\n\nD. Create an AWS PrivateLink connection between the egress VPC and the spoke VPCs. Configure the required routing to allow access to the internet.\n- PrivateLink is designed for private connectivity between VPCs and AWS services, not for general internet access.\n- The requirement is to route traffic from the private subnets of the spoke VPCs"
  },
  "248": {
    "question": "An education company is running a web application used by college students around the world. The application runs in an Amazon Elastic Container Service (Amazon ECS) cluster in an Auto Scaling group behind an Application Load Balancer (ALB). A system administrator detects a weekly spike in the number of failed login attempts, which overwhelm the application's authentication service. All the failed login attempts originate from about 500 different IP addresses that change each week. A solutions architect must prevent the failed login attempts from overwhelming the authentication service.Which solution meets these requirements with the MOST operational efficiency?",
    "choices": [
      "A. Use AWS Firewall Manager to create a security group and security group policy to deny access from the IP addresses.",
      "B. Create an AWS WAF web ACL with a rate-based rule, and set the rule action to Block. Connect the web ACL to the ALB.",
      "C. Use AWS Firewall Manager to create a security group and security group policy to allow access only to specific CIDR ranges.",
      "D. Create an AWS WAF web ACL with an IP set match rule, and set the rule action to Block. Connect the web ACL to the ALB."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\nThe correct answer is B. Create an AWS WAF web ACL with a rate-based rule, and set the rule action to Block. Connect the web ACL to the ALB.\n\nThis solution is the most operationally efficient because:\n- AWS WAF rate-based rules allow you to monitor the rate of requests from different IP addresses and block excessive requests, which is exactly what is needed to mitigate the weekly spike in failed login attempts.\n- Configuring the rate-based rule to block the requests at the ALB level means the traffic will not reach the application's authentication service, reducing the load on the service.\n- The rate-based rule can automatically adjust to changing patterns of attack (the IP addresses changing weekly) without the need for manual updates, making it a more efficient solution.\n- Integrating the WAF web ACL with the ALB provides a centralized and effective way to protect the application from the failed login attempt attacks.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Use AWS Firewall Manager to create a security group and security group policy to deny access from the IP addresses.\n- This solution is not as operationally efficient as the correct answer because it requires manually updating the security group policy with the changing IP addresses each week.\n- Security groups are more suitable for controlling access at the instance/container level, whereas the failed login attempts need to be addressed at the application load balancer level.\n\nC. Use AWS Firewall Manager to create a security group and security group policy to allow access only to specific CIDR ranges.\n- Similar to option A, this solution is not as operationally efficient because it requires manually updating the security group policy with the CIDR ranges, which may change weekly.\n- Allowing access only to specific CIDR ranges may be too restrictive and could potentially block legitimate user access.\n\nD. Create an AWS WAF web ACL with an IP set match rule, and set the rule action to Block. Connect the web ACL to the ALB.\n- This solution is less efficient than the correct answer because the IP set match rule requires manually updating the IP set with the changing IP addresses each week.\n- The rate-based rule in the correct answer is more flexible and can automatically adjust to the changing"
  },
  "249": {
    "question": "A company operates an on-premises software-as-a-service (SaaS) solution that ingests several files daily. The company provides multiple public SFTP endpoints to its customers to facilitate the file transfers. The customers add the SFTP endpoint IP addresses to their firewall allow list for outbound traffic. Changes to the SFTP endpoint IP addresses are not permitted.The company wants to migrate the SaaS solution to AWS and decrease the operational overhead of the file transfer service.Which solution meets these requirements?",
    "choices": [
      "A. Register the customer-owned block of IP addresses in the company's AWS account. Create Elastic IP addresses from the address pool and assign them to an AWS Transfer for SFTP endpoint. Use AWS Transfer to store the files in Amazon S3.",
      "B. Add a subnet containing the customer-owned block of IP addresses to a VPC. Create Elastic IP addresses from the address pool and assign them to an Application Load Balancer (ALB). Launch EC2 instances hosting FTP services in an Auto Scaling group behind the ALStore the files in attached Amazon Elastic Block Store (Amazon EBS) volumes.",
      "C. Register the customer-owned block of IP addresses with Amazon Route 53. Create alias records in Route 53 that point to a Network Load Balancer (NLB). Launch EC2 instances hosting FTP services in an Auto Scaling group behind the NLB. Store the files in Amazon S3.",
      "D. Register the customer-owned block of IP addresses in the company\u2019s AWS account. Create Elastic IP addresses from the address pool and assign them to an Amazon S3 VPC endpoint. Enable SFTP support on the S3 bucket."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\nThe correct answer is A because it meets all the requirements stated in the question:\n- It allows the company to register the customer-owned block of IP addresses in their AWS account, which enables the use of these existing IP addresses within AWS. This ensures that customers don't need to update their firewall allow lists.\n- It creates Elastic IP addresses from the address pool and assigns them to an AWS Transfer for SFTP endpoint. This allows the company to maintain the existing IP addresses for customers while using a managed SFTP service.\n- AWS Transfer for SFTP is used to store the files in Amazon S3, which decreases the operational overhead of the file transfer service by removing the need to manage file storage infrastructure.\n\n2. Explanations of why the incorrect choices are wrong:\nB. This option uses EC2 instances hosting FTP services behind an Application Load Balancer (ALB), which does not meet the requirement of using a managed SFTP service.\nC. This option uses EC2 instances hosting FTP services behind a Network Load Balancer (NLB), which also does not meet the requirement of using a managed SFTP service.\nD. This option uses an Amazon S3 VPC endpoint with SFTP support, but it does not maintain the existing customer-owned IP addresses, as required in the question."
  },
  "250": {
    "question": "A company has a new application that needs to run on five Amazon EC2 instances in a single AWS Region. The application requires high-throughput, low-latency network connections between all of the EC2 instances where the application will run. There is no requirement for the application to be fault tolerant.Which solution will meet these requirements?",
    "choices": [
      "A. Launch five new EC2 instances into a cluster placement group. Ensure that the EC2 instance type supports enhanced networking.",
      "B. Launch five new EC2 instances into an Auto Scaling group in the same Availability Zone. Attach an extra elastic network interface to each EC2 instance.",
      "C. Launch five new EC2 instances into a partition placement group. Ensure that the EC2 instance type supports enhanced networking.",
      "D. Launch five new EC2 instances into a spread placement group. Attach an extra elastic network interface to each EC2 instance."
    ],
    "answer": "A",
    "explanation": "1. Clear explanation of why the correct answer (A) is right:\n\nThe correct answer is A. Launch five new EC2 instances into a cluster placement group. Ensure that the EC2 instance type supports enhanced networking.\n\nA cluster placement group is the best solution for this scenario because it places the EC2 instances in the same Availability Zone, which provides high-throughput, low-latency network connections between all the instances. Cluster placement groups are specifically designed to deliver the lowest possible latency and the highest possible network throughput, which meets the requirement for high-throughput, low-latency network connections.\n\nAdditionally, ensuring that the EC2 instance type supports enhanced networking further improves the network performance, as enhanced networking enables features like single root I/O virtualization (SR-IOV) and Elastic Fabric Adapter (EFA), which can significantly boost network performance.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nB. Launch five new EC2 instances into an Auto Scaling group in the same Availability Zone. Attach an extra elastic network interface to each EC2 instance.\nThis option is incorrect because attaching an extra elastic network interface to each instance does not necessarily provide the high-throughput, low-latency network connections required. Auto Scaling groups are designed for scaling, not specifically for optimizing network performance between instances.\n\nC. Launch five new EC2 instances into a partition placement group. Ensure that the EC2 instance type supports enhanced networking.\nThis option is incorrect because a partition placement group is designed to provide fault tolerance, not necessarily high-throughput, low-latency network connections. Partition placement groups spread instances across different partitions, which can reduce network performance between instances.\n\nD. Launch five new EC2 instances into a spread placement group. Attach an extra elastic network interface to each EC2 instance.\nThis option is incorrect because a spread placement group is designed to spread instances across different underlying hardware to provide fault tolerance, not to optimize network performance. Attaching an extra elastic network interface does not address the need for high-throughput, low-latency network connections between the instances."
  },
  "251": {
    "question": "A company is creating a REST API to share information with six of its partners based in the United States. The company has created an Amazon API Gateway Regional endpoint. Each of the six partners will access the API once per day to post daily sales figures.After initial deployment, the company observes 1,000 requests per second originating from 500 different IP addresses around the world. The company believes this traffic is originating from a botnet and wants to secure its API while minimizing cost.Which approach should the company take to secure its API?",
    "choices": [
      "A. Create an Amazon CloudFront distribution with the API as the origin. Create an AWS WAF web ACL with a rule to block clients that submit more than five requests per day. Associate the web ACL with the CloudFront distribution. Configure CloudFront with an origin access identity (OAI) and associate it with the distribution. Configure API Gateway to ensure only the OAI can run the POST method.",
      "B. Create an Amazon CloudFront distribution with the API as the origin. Create an AWS WAF web ACL with a rule to block clients that submit more than five requests per day. Associate the web ACL with the CloudFront distribution. Add a custom header to the CloudFront distribution populated with an API key. Configure the API to require an API key on the POST method.",
      "C. Create an AWS WAF web ACL with a rule to allow access to the IP addresses used by the six partners. Associate the web ACL with the API. Create a resource policy with a request limit and associate it with the API. Configure the API to require an API key on the POST method.",
      "D. Create an AWS WAF web ACL with a rule to allow access to the IP addresses used by the six partners. Associate the web ACL with the API. Create a usage plan with a request limit and associate it with the API. Create an API key and add it to the usage plan."
    ],
    "answer": "D",
    "explanation": "Explanation of the correct answer (D):\n\nThe correct answer is D because it provides the most comprehensive and effective approach to securing the API while minimizing cost.\n\n1. Create an AWS WAF web ACL with a rule to allow access to the IP addresses used by the six partners. This ensures that only the authorized partners can access the API, filtering out the unwanted traffic from the suspected botnet.\n\n2. Associate the WAF web ACL with the API. This applies the IP-based access control to the API, preventing unauthorized access.\n\n3. Create a usage plan with a request limit and associate it with the API. The usage plan allows you to set a rate limit and a quota on the number of requests that can be made to the API, helping to mitigate the impact of potential abuse or overuse.\n\n4. Create an API key and add it to the usage plan. The API key provides an additional layer of security, ensuring that only authorized partners with the valid API key can access the API. This helps protect the API from unauthorized usage.\n\nExplanations of why the incorrect choices are wrong:\n\nA. This approach is unnecessary and more expensive. Using CloudFront and a more complex WAF configuration with an origin access identity (OAI) is not required for this use case, as the API Gateway regional endpoint should be sufficient. Additionally, the request limit of five requests per day is too low for the intended use of the API.\n\nB. This approach is also unnecessary and more expensive. Using CloudFront is not required, and the API key alone does not provide the needed request throttling and quota management features.\n\nC. This approach is not effective. While the WAF web ACL with IP-based access control is a good start, the resource policy in API Gateway does not support request limiting. The API key requirement alone is also not sufficient to manage the usage and prevent potential abuse."
  },
  "252": {
    "question": "A company uses an Amazon Aurora PostgreSQL DB cluster for applications in a single AWS Region. The company's database team must monitor all data activity on all the databases.Which solution will achieve this goal?",
    "choices": [
      "A. Set up an AWS Database Migration Service (AWS DMS) change data capture (CDC) task. Specify the Aurora DB cluster as the source. Specify Amazon Kinesis Data Firehose as the target. Use Kinesis Data Firehose to upload the data into an Amazon OpenSearch Service cluster for further analysis.",
      "B. Start a database activity stream on the Aurora DB cluster to capture the activity stream in Amazon EventBridge. Define an AWS Lambda function as a target for EventBridge. Program the Lambda function to decrypt the messages from EventBridge and to publish all database activity to Amazon S3 for further analysis.",
      "C. Start a database activity stream on the Aurora DB cluster to push the activity stream to an Amazon Kinesis data stream. Configure Amazon Kinesis Data Firehose to consume the Kinesis data stream and to deliver the data to Amazon S3 for further analysis.",
      "D. Set up an AWS Database Migration Service (AWS DMS) change data capture (CDC) task. Specify the Aurora DB cluster as the source. Specify Amazon Kinesis Data Firehose as the target. Use Kinesis Data Firehose to upload the data into an Amazon Redshift cluster. Run queries on the Amazon Redshift data to determine database activities on the Aurora database."
    ],
    "answer": "C",
    "explanation": "1. Clear explanation of why the correct answer (C) is right:\n\nThe correct answer is C because it leverages the native database activity stream feature of Amazon Aurora PostgreSQL to capture and stream the database activity data in real-time. Specifically:\n\n- The database activity stream is configured on the Aurora DB cluster to push the activity data to an Amazon Kinesis data stream.\n- Amazon Kinesis Data Firehose is then used to consume the Kinesis data stream and deliver the data to Amazon S3 for further analysis.\n\nThis approach meets the stated goal of the database team to monitor all data activity on all the databases, as the activity stream captures the relevant information, and the Kinesis/Firehose pipeline ensures the data is reliably delivered to S3 for the team to analyze.\n\n2. Brief explanations of why the incorrect choices are wrong:\n\nA. This solution uses AWS DMS and Kinesis Data Firehose to capture changes, but it does not leverage the native database activity stream feature of Aurora, which is a more direct and efficient way to monitor all database activity.\n\nB. This solution uses EventBridge instead of the native Kinesis-based activity stream, which is not the recommended approach according to the AWS documentation.\n\nD. This solution also uses AWS DMS and Kinesis Data Firehose, similar to A, but it sends the data to Amazon Redshift instead of S3, which is not the requirement stated in the question."
  },
  "253": {
    "question": "A company is running an application on Amazon EC2 instances in the AWS Cloud. The application is using a MongoDB database with a replica set as its data tier. The MongoDB database is installed on systems in the company\u2019s on-premises data center and is accessible through an AWS Direct Connect connection to the data center environment.A solutions architect must migrate the on-premises MongoDB database to Amazon DocumentDB (with MongoDB compatibility).Which strategy should the solutions architect choose to perform this migration?",
    "choices": [
      "A. Create a fleet of EC2 instances. Install MongoDB Community Edition on the EC2 instances, and create a database. Configure continuous synchronous replication with the database that is running in the on-premises data center.",
      "B. Create an AWS Database Migration Service (AWS DMS) replication instance. Create a source endpoint for the on-premises MongoDB database by using change data capture (CDC). Create a target endpoint for the Amazon DocumentDB database. Create and run a DMS migration task.",
      "C. Create a data migration pipeline by using AWS Data Pipeline. Define data nodes for the on-premises MongoDB database and the Amazon DocumentDB database. Create a scheduled task to run the data pipeline.",
      "D. Create a source endpoint for the on-premises MongoDB database by using AWS Glue crawlers. Configure continuous asynchronous replication between the MongoDB database and the Amazon DocumentDB database."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B, which suggests using AWS Database Migration Service (AWS DMS) to migrate the on-premises MongoDB database to Amazon DocumentDB.\n\nAWS DMS is a managed service that can perform database migrations from various database engines to Amazon DocumentDB, which is a MongoDB-compatible database service on AWS. By using AWS DMS, the solutions architect can create a source endpoint for the on-premises MongoDB database and configure change data capture (CDC) to capture and replicate the changes in real-time to the target Amazon DocumentDB database. This approach ensures minimal downtime and data loss during the migration process, which is the key requirement in this scenario.\n\n2. Explanations of the incorrect choices:\n\nA. This option suggests installing MongoDB Community Edition on EC2 instances and configuring continuous synchronous replication with the on-premises database. However, this approach would not be suitable for the given task, as the goal is to migrate the database to Amazon DocumentDB, which is a managed service and not a self-hosted MongoDB deployment.\n\nC. Using AWS Data Pipeline to create a data migration pipeline is not the recommended approach in this case, as Data Pipeline is designed for batch data processing, not for real-time database migrations. The requirement here is to perform a seamless, low-downtime migration, which is better suited for a dedicated database migration service like AWS DMS.\n\nD. While creating a source endpoint for the on-premises MongoDB database using AWS Glue crawlers and configuring continuous asynchronous replication is a valid approach, it may not be the most efficient strategy for a real-time database migration. Asynchronous replication may lead to potential data inconsistencies or delays, which is not desirable for a mission-critical production database migration."
  },
  "254": {
    "question": "A financial services company loaded millions of historical stock trades into an Amazon DynamoDB table. The table uses on-demand capacity mode. Once each day at midnight, a few million new records are loaded into the table. Application read activity against the table happens in bursts throughout the day. and a limited set of keys are repeatedly looked up. The company needs to reduce costs associated with DynamoDB.Which strategy should a solutions architect recommend to meet this requirement?",
    "choices": [
      "A. Deploy an Amazon ElastiCache cluster in front of the DynamoDB table",
      "B. Deploy DynamoDB Accelerator (DAX). Configure DynamoDB auto scaling. Purchase Savings Plans in Cost Explorer.",
      "C. Use provisioned capacity mode. Purchase Savings Plans in Cost Explorer.",
      "D. Deploy DynamoDB Accelerator (DAX). Use provisioned capacity mode. Configure DynamoDB auto scaling."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (D):\n\nThe correct answer is D. Deploy DynamoDB Accelerator (DAX), use provisioned capacity mode, and configure DynamoDB auto scaling.\n\nThis recommendation addresses the key requirements in the question:\n\n- Repeated lookups: DynamoDB Accelerator (DAX) is designed to provide in-memory caching, which can significantly improve the performance and reduce the cost of frequently accessed data, making it the ideal choice for the limited set of keys that are repeatedly looked up.\n\n- Bursts in application read activity: Using provisioned capacity mode, instead of on-demand capacity mode, allows the company to provision the required read and write capacity units in advance, smoothing out the bursts in read activity and providing more predictable pricing.\n\n- Reducing costs: Configuring DynamoDB auto scaling can automatically adjust the provisioned capacity based on the actual usage, ensuring that the company only pays for the resources they need, thereby reducing costs.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Deploy an Amazon ElastiCache cluster in front of the DynamoDB table:\nThis option may work, but it is not the most efficient or cost-effective solution. ElastiCache is a general-purpose caching solution, whereas DAX is specifically designed for DynamoDB, providing better integration and more efficient caching for DynamoDB workloads.\n\nB. Deploy DynamoDB Accelerator (DAX), configure DynamoDB auto scaling, and purchase Savings Plans in Cost Explorer:\nWhile the first two recommendations (DAX and DynamoDB auto scaling) are correct, the third recommendation, purchasing Savings Plans in Cost Explorer, is not relevant for DynamoDB. Savings Plans are primarily used for EC2 and Fargate, not for DynamoDB.\n\nC. Use provisioned capacity mode and purchase Savings Plans in Cost Explorer:\nThis option is partially correct, as using provisioned capacity mode can help address the bursts in application read activity. However, it does not address the repeated lookups issue, which is better solved by deploying DAX. Additionally, the recommendation to purchase Savings Plans in Cost Explorer is not relevant for DynamoDB."
  },
  "255": {
    "question": "A company is creating a centralized logging service running on Amazon EC2 that will receive and analyze logs from hundreds of AWS accounts. AWS PrivateLink is being used to provide connectivity between the client services and the logging service.In each AWS account with a client, an interface endpoint has been created for the logging service and is available. The logging service running on EC2 instances with a Network Load Balancer (NLB) are deployed in different subnets. The clients are unable to submit logs using the VPC endpoint.Which combination of steps should a solutions architect take to resolve this issue? (Choose two.)",
    "choices": [
      "A. Check that the NACL is attached to the logging service subnet to allow communications to and from the NLB subnets. Check that the NACL is attached to the NLB subnet to allow communications to and from the logging service subnets running on EC2 instances.",
      "B. Check that the NACL is attached to the logging service subnets to allow communications to and from the interface endpoint subnets. Check that the NACL is attached to the interface endpoint subnet to allow communications to and from the logging service subnets running on EC2 instances.",
      "C. Check the security group for the logging service running on the EC2 instances to ensure it allows ingress from the NLB subnets.",
      "D. Check the security group for the logging service running on EC2 instances to ensure it allows ingress from the clients.",
      "E. Check the security group for the NLB to ensure it allows ingress from the interface endpoint subnets."
    ],
    "answer": "AC",
    "explanation": "1. Clear explanation of why the correct answer (A and C) is right:\n\nThe correct answer is A and C because:\n\nA. Checking the Network ACL (NACL) settings is crucial to ensure the proper communication between the client services and the logging service. The NACL should be configured to allow communication to and from the NLB subnets and the logging service subnets running on EC2 instances.\n\nC. Checking the security group of the logging service running on EC2 instances is necessary to ensure it allows ingress from the NLB subnets. This is because the NLB forwards the client requests to the logging service, and the EC2 instances must be able to receive the traffic from the NLB.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nB. Checking the NACL settings for the interface endpoint subnets and the logging service subnets is not the primary issue here. The main problem lies in the communication between the NLB and the logging service EC2 instances, not the interface endpoints.\n\nD. Checking the security group of the logging service to allow ingress from the clients is not necessary because the clients are connecting to the logging service through the VPC endpoint, which is handled by the NLB.\n\nE. Checking the security group of the NLB to ensure it allows ingress from the interface endpoint subnets is not the correct solution. The NLB should be configured to allow ingress from the client's interface endpoint subnets, not the other way around."
  },
  "256": {
    "question": "A company has millions of objects in an Amazon S3 bucket. The objects are in the S3 Standard storage class. All the S3 objects are accessed frequently. The number of users and applications that access the objects is increasing rapidly. The objects are encrypted with server-side encryption with AWS KMS keys (SSE-KMS).A solutions architect reviews the company\u2019s monthly AWS invoice and notices that AWS KMS costs are increasing because of the high number of requests from Amazon S3. The solutions architect needs to optimize costs with minimal changes to the application.Which solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      "A. Create a new S3 bucket that has server-side encryption with customer-provided keys (SSE-C) as the encryption type. Copy the existing objects to the new S3 bucket. Specify SSE-C.",
      "B. Create a new S3 bucket that has server-side encryption with Amazon S3 managed keys (SSE-S3) as the encryption type. Use S3 Batch Operations to copy the existing objects to the new S3 bucket. Specify SSE-S3.",
      "C. Use AWS CloudHSM to store the encryption keys. Create a new S3 bucket. Use S3 Batch Operations to copy the existing objects to the new S3 bucket. Encrypt the objects by using the keys from CloudHSM.",
      "D. Use the S3 Intelligent-Tiering storage class for the S3 bucket. Create an S3 Intelligent-Tiering archive configuration to transition objects that are not accessed for 90 days to S3 Glacier Deep Archive."
    ],
    "answer": "B",
    "explanation": "Explanation:\n\n1. Correct Answer: B\n\nThe correct answer is B, \"Create a new S3 bucket that has server-side encryption with Amazon S3 managed keys (SSE-S3) as the encryption type. Use S3 Batch Operations to copy the existing objects to the new S3 bucket. Specify SSE-S3.\"\n\nThis solution is the most optimal because it addresses the issue of increasing AWS KMS costs while minimizing operational overhead. By switching to SSE-S3 encryption, which uses Amazon S3-managed keys, the application will no longer incur charges for AWS KMS API requests. Additionally, using S3 Batch Operations to copy the existing objects to the new S3 bucket with the new encryption type is a relatively simple and automated process, reducing the operational overhead.\n\n2. Explanations of why each incorrect choice is wrong:\n\nA. \"Create a new S3 bucket that has server-side encryption with customer-provided keys (SSE-C) as the encryption type. Copy the existing objects to the new S3 bucket. Specify SSE-C.\"\nThis option is not the best choice because it requires the company to manage the encryption keys, which adds operational overhead. Additionally, it does not address the issue of increasing AWS KMS costs, as the application will still be using encryption keys that are not managed by Amazon S3.\n\nC. \"Use AWS CloudHSM to store the encryption keys. Create a new S3 bucket. Use S3 Batch Operations to copy the existing objects to the new S3 bucket. Encrypt the objects by using the keys from CloudHSM.\"\nThis option is not the optimal choice because it introduces additional complexity by requiring the use of AWS CloudHSM, which adds operational overhead. While it can help reduce AWS KMS costs, the overall complexity and additional management requirements make it less desirable compared to the simpler solution in option B.\n\nD. \"Use the S3 Intelligent-Tiering storage class for the S3 bucket. Create an S3 Intelligent-Tiering archive configuration to transition objects that are not accessed for 90 days to S3 Glacier Deep Archive.\"\nThis option is not the best choice because it does not address the issue of increasing AWS KMS costs. The"
  },
  "257": {
    "question": "A media storage application uploads user photos to Amazon S3 for processing by AWS Lambda functions. Application state is stored in Amazon DynamoDB tables. Users are reporting that some uploaded photos are not being processed properly. The application developers trace the logs and find that Lambda is experiencing photo processing issues when thousands of users upload photos simultaneously. The issues are the result of Lambda concurrency limits and the performance of DynamoDB when data is saved.Which combination of actions should a solutions architect take to increase the performance and reliability of the application? (Choose two.)",
    "choices": [
      "A. Evaluate and adjust the RCUs for the DynamoDB tables.",
      "B. Evaluate and adjust the WCUs for the DynamoDB tables.",
      "C. Add an Amazon ElastiCache layer to increase the performance of Lambda functions.",
      "D. Add an Amazon Simple Queue Service (Amazon SQS) queue and reprocessing logic between Amazon S3 and the Lambda functions.",
      "E. Use S3 Transfer Acceleration to provide lower latency to users."
    ],
    "answer": "BD",
    "explanation": "1. Explanation of the correct answer (BD):\n\nB. Evaluate and adjust the WCUs for the DynamoDB tables:\nThe question states that the performance issues are related to the \"performance of DynamoDB when data is saved.\" This indicates that the write capacity units (WCUs) of the DynamoDB tables may be insufficient to handle the sudden influx of photo uploads. Adjusting the WCUs can help improve the write throughput and ensure that the data is saved to DynamoDB reliably, reducing the performance issues.\n\nD. Add an Amazon SQS queue and reprocessing logic between Amazon S3 and the Lambda functions:\nThe question mentions that the application is experiencing issues due to \"Lambda concurrency limits.\" By adding an Amazon SQS queue between S3 and the Lambda functions, the application can decouple the photo upload from the processing, allowing the Lambda functions to scale independently. The SQS queue also provides retry capabilities, ensuring that no photos are lost due to temporary processing failures.\n\n2. Explanations of the incorrect choices:\n\nA. Evaluate and adjust the RCUs for the DynamoDB tables:\nWhile adjusting the RCUs (read capacity units) can help improve the performance of data retrieval, the question specifically states that the issues are related to the \"performance of DynamoDB when data is saved.\" Therefore, adjusting the RCUs alone would not address the root cause of the problem.\n\nC. Add an Amazon ElastiCache layer to increase the performance of Lambda functions:\nThe question does not indicate that the performance issues are related to the Lambda functions themselves. The issues are described as being caused by \"Lambda concurrency limits\" and the \"performance of DynamoDB when data is saved.\" Adding an ElastiCache layer would not directly address these specific problems.\n\nE. Use S3 Transfer Acceleration to provide lower latency to users:\nS3 Transfer Acceleration is designed to improve the upload and download performance of objects in Amazon S3, particularly for users located in different geographic regions. However, the question states that the issues are related to the processing of the uploaded photos, not the upload performance itself. Therefore, using S3 Transfer Acceleration would not directly address the root cause of the problem."
  },
  "258": {
    "question": "A company runs an application in an on-premises data center. The application gives users the ability to upload media files. The files persist in a file server. The web application has many users. The application server is overutilized, which causes data uploads to fail occasionally. The company frequently adds new storage to the file server. The company wants to resolve these challenges by migrating the application to AWS.Users from across the United States and Canada access the application. Only authenticated users should have the ability to access the application to upload files. The company will consider a solution that refactors the application, and the company needs to accelerate application development.Which solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      "A. Use AWS Application Migration Service to migrate the application server to Amazon EC2 instances. Create an Auto Scaling group for the EC2 instances. Use an Application Load Balancer to distribute the requests. Modify the application to use Amazon S3 to persist the files. Use Amazon Cognito to authenticate users.",
      "B. Use AWS Application Migration Service to migrate the application server to Amazon EC2 instances. Create an Auto Scaling group for the EC2 instances. Use an Application Load Balancer to distribute the requests. Set up AWS IAM Identity Center (AWS Single Sign-On) to give users the ability to sign in to the application. Modify the application to use Amazon S3 to persist the files.",
      "C. Create a static website for uploads of media files. Store the static assets in Amazon S3. Use AWS AppSync to create an API. Use AWS Lambda resolvers to upload the media files to Amazon S3. Use Amazon Cognito to authenticate users.",
      "D. Use AWS Amplify to create a static website for uploads of media files. Use Amplify Hosting to serve the website through Amazon CloudFront. Use Amazon S3 to store the uploaded media files. Use Amazon Cognito to authenticate users."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (D):\n\nThe correct answer is option D, which uses AWS Amplify to create a static website for media file uploads, leveraging the following AWS services:\n\n- AWS Amplify: Provides a simplified development experience for building and deploying serverless, scalable web applications.\n- Amazon S3: Used for storing the uploaded media files.\n- Amazon Cognito: Handles user authentication, ensuring only authenticated users can access the application.\n- Amazon CloudFront: Serves the static website content from the S3 bucket, providing low-latency access for users across the United States and Canada.\n\nThis solution addresses the key requirements of the problem statement:\n\n- Scalability: The serverless architecture and auto-scaling capabilities of the AWS services minimize operational overhead.\n- Reliability: Amazon S3 and CloudFront provide highly reliable and durable storage and content delivery.\n- User authentication: Amazon Cognito handles user authentication, ensuring only authorized users can access the application.\n- Accelerated development: The Amplify framework simplifies the development process and integrates the necessary AWS services, allowing the company to focus on the application logic rather than infrastructure management.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option uses Amazon EC2 instances and an Application Load Balancer, which introduces more operational overhead compared to the serverless solution in option D. Additionally, the application needs to be modified to use Amazon S3 for file storage, which may require more development effort.\n\nB. Similar to option A, this solution uses Amazon EC2 instances and an Application Load Balancer, which has more operational overhead than the serverless approach in option D. While it uses AWS IAM Identity Center (AWS Single Sign-On) for user authentication, it still requires more infrastructure management compared to the Cognito-based solution in option D.\n\nC. This option creates a static website and uses AWS AppSync and AWS Lambda for the file upload functionality. While this is a serverless approach, it may require more development effort compared to the Amplify-based solution in option D, which provides a more streamlined development experience."
  },
  "259": {
    "question": "A company has an application that is deployed on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances are part of an Auto Scaling group. The application has unpredictable workloads and frequently scales out and in. The company\u2019s development team wants to analyze application logs to find ways to improve the application's performance. However, the logs are no longer available after instances scale in.Which solution will give the development team the ability to view the application logs after a scale-in event?",
    "choices": [
      "A. Enable access logs for the ALB. Store the logs in an Amazon S3 bucket.",
      "B. Configure the EC2 instances to publish logs to Amazon CloudWatch Logs by using the unified CloudWatch agent.",
      "C. Modify the Auto Scaling group to use a step scaling policy.",
      "D. Instrument the application with AWS X-Ray tracing."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B - Configure the EC2 instances to publish logs to Amazon CloudWatch Logs by using the unified CloudWatch agent. This is the best solution because it allows the development team to view the application logs even after the instances have scaled in.\n\nThe key points are:\n- The unified CloudWatch agent can be configured to collect and publish logs from the EC2 instances to Amazon CloudWatch Logs.\n- CloudWatch Logs is a centralized logging service that retains logs even after the underlying EC2 instances have been terminated or scaled in.\n- This allows the development team to access the application logs from CloudWatch Logs, regardless of the scaling activity of the EC2 instances.\n\n2. Explanations of the incorrect choices:\n\nA. Enable access logs for the ALB. Store the logs in an Amazon S3 bucket:\n- This solution only captures the access logs for the Application Load Balancer (ALB), which contain information about the requests sent to the load balancer.\n- It does not capture the application-level logs from the EC2 instances, which are the logs the development team needs to analyze for performance improvements.\n\nC. Modify the Auto Scaling group to use a step scaling policy:\n- Changing the Auto Scaling group's scaling policy would not address the issue of retaining application logs after scale-in events.\n- The scaling policy determines how the instances scale in and out, but it does not affect the availability of application logs.\n\nD. Instrument the application with AWS X-Ray tracing:\n- AWS X-Ray is a service for tracing and analyzing distributed applications, but it does not provide a solution for storing application logs.\n- X-Ray is more focused on tracing the request flow through the application, not on storing the output logs from the application."
  },
  "260": {
    "question": "A company runs an unauthenticated static website (www.example.com) that includes a registration form for users. The website uses Amazon S3 for hosting and uses Amazon CloudFront as the content delivery network with AWS WAF configured. When the registration form is submitted, the website calls an Amazon API Gateway API endpoint that invokes an AWS Lambda function to process the payload and forward the payload to an external API call.During testing, a solutions architect encounters a cross-origin resource sharing (CORS) error. The solutions architect confirms that the CloudFront distribution origin has the Access-Control-Allow-Origin header set to www.example.com.What should the solutions architect do to resolve the error?",
    "choices": [
      "A. Change the CORS configuration on the S3 bucket. Add rules for CORS to the AllowedOrigin element for www.example.com.",
      "B. Enable the CORS setting in AWS WAF. Create a web ACL rule in which the Access-Control-Allow-Origin header is set to www.example.com.",
      "C. Enable the CORS setting on the API Gateway API endpoint. Ensure that the API endpoint is configured to return all responses that have the Access-Control-Allow-Origin header set to www.example.com.",
      "D. Enable the CORS setting on the Lambda function. Ensure that the return code of the function has the Access-Control-Allow-Origin header set to www.example.com."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C. Enable the CORS setting on the API Gateway API endpoint. Ensure that the API endpoint is configured to return all responses that have the Access-Control-Allow-Origin header set to www.example.com.\n\nThis is the correct answer because the CORS error is likely occurring at the API Gateway level, where the website is making a cross-origin request to the API endpoint. To resolve this, the API Gateway API endpoint needs to be configured to allow the www.example.com origin to access the API. This is done by enabling CORS settings on the API Gateway API and setting the Access-Control-Allow-Origin header to www.example.com.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Change the CORS configuration on the S3 bucket. Add rules for CORS to the AllowedOrigin element for www.example.com.\nThis is incorrect because the CORS issue is likely occurring at the API Gateway level, not at the S3 bucket level. Configuring CORS on the S3 bucket would not resolve the CORS error for the API Gateway API endpoint.\n\nB. Enable the CORS setting in AWS WAF. Create a web ACL rule in which the Access-Control-Allow-Origin header is set to www.example.com.\nThis is incorrect because the CORS issue is specific to the API Gateway API endpoint, not the AWS WAF. Configuring CORS in AWS WAF would not resolve the CORS error for the API Gateway API endpoint.\n\nD. Enable the CORS setting on the Lambda function. Ensure that the return code of the function has the Access-Control-Allow-Origin header set to www.example.com.\nThis is incorrect because the CORS issue is likely occurring at the API Gateway level, not the Lambda function level. Configuring CORS on the Lambda function would not resolve the CORS error for the API Gateway API endpoint."
  },
  "261": {
    "question": "A company is planning to migrate an application from on premises to the AWS Cloud. The company will begin the migration by moving the application\u2019s underlying data storage to AWS. The application data is stored on a shared file system on premises, and the application servers connect to the shared file system through SMB.A solutions architect must implement a solution that uses an Amazon S3 bucket for shared storage. Until the application is fully migrated and code is rewritten to use native Amazon S3 APIs, the application must continue to have access to the data through SMB. The solutions architect must migrate the application data to AWS to its new location while still allowing the on-premises application to access the data.Which solution will meet these requirements?",
    "choices": [
      "A. Create a new Amazon FSx for Windows File Server file system. Configure AWS DataSync with one location for the on-premises file share and one location for the new Amazon FSx file system. Create a new DataSync task to copy the data from the on-premises file share location to the Amazon FSx file system.",
      "B. Create an S3 bucket for the application. Copy the data from the on-premises storage to the S3 bucket.",
      "C. Deploy an AWS Server Migration Service (AWS SMS) VM to the on-premises environment. Use AWS SMS to migrate the file storage server from on premises to an Amazon EC2 instance.",
      "D. Create an S3 bucket for the application. Deploy a new AWS Storage Gateway file gateway on an on-premises VM. Create a new file share that stores data in the S3 bucket and is associated with the file gateway. Copy the data from the on-premises storage to the new file gateway endpoint."
    ],
    "answer": "D",
    "explanation": "Sure, here's a clear explanation of the correct answer and why the other choices are incorrect:\n\n1. Correct Answer: D\n\nExplanation:\nOption D is the correct answer because it meets all the requirements outlined in the question:\n\n- It creates an S3 bucket for the application's data storage, as required.\n- It deploys an AWS Storage Gateway file gateway on-premises, which allows the on-premises application to continue accessing the data through SMB, even though the data is now stored in the S3 bucket.\n- The file gateway creates a file share that stores the data in the S3 bucket, effectively bridging the on-premises file system and the S3 bucket.\n- The data can then be copied from the on-premises storage to the new file gateway endpoint, migrating the data to the AWS cloud while still maintaining SMB access.\n\n2. Incorrect Choices:\n\nA. This option uses an Amazon FSx for Windows File Server file system, which does not meet the requirement of using an S3 bucket for shared storage.\n\nB. This option simply copies the data from the on-premises storage to an S3 bucket, but does not provide a solution for maintaining SMB access for the on-premises application.\n\nC. This option uses AWS Server Migration Service (SMS) to migrate the file storage server to an Amazon EC2 instance, but does not address the requirement of using an S3 bucket for shared storage or maintaining SMB access.\n\nIn summary, Option D is the correct answer because it provides a comprehensive solution that uses an S3 bucket for shared storage while still allowing the on-premises application to access the data through SMB, which meets all the requirements outlined in the question."
  },
  "262": {
    "question": "A company wants to manage the costs associated with a group of 20 applications that are infrequently used, but are still business-critical, by migrating to AWS. The applications are a mix of Java and Node.js spread across different instance clusters. The company wants to minimize costs while standardizing by using a single deployment methodology.Most of the applications are part of month-end processing routines with a small number of concurrent users, but they are occasionally run at other times. Average application memory consumption is less than 1 GB. though some applications use as much as 2.5 GB of memory during peak processing. The most important application in the group is a billing report written in Java that accesses multiple data sources and often runs for several hours.Which is the MOST cost-effective solution?",
    "choices": [
      "A. Deploy a separate AWS Lambda function for each application. Use AWS CloudTrail logs and Amazon CloudWatch alarms to verify completion of critical jobs.",
      "B. Deploy Amazon ECS containers on Amazon EC2 with Auto Scaling configured for memory utilization of 75%. Deploy an ECS task for each application being migrated with ECS task scaling. Monitor services and hosts by using Amazon CloudWatch.",
      "C. Deploy AWS Elastic Beanstalk for each application with Auto Scaling to ensure that all requests have sufficient resources. Monitor each AWS Elastic Beanstalk deployment by using CloudWatch alarms.",
      "D. Deploy a new Amazon EC2 instance cluster that co-hosts all applications by using EC2 Auto Scaling and Application Load Balancers. Scale cluster size based on a custom metric set on instance memory utilization. Purchase 3-year Reserved Instance reservations equal to the GroupMaxSize parameter of the Auto Scaling group."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B, which proposes deploying Amazon ECS containers on Amazon EC2 with Auto Scaling configured for memory utilization of 75%. This is the most cost-effective solution for the given scenario for the following reasons:\n\n- ECS containers on EC2 provide a flexible and scalable solution that can accommodate the varying resource requirements of the different applications (Java and Node.js).\n- Auto Scaling based on memory utilization ensures that resources are provisioned dynamically, allowing the solution to scale up and down based on demand, thereby minimizing costs.\n- ECS task scaling allows for individual application scaling, further optimizing resource utilization.\n- Monitoring services and hosts using Amazon CloudWatch provides visibility into the infrastructure, enabling better cost management and troubleshooting.\n\n2. Explanations of why the other choices are incorrect:\n\nA. Deploying a separate AWS Lambda function for each application may not be the most cost-effective solution, as Lambda is designed for short-lived, event-driven workloads. The long-running nature of some of the applications, such as the billing report, may not be well-suited for Lambda, leading to potentially higher costs.\n\nC. Deploying AWS Elastic Beanstalk for each application may not be the most cost-effective solution. While Elastic Beanstalk provides a managed platform, it still utilizes EC2 instances, and having a separate deployment for each application may result in higher overall costs compared to the ECS-based solution in option B.\n\nD. Deploying a new Amazon EC2 instance cluster that co-hosts all applications is a valid approach, but it may not be as cost-effective as the ECS-based solution in option B. The ECS-based solution provides more fine-grained control over resource allocation and scaling, potentially leading to better cost optimization."
  },
  "263": {
    "question": "A solutions architect needs to review the design of an Amazon EMR cluster that is using the EMR File System (EMRFS). The cluster performs tasks that are critical to business needs. The cluster is running Amazon EC2 On-Demand Instances at all times for all task, primary, and core nodes. The EMR tasks run each morning, starting at 1:00 AM. and take 6 hours to finish running. The amount of time to complete the processing is not a priority because the data is not referenced until late in the day.The solutions architect must review the architecture and suggest a solution to minimize the compute costs.Which solution should the solutions architect recommend to meet these requirements?",
    "choices": [
      "A. Launch all task, primary, and core nodes on Spot Instances in an instance fleet. Terminate the cluster, including all instances, when the processing is completed.",
      "B. Launch the primary and core nodes on On-Demand Instances. Launch the task nodes on Spot Instances in an instance fleet. Terminate the cluster, including all instances, when the processing is completed. Purchase Compute Savings Plans to cover the On-Demand Instance usage.",
      "C. Continue to launch all nodes on On-Demand Instances. Terminate the cluster, including all instances, when the processing is completed. Purchase Compute Savings Plans to cover the On-Demand Instance usage.",
      "D. Launch the primary and core nodes on On-Demand Instances. Launch the task nodes on Spot Instances in an instance fleet. Terminate only the task node instances when the processing is completed. Purchase Compute Savings Plans to cover the On-Demand Instance usage."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (D):\n\nThe correct answer is D because it best meets the requirements to minimize compute costs while maintaining the necessary availability for the critical EMR tasks.\n\nKey points:\n- The EMR tasks run for 6 hours each morning, starting at 1 AM, and the data is not referenced until later in the day.\n- Since the data is stored in EMRFS (S3), there is no need to keep the entire cluster running after the tasks complete.\n- Launching the primary and core nodes on On-Demand Instances ensures they are available when the tasks need to run in the morning.\n- Launching the task nodes on Spot Instances in an instance fleet allows the solutions architect to take advantage of the cost savings of Spot Instances for the task nodes, which can be terminated once the processing is complete.\n- Purchasing Compute Savings Plans to cover the On-Demand Instance usage for the primary and core nodes further minimizes the compute costs.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Launching all task, primary, and core nodes on Spot Instances in an instance fleet, and terminating the entire cluster when the processing is complete, is not the best solution. This approach risks losing the primary and core nodes, which are required to be available for the critical EMR tasks.\n\nB. Launching the primary and core nodes on On-Demand Instances, and the task nodes on Spot Instances in an instance fleet, is a good approach. However, terminating the entire cluster, including the primary and core nodes, when the processing is complete is not necessary since the data is stored in EMRFS (S3). Purchasing Compute Savings Plans is a good idea, but it's not required to be done in this specific scenario.\n\nC. Continuing to launch all nodes on On-Demand Instances and terminating the cluster when the processing is complete does not take advantage of the cost savings opportunities provided by Spot Instances for the task nodes. While purchasing Compute Savings Plans can help reduce the costs, it's not the most optimal solution."
  },
  "264": {
    "question": "A company has migrated a legacy application to the AWS Cloud. The application runs on three Amazon EC2 instances that are spread across three Availability Zones. One EC2 instance is in each Availability Zone. The EC2 instances are running in three private subnets of the VPC and are set up as targets for an Application Load Balancer (ALB) that is associated with three public subnets.The application needs to communicate with on-premises systems. Only traffic from IP addresses in the company's IP address range are allowed to access the on-premises systems. The company\u2019s security team is bringing only one IP address from its internal IP address range to the cloud. The company has added this IP address to the allow list for the company firewall. The company also has created an Elastic IP address for this IP address.A solutions architect needs to create a solution that gives the application the ability to communicate with the on-premises systems. The solution also must be able to mitigate failures automatically.Which solution will meet these requirements?",
    "choices": [
      "A. Deploy three NAT gateways, one in each public subnet. Assign the Elastic IP address to the NAT gateways. Turn on health checks for the NAT gateways. If a NAT gateway fails a health check, recreate the NAT gateway and assign the Elastic IP address to the new NAT gateway.",
      "B. Replace the ALB with a Network Load Balancer (NLB). Assign the Elastic IP address to the NLTurn on health checks for the NLIn the case of a failed health check, redeploy the NLB in different subnets.",
      "C. Deploy a single NAT gateway in a public subnet. Assign the Elastic IP address to the NAT gateway. Use Amazon CloudWatch with a custom metric to monitor the NAT gateway. If the NAT gateway is unhealthy, invoke an AWS Lambda function to create a new NAT gateway in a different subnet. Assign the Elastic IP address to the new NAT gateway.",
      "D. Assign the Elastic IP address to the ALB. Create an Amazon Route 53 simple record with the Elastic IP address as the value. Create a Route 53 health check. In the case of a failed health check, recreate the ALB in different subnets."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C because it provides the most robust and scalable solution to the problem.\n\nThe key aspects of the solution are:\n\n- Deploying a single NAT Gateway in a public subnet and assigning the Elastic IP address to it. This ensures that all outbound traffic from the private EC2 instances goes through the single whitelisted IP address, as required by the on-premises systems.\n\n- Using Amazon CloudWatch with a custom metric to monitor the health of the NAT Gateway. This allows for automatic detection of any issues with the NAT Gateway.\n\n- Invoking an AWS Lambda function to recreate the NAT Gateway in a different subnet if the health check fails. This provides automatic failover and high availability, ensuring that the application can continue to communicate with the on-premises systems.\n\nThis solution is simple, efficient, and addresses the key requirements of the problem statement, including the need for a single whitelisted IP address and automatic failure mitigation.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This solution is unnecessarily complex, as it requires deploying three NAT Gateways, one in each public subnet. This is overkill for the given requirement, where only a single whitelisted IP address is needed for communication with the on-premises systems.\n\nB. Replacing the Application Load Balancer (ALB) with a Network Load Balancer (NLB) is not necessary to solve this problem. The ALB is already set up correctly to distribute traffic across the three EC2 instances in different Availability Zones. The issue here is the communication with the on-premises systems, not the load balancing within the VPC.\n\nD. Assigning the Elastic IP address directly to the ALB is not a viable solution, as the ALB is a public-facing resource and would not provide the required control over the outbound traffic IP address. Additionally, recreating the ALB in different subnets is not an appropriate way to handle NAT Gateway failures, as the NAT Gateway is a separate component responsible for internet connectivity."
  },
  "265": {
    "question": "A company uses AWS Organizations to manage more than 1,000 AWS accounts. The company has created a new developer organization. There are 540 developer member accounts that must be moved to the new developer organization. All accounts are set up with all the required information so that each account can be operated as a standalone account.Which combination of steps should a solutions architect take to move all of the developer accounts to the new developer organization? (Choose three.)",
    "choices": [
      "A. Call the MoveAccount operation in the Organizations API from the old organization's management account to migrate the developer accounts to the new developer organization.",
      "B. From the management account, remove each developer account from the old organization using the RemoveAccountFromOrganization operation in the Organizations API.",
      "C. From each developer account, remove the account from the old organization using the RemoveAccountFromOrganization operation in the Organizations API.",
      "D. Sign in to the new developer organization's management account and create a placeholder member account that acts as a target for the developer account migration.",
      "E. Call the InviteAccountToOrganization operation in the Organizations API from the new developer organization's management account to send invitations to the developer accounts.",
      "F. Have each developer sign in to their account and confirm to join the new developer organization."
    ],
    "answer": "BEF",
    "explanation": "1. Clear explanation of why the correct answer is right (BEF):\n\nB. Remove the developer accounts from the old organization using the RemoveAccountFromOrganization operation in the Organizations API. This is a valid step because the accounts are already set up to operate as standalone accounts, so they can be safely removed from the old organization.\n\nE. Invite the developer accounts to the new organization using the InviteAccountToOrganization operation in the Organizations API. This is a necessary step to move the accounts to the new organization.\n\nF. Have each developer sign in to their account and confirm to join the new developer organization. This is a required step to complete the migration, as the developer accounts need to accept the invitation to join the new organization.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nA. Calling the MoveAccount operation is not a valid step because the accounts need to be first removed from the old organization before they can be moved to the new organization.\n\nC. Removing the accounts from the old organization using the RemoveAccountFromOrganization operation from each individual developer account is not a recommended approach, as it would require manual intervention for each of the 540 accounts. It's more efficient to perform this action from the old organization's management account.\n\nD. Creating a placeholder member account in the new organization is not necessary, as the accounts can be directly invited to the new organization without the need for a placeholder."
  },
  "266": {
    "question": "A company\u2019s interactive web application uses an Amazon CloudFront distribution to serve images from an Amazon S3 bucket. Occasionally, third-party tools ingest corrupted images into the S3 bucket. This image corruption causes a poor user experience in the application later. The company has successfully implemented and tested Python logic to detect corrupt images.A solutions architect must recommend a solution to integrate the detection logic with minimal latency between the ingestion and serving.Which solution will meet these requirements?",
    "choices": [
      "A. Use a Lambda@Edge function that is invoked by a viewer-response event.",
      "B. Use a Lambda@Edge function that is invoked by an origin-response event.",
      "C. Use an S3 event notification that invokes an AWS Lambda function.",
      "D. Use an S3 event notification that invokes an AWS Step Functions state machine."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\nThe correct answer is C. Use an S3 event notification that invokes an AWS Lambda function.\n\nThis solution is the best fit for the given requirements because it allows the detection logic to be integrated with the ingestion process. When a new image is uploaded to the S3 bucket, an S3 event notification can be used to trigger an AWS Lambda function that runs the Python logic to detect corrupted images. This way, the corruption check happens as soon as the image is ingested, before it is served to the users, minimizing the latency between ingestion and serving.\n\n2. Explanations of the incorrect choices:\n\nA. Use a Lambda@Edge function that is invoked by a viewer-response event:\nThis solution is incorrect because the viewer-response event is triggered when the CloudFront distribution responds to a client request, which is too late in the process to detect and handle corrupted images. The detection logic should be integrated with the ingestion process, not the serving process.\n\nB. Use a Lambda@Edge function that is invoked by an origin-response event:\nThis solution is incorrect because it would add unnecessary latency. The origin-response event is triggered when the CloudFront distribution receives a response from the S3 bucket, which means the detection logic would run every time an image is fetched from the origin. This is less efficient than running the detection logic at the time of ingestion, as in the correct solution.\n\nD. Use an S3 event notification that invokes an AWS Step Functions state machine:\nThis solution is incorrect because S3 event notifications can only be configured to trigger AWS Lambda functions, Amazon SNS topics, Amazon SQS queues, or Amazon EventBridge rules. They cannot directly invoke an AWS Step Functions state machine. To use Step Functions, you would need to first trigger a Lambda function from the S3 event notification, and then have the Lambda function invoke the Step Functions state machine."
  },
  "267": {
    "question": "A company has an application that runs on Amazon EC2 instances in an Amazon EC2 Auto Scaling group. The company uses AWS CodePipeline to deploy the application. The instances that run in the Auto Scaling group are constantly changing because of scaling events.When the company deploys new application code versions, the company installs the AWS CodeDeploy agent on any new target EC2 instances and associates the instances with the CodeDeploy deployment group. The application is set to go live within the next 24 hours.What should a solutions architect recommend to automate the application deployment process with the LEAST amount of operational overhead?",
    "choices": [
      "A. Configure Amazon EventBridge to invoke an AWS Lambda function when a new EC2 instance is launched into the Auto Scaling group. Code the Lambda function to associate the EC2 instances with the CodeDeploy deployment group.",
      "B. Write a script to suspend Amazon EC2 Auto Scaling operations before the deployment of new code. When the deployment is complete, create a new AMI and configure the Auto Scaling group's launch template to use the new AMI for new launches. Resume Amazon EC2 Auto Scaling operations.",
      "C. Create a new AWS CodeBuild project that creates a new AMI that contains the new code. Configure CodeBuild to update the Auto Scaling group\u2019s launch template to the new AMI. Run an Amazon EC2 Auto Scaling instance refresh operation.",
      "D. Create a new AMI that has the CodeDeploy agent installed. Configure the Auto Scaling group\u2019s launch template to use the new AMI. Associate the CodeDeploy deployment group with the Auto Scaling group instead of the EC2 instances."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (Option D):\n\nThe correct answer is Option D: \"Create a new AMI that has the CodeDeploy agent installed. Configure the Auto Scaling group's launch template to use the new AMI. Associate the CodeDeploy deployment group with the Auto Scaling group instead of the EC2 instances.\"\n\nThis solution is the most effective in automating the application deployment process with the least amount of operational overhead. By creating a new AMI with the CodeDeploy agent pre-installed, the company can ensure that all new instances launched by the Auto Scaling group will have the necessary agent. This eliminates the need to manually install the agent on each new instance. Additionally, by associating the CodeDeploy deployment group with the Auto Scaling group, CodeDeploy will automatically deploy the application to any new instances, further reducing the operational overhead.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option requires the use of a Lambda function to associate the EC2 instances with the CodeDeploy deployment group, which adds an additional layer of complexity and operational overhead. It is more efficient to leverage the auto-association capabilities of CodeDeploy by associating the deployment group with the Auto Scaling group.\n\nB. This option involves suspending Auto Scaling operations, creating a new AMI, and updating the Auto Scaling group's launch template. While this approach may work, it requires more manual steps and operational overhead compared to the recommended solution in Option D.\n\nC. This option uses AWS CodeBuild to create a new AMI and update the Auto Scaling group's launch template. While this is a valid approach, it still requires more manual steps and operational overhead than the recommended solution in Option D, which directly associates the CodeDeploy deployment group with the Auto Scaling group."
  },
  "268": {
    "question": "A company has a website that runs on four Amazon EC2 instances that are behind an Application Load Balancer (ALB). When the ALB detects that an EC2 instance is no longer available, an Amazon CloudWatch alarm enters the ALARM state. A member of the company's operations team then manually adds a new EC2 instance behind the ALB.A solutions architect needs to design a highly available solution that automatically handles the replacement of EC2 instances. The company needs to minimize downtime during the switch to the new solution.Which set of steps should the solutions architect take to meet these requirements?",
    "choices": [
      "A. Delete the existing ALB. Create an Auto Scaling group that is configured to handle the web application traffic. Attach a new launch template to the Auto Scaling group. Create a new ALB. Attach the Auto Scaling group to the new ALB. Attach the existing EC2 instances to the Auto Scaling group.",
      "B. Create an Auto Scaling group that is configured to handle the web application traffic. Attach a new launch template to the Auto Scaling group. Attach the Auto Scaling group to the existing ALAttach the existing EC2 instances to the Auto Scaling group.",
      "C. Delete the existing ALB and the EC2 instances. Create an Auto Scaling group that is configured to handle the web application traffic. Attach a new launch template to the Auto Scaling group. Create a new ALB. Attach the Auto Scaling group to the new ALB. Wait for the Auto Scaling group to launch the minimum number of EC2 instances.",
      "D. Create an Auto Scaling group that is configured to handle the web application traffic. Attach a new launch template to the Auto Scaling group. Attach the Auto Scaling group to the existing ALB. Wait for the existing ALB to register the existing EC2 instances with the Auto Scaling group."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B because it outlines the most appropriate steps to design a highly available solution that automatically handles the replacement of EC2 instances while minimizing downtime.\n\nThe key steps are:\n\n- Create an Auto Scaling group that is configured to handle the web application traffic.\n- Attach a new launch template to the Auto Scaling group.\n- Attach the Auto Scaling group to the existing Application Load Balancer (ALB).\n\nThis approach allows the Auto Scaling group to automatically manage the EC2 instances behind the existing ALB, replacing instances as needed without disrupting the overall service. The existing ALB can continue to distribute traffic to the instances, ensuring minimal downtime during the transition.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This choice is incorrect because deleting the existing ALB would increase downtime, which goes against the requirement to minimize downtime. Creating a new ALB and attaching the Auto Scaling group to it is also an unnecessary step.\n\nC. This choice is incorrect because deleting the existing EC2 instances and ALB would also increase downtime, which goes against the requirement. Creating a new ALB and waiting for the Auto Scaling group to launch the minimum number of instances could also lead to increased downtime.\n\nD. This choice is incorrect because it suggests waiting for the existing ALB to register the EC2 instances with the Auto Scaling group, which could lead to increased downtime. The preferred approach is to directly attach the Auto Scaling group to the existing ALB, as outlined in the correct answer (B)."
  },
  "269": {
    "question": "A company uses AWS CloudFormation to deploy applications within multiple VPCs that are all attached to a transit gateway. Each VPC that sends traffic to the public internet must send the traffic through a shared services VPC. Each subnet within a VPC uses the default VPC route table, and the traffic is routed to the transit gateway. The transit gateway uses its default route table for any VPC attachment.A security audit reveals that an Amazon EC2 instance that is deployed within a VPC can communicate with an EC2 instance that is deployed in any of the company's other VPCs. A solutions architect needs to limit the traffic between the VPCs. Each VPC must be able to communicate only with a predefined, limited set of authorized VPCs.What should the solutions architect do to meet these requirements?",
    "choices": [
      "A. Update the network ACL of each subnet within a VPC to allow outbound traffic only to the authorized VPCs. Remove all deny rules except the default deny rule.",
      "B. Update all the security groups that are used within a VPC to deny outbound traffic to security groups that are used within the unauthorized VPCs.",
      "C. Create a dedicated transit gateway route table for each VPC attachment. Route traffic only to the authorized VPCs.",
      "D. Update the main route table of each VPC to route traffic only to the authorized VPCs through the transit gateway."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C: Create a dedicated transit gateway route table for each VPC attachment. Route traffic only to the authorized VPCs.\n\nThis is the correct approach because it allows the solutions architect to have fine-grained control over the routing of traffic between VPCs. By creating a separate transit gateway route table for each VPC attachment, the architect can specify the allowed routes for each VPC, ensuring that communication is restricted to only the authorized VPCs. This provides a secure and controlled network environment, as the transit gateway will only route traffic to the VPCs specified in the corresponding route table.\n\n2. Explanations of why the other choices are incorrect:\n\nA. Update the network ACL of each subnet within a VPC to allow outbound traffic only to the authorized VPCs. Remove all deny rules except the default deny rule.\nThis is incorrect because network ACLs are not the appropriate mechanism to control traffic between VPCs. Network ACLs operate at the subnet level and are designed to control traffic entering or leaving a subnet, not between VPCs. Updating the network ACLs would not effectively limit the communication between VPCs.\n\nB. Update all the security groups that are used within a VPC to deny outbound traffic to security groups that are used within the unauthorized VPCs.\nThis is incorrect because security groups are not designed to control traffic between VPCs. Security groups are applied to individual EC2 instances and control traffic to and from those instances. Updating the security groups within a VPC would not prevent communication between VPCs, as the traffic would still be routed through the transit gateway.\n\nD. Update the main route table of each VPC to route traffic only to the authorized VPCs through the transit gateway.\nThis is incorrect because it does not provide the necessary level of control. Updating the main route table of each VPC would only control the routing within the VPC, but it would not prevent communication between VPCs through the transit gateway. The transit gateway's default route table would still allow communication between all VPCs, which does not meet the requirement to limit traffic to only the authorized VPCs."
  },
  "270": {
    "question": "A company is expanding. The company plans to separate its resources into hundreds of different AWS accounts in multiple AWS Regions. A solutions architect must recommend a solution that denies access to any operations outside of specifically designated Regions.Which solution will meet these requirements?",
    "choices": [
      "A. Create IAM roles for each account. Create IAM policies with conditional allow permissions that include only approved Regions for the accounts.",
      "B. Create an organization in AWS Organizations. Create IAM users for each account. Attach a policy to each user to block access to Regions where an account cannot deploy infrastructure.",
      "C. Launch an AWS Control Tower landing zone. Create OUs and attach SCPs that deny access to run services outside of the approved Regions.",
      "D. Enable AWS Security Hub in each account. Create controls to specify the Regions where an account can deploy infrastructure."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the Correct Answer (C):\n\nThe correct answer is C, \"Launch an AWS Control Tower landing zone. Create OUs and attach SCPs that deny access to run services outside of the approved Regions.\"\n\nThis is the best solution because:\n- AWS Control Tower provides a centralized way to set up and govern multiple AWS accounts in an organization.\n- Once the organization is set up, you can create Organizational Units (OUs) and attach Service Control Policies (SCPs) to those OUs.\n- SCPs can be used to deny access to specific AWS Regions, ensuring that the accounts within the OU can only deploy infrastructure in the approved Regions.\n- This approach is scalable and easy to manage, as the SCP is applied at the OU level, rather than having to configure policies individually for each IAM user or role.\n\n2. Explanations of the Incorrect Choices:\n\nA. \"Create IAM roles for each account. Create IAM policies with conditional allow permissions that include only approved Regions for the accounts.\"\n- This option is not ideal because it requires configuring the policies individually for each account, which can become unwieldy as the number of accounts grows.\n- Maintaining consistent policies across multiple accounts can be challenging and error-prone.\n\nB. \"Create an organization in AWS Organizations. Create IAM users for each account. Attach a policy to each user to block access to Regions where an account cannot deploy infrastructure.\"\n- This option is not the best because it requires managing policies at the individual user level, which can be time-consuming and difficult to maintain as the organization grows.\n- It's generally better to manage policies at a higher level, such as at the OU or account level, to ensure consistency and ease of management.\n\nD. \"Enable AWS Security Hub in each account. Create controls to specify the Regions where an account can deploy infrastructure.\"\n- This option is not the best because it does not provide a centralized way to manage the policies across multiple accounts.\n- Configuring the controls individually in each account can be error-prone and difficult to maintain.\n\nIn summary, the correct answer (C) is the best solution because it leverages the centralized management capabilities of AWS Control Tower and Service Control Policies to enforce the desired regional access restrictions across multiple AWS accounts."
  },
  "271": {
    "question": "A company wants to refactor its retail ordering web application that currently has a load-balanced Amazon EC2 instance fleet for web hosting, database API services, and business logic. The company needs to create a decoupled, scalable architecture with a mechanism for retaining failed orders while also minimizing operational costs.Which solution will meet these requirements?",
    "choices": [
      "A. Use Amazon S3 for web hosting with Amazon API Gateway for database API services. Use Amazon Simple Queue Service (Amazon SQS) for order queuing. Use Amazon Elastic Container Service (Amazon ECS) for business logic with Amazon SQS long polling for retaining failed orders.",
      "B. Use AWS Elastic Beanstalk for web hosting with Amazon API Gateway for database API services. Use Amazon MQ for order queuing. Use AWS Step Functions for business logic with Amazon S3 Glacier Deep Archive for retaining failed orders.",
      "C. Use Amazon S3 for web hosting with AWS AppSync for database API services. Use Amazon Simple Queue Service (Amazon SQS) for order queuing. Use AWS Lambda for business logic with an Amazon SQS dead-letter queue for retaining failed orders.",
      "D. Use Amazon Lightsail for web hosting with AWS AppSync for database API services. Use Amazon Simple Email Service (Amazon SES) for order queuing. Use Amazon Elastic Kubernetes Service (Amazon EKS) for business logic with Amazon OpenSearch Service for retaining failed orders."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C because it meets all the key requirements of the problem statement:\n\n- **Web Hosting**: Amazon S3 is a highly scalable and cost-effective solution for static web hosting.\n- **Database API Services**: AWS AppSync is a fully managed GraphQL service that can efficiently handle the database API needs.\n- **Order Queuing**: Amazon SQS is a reliable and scalable message queuing service that can be used for order queuing.\n- **Business Logic**: AWS Lambda is a serverless compute service that can be used to implement the business logic, providing a scalable and cost-effective solution.\n- **Retaining Failed Orders**: The use of an Amazon SQS dead-letter queue (DLQ) allows for the retention of failed orders, ensuring no order is lost.\n\n2. Explanations of why the other choices are incorrect:\n\nA. This solution uses Amazon ECS (Elastic Container Service) for business logic, which is a more complex and potentially more costly solution compared to the serverless AWS Lambda approach in the correct answer.\n\nB. This solution uses AWS Step Functions for business logic, which may be overkill for the given requirements. Additionally, the use of Amazon MQ for order queuing and Amazon S3 Glacier Deep Archive for retaining failed orders is not the most optimal and cost-effective solution compared to the correct answer.\n\nD. This solution uses Amazon Lightsail for web hosting, which is a more limited and less scalable solution compared to the Amazon S3 approach in the correct answer. Additionally, the use of Amazon SES for order queuing and Amazon EKS for business logic are not the most appropriate choices for the given requirements."
  },
  "272": {
    "question": "A company hosts a web application on AWS in the us-east-1 Region. The application servers are distributed across three Availability Zones behind an Application Load Balancer. The database is hosted in a MySQL database on an Amazon EC2 instance. A solutions architect needs to design a cross-Region data recovery solution using AWS services with an RTO of less than 5 minutes and an RPO of less than 1 minute. The solutions architect is deploying application servers in us-west-2, and has configured Amazon Route 53 health checks and DNS failover to us-west-2.Which additional step should the solutions architect take?",
    "choices": [
      "A. Migrate the database to an Amazon RDS for MySQL instance with a cross-Region read replica in us-west-2.",
      "B. Migrate the database to an Amazon Aurora global database with the primary in us-east-1 and the secondary in us-west-2.",
      "C. Migrate the database to an Amazon RDS for MySQL instance with a Multi-AZ deployment.",
      "D. Create a MySQL standby database on an Amazon EC2 instance in us-west-2."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\nThe correct answer is B. Migrate the database to an Amazon Aurora global database with the primary in us-east-1 and the secondary in us-west-2.\n\nAmazon Aurora Global Database provides a cross-Region data recovery solution that meets the given requirements of an RTO of less than 5 minutes and an RPO of less than 1 minute. \n\nAurora Global Database automatically replicates data across Regions, providing a secondary read-only copy of the database in the us-west-2 Region. In the event of a failure in the primary Region (us-east-1), the application can automatically failover to the secondary Region (us-west-2) in less than 60 seconds, meeting the RTO requirement. Additionally, the RPO is less than 1 minute due to the near-real-time data replication between the two Regions.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Migrate the database to an Amazon RDS for MySQL instance with a cross-Region read replica in us-west-2:\nThis option does not meet the RTO requirement of less than 5 minutes. Failover to a cross-Region read replica typically takes longer (around 5-10 minutes) due to the time required to promote the replica to become the new primary.\n\nC. Migrate the database to an Amazon RDS for MySQL instance with a Multi-AZ deployment:\nA Multi-AZ deployment of Amazon RDS for MySQL provides high availability within a single Region, but does not meet the requirement for a cross-Region data recovery solution. The RTO for failing over to a standby in a Multi-AZ deployment is typically 60-120 seconds, which does not meet the RTO requirement of less than 5 minutes.\n\nD. Create a MySQL standby database on an Amazon EC2 instance in us-west-2:\nThis option does not provide the automatic failover and data replication capabilities required to meet the RTO and RPO requirements. Manually setting up and maintaining a standby database on an EC2 instance would not be able to achieve the specified RTO and RPO goals."
  },
  "273": {
    "question": "A company is using AWS Organizations to manage multiple accounts. Due to regulatory requirements, the company wants to restrict specific member accounts to certain AWS Regions, where they are permitted to deploy resources. The resources in the accounts must be tagged, enforced based on a group standard, and centrally managed with minimal configuration.What should a solutions architect do to meet these requirements?",
    "choices": [
      "A. Create an AWS Config rule in the specific member accounts to limit Regions and apply a tag policy.",
      "B. From the AWS Billing and Cost Management console, in the management account, disable Regions for the specific member accounts and apply a tag policy on the root.",
      "C. Associate the specific member accounts with the root. Apply a tag policy and an SCP using conditions to limit Regions.",
      "D. Associate the specific member accounts with a new OU. Apply a tag policy and an SCP using conditions to limit Regions."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (D):\n\nThe correct answer is D. Associate the specific member accounts with a new OU, apply a tag policy, and use an SCP (Service Control Policy) with conditions to limit the AWS Regions that can be used.\n\nThis is the best approach because:\n\n- Creating a new OU (Organizational Unit) allows you to isolate the specific member accounts that require region restrictions and tag enforcement.\n- Applying a tag policy at the OU level ensures that the required tagging standards are enforced across all resources in the accounts within that OU.\n- Using an SCP with conditions to limit the allowed Regions restricts the member accounts to only deploy resources in the permitted Regions, meeting the regulatory requirements.\n- This approach allows for centralized management of the policies, with minimal configuration required in the individual member accounts.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Create an AWS Config rule in the specific member accounts to limit Regions and apply a tag policy.\n- This approach would require manual configuration in each member account, which goes against the requirement for centralized management.\n- AWS Config rules are not the appropriate mechanism to enforce Region restrictions. SCPs should be used for that purpose.\n\nB. From the AWS Billing and Cost Management console, in the management account, disable Regions for the specific member accounts and apply a tag policy on the root.\n- Disabling Regions from the Billing console is not the recommended way to enforce Region restrictions. SCPs should be used for this purpose.\n- Applying a tag policy at the root level may not provide the same level of control and isolation as applying it at the OU level.\n\nC. Associate the specific member accounts with the root. Apply a tag policy and an SCP using conditions to limit Regions.\n- Associating the specific member accounts directly with the root may not provide the same level of isolation and management as creating a new OU.\n- While this approach could work, using a dedicated OU (as in the correct answer) provides more flexibility and granularity in managing the specific member accounts."
  },
  "274": {
    "question": "A company has an application that generates reports and stores them in an Amazon S3 bucket. When a user accesses their report, the application generates a signed URL to allow the user to download the report. The company's security team has discovered that the files are public and that anyone can download them without authentication. The company has suspended the generation of new reports until the problem is resolved.Which set of actions will immediately remediate the security issue without impacting the application's normal workflow?",
    "choices": [
      "A. Create an AWS Lambda function that applies a deny all policy for users who are not authenticated. Create a scheduled event to invoke the Lambda function.",
      "B. Review the AWS Trusted Advisor bucket permissions check and implement the recommended actions.",
      "C. Run a script that puts a private ACL on all of the objects in the bucket.",
      "D. Use the Block Public Access feature in Amazon S3 to set the IgnorePublicAcIs option to TRUE on the bucket."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (D):\n\nThe correct answer is D. Use the Block Public Access feature in Amazon S3 to set the IgnorePublicAcls option to TRUE on the bucket.\n\nThis is the right solution because it immediately remediates the security issue without impacting the application's normal workflow. By setting the IgnorePublicAcls option to TRUE, the bucket will ignore any public ACLs applied to the objects, effectively making them private. This will prevent anyone from accessing the reports without proper authentication, even if the objects have public ACLs.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Create an AWS Lambda function that applies a deny all policy for users who are not authenticated. Create a scheduled event to invoke the Lambda function.\nThis is not the right solution because it involves creating a custom Lambda function and a scheduled event to apply the policy. This would be more complex and time-consuming than the simpler solution provided in the correct answer (D).\n\nB. Review the AWS Trusted Advisor bucket permissions check and implement the recommended actions.\nThis is not the immediate solution to the problem. Reviewing the Trusted Advisor recommendations and implementing them may take time, and the security issue would still be present during that period.\n\nC. Run a script that puts a private ACL on all of the objects in the bucket.\nThis is not the right solution because running a script to update the ACLs on all existing objects is a temporary and potentially disruptive solution. It doesn't address the root cause, which is the public accessibility of the files. Additionally, any new files uploaded after the script is executed would still be publicly accessible, making this a suboptimal solution."
  },
  "275": {
    "question": "A company is planning to migrate an Amazon RDS for Oracle database to an RDS for PostgreSQL DB instance in another AWS account. A solutions architect needs to design a migration strategy that will require no downtime and that will minimize the amount of time necessary to complete the migration. The migration strategy must replicate all existing data and any new data that is created during the migration. The target database must be identical to the source database at completion of the migration process.All applications currently use an Amazon Route 53 CNAME record as their endpoint for communication with the RDS for Oracle DB instance. The RDS for Oracle DB instance is in a private subnet.Which combination of steps should the solutions architect take to meet these requirements? (Choose three.)",
    "choices": [
      "A. Create a new RDS for PostgreSQL DB instance in the target account. Use the AWS Schema Conversion Tool (AWS SCT) to migrate the database schema from the source database to the target database.",
      "B. Use the AWS Schema Conversion Tool (AWS SCT) to create a new RDS for PostgreSQL DB instance in the target account with the schema and initial data from the source database.",
      "C. Configure VPC peering between the VPCs in the two AWS accounts to provide connectivity to both DB instances from the target account. Configure the security groups that are attached to each DB instance to allow traffic on the database port from the VPC in the target account.",
      "D. Temporarily allow the source DB instance to be publicly accessible to provide connectivity from the VPC in the target account. Configure the security groups that are attached to each DB instance to allow traffic on the database port from the VPC in the target account.",
      "E. Use AWS Database Migration Service (AWS DMS) in the target account to perform a full load plus change data capture (CDC) migration from the source database to the target database. When the migration is complete, change the CNAME record to point to the target DB instance endpoint.",
      "F. Use AWS Database Migration Service (AWS DMS) in the target account to perform a change data capture (CDC) migration from the source database to the target database. When the migration is complete, change the CNAME record to point to the target DB instance endpoint."
    ],
    "answer": "ACE",
    "explanation": "1. Explanation of the correct answer (ACE):\n\nA. Create a new RDS for PostgreSQL DB instance in the target account. Use the AWS Schema Conversion Tool (AWS SCT) to migrate the database schema from the source database to the target database.\nThis is a necessary step to set up the target PostgreSQL database with the correct schema. The AWS SCT tool can be used to automate the schema conversion process from Oracle to PostgreSQL.\n\nC. Configure VPC peering between the VPCs in the two AWS accounts to provide connectivity to both DB instances from the target account. Configure the security groups that are attached to each DB instance to allow traffic on the database port from the VPC in the target account.\nThis is required to enable network connectivity between the source Oracle database and the target PostgreSQL database in the different AWS accounts. VPC peering and security group configurations ensure the migration process can access both databases.\n\nE. Use AWS Database Migration Service (AWS DMS) in the target account to perform a full load plus change data capture (CDC) migration from the source database to the target database. When the migration is complete, change the CNAME record to point to the target DB instance endpoint.\nAWS DMS enables a zero-downtime migration by performing a full data load from the source to the target database, followed by a CDC process to capture and apply any changes made to the source database during the migration. This ensures the target database is an exact replica of the source at the end of the migration. Updating the CNAME record to point to the new PostgreSQL endpoint completes the migration.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. Use the AWS Schema Conversion Tool (AWS SCT) to create a new RDS for PostgreSQL DB instance in the target account with the schema and initial data from the source database.\nThe AWS SCT can only migrate the schema, not create the RDS instance itself. Creating the RDS instance is a separate step.\n\nD. Temporarily allow the source DB instance to be publicly accessible to provide connectivity from the VPC in the target account. Configure the security groups that are attached to each DB instance to allow traffic on the database port from the VPC in the target account.\nMaking the source database publicly accessible, even temporarily, is a security risk and should be avoided. The correct approach"
  },
  "276": {
    "question": "A company has implemented an ordering system using an event-driven architecture. During initial testing, the system stopped processing orders. Further log analysis revealed that one order message in an Amazon Simple Queue Service (Amazon SQS) standard queue was causing an error on the backend and blocking all subsequent order messages. The visibility timeout of the queue is set to 30 seconds, and the backend processing timeout is set to 10 seconds. A solutions architect needs to analyze faulty order messages and ensure that the system continues to process subsequent messages.Which step should the solutions architect take to meet these requirements?",
    "choices": [
      "A. Increase the backend processing timeout to 30 seconds to match the visibility timeout.",
      "B. Reduce the visibility timeout of the queue to automatically remove the faulty message.",
      "C. Configure a new SQS FIFO queue as a dead-letter queue to isolate the faulty messages.",
      "D. Configure a new SQS standard queue as a dead-letter queue to isolate the faulty messages."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (D):\n\nThe correct answer is D. Configure a new SQS standard queue as a dead-letter queue to isolate the faulty messages.\n\nThe key reasons why this is the correct step are:\n\n- The question specifies that the existing queue is an SQS standard queue. As per the AWS documentation, \"the dead-letter queue of a standard queue must also be a standard queue.\"\n- Configuring a new SQS standard queue as a dead-letter queue allows the system to isolate the faulty message and continue processing subsequent messages in the main queue without interruption.\n- This approach ensures that the ordering of messages is preserved, as a standard queue does not have the strict FIFO ordering requirements of an SQS FIFO queue.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Increase the backend processing timeout to 30 seconds to match the visibility timeout:\nThis would not solve the issue, as the problem is not related to the processing timeout, but rather the faulty message causing an error on the backend. Increasing the timeout would just delay the error, not resolve it.\n\nB. Reduce the visibility timeout of the queue to automatically remove the faulty message:\nReducing the visibility timeout would not be the correct approach, as it would not isolate the faulty message. The message would still be reprocessed and cause the same error, potentially blocking the entire queue.\n\nC. Configure a new SQS FIFO queue as a dead-letter queue to isolate the faulty messages:\nThis is incorrect because the existing queue is a standard queue, and as per the AWS documentation, \"the dead-letter queue of a standard queue must also be a standard queue.\" Configuring a FIFO queue as the dead-letter queue would not be the correct solution."
  },
  "277": {
    "question": "A company has automated the nightly retraining of its machine learning models by using AWS Step Functions. The workflow consists of multiple steps that use AWS Lambda. Each step can fail for various reasons, and any failure causes a failure of the overall workflow.A review reveals that the retraining has failed multiple nights in a row without the company noticing the failure. A solutions architect needs to improve the workflow so that notifications are sent for all types of failures in the retraining process.Which combination of steps should the solutions architect take to meet these requirements? (Choose three.)",
    "choices": [
      "A. Create an Amazon Simple Notification Service (Amazon SNS) topic with a subscription of type \"Email\" that targets the team's mailing list.",
      "B. Create a task named \"Email\" that forwards the input arguments to the SNS topic.",
      "C. Add a Catch field to all Task, Map, and Parallel states that have a statement of \"ErrorEquals\": [ \"States.ALL\" ] and \"Next\u201d: \"Email\".",
      "D. Add a new email address to Amazon Simple Email Service (Amazon SES). Verify the email address.",
      "E. Create a task named \"Email\" that forwards the input arguments to the SES email address.",
      "F. Add a Catch field to all Task, Map, and Parallel states that have a statement of \"ErrorEquals\": [ \"States.Runtime\" ] and \"Next\": \"Email\"."
    ],
    "answer": "ABC",
    "explanation": "1. Explanation of the correct answer (A, B, C):\n\nA. Create an Amazon SNS topic with an email subscription: This allows the team to receive notifications whenever the retraining workflow fails, as SNS can be used to trigger email notifications.\n\nB. Create a \"Email\" task that forwards the input arguments to the SNS topic: This task can be added at the end of the workflow to send the notification after a failure has occurred.\n\nC. Add a Catch field to all Task, Map, and Parallel states with \"ErrorEquals\": [\"States.ALL\"] and \"Next\": \"Email\": This ensures that the \"Email\" task is executed whenever any step in the workflow fails, regardless of the specific error type. The \"States.ALL\" error matches all possible errors that can occur.\n\n2. Explanations of the incorrect choices:\n\nD. Add a new email address to Amazon SES and verify it: This is not necessary, as the SNS topic can be used to send emails directly without the need for SES.\n\nE. Create a \"Email\" task that forwards the input arguments to the SES email address: This is incorrect because it does not leverage the SNS topic, which is a more appropriate solution for sending notifications in this case.\n\nF. Add a Catch field to all Task, Map, and Parallel states with \"ErrorEquals\": [\"States.Runtime\"] and \"Next\": \"Email\": This is incorrect because it only catches \"States.Runtime\" errors, which is a subset of the possible errors that can occur. The requirement is to catch all types of failures, so \"States.ALL\" is the appropriate choice."
  },
  "278": {
    "question": "A company plans to deploy a new private intranet service on Amazon EC2 instances inside a VPC. An AWS Site-to-Site VPN connects the VPC to the company's on-premises network. The new service must communicate with existing on-premises services. The on-premises services are accessible through the use of hostnames that reside in the company.example DNS zone. This DNS zone is wholly hosted on premises and is available only on the company's private network.A solutions architect must ensure that the new service can resolve hostnames on the company.example domain to integrate with existing services.Which solution meets these requirements?",
    "choices": [
      "A. Create an empty private zone in Amazon Route 53 for company.example. Add an additional NS record to the company's on-premises company.example zone that points to the authoritative name servers for the new private zone in Route 53.",
      "B. Turn on DNS hostnames for the VPC. Configure a new outbound endpoint with Amazon Route 53 Resolver. Create a Resolver rule to forward requests for company.example to the on-premises name servers.",
      "C. Turn on DNS hostnames for the VPConfigure a new inbound resolver endpoint with Amazon Route 53 Resolver. Configur&the on-premises DNS server to forward requests for company.example to the new resolver.",
      "D. Use AWS Systems Manager to configure a run document that will install a hosts file that contains any required hostnames. Use an Amazon EventBridge rule to run the document when an instance is entering the running state."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (Option B):\n\nOption B is the correct answer because it leverages Amazon Route 53 Resolver to handle the DNS resolution between the VPC and the on-premises network. The key steps are:\n\n- Turn on DNS hostnames for the VPC, which enables the EC2 instances to have DNS resolution capabilities.\n- Configure a new outbound endpoint with Amazon Route 53 Resolver. This allows the VPC to resolve DNS queries for external domains, including the company.example domain hosted on-premises.\n- Create a Resolver rule to forward requests for the company.example domain to the on-premises name servers. This ensures that the VPC-based service can resolve hostnames in the company.example DNS zone, which is only available on the company's private network.\n\n2. Explanations of why the other options are incorrect:\n\nOption A is incorrect because it involves creating an empty private zone in Amazon Route 53 for the company.example domain. This does not address the requirement that the company.example DNS zone is wholly hosted on-premises and only available on the company's private network.\n\nOption C is incorrect because it involves configuring a new inbound resolver endpoint with Amazon Route 53 Resolver. This approach would allow the on-premises DNS server to query the VPC's DNS server, which is the opposite of the requirement, where the VPC-based service needs to resolve hostnames in the company.example domain.\n\nOption D is incorrect because it involves using AWS Systems Manager to configure a hosts file on the EC2 instances. This is not a scalable or flexible solution, as it requires manual management of the hosts file on each instance. It also does not leverage the capabilities of Amazon Route 53 Resolver to handle the DNS resolution between the VPC and the on-premises network."
  },
  "279": {
    "question": "A company uses AWS CloudFormation to deploy applications within multiple VPCs that are all attached to a transit gateway. Each VPC that sends traffic to the public internet must send the traffic through a shared services VPC. Each subnet within a VPC uses the default VPC route table, and the traffic is routed to the transit gateway. The transit gateway uses its default route table for any VPC attachment.A security audit reveals that an Amazon EC2 instance that is deployed within a VPC can communicate with an EC2 instance that is deployed in any of the company's other VPCs. A solutions architect needs to limit the traffic between the VPCs. Each VPC must be able to communicate only with a predefined, limited set of authorized VPCs.What should the solutions architect do to meet these requirements?",
    "choices": [
      "A. Update the network ACL of each subnet within a VPC to allow outbound traffic only to the authorized VPCs. Remove all deny rules except the default deny rule.",
      "B. Update all the security groups that are used within a VPC to deny outbound traffic to security groups that are used within the unauthorized VPCs.",
      "C. Create a dedicated transit gateway route table for each VPC attachment. Route traffic only to the authorized VPCs.",
      "D. Update the main route table of each VPC to route traffic only to the authorized VPCs through the transit gateway."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C, which suggests creating a dedicated transit gateway route table for each VPC attachment. This is the best approach to meet the requirement of limiting traffic between VPCs and allowing communication only with a predefined, limited set of authorized VPCs.\n\nThe key reasons why this is the correct solution:\n\n- By creating a dedicated transit gateway route table for each VPC attachment, the solutions architect can precisely control the routing of traffic between VPCs. This allows them to specify the authorized VPCs that each VPC can communicate with, effectively limiting the communication to only the desired set of VPCs.\n- The default transit gateway route table is used for all VPC attachments, which is why the current setup allows EC2 instances in any VPC to communicate with each other. Creating separate route tables for each VPC attachment breaks this default behavior and enables the architect to customize the routing rules.\n- This solution provides a flexible and granular approach to controlling inter-VPC communication, as the routing rules can be easily updated for each VPC attachment as the network requirements change.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Updating the network ACL of each subnet within a VPC to allow outbound traffic only to the authorized VPCs is not the best solution. Network ACLs are stateless and operate at the subnet level, while the requirement is to control communication between VPCs, which is a higher-level concern that is better addressed at the VPC or transit gateway level.\n\nB. Updating all the security groups used within a VPC to deny outbound traffic to security groups used within the unauthorized VPCs is not the optimal solution. Security groups are applied at the instance level and cannot directly control communication between VPCs. This approach would be cumbersome to manage and maintain as the number of VPCs and instances grows.\n\nD. Updating the main route table of each VPC to route traffic only to the authorized VPCs through the transit gateway is not the best solution. This approach would require updating the route tables of all VPCs, which can be less efficient and flexible than creating dedicated transit gateway route tables for each VPC attachment. Additionally, it may not provide the same level of granular control over the routing rules as the correct solution."
  },
  "280": {
    "question": "A company has a Windows-based desktop application that is packaged and deployed to the users' Windows machines. The company recently acquired another company that has employees who primarily use machines with a Linux operating system. The acquiring company has decided to migrate and rehost the Windows-based desktop application to AWS.All employees must be authenticated before they use the application. The acquiring company uses Active Directory on premises but wants a simplified way to manage access to the application on AWS for all the employees.Which solution will rehost the application on AWS with the LEAST development effort?",
    "choices": [
      "A. Set up and provision an Amazon Workspaces virtual desktop for every employee. Implement authentication by using Amazon Cognito identity pools. Instruct employees to run the application from their provisioned Workspaces virtual desktops.",
      "B. Create an Auto Scaling group of Windows-based Amazon EC2 instances. Join each EC2 instance to the company\u2019s Active Directory domain. Implement authentication by using the Active Directory that is running on premises. Instruct employees to run the application by using a Windows remote desktop.",
      "C. Use an Amazon AppStream 2.0 image builder to create an image that includes the application and the required configurations. Provision an AppStream 2.0 On-Demand fleet with dynamic Fleet Auto Scaling policies for running the image. Implement authentication by using AppStream 2.0 user pools. Instruct the employees to access the application by starting browser-based AppStream 2.0 streaming sessions.",
      "D. Refactor and containerize the application to run as a web-based application. Run the application in Amazon Elastic Container Service (Amazon ECS) on AWS Fargate with step scaling policies. Implement authentication by using Amazon Cognito user pools. Instruct the employees to run the application from their browsers."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C because Amazon AppStream 2.0 is the best solution that requires the least development effort to migrate and rehost the Windows-based desktop application to AWS for all employees, including those using Linux machines.\n\nAppStream 2.0 allows you to package the desktop application as an image, which can then be provisioned and scaled on-demand using an AppStream 2.0 fleet. This eliminates the need to set up and manage individual virtual desktops (as in option A) or EC2 instances (as in option B), which would require more effort.\n\nAdditionally, AppStream 2.0 provides built-in authentication through user pools, which simplifies the access management process compared to integrating with the on-premises Active Directory (as in option B) or implementing Amazon Cognito (as in option D).\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option requires setting up and provisioning an Amazon Workspaces virtual desktop for every employee, which would be more complex and require more effort than the AppStream 2.0 solution.\n\nB. This option requires joining each EC2 instance to the company's on-premises Active Directory domain, which adds complexity and maintenance overhead compared to the AppStream 2.0 solution with user pool authentication.\n\nD. This option requires refactoring and containerizing the application to run as a web-based application, which would involve a significant development effort compared to the AppStream 2.0 solution that can directly use the existing Windows-based desktop application."
  },
  "281": {
    "question": "A company has registered 10 new domain names. The company uses the domains for online marketing. The company needs a solution that will redirect online visitors to a specific URL for each domain. All domains and target URLs are defined in a JSON document. All DNS records are managed by Amazon Route 53.A solutions architect must implement a redirect service that accepts HTTP and HTTPS requests.Which combination of steps should the solutions architect take to meet these requirements with the LEAST amount of operational effort? (Choose three.)",
    "choices": [
      "A. Create a dynamic webpage that runs on an Amazon EC2 instance. Configure the webpage to use the JSON document in combination with the event message to look up and respond with a redirect URL.",
      "B. Create an Application Load Balancer that includes HTTP and HTTPS listeners.",
      "C. Create an AWS Lambda function that uses the JSON document in combination with the event message to look up and respond with a redirect URL.",
      "D. Use an Amazon API Gateway API with a custom domain to publish an AWS Lambda function.",
      "E. Create an Amazon CloudFront distribution. Deploy a Lambda@Edge function.",
      "F. Create an SSL certificate by using AWS Certificate Manager (ACM). Include the domains as Subject Alternative Names."
    ],
    "answer": "CEF",
    "explanation": "1. Explanation of the correct answer (CEF):\n\nC. Create an AWS Lambda function that uses the JSON document in combination with the event message to look up and respond with a redirect URL.\nThis is the correct choice because an AWS Lambda function can handle the lookup and redirect logic without the need for a dedicated web server, which reduces operational effort. The Lambda function can leverage the provided JSON document to look up the target URLs for each domain and return the appropriate redirect response.\n\nE. Create an Amazon CloudFront distribution. Deploy a Lambda@Edge function.\nThis is also a correct choice because a CloudFront distribution with a Lambda@Edge function can handle the redirect logic in a serverless manner. The Lambda@Edge function can perform the same lookup and redirect functionality as the standalone Lambda function in choice C, but with the added benefit of being executed at the edge, closer to the user, which can improve latency and performance.\n\nF. Create an SSL certificate by using AWS Certificate Manager (ACM). Include the domains as Subject Alternative Names.\nThis is the third correct choice because it addresses the requirement of handling both HTTP and HTTPS requests. By creating an SSL certificate with the domains as Subject Alternative Names, the redirect service can securely handle HTTPS requests, which is necessary for the company's online marketing needs.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Create a dynamic webpage that runs on an Amazon EC2 instance. Configure the webpage to use the JSON document in combination with the event message to look up and respond with a redirect URL.\nThis is incorrect because it would require the setup and maintenance of an EC2 instance, which would increase the operational effort compared to a serverless solution like AWS Lambda.\n\nB. Create an Application Load Balancer that includes HTTP and HTTPS listeners.\nThis is incorrect because while an Application Load Balancer can handle the HTTP and HTTPS listeners, it does not provide the redirect logic on its own. The redirect functionality would still need to be implemented, either in a separate service (like the Lambda function in choice C) or in the backend (which would increase operational effort).\n\nD. Use an Amazon API Gateway API with a custom domain to publish an AWS Lambda function.\nThis is incorrect because it adds an additional service (API Gateway) that is not strictly necessary for the requirements. The redirect functionality can be implemented directly in"
  },
  "282": {
    "question": "A large company recently experienced an unexpected increase in Amazon RDS and Amazon DynamoDB costs. The company needs to increase visibility into details of AWS Billing and Cost Management. There are various accounts associated with AWS Organizations, including many development and production accounts. There is no consistent tagging strategy across the organization, but there are guidelines in place that require all infrastructure to be deployed using AWS CloudFormation with consistent tagging. Management requires cost center numbers and project ID numbers for all existing and future DynamoDB tables and RDS instances.Which strategy should the solutions architect provide to meet these requirements?",
    "choices": [
      "A. Use Tag Editor to tag existing resources. Create cost allocation tags to define the cost center and project ID and allow 24 hours for tags to propagate to existing resources.",
      "B. Use an AWS Config rule to alert the finance team of untagged resources. Create a centralized AWS Lambda based solution to tag untagged RDS databases and DynamoDB resources every hour using a cross-account role.",
      "C. Use Tag Editor to tag existing resources. Create cost allocation tags to define the cost center and project ID. Use SCPs to restrict resource creation that do not have the cost center and project ID on the resource.",
      "D. Create cost allocation tags to define the cost center and project ID and allow 24 hours for tags to propagate to existing resources. Update existing federated roles to restrict privileges to provision resources that do not include the cost center and project ID on the resource."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C because it addresses both the requirement to tag existing resources and the requirement to enforce consistent tagging for future resource creation.\n\nThe key aspects of this solution are:\n\n- Use Tag Editor to tag existing resources with the required cost center and project ID tags. This addresses the need to tag existing resources.\n- Create cost allocation tags to define the cost center and project ID. This ensures the desired tags are in place.\n- Use Service Control Policies (SCPs) in AWS Organizations to restrict resource creation that does not include the required cost center and project ID tags. This enforces the consistent tagging strategy for future resource deployments.\n\nThe SCP-based approach ensures that all new resources are provisioned with the required tags, meeting the management's requirement for cost center and project ID on all DynamoDB tables and RDS instances.\n\n2. Explanations of the incorrect choices:\n\nA. This option only addresses tagging of existing resources, but does not enforce the tagging requirement for future resource creation. It relies on the 24-hour tag propagation delay, which may not be suitable for the company's needs.\n\nB. This option involves creating a custom Lambda-based solution to retroactively tag untagged resources. While this can work, it is a more complex and ongoing maintenance-intensive solution compared to the SCP-based approach in the correct answer.\n\nD. This option updates the federated roles to restrict privileges, but does not address the need to tag existing resources. It also relies on the 24-hour tag propagation delay, similar to option A."
  },
  "283": {
    "question": "A company is planning to migrate an application from on premises to the AWS Cloud. The company will begin the migration by moving the application\u2019s underlying data storage to AWS. The application data is stored on a shared file system on premises, and the application servers connect to the shared file system through SMB.A solutions architect must implement a solution that uses an Amazon S3 bucket for shared storage. Until the application is fully migrated and code is rewritten to use native Amazon S3 APIs, the application must continue to have access to the data through SMB. The solutions architect must migrate the application data to AWS to its new location while still allowing the on-premises application to access the data.Which solution will meet these requirements?",
    "choices": [
      "A. Create a new Amazon FSx for Windows File Server file system. Configure AWS DataSync with one location for the on-premises file share and one location for the new Amazon FSx file system. Create a new DataSync task to copy the data from the on-premises file share location to the Amazon FSx file system.",
      "B. Create an S3 bucket for the application. Copy the data from the on-premises storage to the S3 bucket.",
      "C. Deploy an AWS Server Migration Service (AWS SMS) VM to the on-premises environment. Use AWS SMS to migrate the file storage server from on premises to an Amazon EC2 instance.",
      "D. Create an S3 bucket for the application. Deploy a new AWS Storage Gateway file gateway on an on-premises VM. Create a new file share that stores data in the S3 bucket and is associated with the file gateway. Copy the data from the on-premises storage to the new file gateway endpoint."
    ],
    "answer": "D",
    "explanation": "Explanation of the correct answer (Option D):\n\nOption D is the correct answer because it meets all the requirements of the scenario:\n\n1. Create an S3 bucket for the application: This provides a shared storage location for the application's data, as required by the question.\n\n2. Deploy a new AWS Storage Gateway file gateway on an on-premises VM: The file gateway allows the on-premises application to access the data in the S3 bucket through the SMB protocol, which is required to maintain access to the data during the migration.\n\n3. Create a new file share that stores data in the S3 bucket and is associated with the file gateway: This creates a bridge between the on-premises file system and the S3 bucket, allowing the application to continue accessing the data through the familiar SMB protocol.\n\n4. Copy the data from the on-premises storage to the new file gateway endpoint: This ensures that all the application data is migrated to the new shared storage location in S3.\n\nExplanations of why the other options are incorrect:\n\nOption A:\n- This option uses Amazon FSx for Windows File Server, which is a managed Windows file system, instead of the required S3 bucket.\n- The question specifically states that the solution must use an S3 bucket for shared storage.\n\nOption B:\n- This option simply copies the data from the on-premises storage to the S3 bucket, but it does not provide a solution for the on-premises application to continue accessing the data through SMB.\n- The question requires a solution that allows the on-premises application to access the data through SMB until the application is fully migrated.\n\nOption C:\n- This option uses AWS Server Migration Service (AWS SMS) to migrate the file storage server from on-premises to an Amazon EC2 instance, but it does not leverage S3 for shared storage as required by the question.\n- The question specifically states that the solution must use an S3 bucket for shared storage."
  },
  "284": {
    "question": "A company operates quick-service restaurants. The restaurants follow a predictable model with high sales traffic for 4 hours daily. Sales traffic is lower outside of those peak hours.The point of sale and management platform is deployed in the AWS Cloud and has a backend that is based on Amazon DynamoDB. The database table uses provisioned throughput mode with 100,000 RCUs and 80,000 WCUs to match known peak resource consumption.The company wants to reduce its DynamoDB cost and minimize the operational overhead for the IT staff.Which solution meets these requirements MOST cost-effectively?",
    "choices": [
      "A. Reduce the provisioned RCUs and WCUs.",
      "B. Change the DynamoDB table to use on-demand capacity.",
      "C. Enable Dynamo DB auto scaling for the table.",
      "D. Purchase 1-year reserved capacity that is sufficient to cover the peak load for 4 hours each day."
    ],
    "answer": "C",
    "explanation": "1. Clear explanation of the correct answer:\n\nThe correct answer is B. Change the DynamoDB table to use on-demand capacity.\n\nThis is the most cost-effective solution because the question states that the company has a predictable model with high sales traffic for 4 hours daily and lower traffic outside of those peak hours. With this usage pattern, on-demand capacity is more cost-effective than maintaining a high provisioned capacity all the time.\n\nThe on-demand capacity mode in DynamoDB automatically adjusts the provisioned throughput based on the actual usage, so the company only pays for the resources it consumes. This eliminates the need to over-provision resources to handle the peak load, which can be costly during the off-peak hours.\n\n2. Explanations of why each incorrect choice is wrong:\n\nA. Reduce the provisioned RCUs and WCUs:\nThis is not the most cost-effective solution because the question states the current provisioned capacity of 100,000 RCUs and 80,000 WCUs is set to match the known peak resource consumption. Reducing the provisioned capacity would not be sufficient to handle the peak loads and could lead to throttling during those periods.\n\nC. Enable DynamoDB auto scaling for the table:\nWhile auto scaling can be a good solution, it is not the most cost-effective in this case. The question states that the current provisioned capacity is set to match the known peak resource consumption. Auto scaling would still require maintaining a high provisioned capacity to handle the peak loads, which would be costly during the off-peak hours.\n\nD. Purchase 1-year reserved capacity that is sufficient to cover the peak load for 4 hours each day:\nThis is not the most cost-effective solution because it requires the company to purchase reserved capacity for the entire day, even though the peak load only lasts for 4 hours. The company would be paying for unused capacity during the off-peak hours, which would be more costly than the on-demand capacity approach."
  },
  "285": {
    "question": "A company hosts a blog post application on AWS using Amazon API Gateway, Amazon DynamoDB, and AWS Lambda. The application currently does not use API keys to authorize requests. The API model is as follows:GET /posts/{postId}: to get post detailsGET /users/{userId}: to get user detailsGET /comments/{commentId}: to get comments detailsThe company has noticed users are actively discussing topics in the comments section, and the company wants to increase user engagement by making the comments appear in real time.Which design should be used to reduce comment latency and improve user experience?",
    "choices": [
      "A. Use edge-optimized API with Amazon CloudFront to cache API responses.",
      "B. Modify the blog application code to request GET/comments/{commentId} every 10 seconds.",
      "C. Use AWS AppSync and leverage WebSockets to deliver comments.",
      "D. Change the concurrency limit of the Lambda functions to lower the API response time."
    ],
    "answer": "C",
    "explanation": "1. Explanation for the correct answer (C):\nThe correct answer is C. Use AWS AppSync and leverage WebSockets to deliver comments.\n\nAWS AppSync is a fully managed GraphQL service that simplifies the development of real-time and offline-capable applications. By using AppSync and leveraging the WebSockets protocol, the blog application can deliver comments in real-time, reducing comment latency and improving the user experience. WebSockets provide a persistent, bidirectional communication channel between the client and the server, allowing immediate updates to be pushed to the clients as new comments are posted, without the need for the clients to repeatedly poll the API.\n\n2. Explanations for the incorrect choices:\n\nA. Use edge-optimized API with Amazon CloudFront to cache API responses.\nThis option is not suitable for delivering real-time comments. CloudFront is a content delivery network (CDN) that caches static content, but it does not provide real-time data synchronization capabilities. The comments would still be subject to the latency of the underlying API calls, and users would not see the comments appear instantly as they are posted.\n\nB. Modify the blog application code to request GET/comments/{commentId} every 10 seconds.\nThis option is also not suitable for delivering real-time comments. Polling the API every 10 seconds would still result in a delayed user experience, as the comments would only be updated at fixed intervals, rather than being pushed in real-time. Frequent polling can also put additional load on the API and the underlying infrastructure.\n\nD. Change the concurrency limit of the Lambda functions to lower the API response time.\nAdjusting the concurrency limit of the Lambda functions may help reduce the API response time, but it does not address the real-time requirement for the comments. The comments would still be subject to the latency of the underlying API calls, and users would not see the comments appear instantly as they are posted."
  },
  "286": {
    "question": "A company manages hundreds of AWS accounts centrally in an organization in AWS Organizations. The company recently started to allow product teams to create and manage their own S3 access points in their accounts. The S3 access points can be accessed only within VPCs, not on the internet.What is the MOST operationally efficient way to enforce this requirement?",
    "choices": [
      "A. Set the S3 access point resource policy to deny the s3:CreateAccessPoint action unless the s3:AccessPointNetworkOrigin condition key evaluates to VPC.",
      "B. Create an SCP at the root level in the organization to deny the s3:CreateAccessPoint action unless the s3:AccessPointNetworkOrigin condition key evaluates to VPC.",
      "C. Use AWS CloudFormation StackSets to create a new IAM policy in each AWS account that allows the s3:CreateAccessPoint action only if the s3:AccessPointNetworkOrigin condition key evaluates to VPC.",
      "D. Set the S3 bucket policy to deny the s3:CreateAccessPoint action unless the s3:AccessPointNetworkOrigin condition key evaluates to VPC."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B: Create an SCP at the root level in the organization to deny the s3:CreateAccessPoint action unless the s3:AccessPointNetworkOrigin condition key evaluates to VPC.\n\nThis is the most operationally efficient way to enforce the requirement because:\n\n- The company manages hundreds of AWS accounts centrally in an AWS Organization, so a centralized policy at the organizational level is the most efficient approach.\n- Using an SCP (Service Control Policy) at the root level of the organization allows the policy to be applied to all member accounts, ensuring the requirement is enforced consistently across the organization.\n- The SCP can be used to deny the s3:CreateAccessPoint action unless the s3:AccessPointNetworkOrigin condition key evaluates to VPC, which directly implements the requirement that S3 access points can only be accessed within VPCs.\n- This approach is more efficient than managing individual IAM policies in each account or using CloudFormation StackSets, as it provides a single, centralized point of control and enforcement.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Set the S3 access point resource policy to deny the s3:CreateAccessPoint action unless the s3:AccessPointNetworkOrigin condition key evaluates to VPC:\n   - This approach is not the most operationally efficient because it would need to be applied to each individual S3 access point, rather than a centralized policy at the organizational level.\n   - Managing individual resource policies across hundreds of accounts would be more complex and time-consuming than a single organizational-level SCP.\n\nC. Use AWS CloudFormation StackSets to create a new IAM policy in each AWS account that allows the s3:CreateAccessPoint action only if the s3:AccessPointNetworkOrigin condition key evaluates to VPC:\n   - This approach is less efficient than the SCP-based solution because it requires managing and deploying the IAM policy to each individual account, rather than a single organizational-level policy.\n   - Using CloudFormation StackSets adds additional complexity and overhead compared to a straightforward SCP.\n\nD. Set the S3 bucket policy to deny the s3:"
  },
  "287": {
    "question": "A solutions architect must update an application environment within AWS Elastic Beanstalk using a blue/green deployment methodology. The solutions architect creates an environment that is identical to the existing application environment and deploys the application to the new environment.What should be done next to complete the update?",
    "choices": [
      "A. Redirect to the new environment using Amazon Route 53.",
      "B. Select the Swap Environment URLs option.",
      "C. Replace the Auto Scaling launch configuration.",
      "D. Update the DNS records to point to the green environment."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer:\n\nThe correct answer is B. Select the Swap Environment URLs option.\n\nIn a blue/green deployment strategy, a new environment (green) is created that mirrors the existing production environment (blue). Once the new environment is ready, the traffic is switched from the blue environment to the green environment. This is typically done by swapping the environment URLs, which updates the DNS records to point to the new environment.\n\nThe Swap Environment URLs option in AWS Elastic Beanstalk is the correct way to complete the blue/green deployment process. This operation seamlessly switches the URLs of the two environments, redirecting traffic from the old (blue) environment to the new (green) environment without any downtime.\n\n2. Explanations of the incorrect choices:\n\nA. Redirect to the new environment using Amazon Route 53:\nThis is not the correct approach for a blue/green deployment in Elastic Beanstalk. The Swap Environment URLs option is the recommended way to switch traffic between the two environments, as it updates the DNS records automatically.\n\nC. Replace the Auto Scaling launch configuration:\nReplacing the Auto Scaling launch configuration is not the correct step to complete the blue/green deployment. The launch configuration is used to define the EC2 instances that make up the Elastic Beanstalk environment. It is not directly related to the process of swapping the environment URLs.\n\nD. Update the DNS records to point to the green environment:\nManually updating the DNS records to point to the green environment is not the correct approach. The Swap Environment URLs option in Elastic Beanstalk handles the DNS update automatically, ensuring a seamless transition between the blue and green environments."
  },
  "288": {
    "question": "A company is building an image service on the web that will allow users to upload and search random photos. At peak usage, up to 10,000 users worldwide will upload their images. The will then overlay text on the uploaded images, which will then be published on the company website.Which design should a solutions architect implement?",
    "choices": [
      "A. Store the uploaded images in Amazon Elastic File System (Amazon EFS). Send application log information about each image to Amazon CloudWatch Logs. Create a fleet of Amazon EC2 instances that use CloudWatch Logs to determine which images need to be processed. Place processed images in another directory in Amazon EFS. Enable Amazon CloudFront and configure the origin to be the one of the EC2 instances in the fleet.",
      "B. Store the uploaded images in an Amazon S3 bucket and configure an S3 bucket event notification to send a message to Amazon Simple Notification Service (Amazon SNS). Create a fleet of Amazon EC2 instances behind an Application Load Balancer (ALB) to pull messages from Amazon SNS to process the images and place them in Amazon Elastic File System (Amazon EFS). Use Amazon CloudWatch metrics for the SNS message volume to scale out EC2 instances. Enable Amazon CloudFront and configure the origin to be the ALB in front of the EC2 instances.",
      "C. Store the uploaded images in an Amazon S3 bucket and configure an S3 bucket event notification to send a message to the Amazon Simple Queue Service (Amazon SQS) queue. Create a fleet of Amazon EC2 instances to pull messages from the SQS queue to process the images and place them in another S3 bucket. Use Amazon CloudWatch metrics for queue depth to scale out EC2 instances. Enable Amazon CloudFront and configure the origin to be the S3 bucket that contains the processed images.",
      "D. Store the uploaded images on a shared Amazon Elastic Block Store (Amazon EBS) volume mounted to a fleet of Amazon EC2 Spot instances. Create an Amazon DynamoDB table that contains information about each uploaded image and whether it has been processed. Use an Amazon EventBridge rule to scale out EC2 instances. Enable Amazon CloudFront and configure the origin to reference an Elastic Load Balancer in front of the fleet of EC2 instances."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (Option C):\n\nOption C is the correct choice because it leverages the most appropriate AWS services for the given use case:\n\n- Storing the uploaded images in an Amazon S3 bucket provides highly scalable and durable storage, which is well-suited for handling the expected 10,000 concurrent users.\n- Configuring an S3 bucket event notification to send a message to an Amazon SQS queue decouples the image upload process from the processing of the images. This allows the system to scale independently and handle the high volume of image uploads efficiently.\n- Creating a fleet of Amazon EC2 instances to pull messages from the SQS queue and process the images is a reliable and scalable approach. The EC2 instances can then store the processed images in another S3 bucket.\n- Using Amazon CloudWatch metrics to scale out the EC2 instances based on the depth of the SQS queue ensures that the system can handle the expected peak usage.\n- Enabling Amazon CloudFront and configuring the origin to be the S3 bucket with the processed images improves the global availability and performance of the image delivery, which is crucial for a web-based image service.\n\n2. Explanations of why the incorrect choices are wrong:\n\nOption A:\n- Using Amazon EFS for storing the uploaded images is not the most suitable choice, as S3 is a better fit for handling large volumes of data and providing durability.\n- Sending application log information to CloudWatch Logs and using it to determine which images need to be processed is an unnecessary overhead, as the SQS queue in Option C provides a more efficient way to distribute the workload.\n- Placing the processed images in another directory in EFS is not the most scalable approach, as S3 is better suited for storing and serving the processed images.\n\nOption B:\n- Using an Application Load Balancer (ALB) in front of the EC2 instances is an unnecessary complexity, as the SQS queue in Option C provides a more efficient way to distribute the workload.\n- Configuring the CloudFront origin to be the ALB is less efficient than directly using the S3 bucket with the processed images, as it adds an additional layer of indirection.\n\nOption D:\n- Using a"
  },
  "289": {
    "question": "A company has deployed its database on an Amazon RDS for MySQL DB instance in the us-east-1 Region. The company needs to make its data available to customers in Europe. The customers in Europe must have access to the same data as customers in the United States (US) and will not tolerate high application latency or stale data. The customers in Europe and the customers in the US need to write to the database. Both groups of customers need to see updates from the other group in real time.Which solution will meet these requirements?",
    "choices": [
      "A. Create an Amazon Aurora MySQL replica of the RDS for MySQL DB instance. Pause application writes to the RDS DB instance. Promote the Aurora Replica to a standalone DB cluster. Reconfigure the application to use the Aurora database and resume writes. Add eu-west-1 as a secondary Region to the DB cluster. Enable write forwarding on the DB cluster. Deploy the application in eu-west-1. Configure the application to use the Aurora MySQL endpoint in eu-west-1.",
      "B. Add a cross-Region replica in eu-west-1 for the RDS for MySQL DB instance. Configure the replica to replicate write queries back to the primary DB instance. Deploy the application in eu-west-1. Configure the application to use the RDS for MySQL endpoint in eu-west-1.",
      "C. Copy the most recent snapshot from the RDS for MySQL DB instance to eu-west-1. Create a new RDS for MySQL DB instance in eu-west-1 from the snapshot. Configure MySQL logical replication from us-east-1 to eu-west-1. Enable write forwarding on the DB cluster. Deploy the application in eu-wes&1. Configure the application to use the RDS for MySQL endpoint in eu-west-1.",
      "D. Convert the RDS for MySQL DB instance to an Amazon Aurora MySQL DB cluster. Add eu-west-1 as a secondary Region to the DB cluster. Enable write forwarding on the DB cluster. Deploy the application in eu-west-1. Configure the application to use the Aurora MySQL endpoint in eu-west-1."
    ],
    "answer": "A",
    "explanation": "1. Clear explanation of why the correct answer (A) is right:\n\nThe correct answer is A because it provides the most comprehensive solution to meet the requirements stated in the question:\n\n- It creates an Amazon Aurora MySQL replica of the existing RDS for MySQL DB instance. This allows for a seamless migration to Aurora with minimal downtime.\n- It pauses application writes to the RDS DB instance, promotes the Aurora replica to a standalone DB cluster, and reconfigures the application to use the Aurora database. This ensures a consistent transition without data loss.\n- It adds the eu-west-1 Region as a secondary Region to the Aurora DB cluster, enabling cross-Region replication and write forwarding. This satisfies the requirement for customers in Europe to have access to the same data as customers in the US, with low latency and real-time updates.\n- The application is then deployed in the eu-west-1 Region and configured to use the Aurora MySQL endpoint there, providing the desired low-latency access for European customers.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nB. This solution only adds a cross-Region replica in eu-west-1 for the RDS for MySQL DB instance, but does not convert the database to Aurora MySQL. This means the European customers would still experience higher latency and potential data staleness compared to the Aurora solution in A.\n\nC. This solution involves creating a new RDS for MySQL DB instance in eu-west-1 from a snapshot, and then configuring MySQL logical replication. While this provides a secondary database in Europe, it does not offer the same level of cross-Region replication and write forwarding capabilities as the Aurora solution in A.\n\nD. This solution suggests converting the RDS for MySQL DB instance to an Amazon Aurora MySQL DB cluster, which is not possible natively. The question states that the company has already deployed its database on an RDS for MySQL DB instance, and there is no native way to convert it to Aurora MySQL. The solution in A is more realistic and achievable."
  },
  "290": {
    "question": "A company is serving files to its customers through an SFTP server that is accessible over the internet. The SFTP server is running on a single Amazon EC2 instance with an Elastic IP address attached. Customers connect to the SFTP server through its Elastic IP address and use SSH for authentication. The EC2 instance also has an attached security group that allows access from all customer IP addresses.A solutions architect must implement a solution to improve availability, minimize the complexity of infrastructure management, and minimize the disruption to customers who access files. The solution must not change the way customers connect.Which solution will meet these requirements?",
    "choices": [
      "A. Disassociate the Elastic IP address from the EC2 instance. Create an Amazon S3 bucket to be used for SFTP file hosting. Create an AWS Transfer Family server. Configure the Transfer Family server with a publicly accessible endpoint. Associate the SFTP Elastic IP address with the new endpoint. Point the Transfer Family server to the S3 bucket. Sync all files from the SFTP server to the S3 bucket.",
      "B. Disassociate the Elastic IP address from the EC2 instance. Create an Amazon S3 bucket to be used for SFTP file hosting. Create an AWS Transfer Family server. Configure the Transfer Family server with a VPC-hosted, internet-facing endpoint. Associate the SFTP Elastic IP address with the new endpoint. Attach the security group with customer IP addresses to the new endpoint. Point the Transfer Family server to the S3 bucket. Sync all files from the SFTP server to the S3 bucket.",
      "C. Disassociate the Elastic IP address from the EC2 instance. Create a new Amazon Elastic File System (Amazon EFS) file system to be used for SFTP file hosting. Create an AWS Fargate task definition to run an SFTP server. Specify the EFS file system as a mount in the task definition. Create a Fargate service by using the task definition, and place a Network Load Balancer (NLB) in front of the service. When configuring the service, attach the security group with customer IP addresses to the tasks that run the SFTP server. Associate the Elastic IP address with the NLB. Sync all files from the SFTP server to the S3 bucket.",
      "D. Disassociate the Elastic IP address from the EC2 instance. Create a multi-attach Amazon Elastic Block Store (Amazon EBS) volume to be used for SFTP file hosting. Create a Network Load Balancer (NLB) with the Elastic IP address attached. Create an Auto Scaling group with EC2 instances that run an SFTP server. Define in the Auto Scaling group that instances that are launched should attach the new multi-attach EBS volume. Configure the Auto Scaling group to automatically add instances behind the NLB. Configure the Auto Scaling group to use the security group that allows customer IP addresses for the EC2 instances that the Auto Scaling group launches. Sync all files from the SFTP server to the new multi-attach EBS volume."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B because it meets all the requirements stated in the question:\n\n- It improves availability by migrating the SFTP server from a single EC2 instance to an AWS Transfer Family server, which is a fully managed service that can provide better availability and scalability.\n- It minimizes the complexity of infrastructure management by using the managed AWS Transfer Family service, which handles the provisioning and maintenance of the SFTP server infrastructure.\n- It minimizes the disruption to customers by using a publicly accessible endpoint for the Transfer Family server and associating the existing Elastic IP address with the new endpoint. This allows customers to continue connecting to the SFTP server using the same IP address, without changing their existing workflows.\n- It also maintains the security by attaching the existing security group that allows access from all customer IP addresses to the new Transfer Family server endpoint.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This solution is incorrect because it uses an Amazon S3 bucket as the storage backend, which would require changes to the way customers connect to the SFTP server. The question specifically states that the solution must not change the way customers connect.\n\nC. This solution is incorrect because it uses an Amazon EFS file system and an AWS Fargate task to run the SFTP server, which adds unnecessary complexity to the infrastructure. The question specifically states that the solution must minimize the complexity of infrastructure management.\n\nD. This solution is incorrect because it uses a multi-attach Amazon EBS volume and an Auto Scaling group to manage the SFTP server instances, which adds unnecessary complexity to the infrastructure. The question specifically states that the solution must minimize the complexity of infrastructure management."
  },
  "291": {
    "question": "An events company runs a ticketing platform on AWS. The company\u2019s customers configure and schedule their events on the platform. The events result in large increases of traffic to the platform. The company knows the date and time of each customer\u2019s events.The company runs the platform on an Amazon Elastic Container Service (Amazon ECS) cluster. The ECS cluster consists of Amazon EC2 On-Demand Instances that are in an Auto Scaling group. The Auto Scaling group uses a predictive scaling policy.The ECS cluster makes frequent requests to an Amazon S3 bucket to download ticket assets. The ECS cluster and the S3 bucket are in the same AWS Region and the same AWS account. Traffic between the ECS cluster and the S3 bucket flows across a NAT gateway.The company needs to optimize the cost of the platform without decreasing the platform's availability.Which combination of steps will meet these requirements? (Choose two.)",
    "choices": [
      "A. Create a gateway VPC endpoint for the S3 bucket.",
      "B. Add another ECS capacity provider that uses an Auto Scaling group of Spot Instances. Configure the new capacity provider strategy to have the same weight as the existing capacity provider strategy.",
      "C. Create On-Demand Capacity Reservations for the applicable instance type for the time period of the scheduled scaling policies.",
      "D. Enable S3 Transfer Acceleration on the S3 bucket.",
      "E. Replace the predictive scaling policy with scheduled scaling policies for the scheduled events."
    ],
    "answer": "AE",
    "explanation": "1. Explanation of the correct answer (A and E):\n\nA. Create a gateway VPC endpoint for the S3 bucket.\nThis option is correct because it allows the ECS cluster to communicate with the S3 bucket directly through the VPC, bypassing the NAT gateway. This reduces the data transfer costs and latency between the ECS cluster and the S3 bucket, as the traffic stays within the AWS network instead of going through the public internet.\n\nE. Replace the predictive scaling policy with scheduled scaling policies for the scheduled events.\nThis option is correct because it allows the company to scale the ECS cluster based on the known schedule of customer events. By using scheduled scaling policies instead of the predictive scaling policy, the company can ensure the appropriate amount of capacity is available during the high-traffic periods, while avoiding unnecessary scaling during low-traffic periods. This helps optimize the cost of the platform without decreasing its availability.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. Add another ECS capacity provider that uses an Auto Scaling group of Spot Instances. Configure the new capacity provider strategy to have the same weight as the existing capacity provider strategy.\nThis option is incorrect because the company is trying to optimize costs without decreasing the platform's availability. Spot Instances can be interrupted, which could lead to decreased availability during customer events.\n\nC. Create On-Demand Capacity Reservations for the applicable instance type for the time period of the scheduled scaling policies.\nThis option is incorrect because On-Demand Capacity Reservations are more expensive than using the Auto Scaling group with a predictive scaling policy or scheduled scaling policies. The company's goal is to optimize costs, and this option does not align with that.\n\nD. Enable S3 Transfer Acceleration on the S3 bucket.\nThis option is incorrect because it does not address the high data transfer costs between the ECS cluster and the S3 bucket. S3 Transfer Acceleration primarily helps with improving transfer speeds for long-distance or high-latency connections, which is not the issue in this case since the ECS cluster and S3 bucket are in the same Region and account."
  },
  "292": {
    "question": "A company needs to migrate an on-premises SFTP site to AWS. The SFTP site currently runs on a Linux VM. Uploaded files are made available to downstream applications through an NFS share.As part of the migration to AWS, a solutions architect must implement high availability. The solution must provide external vendors with a set of static public IP addresses that the vendors can allow. The company has set up an AWS Direct Connect connection between its on-premises data center and its VPC.Which solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      "A. Create an AWS Transfer Family server. Configure an internet-facing VPC endpoint for the Transfer Family server. Specify an Elastic IP address for each subnet. Configure the Transfer Family server to place files into an Amazon Elastic File System (Amazon EFS) file system that is deployed across multiple Availability Zones. Modify the configuration on the downstream applications that access the existing NFS share to mount the EFS endpoint instead.",
      "B. Create an AWS Transfer Family server. Configure a publicly accessible endpoint for the Transfer Family server. Configure the Transfer Family server to place files into an Amazon Elastic File System (Amazon EFS) file system that is deployed across multiple Availability Zones. Modify the configuration on the downstream applications that access the existing NFS share to mount the EFS endpoint instead.",
      "C. Use AWS Application Migration Service to migrate the existing Linux VM to an Amazon EC2 instance. Assign an Elastic IP address to the EC2 instance. Mount an Amazon Elastic File System (Amazon EFS) file system to the EC2 instance. Configure the SFTP server to place files in the EFS file system. Modify the configuration on the downstream applications that access the existing NFS share to mount the EFS endpoint instead.",
      "D. Use AWS Application Migration Service to migrate the existing Linux VM to an AWS Transfer Family server. Configure a publicly accessible endpoint for the Transfer Family server. Configure the Transfer Family server to place files into an Amazon FSx for Lustre file system that is deployed across multiple Availability Zones. Modify the configuration on the downstream applications that access the existing NFS share to mount the FSx for Lustre endpoint instead."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\n\nThe correct answer is A because it provides the optimal solution to meet the requirements with the least operational overhead.\n\n- AWS Transfer Family provides a fully managed SFTP service, eliminating the need to manage and maintain SFTP servers, reducing operational overhead compared to an EC2-based solution.\n- Configuring an internet-facing VPC endpoint for the Transfer Family server allows the assignment of static public IP addresses, which can be provided to external vendors to allow access.\n- By configuring the Transfer Family server to place files into an Amazon Elastic File System (Amazon EFS) file system deployed across multiple Availability Zones, it ensures high availability of the file storage.\n- Modifying the downstream applications to mount the EFS endpoint instead of the existing NFS share allows a seamless migration, as EFS is NFS-compatible.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. This option is incorrect because the publicly accessible endpoint provided by AWS Transfer Family cannot be configured with static public IP addresses. The IP addresses are managed by AWS and are subject to change.\n\nC. This option is incorrect because it involves manually migrating the existing Linux VM to an EC2 instance, which introduces additional operational overhead compared to the managed AWS Transfer Family service.\n\nD. This option is incorrect because it uses AWS FSx for Lustre instead of the more general-purpose Amazon EFS, which may not be the best fit for the company's use case, and it still does not provide static public IP addresses for external vendors."
  },
  "293": {
    "question": "A solutions architect has an operational workload deployed on Amazon EC2 instances in an Auto Scaling group. The VPC architecture spans two Availability Zones (AZ) with a subnet in each that the Auto Scaling group is targeting. The VPC is connected to an on-premises environment and connectivity cannot be interrupted. The maximum size of the Auto Scaling group is 20 instances in service. The VPC IPv4 addressing is as follows:VPC CIDR: 10.0.0.0/23 -AZ1 subnet CIDR: 10.0.0.0/24 -AZ2 subnet CIDR: 10.0.1.0/24 -Since deployment, a third AZ has become available in the Region. The solutions architect wants to adopt the new AZ without adding additional IPv4 address space and without service downtime. Which solution will meet these requirements?",
    "choices": [
      "A. Update the Auto Scaling group to use the AZ2 subnet only. Delete and re-create the AZ1 subnet using half the previous address space. Adjust the Auto Scaling group to also use the new AZ1 subnet. When the instances are healthy, adjust the Auto Scaling group to use the AZ1 subnet only. Remove the current AZ2 subnet. Create a new AZ2 subnet using the second half of the address space from the original AZ1 subnet. Create a new AZ3 subnet using half the original AZ2 subnet address space, then update the Auto Scaling group to target all three new subnets.",
      "B. Terminate the EC2 instances in the AZ1 subnet. Delete and re-create the AZ1 subnet using half the address space. Update the Auto Scaling group to use this new subnet. Repeat this for the second AZ. Define a new subnet in AZ3, then update the Auto Scaling group to target all three new subnets.",
      "C. Create a new VPC with the same IPv4 address space and define three subnets, with one for each AZ. Update the existing Auto Scaling group to target the new subnets in the new VPC.",
      "D. Update the Auto Scaling group to use the AZ2 subnet only. Update the AZ1 subnet to have half the previous address space. Adjust the Auto Scaling group to also use the AZ1 subnet again. When the instances are healthy, adjust the Auto Scaling group to use the AZ1 subnet only. Update the current AZ2 subnet and assign the second half of the address space from the original AZ1 subnet. Create a new AZ3 subnet using half the original AZ2 subnet address space, then update the Auto Scaling group to target all three new subnets."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\n\nThe correct answer is A because it allows the solutions architect to adopt the new AZ without adding additional IPv4 address space and without service downtime. The key steps are:\n\n1. Update the Auto Scaling group to use the AZ2 subnet only.\n2. Delete and re-create the AZ1 subnet using half the previous address space.\n3. Adjust the Auto Scaling group to also use the new AZ1 subnet.\n4. When the instances are healthy, adjust the Auto Scaling group to use the AZ1 subnet only.\n5. Remove the current AZ2 subnet.\n6. Create a new AZ2 subnet using the second half of the address space from the original AZ1 subnet.\n7. Create a new AZ3 subnet using half the original AZ2 subnet address space.\n8. Update the Auto Scaling group to target all three new subnets.\n\nThis approach allows the solutions architect to gradually migrate the workload to the new subnets without interrupting the service, while also efficiently utilizing the existing address space.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. This option involves terminating all EC2 instances in the AZ1 subnet, which could lead to service downtime. Additionally, deleting and re-creating subnets may require downtime and could be more complex to manage.\n\nC. Creating a new VPC with the same address space is not necessary and would require more complex networking changes, such as updating routing tables and VPC peering, which could also lead to service disruptions.\n\nD. This option is similar to the correct answer, but it does not specify the need to delete and re-create the subnets, which is a critical step to avoid overlapping IP addresses and ensure a seamless transition."
  },
  "294": {
    "question": "A company uses an organization in AWS Organizations to manage the company's AWS accounts. The company uses AWS CloudFormation to deploy all infrastructure. A finance team wants to build a chargeback model. The finance team asked each business unit to tag resources by using a predefined list of project values.When the finance team used the AWS Cost and Usage Report in AWS Cost Explorer and filtered based on project, the team noticed noncompliant project values. The company wants to enforce the use of project tags for new resources.Which solution will meet these requirements with the LEAST effort?",
    "choices": [
      "A. Create a tag policy that contains the allowed project tag values in the organization's management account. Create an SCP that denies the cloudformation:CreateStack API operation unless a project tag is added. Attach the SCP to each OU.",
      "B. Create a tag policy that contains the allowed project tag values in each OU. Create an SCP that denies the cloudformation:CreateStack API operation unless a project tag is added. Attach the SCP to each OU.",
      "C. Create a tag policy that contains the allowed project tag values in the AWS management account. Create an IAM policy that denies the cloudformation:CreateStack API operation unless a project tag is added. Assign the policy to each user.",
      "D. Use AWS Service Catalog to manage the CloudFormation stacks as products. Use a TagOptions library to control project tag values. Share the portfolio with all OUs that are in the organization."
    ],
    "answer": "A",
    "explanation": "1. Explanation for the correct answer (A):\n\nThe correct answer is A because it uses the right AWS service and approach to enforce the use of project tags for new resources with the least effort.\n\nThe key points are:\n- Create a tag policy in the organization's management account. This allows defining the allowed project tag values centrally and having them applied to all child OUs.\n- Create an SCP (Service Control Policy) that denies the cloudformation:CreateStack API operation unless a project tag is added. This ensures that no new resources can be created without the required project tag.\n- Attaching the SCP to each OU ensures the policy is applied consistently across the organization.\n\nThis approach leverages the capabilities of AWS Organizations to centrally manage and enforce the tagging policy across all accounts in the organization.\n\n2. Explanations for the incorrect choices:\n\nB. This option is incorrect because tag policies should be created in the management account, not in each OU. Applying the SCP to each OU is redundant, as the policy applied at the Root OU level would be inherited by all child OUs.\n\nC. This option is incorrect because it uses IAM policies instead of AWS Organizations policies. IAM policies are applied to individual users, which would require more effort to manage and maintain across the organization. Additionally, the tag policy should be defined in the management account, not the AWS management account.\n\nD. This option is incorrect because it uses AWS Service Catalog, which is a different service than the ones required to meet the given requirements. The question specifically asks for a solution that uses the least effort, and using AWS Service Catalog would likely require more effort compared to the correct solution."
  },
  "295": {
    "question": "An application is deployed on Amazon EC2 instances that run in an Auto Scaling group. The Auto Scaling group configuration uses only one type of instance.CPU and memory utilization metrics show that the instances are underutilized. A solutions architect needs to implement a solution to permanently reduce the EC2 cost and increase the utilization.Which solution will meet these requirements with the LEAST number of configuration changes in the future?",
    "choices": [
      "A. List instance types that have properties that are similar to the properties that the current instances have. Modify the Auto Scaling group's launch template configuration to use multiple instance types from the list.",
      "B. Use the information about the application's CPU and memory utilization to select an instance type that matches the requirements. Modify the Auto Scaling group's configuration by adding the new instance type. Remove the current instance type from the configuration.",
      "C. Use the information about the application's CPU and memory utilization to specify CPU and memory requirements in a new revision of the Auto Scaling group's launch template. Remove the current instance type from the configuration.",
      "D. Create a script that selects the appropriate instance types from the AWS Price List Bulk API. Use the selected instance types to create a new revision of the Auto Scaling group's launch template."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C because it allows the solutions architect to make a targeted change to the instance type specification in the Auto Scaling group's launch template, based on the observed CPU and memory utilization metrics. This approach is more efficient than modifying the Auto Scaling group configuration to use multiple instance types (as in A) or removing the current instance type and adding a new one (as in B).\n\nBy updating the launch template with the appropriate instance type that matches the application's resource requirements, the Auto Scaling group can automatically provision instances of the new, more suitable type. This approach ensures that the instances are properly sized and utilized, reducing the EC2 costs.\n\nImportantly, this solution with the launch template update requires the least number of future configuration changes. If the application's resource requirements change again, the solutions architect can simply update the launch template with a new instance type, without needing to modify the Auto Scaling group configuration itself.\n\n2. Explanations of the incorrect choices:\n\nA. This approach of using multiple instance types in the Auto Scaling group configuration is less efficient than the targeted approach in C. It may result in over-provisioning of resources, as the instances would likely be a mix of under-utilized and over-utilized types.\n\nB. Removing the current instance type and adding a new one in the Auto Scaling group configuration is a valid approach, but it requires more frequent configuration changes compared to the launch template update in C. If the application's resource requirements change again, the solutions architect would need to modify the Auto Scaling group configuration once more.\n\nD. Creating a script to select instance types from the AWS Price List Bulk API is an overly complex solution compared to the straightforward launch template update in C. Additionally, it does not directly address the issue of underutilized instances or reduced EC2 costs."
  },
  "296": {
    "question": "A company implements a containerized application by using Amazon Elastic Container Service (Amazon ECS) and Amazon API Gateway The application data is stored in Amazon Aurora databases and Amazon DynamoDB databases. The company automates infrastructure provisioning by using AWS CloudFormation. The company automates application deployment by using AWS CodePipeline.A solutions architect needs to implement a disaster recovery (DR) strategy that meets an RPO of 2 hours and an RTO of 4 hours.Which solution will meet these requirements MOST cost-effectively?",
    "choices": [
      "A. Set up an Aurora global database and DynamoDB global tables to replicate the databases to a secondary AWS Region. In the primary Region and in the secondary Region, configure an API Gateway API with a Regional endpoint. Implement Amazon CloudFront with origin failover to route traffic to the secondary Region during a DR scenario.",
      "B. Use AWS Database Migration Service (AWS DMS), Amazon EventBridge, and AWS Lambda to replicate the Aurora databases to a secondary AWS Region. Use DynamoDB Streams, EventBridge. and Lambda to replicate the DynamoDB databases to the secondary Region. In the primary Region and in the secondary Region, configure an API Gateway API with a Regional endpoint. Implement Amazon Route 53 failover routing to switch traffic from the primary Region to the secondary Region.",
      "C. Use AWS Backup to create backups of the Aurora databases and the DynamoDB databases in a secondary AWS Region. In the primary Region and in the secondary Region, configure an API Gateway API with a Regional endpoint. Implement Amazon Route 53 failover routing to switch traffic from the primary Region to the secondary Region.",
      "D. Set up an Aurora global database and DynamoDB global tables to replicate the databases to a secondary AWS Region. In the primary Region and in the secondary Region, configure an API Gateway API with a Regional endpoint. Implement Amazon Route 53 failover routing to switch traffic from the primary Region to the secondary Region."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C because it meets the RPO and RTO requirements most cost-effectively.\n\nOption C uses AWS Backup to create backups of the Aurora databases and DynamoDB databases in a secondary AWS Region. This approach is more cost-effective than the other options, which involve setting up active database replication between Regions. \n\nWith an RPO of 2 hours and an RTO of 4 hours, using AWS Backup to create regular database backups in a secondary Region and restoring from those backups in a disaster scenario can meet the requirements. The RTO of 4 hours can be achieved by automating the restore process from the backups in the secondary Region.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option uses Aurora global databases and DynamoDB global tables, which provide continuous data replication between Regions. However, this approach is likely more expensive than the backup-based solution in option C, as it requires maintaining active database replication infrastructure.\n\nB. This option uses AWS DMS, EventBridge, and Lambda to replicate the databases to a secondary Region. While this provides a more real-time data replication solution, it is also more complex and costly to implement and maintain compared to the backup-based approach in option C.\n\nD. This option also uses Aurora global databases and DynamoDB global tables, similar to option A. While it meets the RPO and RTO requirements, it is likely more expensive than the backup-based solution in option C.\n\nIn summary, option C is the most cost-effective solution that can still meet the given RPO and RTO requirements, making it the correct choice."
  },
  "297": {
    "question": "A company has a complex web application that leverages Amazon CloudFront for global scalability and performance. Over time, users report that the web application is slowing down.The company's operations team reports that the CloudFront cache hit ratio has been dropping steadily. The cache metrics report indicates that query strings on some URLs are inconsistently ordered and are specified sometimes in mixed-case letters and sometimes in lowercase letters.Which set of actions should the solutions architect take to increase the cache hit ratio as quickly as possible?",
    "choices": [
      "A. Deploy a Lambda@Edge function to sort parameters by name and force them to be lowercase. Select the CloudFront viewer request trigger to invoke the function.",
      "B. Update the CloudFront distribution to disable caching based on query string parameters.",
      "C. Deploy a reverse proxy after the load balancer to post-process the emitted URLs in the application to force the URL strings to be lowercase.",
      "D. Update the CloudFront distribution to specify casing-insensitive query string processing."
    ],
    "answer": "A",
    "explanation": "1. Clear explanation of why the correct answer (A) is right:\n\nThe correct answer is A, which is to deploy a Lambda@Edge function to sort parameters by name and force them to be lowercase. This is the best solution to increase the cache hit ratio as quickly as possible.\n\nThe reason is that Amazon CloudFront considers the case and order of query string parameters when caching the content. If the query strings are inconsistently ordered or specified in mixed-case letters, CloudFront will treat them as different requests, leading to a drop in the cache hit ratio. By deploying a Lambda@Edge function to standardize the query string parameters, CloudFront can effectively cache the content and serve it from the edge locations, improving the application's performance.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nB. Update the CloudFront distribution to disable caching based on query string parameters:\nThis is not the correct solution because disabling caching based on query string parameters will exacerbate the issue. It will cause all requests with query strings to be forwarded to the origin, leading to higher latency and reduced performance.\n\nC. Deploy a reverse proxy after the load balancer to post-process the emitted URLs in the application to force the URL strings to be lowercase:\nThis is not the correct solution because it does not address the issue of inconsistent query string parameters. Post-processing the URLs in the application will not improve the cache hit ratio on the CloudFront distribution.\n\nD. Update the CloudFront distribution to specify casing-insensitive query string processing:\nThis is not the correct solution because Amazon CloudFront does not support casing-insensitive query string processing. The case and order of the query string parameters are still considered when caching the content, so this solution will not address the root cause of the problem."
  },
  "298": {
    "question": "A company runs an ecommerce application in a single AWS Region. The application uses a five-node Amazon Aurora MySQL DB cluster to store information about customers and their recent orders. The DB cluster experiences a large number of write transactions throughout the day.The company needs to replicate the data in the Aurora database to another Region to meet disaster recovery requirements. The company has an RPO of 1 hour.Which solution will meet these requirements with the LOWEST cost?",
    "choices": [
      "A. Modify the Aurora database to be an Aurora global database. Create a second Aurora database in another Region.",
      "B. Enable the Backtrack feature for the Aurora database. Create an AWS Lambda function that runs daily to copy the snapshots of the database to a backup Region.",
      "C. Use AWS Database Migration Service (AWS DMS). Create a DMS change data capture (CDC) task that replicates the ongoing changes from the Aurora database to an Amazon S3 bucket in another Region.",
      "D. Turn off automated Aurora backups. Configure Aurora backups with a backup frequency of 1 hour. Specify another Region as the destination Region. Select the Aurora database as the resource assignment."
    ],
    "answer": "C",
    "explanation": "1. Explanation of why the correct answer (C) is right:\n\nThe correct answer is C. Use AWS Database Migration Service (AWS DMS) to create a CDC (Change Data Capture) task that replicates the ongoing changes from the Aurora database to an Amazon S3 bucket in another Region.\n\nThis solution meets the disaster recovery requirement of replicating the data to another Region. The CDC task in AWS DMS will continuously replicate the changes in the Aurora database to an S3 bucket in the other Region, ensuring a low RPO (Recovery Point Objective) of 1 hour as required.\n\nThis is the lowest-cost solution compared to the other options because:\n- It does not require setting up a second Aurora database in another Region (option A), which would incur additional database costs.\n- It does not require the use of a custom Lambda function to copy database snapshots (option B), which would incur additional Lambda function costs.\n- It does not require configuring custom backups with a 1-hour frequency and backup destination Region (option D), which would incur additional backup storage and data transfer costs.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Modify the Aurora database to be an Aurora global database and create a second Aurora database in another Region:\nThis solution would be more costly as it requires setting up a second Aurora database in another Region, which incurs additional database costs.\n\nB. Enable the Backtrack feature for the Aurora database and create an AWS Lambda function that runs daily to copy the snapshots of the database to a backup Region:\nThis solution is more costly as it requires the use of a custom Lambda function to copy the database snapshots, which incurs additional Lambda function costs.\n\nD. Turn off automated Aurora backups, configure Aurora backups with a 1-hour frequency, and specify another Region as the destination Region:\nThis solution is more costly as it requires configuring custom backups with a 1-hour frequency and backup destination Region, which incurs additional backup storage and data transfer costs."
  },
  "299": {
    "question": "A company has set up its entire infrastructure on AWS. The company uses Amazon EC2 instances to host its ecommerce website and uses Amazon S3 to store static data. Three engineers at the company handle the cloud administration and development through one AWS account. Occasionally, an engineer alters an EC2 security group configuration of another engineer and causes noncompliance issues in the environment.A solutions architect must set up a system that tracks changes that the engineers make. The system must send alerts when the engineers make noncompliant changes to the security settings for the EC2 instances.What is the FASTEST way for the solutions architect to meet these requirements?",
    "choices": [
      "A. Set up AWS Organizations for the company. Apply SCPs to govern and track noncompliant security group changes that are made to the AWS account.",
      "B. Enable AWS CloudTrail to capture the changes to EC2 security groups. Enable Amazon CloudWatch rules to provide alerts when noncompliant security settings are detected.",
      "C. Enable SCPs on the AWS account to provide alerts when noncompliant security group changes are made to the environment.",
      "D. Enable AWS Config on the EC2 security groups to track any noncompliant changes. Send the changes as alerts through an Amazon Simple Notification Service (Amazon SNS) topic."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (D):\n\nThe correct answer is D. Enable AWS Config on the EC2 security groups to track any noncompliant changes, and send the changes as alerts through an Amazon Simple Notification Service (Amazon SNS) topic.\n\nThis is the fastest way to meet the requirements because:\n- AWS Config can automatically detect and track changes made to the EC2 security groups, including noncompliant changes.\n- AWS Config can be configured to send notifications through an Amazon SNS topic whenever a noncompliant change is detected.\n- This setup is straightforward and can be implemented quickly, without the need to set up additional services like CloudTrail and CloudWatch.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Set up AWS Organizations for the company and apply SCPs to govern and track noncompliant security group changes: This is not the fastest solution, as it requires setting up AWS Organizations, which is a more complex and time-consuming process.\n\nB. Enable AWS CloudTrail to capture the changes to EC2 security groups, and enable Amazon CloudWatch rules to provide alerts when noncompliant security settings are detected: This solution is not the fastest because it requires multiple steps, including setting up CloudTrail, CloudWatch logs, CloudWatch metric filters, and CloudWatch alarms, before the notification through Amazon SNS can be set up.\n\nC. Enable SCPs on the AWS account to provide alerts when noncompliant security group changes are made to the environment: This solution does not meet the requirement of tracking the changes made by the engineers, as SCPs only provide alerts, not a comprehensive tracking system."
  },
  "300": {
    "question": "A company is planning to migrate several applications to AWS. The company does not have a good understanding of its entire application estate. The estate consists of a mixture of physical machines and VMs.One application that the company will migrate has many dependencies that are sensitive to latency. The company is unsure what all the dependencies are. However the company knows that the low-latency communications use a custom IP-based protocol that runs on port 1000. The company wants to migrate the application and these dependencies together to move all the low-latency interfaces to AWS at the same time.The company has installed the AWS Application Discovery Agent and has been collecting data for several months.What should the company do to identify the dependencies that need to be migrated in the same phase as the application?",
    "choices": [
      "A. Use AWS Migration Hub and select the servers that host the application. Visualize the network graph to find servers that interact with the application. Turn on data exploration in Amazon Athena. Query the data that is transferred between the servers to identify the servers that communicate on port 1000. Return to Migration Hub. Create a move group that is based on the findings from the Athena queries.",
      "B. Use AWS Application Migration Service and select the servers that host the application. Visualize the network graph to find servers that interact with the application. Configure Application Migration Service to launch test instances for all the servers that interact with the application. Perform acceptance tests on the test instances. If no issues are identified, create a move group that is based on the tested servers.",
      "C. Use AWS Migration Hub and select the servers that host the application. Turn on data exploration in Network Access Analyzer. Use the Network Access Analyzer console to select the servers that host the application. Select a Network Access Scope of port 1000 and note the matching servers. Return to Migration Hub. Create a move group that is based on the findings from Network Access Analyzer.",
      "D. Use AWS Migration Hub and select the servers that host the application. Push the Amazon CloudWalch agent to the identified servers by using the AWS Application Discovery Agent. Export the CloudWatch logs that the agents collect to Amazon S3. Use Amazon Athena to query the logs to find servers that communicate on port 1000. Return to Migration Hub Create a move group that is based on the findings from the Athena queries."
    ],
    "answer": "A",
    "explanation": "Explanation of the correct answer (A):\n\nThe correct answer is A because it outlines the most appropriate steps to identify the dependencies that need to be migrated along with the application.\n\nThe key points are:\n\n1. Using AWS Migration Hub to visualize the network graph and identify the servers that interact with the application.\n2. Leveraging Amazon Athena to explore the data collected by the AWS Application Discovery Agent and query the network traffic to identify the servers that communicate on port 1000 using the custom IP-based protocol.\n3. Creating a move group in Migration Hub based on the findings from the Athena queries, ensuring that all the relevant dependencies are migrated together with the application.\n\nThis approach allows the company to leverage the capabilities of AWS Migration Hub and Athena to effectively identify and group the necessary dependencies for migration, which is crucial for the low-latency communication requirements of the application.\n\nExplanations of the incorrect choices:\n\nB. This option is incorrect because AWS Application Migration Service is primarily used for the lift-and-shift migration of servers, not for dependency mapping. While it can launch test instances, it does not provide the same level of network visualization and data exploration capabilities as Migration Hub and Athena.\n\nC. This option is incorrect because Network Access Analyzer is designed to analyze network access patterns for AWS resources, not on-premises servers. The question states that the company has a mixture of physical machines and VMs, so Network Access Analyzer would not be suitable for this use case.\n\nD. This option is incorrect because the use of Amazon CloudWatch logs and Athena is not the best approach for identifying the specific dependencies that need to be migrated with the application. CloudWatch is primarily used for monitoring and logging, and may not provide the necessary insights for this migration scenario."
  },
  "301": {
    "question": "A company is building an application that will run on an AWS Lambda function. Hundreds of customers will use the application. The company wants to give each customer a quota of requests for a specific time period. The quotas must match customer usage patterns. Some customers must receive a higher quota for a shorter time period.Which solution will meet these requirements?",
    "choices": [
      "A. Create an Amazon API Gateway REST API with a proxy integration to invoke the Lambda function. For each customer, configure an API Gateway usage plan that includes an appropriate request quota. Create an API key from the usage plan for each user that the customer needs.",
      "B. Create an Amazon API Gateway HTTP API with a proxy integration to invoke the Lambda function. For each customer configure an API Gateway usage plan that includes an appropriate request quota Configure route-level throttling for each usage plan. Create an API Key from the usage plan for each user that the customer needs.",
      "C. Create a Lambda function alias for each customer. Include a concurrency limit with an appropriate request quota. Create a Lambda function URL for each function alias. Share the Lambda function URL for each alias with the relevant customer.",
      "D. Create an Application Load Balancer (ALB) in a VPC. Configure the Lambda function as a target for the ALB. Configure an AWS WAF web ACL for the ALB. For each customer configure a rale-based rule that includes an appropriate request quota."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\nThe correct answer is A because it best meets the requirements of the problem statement. By using Amazon API Gateway with a REST API and a proxy integration to the Lambda function, the solution can provide the following features:\n\n- Ability to configure a separate usage plan with appropriate request quotas for each customer. This allows the company to match the quota to the specific usage patterns of each customer, including the requirement for some customers to have higher quotas for shorter time periods.\n- Creation of API keys for each user within a customer's organization. This enables the enforcement of quotas at the individual user level, as required by the problem statement.\n- The proxy integration with the Lambda function allows the API Gateway to invoke the Lambda function on behalf of each customer, while still enforcing the individual quotas.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. This choice mentions using an API Gateway HTTP API, which does not support the same level of configuration and control as the REST API in the correct answer. Specifically, it does not mention the ability to configure separate usage plans and API keys for each customer, which is a key requirement.\n\nC. This choice proposes using Lambda function aliases and function URLs, which does not provide the same level of control and customization as the API Gateway solution in the correct answer. Configuring a concurrency limit and quota at the function level would not allow the granular control required to match individual customer usage patterns.\n\nD. This choice uses an Application Load Balancer (ALB) to route traffic to the Lambda function, which is a valid approach. However, it does not mention the ability to configure separate usage plans and quotas for each customer, as required by the problem statement. The AWS WAF integration mentioned is also not specifically required by the problem statement."
  },
  "302": {
    "question": "A company has an on-premises Microsoft SQL Server database that writes a nightly 200 GB export to a local drive. The company wants to move the backups to more robust cloud storage on Amazon S3. The company has set up a 10 Gbps AWS Direct Connect connection between the on-premises data center and AWS.Which solution meets these requirements MOST cost-effectively?",
    "choices": [
      "A. Create a new S3 bucket. Deploy an AWS Storage Gateway file gateway within the VPC that is connected to the Direct Connect connection. Create a new SMB file share. Write nightly database exports to the new SMB file share.",
      "B. Create an Amazon FSx for Windows File Server Single-AZ file system within the VPC that is connected to the Direct Connect connection. Create a new SMB file share. Write nightly database exports to an SMB file share on the Amazon FSx file system. Enable nightly backups.",
      "C. Create an Amazon FSx for Windows File Server Multi-AZ file system within the VPC that is connected to the Direct Connect connection. Create a new SMB file share. Write nightly database exports to an SMB file share on the Amazon FSx file system. Enable nightly backups.",
      "D. Create a new S3 bucket. Deploy an AWS Storage Gateway volume gateway within the VPC that is connected to the Direct Connect connection. Create a new SMB file share. Write nightly database exports to the new SMB file share on the volume gateway, and automate copies of this data to an S3 bucket."
    ],
    "answer": "A",
    "explanation": "1. Explanation of why the correct answer (D) is right:\n\nThe most cost-effective solution is to use the AWS Storage Gateway volume gateway (option D). This approach has the following advantages:\n\n- Cost-effectiveness: The volume gateway uses Amazon EBS volumes for local storage, which is typically more cost-effective than using Amazon FSx for Windows File Server (options B and C) for storing large amounts of data.\n- Automated backups: The solution allows for automating the copies of the nightly database exports to Amazon S3, ensuring reliable backups and minimizing manual intervention.\n- Utilization of Direct Connect: Leveraging the existing Direct Connect connection optimizes the network bandwidth for transferring data to S3, minimizing latency and potential data transfer charges.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. File Gateway vs. Volume Gateway:\nThe question specifically asks for a solution to move the backups to Amazon S3, which is better suited for a Volume Gateway rather than a File Gateway. The File Gateway is designed for sharing files over SMB/NFS, not for efficiently transferring large data sets to S3.\n\nB. and C. Amazon FSx for Windows File Server:\nOptions B and C, which involve using Amazon FSx for Windows File Server, are not the most cost-effective solutions. While FSx provides a managed Windows file system, it may be more expensive than using the Volume Gateway approach, especially for storing large amounts of data.\n\nD. Automated backups to S3:\nThis is the most cost-effective solution, as it leverages the existing Direct Connect connection, uses the cost-effective Volume Gateway, and automates the backups to Amazon S3, minimizing manual intervention."
  },
  "303": {
    "question": "An online survey company runs its application in the AWS Cloud. The application is distributed and consists of microservices that run in an automatically scaled Amazon Elastic Container Service (Amazon ECS) cluster. The ECS cluster is a target for an Application Load Balancer (ALB). The ALB is a custom origin for an Amazon CloudFront distribution.The company has a survey that contains sensitive data. The sensitive data must be encrypted when it moves through the application. The application's data-handling microservice is the only microservice that should be able to decrypt the dataWhich solution will meet these requirements?",
    "choices": [
      "A. Create a symmetric AWS Key Management Service (AWS KMS) key that is dedicated to the data-handling microservice. Create a field-level encryption profile and a configuration. Associate the KMS key and the configuration with the CloudFront cache behavior.",
      "B. Create an RSA key pair that is dedicated to the data-handing microservice. Upload the public key to the CloudFront distribution. Create a field-level encryption profile and a configuration. Add the configuration to the CloudFront cache behavior.",
      "C. Create a symmetric AWS Key Management Service (AWS KMS) key that is dedicated to the data-handling microservice. Create a Lambda@Edge function. Program the function to use the KMS key to encrypt the sensitive data.",
      "D. Create an RSA key pair that is dedicated to the data-handling microservice. Create a Lambda@Edge function. Program the function to use the private key of the RSA key pair to encrypt the sensitive data."
    ],
    "answer": "B",
    "explanation": "1. Explanation of why the correct answer (B) is right:\n\nThe correct answer is B because it aligns with the requirements specified in the question. The question states that the sensitive data must be encrypted when it moves through the application, and the data-handling microservice should be the only one able to decrypt the data.\n\nBy creating an RSA key pair dedicated to the data-handling microservice, you can use the public key to encrypt the sensitive data as it travels through the application, while the private key is only accessible to the data-handling microservice, which can then decrypt the data.\n\nThis approach uses field-level encryption in CloudFront, which allows you to encrypt specific fields in the data before it is transmitted to the client. The field-level encryption configuration, which includes the public key, is added to the CloudFront cache behavior, ensuring that the sensitive data is encrypted at the edge.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option uses a symmetric AWS KMS key, which would not meet the requirement of the data-handling microservice being the only one able to decrypt the data. Symmetric keys are shared, so other microservices could potentially access the key and decrypt the data.\n\nC. This option uses a symmetric AWS KMS key and a Lambda@Edge function to encrypt the data. While this would meet the encryption requirement, it would not satisfy the need for the data-handling microservice to be the only one able to decrypt the data, as the Lambda@Edge function could potentially be accessed by other microservices.\n\nD. This option uses an RSA key pair and a Lambda@Edge function to encrypt the data. While it uses asymmetric encryption like the correct answer, the Lambda@Edge function could potentially be accessed by other microservices, which would not meet the requirement of the data-handling microservice being the only one able to decrypt the data.\n\nIn summary, the correct answer (B) is the most appropriate solution because it uses field-level encryption with an RSA key pair, which ensures that the sensitive data is encrypted as it moves through the application, and the private key is only accessible to the data-handling microservice, meeting the specified requirements."
  },
  "304": {
    "question": "A solutions architect is determining the DNS strategy for an existing VPC. The VPC is provisioned to use the 10.24.34.0/24 CIDR block. The VPC also uses Amazon Route 53 Resolver for DNS. New requirements mandate that DNS queries must use private hosted zones. Additionally instances that have public IP addresses must receive corresponding public hostnamesWhich solution will meet these requirements to ensure that the domain names are correctly resolved within the VPC?",
    "choices": [
      "A. Create a private hosted zone. Activate the enableDnsSupport attribute and the enableDnsHostnames attribute for the VPC. Update the VPC DHCP options set to include domain-name-servers=10.24.34.2.",
      "B. Create a private hosted zone Associate the private hosted zone with the VPC. Activate the enableDnsSupport attribute and the enableDnsHostnames attribute for the VPC. Create a new VPC DHCP options set, and configure domain-name-servers=AmazonProvidedDNS. Associate the new DHCP options set with the VPC.",
      "C. Deactivate the enableDnsSupport attribute for the VPActivate the enableDnsHostnames attribute for the VPCreate a new VPC DHCP options set, and configure doman-name-servers=10.24.34.2. Associate the new DHCP options set with the VPC.",
      "D. Create a private hosted zone. Associate the private hosted zone with the VPC. Activate the enableDnsSupport attribute for the VPC. Deactivate the enableDnsHostnames attribute for the VPC. Update the VPC DHCP options set to include domain-name-servers=AmazonProvidedDNS."
    ],
    "answer": "B",
    "explanation": "Here's a concise technical explanation:\n\n1. Correct Answer: B\n\nExplanation for Option B:\n- This option correctly creates a private hosted zone in Route 53 and associates it with the VPC.\n- It activates both the `enableDnsSupport` and `enableDnsHostnames` attributes for the VPC, which enables DNS resolution within the VPC and allows instances to receive public hostnames if they have public IP addresses.\n- It creates a new VPC DHCP options set and configures `domain-name-servers=AmazonProvidedDNS`, which ensures instances receive the correct DNS server information.\n\n2. Explanations for Incorrect Answers:\n\nOption A:\n- Incorrect because it uses a custom DNS server (10.24.34.2) instead of the Amazon-provided DNS, as required by the new requirements.\n\nOption C:\n- Incorrect because it deactivates the `enableDnsSupport` attribute, which is required for DNS resolution within the VPC.\n- It also does not create a private hosted zone, as required by the new requirements.\n\nOption D:\n- Incorrect because it deactivates the `enableDnsHostnames` attribute, which is required to assign public hostnames to instances with public IP addresses.\n- It also updates the DHCP options set to use `AmazonProvidedDNS`, which is not necessary since the question states that the VPC is already using Amazon Route 53 Resolver for DNS."
  },
  "305": {
    "question": "A data analytics company has an Amazon Redshift cluster that consists of several reserved nodes. The cluster is experiencing unexpected bursts of usage because a team of employees is compiling a deep audit analysis report. The queries to generate the report are complex read queries and are CPU intensive.Business requirements dictate that the cluster must be able to service read and write queries at all times. A solutions architect must devise a solution that accommodates the bursts of usage.Which solution meets these requirements MOST cost-effectively?",
    "choices": [
      "A. Provision an Amazon EMR cluster Offload the complex data processing tasks.",
      "B. Deploy an AWS Lambda function to add capacity to the Amazon Redshift cluster by using a classic resize operation when the cluster\u2019s CPU metrics in Amazon CloudWatch reach 80%.",
      "C. Deploy an AWS Lambda function to add capacity to the Amazon Redshift cluster by using an elastic resize operation when the cluster\u2019s CPU metrics in Amazon CloudWatch reach 80%.",
      "D. Turn on the Concurrency Scaling feature for the Amazon Redshift cluster."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (D):\n\nThe correct answer is D. Turn on the Concurrency Scaling feature for the Amazon Redshift cluster.\n\nConcurrency Scaling is the most cost-effective solution that meets the given requirements. It allows the Amazon Redshift cluster to automatically scale up and down its concurrency level based on usage, without the need for manual resizing. This addresses the issue of unexpected bursts of usage from the complex read-heavy queries, while ensuring that the cluster can service both read and write queries at all times.\n\nConcurrency Scaling is a feature in Amazon Redshift that dynamically adds or removes compute resources to the cluster as needed, without interrupting ongoing queries or requiring you to manually resize the cluster. This allows the cluster to handle increased concurrency and CPU-intensive workloads, such as the deep audit analysis report, in a cost-effective manner.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Provision an Amazon EMR cluster and Offload the complex data processing tasks:\nThis solution is not the most cost-effective, as it would require setting up and managing a separate EMR cluster in addition to the existing Amazon Redshift cluster. It also does not address the need to handle both read and write queries on the Redshift cluster.\n\nB. Deploy an AWS Lambda function to add capacity to the Amazon Redshift cluster by using a classic resize operation when the cluster's CPU metrics in Amazon CloudWatch reach 80%:\nThis solution is not as cost-effective as Concurrency Scaling, as it requires manual intervention to trigger the classic resize operation. It also introduces the risk of potential downtime during the resize process, which may impact the ability to service both read and write queries.\n\nC. Deploy an AWS Lambda function to add capacity to the Amazon Redshift cluster by using an elastic resize operation when the cluster's CPU metrics in Amazon CloudWatch reach 80%:\nThis solution is similar to the previous one, but it uses the elastic resize operation instead of the classic resize. While the elastic resize is faster and less disruptive, it still requires manual intervention and does not provide the same level of cost-effectiveness as Concurrency Scaling."
  },
  "306": {
    "question": "A research center is migrating to the AWS Cloud and has moved its on-premises 1 PB object storage to an Amazon S3 bucket. One hundred scientists are using this object storage to store their work-related documents. Each scientist has a personal folder on the object store. All the scientists are members of a single IAM user group.The research center's compliance officer is worried that scientists will be able to access each other's work. The research center has a strict obligation to report on which scientist accesses which documents. The team that is responsible for these reports has little AWS experience and wants a ready-to-use solution that minimizes operational overhead.Which combination of actions should a solutions architect take to meet these requirements? (Choose two.)",
    "choices": [
      "A. Create an identity policy that grants the user read and write access. Add a condition that specifies that the S3 paths must be prefixed with $(aws:username). Apply the policy on the scientists\u2019 IAM user group.",
      "B. Configure a trail with AWS CloudTrail to capture all object-level events in the S3 bucket. Store the trail output in another S3 bucket. Use Amazon Athena to query the logs and generate reports.",
      "C. Enable S3 server access logging. Configure another S3 bucket as the target for log delivery. Use Amazon Athena to query the logs and generate reports.",
      "D. Create an S3 bucket policy that grants read and write access to users in the scientists\u2019 IAM user group.",
      "E. Configure a trail with AWS CloudTrail to capture all object-level events in the S3 bucket and write the events to Amazon CloudWatch. Use the Amazon Athena CloudWatch connector to query the logs and generate reports."
    ],
    "answer": "AB",
    "explanation": "1. Explanation of the correct answer (AB):\n\na. Option A is correct because it creates an identity policy that grants the user read and write access, but with a condition that the S3 paths must be prefixed with $(aws:username). This ensures that each scientist can only access their own personal folder, addressing the research center's compliance requirement of preventing scientists from accessing each other's work.\n\nb. Option B is correct because it configures a trail with AWS CloudTrail to capture all object-level events in the S3 bucket. This provides a detailed audit trail of which scientist accessed which documents, allowing the research center to generate the required reports and fulfill the strict obligation to report on document access.\n\n2. Explanations of why the incorrect choices are wrong:\n\na. Option D is incorrect because a bucket policy that grants read and write access to the user group would allow scientists to access each other's folders, which is against the research center's compliance requirement.\n\nb. Option C is incorrect because enabling S3 server access logging and using Athena to query the logs would only provide access information, but not the specific user-to-document mapping required for the reporting obligation.\n\nc. Option E is incorrect because while it configures a CloudTrail trail and uses the Athena CloudWatch connector to query the logs, it does not address the issue of scientists accessing each other's folders. The policy in Option A is still required to ensure data isolation.\n\nIn summary, the correct answer is AB because it combines the proper access control mechanism (Option A) with the necessary auditing and reporting capabilities (Option B) to meet the research center's compliance requirements."
  },
  "307": {
    "question": "A company uses AWS Organizations to manage a multi-account structure. The company has hundreds of AWS accounts and expects the number of accounts to increase. The company is building a new application that uses Docker images. The company will push the Docker images to Amazon Elastic Container Registry (Amazon ECR). Only accounts that are within the company\u2019s organization should have access to the images.The company has a CI/CD process that runs frequently. The company wants to retain all the tagged images. However, the company wants to retain only the five most recent untagged images.Which solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      "A. Create a private repository in Amazon ECR. Create a permissions policy for the repository that allows only required ECR operations. Include a condition to allow the ECR operations if the value of the aws:PrincipalOrglD condition key is equal to the ID of the company\u2019s organization. Add a lifecycle rule to the ECR repository that deletes all untagged images over the count of five",
      "B. Create a public repository in Amazon ECR. Create an IAM role in the ECR account. Set permissions so that any account can assume the role if the value of the aws:PrincipalOrglD condition key is equal to the ID of the company\u2019s organization. Add a lifecycle rule to the ECR repository that deletes all untagged images over the count of five.",
      "C. Create a private repository in Amazon ECR. Create a permissions policy for the repository that includes only required ECR operations. Include a condition to allow the ECR operations for all account IDs in the organization Schedule a daily Amazon EventBridge rule to invoke an AWS Lambda function that deletes all untagged images over the count of five.",
      "D. Create a public repository in Amazon ECR. Configure Amazon ECR to use an interface VPC endpoint with an endpoint policy that includes the required permissions for images that the company needs to pull. Include a condition to allow the ECR operations for all account IDs in the company\u2019s organization. Schedule a daily Amazon EventBridge rule to invoke an AWS Lambda function that deletes all untagged images over the count of five."
    ],
    "answer": "A",
    "explanation": "1. Explanation of why the correct answer (A) is right:\n\nThe correct answer is A because it meets all the requirements with the least operational overhead:\n\n- It creates a private repository in Amazon ECR, which ensures that only accounts within the company's AWS organization can access the Docker images.\n- It creates a permissions policy for the repository that allows only the required ECR operations, reducing the risk of over-permissioning.\n- The policy includes a condition to allow the ECR operations if the `aws:PrincipalOrgId` condition key matches the ID of the company's organization. This is a scalable approach as the number of accounts in the organization grows, as opposed to individually managing access for each account.\n- It adds a lifecycle rule to the ECR repository to automatically delete all untagged images over the count of five, reducing the operational overhead of manual image management.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. This option creates a public repository in Amazon ECR, which is not desirable as it exposes the Docker images to the public. The solution also involves creating an IAM role and managing permissions, which adds more operational overhead compared to the correct answer.\n\nC. This option creates a private repository in Amazon ECR, which is good. However, it includes a condition to allow ECR operations for all account IDs in the organization, which is less scalable than the correct answer's approach of using the `aws:PrincipalOrgId` condition key. Additionally, it requires setting up a scheduled Amazon EventBridge rule and an AWS Lambda function to delete untagged images, which adds more operational overhead compared to the lifecycle rule in the correct answer.\n\nD. This option creates a public repository in Amazon ECR, which is not desirable. It also configures an interface VPC endpoint with an endpoint policy, which adds unnecessary complexity and operational overhead compared to the correct answer's simple permissions policy."
  },
  "308": {
    "question": "A solutions architect is reviewing a company's process for taking snapshots of Amazon RDS DB instances. The company takes automatic snapshots every day and retains the snapshots for 7 days.The solutions architect needs to recommend a solution that takes snapshots every 6 hours and retains the snapshots for 30 days. The company uses AWS Organizations to manage all of its AWS accounts. The company needs a consolidated view of the health of the RDS snapshots.Which solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      "A. Turn on the cross-account management feature in AWS Backup. Create a backup plan that specifies the frequency and retention requirements. Add a tag to the DB instances. Apply the backup plan by using tags. Use AWS Backup to monitor the status of the backups.",
      "B. Turn on the cross-account management feature in Amazon RDS. Create a snapshot global policy that specifies the frequency and retention requirements. Use the RDS console in the management account to monitor the status of the backups.",
      "C. Turn on the cross-account management feature in AWS CloudFormation. From the management account, deploy a CloudFormation stack set that contains a backup plan from AWS Backup that specifies the frequency and retention requirements. Create an AWS Lambda function in the management account to monitor the status of the backups. Create an Amazon EventBridge rule in each account to run the Lambda function on a schedule.",
      "D. Configure AWS Backup in each account. Create an Amazon Data Lifecycle Manager lifecycle policy that specifies the frequency and retention requirements. Specify the DB instances as the target resource Use the Amazon Data Lifecycle Manager console in each member account to monitor the status of the backups."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\nThe correct answer is A because it uses the AWS Backup service, which provides the least operational overhead while meeting all the requirements. Specifically:\n\n- AWS Backup supports cross-account management, allowing a centralized view of backups across all accounts.\n- The backup plan in AWS Backup can be configured to take snapshots every 6 hours and retain them for 30 days, meeting the frequency and retention requirements.\n- Applying the backup plan using tags simplifies the process and reduces operational overhead.\n- The AWS Backup service provides a consolidated view of the health of the RDS snapshots, meeting the need for a centralized monitoring solution.\n\n2. Explanations of why the other choices are incorrect:\n\nB. This option uses the cross-account management feature in Amazon RDS. However, RDS does not provide the ability to create a snapshot policy that can take snapshots every 6 hours and retain them for 30 days. The RDS console also does not provide a consolidated view of backups across multiple accounts.\n\nC. This option uses AWS CloudFormation and AWS Lambda to achieve the requirements. While it is possible to implement the desired functionality, it would require more operational overhead compared to using the fully-managed AWS Backup service. Additionally, the need to create and maintain a custom Lambda function adds complexity.\n\nD. This option uses AWS Backup and Amazon Data Lifecycle Manager in each individual account. While it meets the requirements, it does not provide a centralized view of backups across all accounts, which is a key requirement in the question. The need to monitor the status of backups in each account separately also increases the operational overhead."
  },
  "309": {
    "question": "A company is using AWS Organizations with a multi-account architecture. The company's current security configuration for the account architecture includes SCPs, resource-based policies, identity-based policies, trust policies, and session policies.A solutions architect needs to allow an IAM user in Account A to assume a role in Account B.Which combination of steps must the solutions architect take to meet this requirement? (Choose three.)",
    "choices": [
      "A. Configure the SCP for Account A to allow the action.",
      "B. Configure the resource-based policies to allow the action.",
      "C. Configure the identity-based policy on the user in Account A to allow the action.",
      "D. Configure the identity-based policy on the user in Account B to allow the action.",
      "E. Configure the trust policy on the target role in Account B to allow the action.",
      "F. Configure the session policy to allow the action and to be passed programmatically by the GetSessionToken API operation."
    ],
    "answer": "ACE",
    "explanation": "1. Explanation of the correct answer (ACE):\n\nA. Configure the SCP for Account A to allow the action.\nThis is correct because Service Control Policies (SCPs) are used to manage permissions at the AWS Organizations level. If there is an SCP that denies cross-account role assumption, it must be modified to allow the sts:AssumeRole action for Account A.\n\nC. Configure the identity-based policy on the user in Account A to allow the action.\nThis is correct because the IAM user in Account A needs permission to assume the role in Account B. This is done by attaching an identity-based policy that allows the sts:AssumeRole action on the target role in Account B.\n\nE. Configure the trust policy on the target role in Account B to allow the action.\nThis is correct because the IAM role in Account B must trust the user from Account A. This is done by adding a trust policy to the role, specifying Account A's user or a specific IAM principal from Account A.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. Configure the resource-based policies to allow the action.\nIncorrect, because resource-based policies are used to manage permissions on the resource itself, not for cross-account role assumption. Resource-based policies are not the appropriate mechanism to allow an IAM user in Account A to assume a role in Account B.\n\nD. Configure the identity-based policy on the user in Account B to allow the action.\nIncorrect, because the user in Account A needs permission to assume the role in Account B. Configuring the identity-based policy on the user in Account B would not achieve the desired outcome.\n\nF. Configure the session policy to allow the action and to be passed programmatically by the GetSessionToken API operation.\nIncorrect, because session policies cannot be passed programmatically using the GetSessionToken API operation. Session policies are passed programmatically using the AssumeRole, AssumeRoleWithSAML, AssumeRoleWithWebIdentity, and GetFederationToken API operations."
  },
  "310": {
    "question": "A company wants to use Amazon S3 to back up its on-premises file storage solution. The company\u2019s on-premises file storage solution supports NFS, and the company wants its new solution to support NFS. The company wants to archive the backup files after 5 days. If the company needs archived files for disaster recovery, the company is willing to wait a few days for the retrieval of those files.Which solution meets these requirements MOST cost-effectively?",
    "choices": [
      "A. Deploy an AWS Storage Gateway file gateway that is associated with an S3 bucket. Move the files from the on-premises file storage solution to the file gateway. Create an S3 Lifecycle rule to move the files to S3 Standard-Infrequent Access (S3 Standard-IA) after 5 days.",
      "B. Deploy an AWS Storage Gateway volume gateway that is associated with an S3 bucket. Move the files from the on-premises file storage solution to the volume gateway. Create an S3 Lifecycle rule to move the files to S3 Glacier Deep Archive after 5 days.",
      "C. Deploy an AWS Storage Gateway tape gateway that is associated with an S3 bucket. Move the files from the on-premises file storage solution to the tape gateway. Create an S3 Lifecycle rule to move the files to S3 Standard-Infrequent Access (S3 Standard-IA) after 5 days.",
      "D. Deploy an AWS Storage Gateway file gateway that is associated with an S3 bucket. Move the files from the on-premises file storage solution to the file gateway. Create an S3 Lifecycle rule to move the files to S3 Glacier Deep Archive after 5 days."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (Option D):\n\nThe correct answer is Option D, which involves deploying an AWS Storage Gateway file gateway associated with an S3 bucket, moving the files from the on-premises file storage solution to the file gateway, and creating an S3 Lifecycle rule to move the files to S3 Glacier Deep Archive after 5 days.\n\nThis solution meets all the requirements most cost-effectively:\n\n- The file gateway supports NFS, which meets the company's requirement for a file-based storage solution.\n- Moving the files to the file gateway, which is then backed up to S3, meets the company's need for cloud-based backup.\n- Configuring an S3 Lifecycle rule to move the files to Glacier Deep Archive after 5 days meets the requirement for archiving the backup files and reducing storage costs.\n- Glacier Deep Archive is the most cost-effective option for long-term, infrequent data retrieval, which aligns with the company's willingness to wait a few days for the retrieval of archived files during a disaster recovery scenario.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option uses S3 Standard-IA instead of Glacier Deep Archive, which is less cost-effective for long-term, infrequent data retrieval.\n\nB. This option uses a volume gateway instead of a file gateway, which does not support the company's requirement for an NFS-based storage solution.\n\nC. This option uses S3 Standard-IA instead of Glacier Deep Archive, which is less cost-effective for long-term, infrequent data retrieval. Additionally, the tape gateway does not support the company's requirement for an NFS-based storage solution."
  },
  "311": {
    "question": "A company runs its application on Amazon EC2 instances and AWS Lambda functions. The EC2 instances experience a continuous and stable load. The Lambda functions experience a varied and unpredictable load. The application includes a caching layer that uses an Amazon MemoryDB for Redis cluster.A solutions architect must recommend a solution to minimize the company's overall monthly costs.Which solution will meet these requirements?",
    "choices": [
      "A. Purchase an EC2 instance Savings Plan to cover the EC2 instances. Purchase a Compute Savings Plan for Lambda to cover the minimum expected consumption of the Lambda functions. Purchase reserved nodes to cover the MemoryDB cache nodes.",
      "B. Purchase a Compute Savings Plan to cover the EC2 instances. Purchase Lambda reserved concurrency to cover the expected Lambda usage. Purchase reserved nodes to cover the MemoryDB cache nodes.",
      "C. Purchase a Compute Savings Plan to cover the entire expected cost of the EC2 instances, Lambda functions, and MemoryDB cache nodes.",
      "D. Purchase a Compute Savings Plan to cover the EC2 instances and the MemoryDB cache nodes. Purchase Lambda reserved concurrency to cover the expected Lambda usage."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\n\nThe correct answer is A, which recommends the following solution:\n- Purchase an EC2 instance Savings Plan to cover the EC2 instances\n- Purchase a Compute Savings Plan for Lambda to cover the minimum expected consumption of the Lambda functions\n- Purchase reserved nodes to cover the MemoryDB cache nodes\n\nThis solution is the most cost-effective because it addresses each component of the application separately with the appropriate cost-saving measures:\n\n- EC2 instances have a continuous and stable load, so an EC2 instance Savings Plan provides a discounted rate for these instances.\n- Lambda functions have a varied and unpredictable load, so a Compute Savings Plan for Lambda covers the minimum expected consumption, which is more cost-effective than on-demand pricing.\n- MemoryDB cache nodes have a stable and predictable load, so reserved nodes provide a discounted rate for these nodes.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. This option recommends purchasing a Compute Savings Plan to cover the EC2 instances and Lambda reserved concurrency to cover the expected Lambda usage. However, Compute Savings Plans do not cover the MemoryDB cache nodes, making this solution less cost-effective than the correct answer.\n\nC. This option recommends purchasing a Compute Savings Plan to cover the entire expected cost of the EC2 instances, Lambda functions, and MemoryDB cache nodes. While this may provide some cost savings, it does not address the different usage patterns of the various components (EC2 instances, Lambda functions, and MemoryDB cache nodes) and misses the opportunity to optimize the cost-saving measures for each component individually.\n\nD. This option recommends purchasing a Compute Savings Plan to cover the EC2 instances and MemoryDB cache nodes, and Lambda reserved concurrency to cover the expected Lambda usage. Similar to choice B, this solution does not address the MemoryDB cache nodes with the most cost-effective option (reserved nodes), making it less optimal than the correct answer."
  },
  "312": {
    "question": "A company is launching a new online game on Amazon EC2 instances. The game must be available globally. The company plans to run the game in three AWS Regions us-east-1, eu-west-1, and ap-southeast-1. The game's leaderboards, player inventory and event status must be available across Regions.A solutions architect must design a solution that will give any Region the ability to scale to handle the load of all Regions. Additionally, users must automatically connect to the Region that provides the least latency.Which solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      "A. Create an EC2 Spot Fleet. Attach the Spot Fleet to a Network Load Balancer (NLB) in each Region. Create an AWS Global Accelerator IP address that points to the NLB. Create an Amazon Route 53 latency-based routing entry for the Global Accelerator IP address. Save the game metadata to an Amazon RDS for MySQL DB instance in each Region. Set up a read replica in the other Regions.",
      "B. Create an Auto Scaling group for the EC2 instances Attach the Auto Scaling group to a Network Load Balancer (NLB) in each Region. For each Region, create an Amazon Route 53 entry that uses geoproximity routing and points to the NLB in that Region. Save the game metadata to MySQL databases on EC2 instances in each Region. Set up replication between the database EC2 instances in each Region.",
      "C. Create an Auto Scaling group for the EC2 instances. Attach the Auto Scaling group to a Network Load Balancer (NLB) in each Region. For each Region, create an Amazon Route 53 entry that uses latency-based routing and points to the NLB in that Region. Save the game metadata to an Amazon DynamoDB global table.",
      "D. Use EC2 Global View. Deploy the EC2 instances to each Region. Attach the instances to a Network Load Balancer (NLB). Deploy a DNS server on an EC2 instance in each Region. Set up custom logic on each DNS server to redirect the user to the Region that provides the lowest latency. Save the game metadata to an Amazon Aurora global database."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is option C, which proposes the following solution:\n\n- Create an Auto Scaling group for the EC2 instances and attach it to a Network Load Balancer (NLB) in each Region.\n- For each Region, create an Amazon Route 53 entry that uses latency-based routing and points to the NLB in that Region.\n- Save the game metadata to an Amazon DynamoDB global table.\n\nThis solution meets the requirements in the most efficient way:\n\n- The Auto Scaling group in each Region can scale to handle the load of all Regions, satisfying the first requirement.\n- The use of latency-based routing with Amazon Route 53 automatically connects users to the Region with the lowest latency, addressing the second requirement.\n- Storing the game metadata in a DynamoDB global table eliminates the need for manual replication between Regions, reducing operational overhead.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This solution uses an NLB in each Region and a Global Accelerator IP address, which adds an additional layer of complexity compared to the simpler latency-based routing with Route 53 in the correct answer. Additionally, the use of an Amazon RDS MySQL database with read replicas also introduces more operational overhead than the DynamoDB global table.\n\nB. This solution uses geoproximity routing with Route 53, which may not be as effective as latency-based routing in automatically directing users to the closest Region. Additionally, the use of database replication between EC2 instances in each Region adds more operational complexity compared to the DynamoDB global table.\n\nD. This solution uses a custom DNS server in each Region to redirect users, which adds more operational overhead than the simpler latency-based routing with Route 53. Additionally, the use of an Amazon Aurora global database may not be as cost-effective or as scalable as the DynamoDB global table for storing game metadata."
  },
  "313": {
    "question": "A company is deploying a third-party firewall appliance solution from AWS Marketplace to monitor and protect traffic that leaves the company's AWS environments. The company wants to deploy this appliance into a shared services VPC and route all outbound internet-bound traffic through the appliances.A solutions architect needs to recommend a deployment method that prioritizes reliability and minimizes failover time between firewall appliances within a single AWS Region. The company has set up routing from the shared services VPC to other VPCs.Which steps should the solutions architect recommend to meet these requirements? (Choose three.)",
    "choices": [
      "A. Deploy two firewall appliances into the shared services VPC, each in a separate Availability Zone.",
      "B. Create a new Network Load Balancer in the shared services VPC. Create a new target group, and attach it to the new Network Load Balancer. Add each of the firewall appliance instances to the target group.",
      "C. Create a new Gateway Load Balancer in the shared services VPCreate a new target group, and attach it to the new Gateway Load Balancer Add each of the firewall appliance instances to the target group.",
      "D. Create a VPC interface endpoint. Add a route to the route table in the shared services VPC. Designate the new endpoint as the next hop for traffic that enters the shared services VPC from other VPCs.",
      "E. Deploy two firewall appliances into the shared services VPC, each in the same Availability Zone.",
      "F. Create a VPC Gateway Load Balancer endpoint. Add a route to the route table in the shared services VPC. Designate the new endpoint as the next hop for traffic that enters the shared services VPC from other VPCs."
    ],
    "answer": "ACF",
    "explanation": "1. Explanation of the correct answer (A, C, F):\n\nA. Deploy two firewall appliances into the shared services VPC, each in a separate Availability Zone.\nThis is recommended to achieve high availability and reliability. By deploying the firewall appliances in separate Availability Zones, the solution can withstand the failure of an entire Availability Zone, ensuring continuous operation.\n\nC. Create a new Gateway Load Balancer in the shared services VPC. Create a new target group, and attach it to the new Gateway Load Balancer. Add each of the firewall appliance instances to the target group.\nThe Gateway Load Balancer is designed to handle traffic entering or leaving the VPC, making it the appropriate choice for load balancing the traffic between the VPC and the firewall appliances. This ensures that the traffic is distributed across the firewall appliances, improving scalability and fault tolerance.\n\nF. Create a VPC Gateway Load Balancer endpoint. Add a route to the route table in the shared services VPC. Designate the new endpoint as the next hop for traffic that enters the shared services VPC from other VPCs.\nThe VPC Gateway Load Balancer endpoint provides a reliable and scalable way to route traffic from other VPCs to the shared services VPC and the firewall appliances. This ensures that all outbound internet-bound traffic from the company's AWS environments is channeled through the firewall appliances.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. Create a new Network Load Balancer in the shared services VPC. Create a new target group, and attach it to the new Network Load Balancer. Add each of the firewall appliance instances to the target group.\nWhile the Network Load Balancer can be used to distribute traffic across the firewall appliances, it is not the optimal choice for handling traffic entering or leaving the VPC. The Gateway Load Balancer (option C) is better suited for this task.\n\nD. Create a VPC interface endpoint. Add a route to the route table in the shared services VPC. Designate the new endpoint as the next hop for traffic that enters the shared services VPC from other VPCs.\nThe VPC interface endpoint is used to access AWS services, not to load balance traffic between fire"
  },
  "314": {
    "question": "A solutions architect needs to migrate an on-premises legacy application to AWS. The application runs on two servers behind a load balancer. The application requires a license file that is associated with the MAC address of the server's network adapter It takes the software vendor 12 hours to send new license files. The application also uses configuration files with a static IP address to access a database server, host names are not supported.Given these requirements, which combination of steps should be taken to implement highly available architecture for the application servers in AWS? (Choose two.)",
    "choices": [
      "A. Create a pool of ENIs. Request license files from the vendor for the pool, and store the license files in Amazon S3. Create a bootstrap automation script to download a license file and attach the corresponding ENI to an Amazon EC2 instance.",
      "B. Create a pool of ENIs. Request license files from the vendor for the pool, store the license files on an Amazon EC2 instance. Create an AMI from the instance and use this AMI for all future EC2 instances.",
      "C. Create a bootstrap automation script to request a new license file from the vendor .When the response is received, apply the license file to an Amazon EC2 instance.",
      "D. Edit the bootstrap automation script to read the database server IP address from the AWS Systems Manager Parameter Store, and inject the value into the local configuration files.",
      "E. Edit an Amazon EC2 instance to include the database server IP address in the configuration files and re-create the AMI to use for all future EC2 stances."
    ],
    "answer": "AD",
    "explanation": "1. Explanation of the correct answer (A and D):\n\nA. This option addresses the licensing requirement by creating a pool of ENIs, requesting license files from the vendor for the pool, and storing them in Amazon S3. The bootstrap automation script can then download the appropriate license file and attach the corresponding ENI to the EC2 instance, ensuring the MAC address requirement is met.\n\nD. This option solves the issue with the static IP address in the configuration files by using the AWS Systems Manager Parameter Store to dynamically retrieve the database server IP address. The bootstrap automation script can then inject this value into the local configuration files, eliminating the need for hardcoded IP addresses.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. Storing the license files on an EC2 instance directly is not recommended, as it introduces a single point of failure and makes it more difficult to manage the license files across multiple instances.\n\nC. Creating a bootstrap automation script to request a new license file from the vendor every time a new instance is launched is not a scalable solution, as it would introduce significant delays and dependencies on the vendor.\n\nE. Hardcoding the database server IP address in the configuration files and recreating the AMI for all future instances is not a flexible solution, as it would require manual intervention every time the database server IP address changes."
  },
  "315": {
    "question": "A company runs its sales reporting application in an AWS Region in the United States. The application uses an Amazon API Gateway Regional API and AWS Lambda functions to generate on-demand reports from data in an Amazon RDS for MySQL database. The frontend of the application is hosted on Amazon S3 and is accessed by users through an Amazon CloudFront distribution. The company is using Amazon Route 53 as the DNS service for the domain. Route 53 is configured with a simple routing policy to route traffic to the API Gateway API.In the next 6 months, the company plans to expand operations to Europe. More than 90% of the database traffic is read-only traffic. The company has already deployed an API Gateway API and Lambda functions in the new Region.A solutions architect must design a solution that minimizes latency for users who download reports.Which solution will meet these requirements?",
    "choices": [
      "A. Use an AWS Database Migration Service (AWS DMS) task with full load to replicate the primary database in the original Region to the database in the new Region. Change the Route 53 record to latency-based routing to connect to the API Gateway API.",
      "B. Use an AWS Database Migration Service (AWS DMS) task with full load plus change data capture (CDC) to replicate the primary database in the original Region to the database in the new Region. Change the Route 53 record to geolocation routing to connect to the API Gateway API.",
      "C. Configure a cross-Region read replica for the RDS database in the new Region Change the Route 53 record to latency-based routing to connect to the API Gateway API.",
      "D. Configure a cross-Region read replica for the RDS database in the new Region. Change the Route 53 record to geolocation routing to connect to the API Gateway API."
    ],
    "answer": "C",
    "explanation": "1. Explanation of why the correct answer (C) is right:\n\nThe correct answer is C because it provides the best solution to minimize latency for users who download reports. Configuring a cross-Region read replica for the RDS database in the new Region allows users in Europe to access the database that is closer to their location, reducing the latency for report downloads. Since the majority of the database traffic is read-only, a read replica setup is an appropriate solution. Additionally, using latency-based routing in Route 53 will direct users to the API Gateway API that is closest to them, further minimizing the latency.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This solution uses AWS DMS to replicate the primary database to the new Region, but it does not leverage the benefits of a read replica. The full load replication may not be as efficient as a read replica, and latency-based routing may not be the best approach for this scenario.\n\nB. Similar to option A, this solution uses AWS DMS with CDC to replicate the database. However, the use of geolocation-based routing may not be the most optimal approach, as it does not take into account the actual latency experienced by the users. Geolocation-based routing may not provide the same level of latency reduction as latency-based routing.\n\nD. This solution configures a cross-Region read replica, which is a good approach. However, the use of geolocation-based routing in Route 53 may not be the most effective in minimizing latency for users who download reports. Latency-based routing would be a better choice, as it can dynamically route users to the API Gateway API that provides the lowest latency."
  },
  "316": {
    "question": "A software company needs to create short-lived test environments to test pull requests as part of its development process. Each test environment consists of a single Amazon EC2 instance that is in an Auto Scaling group.The test environments must be able to communicate with a central server to report test results. The central server is located in an on-premises data center. A solutions architect must implement a solution so that the company can create and delete test environments without any manual intervention. The company has created a transit gateway with a VPN attachment to the on-premises network.Which solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      "A. Create an AWS CloudFormation template that contains a transit gateway attachment and related routing configurations. Create a CloudFormation stack set that includes this template. Use CloudFormation StackSets to deploy a new stack for each VPC in the account. Deploy a new VPC for each test environment.",
      "B. Create a single VPC for the test environments. Include a transit gateway attachment and related routing configurations. Use AWS CloudFormation to deploy all test environments into the VPC.",
      "C. Create a new OU in AWS Organizations for testing. Create an AWS CioudFormation template that contains a VPC, necessary networking resources, a transit gateway attachment, and related routing configurations. Create a CloudFormation stack set that includes this template. Use CloudFormation StackSets for deployments into each account under the testing OU. Create a new account for each test environment.",
      "D. Convert the test environment EC2 instances into Docker images. Use AWS CloudFormation to configure an Amazon Elastic Kubernetes Service (Amazon EKS) cluster in a new VPC, create a transit gateway attachment, and create related routing configurations. Use Kubernetes to manage the deployment and lifecycle of the test environments."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\nThe correct answer is option B because it creates a single VPC for all the test environments, which simplifies the network configuration and reduces the operational overhead compared to the other options. By using a single VPC, the test environments can share the same transit gateway attachment and routing configurations, making it easier to manage and maintain the network setup. This approach is suitable when the requirements do not specify that each test environment needs to have its own isolated network space, and the test environments only need to communicate with a central server in an on-premises data center.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option is not the most optimal solution because it requires creating a new VPC for each test environment, which can increase the operational overhead and complexity of managing the network setup. While this approach may provide more isolation between test environments, it is not necessary in this case, as the requirements do not specify the need for separate VPCs.\n\nC. This option is not recommended because it involves creating a new account for each test environment, which can be overkill and increase the operational overhead. Additionally, creating a new OU in AWS Organizations for testing may not be necessary if the test environments can be adequately isolated within a single VPC.\n\nD. This option is overly complex for the given requirements. Converting the test environment EC2 instances into Docker images and using an Amazon EKS cluster to manage the deployment and lifecycle of the test environments is an unnecessary complication, as the same functionality can be achieved more simply using a single VPC and the existing EC2 instances.\n\nIn summary, option B is the correct answer because it provides the most efficient and least operationally complex solution to meet the requirements, by using a single VPC for all the test environments and sharing the transit gateway attachment and routing configurations."
  },
  "317": {
    "question": "A company is deploying a new API to AWS. The API uses Amazon API Gateway with a Regional API endpoint and an AWS Lambda function for hosting. The API retrieves data from an external vendor API, stores data in an Amazon DynamoDB global table, and retrieves data from the DynamoDB global table The API key for the vendor's API is stored in AWS Secrets Manager and is encrypted with a customer managed key in AWS Key Management Service (AWS KMS). The company has deployed its own API into a single AWS Region.A solutions architect needs to change the API components of the company\u2019s API to ensure that the components can run across multiple Regions in an active-active configuration.Which combination of changes will meet this requirement with the LEAST operational overhead? (Choose three.)",
    "choices": [
      "A. Deploy the API to multiple Regions. Configure Amazon Route 53 with custom domain names that route traffic to each Regional API endpoint. Implement a Route 53 multivalue answer routing policy.",
      "B. Create a new KMS multi-Region customer managed key. Create a new KMS customer managed replica key in each in-scope Region.",
      "C. Replicate the existing Secrets Manager secret to other Regions. For each in-scope Region's replicated secret, select the appropriate KMS key.",
      "D. Create a new AWS managed KMS key in each in-scope Region. Convert an existing key to a multiRegion key. Use the multi-Region key in other Regions.",
      "E. Create a new Secrets Manager secret in each in-scope Region. Copy the secret value from the existing Region to the new secret in each in-scope Region.",
      "F. Modify the deployment process for the Lambda function to repeat the deployment across in-scope Regions. Turn on the multi-Region option for the existing API. Select the Lambda function that is deployed in each Region as the backend for the multi-Region API."
    ],
    "answer": "ABC",
    "explanation": "1. Clear explanation of why the correct answer is ABC:\n\nA. Deploy the API to multiple Regions. Configure Amazon Route 53 with custom domain names that route traffic to each Regional API endpoint. Implement a Route 53 multivalue answer routing policy.\nThis is correct because deploying the API to multiple Regions ensures that the API components can run across multiple Regions in an active-active configuration, meeting the requirement.\n\nB. Create a new KMS multi-Region customer managed key. Create a new KMS customer managed replica key in each in-scope Region.\nThis is correct because creating a new KMS multi-Region customer managed key, along with replica keys in each Region, ensures that the encryption keys used to encrypt the vendor's API secret are accessible across multiple Regions. This allows the use of a single secret in Secrets Manager across all Regions.\n\nC. Replicate the existing Secrets Manager secret to other Regions. For each in-scope Region's replicated secret, select the appropriate KMS key.\nThis is correct because replicating the existing Secrets Manager secret to other Regions, and selecting the appropriate KMS key for each Region's replicated secret, allows the use of a single secret across multiple Regions, reducing operational overhead.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nD. Create a new AWS managed KMS key in each in-scope Region. Convert an existing key to a multiRegion key. Use the multi-Region key in other Regions.\nThis is incorrect because creating AWS managed KMS keys in each Region and converting an existing key to a multi-Region key still requires managing multiple keys across Regions, which increases operational overhead.\n\nE. Create a new Secrets Manager secret in each in-scope Region. Copy the secret value from the existing Region to the new secret in each in-scope Region.\nThis is incorrect because creating a new Secrets Manager secret in each Region and copying the secret value to each Region also increases operational overhead, as it requires managing multiple secrets across Regions.\n\nF. Modify the deployment process for the Lambda function to repeat the deployment across in-scope Regions. Turn on the multi-Region option for the existing API. Select the Lambda function that is deployed in each Region as the backend for the multi-Region API.\nThis is incorrect because the question states that"
  },
  "318": {
    "question": "An online retail company hosts its stateful web-based application and MySQL database in an on-premises data center on a single server. The company wants to increase its customer base by conducting more marketing campaigns and promotions. In preparation, the company wants to migrate its application and database to AWS to increase the reliability of its architecture.Which solution should provide the HIGHEST level of reliability?",
    "choices": [
      "A. Migrate the database to an Amazon RDS MySQL Multi-AZ DB instance. Deploy the application in an Auto Scaling group on Amazon EC2 instances behind an Application Load Balancer. Store sessions in Amazon Neptune",
      "B. Migrate the database to Amazon Aurora MySQL. Deploy the application in an Auto Scaling group on Amazon EC2 instances behind an Application Load Balancer. Store sessions in an Amazon ElastiCache for Redis replication group.",
      "C. Migrate the database to Amazon DocumentDB (with MongoDB compatibility). Deploy the application in an Auto Scaling group on Amazon EC2 instances behind a Network Load Balancer Store sessions in Amazon Kinesis Data Firehose.",
      "D. Migrate the database to an Amazon RDS MariaDB Multi-AZ DB instance. Deploy the application in an Auto Scaling group on Amazon EC2 instances behind an Application Load Balancer. Store sessions in Amazon ElastiCache for Memcached."
    ],
    "answer": "B",
    "explanation": "1. Clear explanation of why the correct answer (B) is right:\n\nThe correct answer is B because it provides the highest level of reliability for the given scenario:\n\n- Migrating the database to Amazon Aurora MySQL provides a highly reliable and scalable database solution. Aurora is a fully managed, self-healing, and highly available database that supports up to 15 read replicas across multiple Availability Zones (AZs). This ensures high availability, fault tolerance, and scalability for the database layer.\n\n- Deploying the application in an Auto Scaling group on Amazon EC2 instances behind an Application Load Balancer ensures the application layer is also highly available and scalable. The Auto Scaling group can automatically scale the application instances based on demand, while the Application Load Balancer distributes traffic across the instances and provides high availability.\n\n- Storing sessions in an Amazon ElastiCache for Redis replication group adds a highly available caching layer. Redis replication provides automatic failover and redundancy, ensuring the session data is reliably stored and accessed.\n\nThe combination of a highly available database (Aurora), scalable and fault-tolerant application layer (Auto Scaling and Application Load Balancer), and a reliable caching solution (ElastiCache for Redis) provides the highest level of reliability for the given scenario.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nA. This solution uses Amazon RDS MySQL and Amazon Neptune, which are not as reliable as the options provided in the correct answer.\n\nC. Amazon DocumentDB is a document-oriented database, which may not be the most suitable choice for a web-based application that typically requires a relational database. Additionally, storing sessions in Amazon Kinesis Data Firehose is not the most appropriate solution for session management.\n\nD. Migrating the database to an Amazon RDS MariaDB instance is not as reliable as the Amazon Aurora MySQL solution in the correct answer. Additionally, using Amazon ElastiCache for Memcached for session storage is less reliable than the Redis replication group in the correct answer."
  },
  "319": {
    "question": "A company\u2019s solutions architect needs to provide secure Remote Desktop connectivity to users for Amazon EC2 Windows instances that are hosted in a VPC. The solution must integrate centralized user management with the company's on-premises Active Directory. Connectivity to the VPC is through the internet. The company has hardware that can be used to establish an AWS Site-to-Site VPN connection.Which solution will meet these requirements MOST cost-effectively?",
    "choices": [
      "A. Deploy a managed Active Directory by using AWS Directory Service for Microsoft Active Directory. Establish a trust with the on-premises Active Directory. Deploy an EC2 instance as a bastion host in the VPC. Ensure that the EC2 instance is joined to the domain. Use the bastion host to access the target instances through RDP.",
      "B. Configure AWS IAM Identity Center (AWS Single Sign-On) to integrate with the on-premises Active Directory by using the AWS Directory Service for Microsoft Active Directory AD Connector. Configure permission sets against user groups for access to AWS Systems Manager. Use Systems Manager Fleet Manager to access the target instances through RDP.",
      "C. Implement a VPN between the on-premises environment and the target VPEnsure that the target instances are joined to the on-premises Active Directory domain over the VPN connection. Configure RDP access through the VPN. Connect from the company\u2019s network to the target instances.",
      "D. Deploy a managed Active Directory by using AWS Directory Service for Microsoft Active Directory. Establish a trust with the on-premises Active Directory. Deploy a Remote Desktop Gateway on AWS by using an AWS Quick Start. Ensure that the Remote Desktop Gateway is joined to the domain. Use the Remote Desktop Gateway to access the target instances through RDP."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B, which recommends using AWS IAM Identity Center (AWS Single Sign-On) to integrate with the on-premises Active Directory and configure permission sets against user groups for access to AWS Systems Manager. This solution is the most cost-effective because it leverages AWS managed services (IAM Identity Center and Systems Manager) to provide secure remote access to the EC2 instances, without the need to deploy and manage additional infrastructure like a bastion host or a Remote Desktop Gateway.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This solution involves deploying a managed Active Directory using AWS Directory Service and setting up a trust with the on-premises Active Directory. It then requires deploying an EC2 instance as a bastion host to access the target instances through RDP. This solution is more complex and requires additional infrastructure management, which makes it less cost-effective compared to the recommended solution.\n\nC. This solution involves implementing a VPN connection between the on-premises environment and the target VPC. While this is a valid approach, it requires the company to manage and maintain the VPN infrastructure, which can be more complex and costly than using AWS managed services like IAM Identity Center and Systems Manager.\n\nD. This solution involves deploying a managed Active Directory using AWS Directory Service and establishing a trust with the on-premises Active Directory. It then requires deploying a Remote Desktop Gateway on AWS to access the target instances through RDP. This solution is more complex and requires additional infrastructure management, which makes it less cost-effective compared to the recommended solution.\n\nIn summary, the correct answer (B) is the most cost-effective solution because it leverages AWS managed services, reducing the need for additional infrastructure management and maintenance, which would be required in the other solutions."
  },
  "320": {
    "question": "A company's compliance audit reveals that some Amazon Elastic Block Store (Amazon EBS) volumes that were created in an AWS account were not encrypted. A solutions architect must implement a solution to encrypt all new EBS volumes at rest.Which solution will meet this requirement with the LEAST effort?",
    "choices": [
      "A. Create an Amazon EventBridge rule to detect the creation of unencrypted EBS volumes. Invoke an AWS Lambda function to delete noncompliant volumes.",
      "B. Use AWS Audit Manager with data encryption.",
      "C. Create an AWS Config rule to detect the creation of a new EBS volume. Encrypt the volume by using AWS Systems Manager Automation.",
      "D. Turn on EBS encryption by default in all AWS Regions."
    ],
    "answer": "D",
    "explanation": "1. Clear explanation of the correct answer (D):\n\nThe correct answer is D. Turn on EBS encryption by default in all AWS Regions.\n\nThis is the most appropriate solution because it proactively prevents the creation of unencrypted EBS volumes, which is the requirement stated in the question. By enabling EBS encryption by default, all new EBS volumes will be automatically encrypted at rest, without requiring any additional configuration or automation. This solution meets the requirement with the least effort, as it is a global setting that applies to all new EBS volumes going forward.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nA. Create an Amazon EventBridge rule to detect the creation of unencrypted EBS volumes. Invoke an AWS Lambda function to delete noncompliant volumes.\nThis solution is incorrect because it only addresses existing unencrypted EBS volumes, not preventing the creation of new unencrypted volumes. It also involves deleting the noncompliant volumes, which may not be the desired approach.\n\nB. Use AWS Audit Manager with data encryption.\nThis solution is incorrect because it does not directly address the requirement to encrypt all new EBS volumes. AWS Audit Manager is a governance, risk, and compliance (GRC) service that helps assess the compliance of your AWS environment, but it does not provide a mechanism to automatically encrypt new EBS volumes.\n\nC. Create an AWS Config rule to detect the creation of a new EBS volume. Encrypt the volume by using AWS Systems Manager Automation.\nThis solution is incorrect because it only addresses the encryption of new EBS volumes, not all new EBS volumes. It also requires additional configuration and automation, which is more effort than the correct solution of turning on EBS encryption by default."
  },
  "321": {
    "question": "A research company is running daily simulations in the AWS Cloud to meet high demand. The simulations run on several hundred Amazon EC2 instances that are based on Amazon Linux 2. Occasionally, a simulation gets stuck and requires a cloud operations engineer to solve the problem by connecting to an EC2 instance through SSH.Company policy states that no EC2 instance can use the same SSH key and that all connections must be logged in AWS CloudTrail.How can a solutions architect meet these requirements?",
    "choices": [
      "A. Launch new EC2 instances, and generate an individual SSH key for each instance. Store the SSH key in AWS Secrets Manager. Create a new IAM policy, and attach it to the engineers\u2019 IAM role with an Allow statement for the GetSecretValue action. Instruct the engineers to fetch the SSH key from Secrets Manager when they connect through any SSH client.",
      "B. Create an AWS Systems Manager document to run commands on EC2 instances to set a new unique SSH key. Create a new IAM policy, and attach it to the engineers\u2019 IAM role with an Allow statement to run Systems Manager documents. Instruct the engineers to run the document to set an SSH key and to connect through any SSH client.",
      "C. Launch new EC2 instances without setting up any SSH key for the instances. Set up EC2 Instance Connect on each instance. Create a new IAM policy, and attach it to the engineers\u2019 IAM role with an Allow statement for the SendSSHPublicKey action. Instruct the engineers to connect to the instance by using a browser-based SSH client from the EC2 console.",
      "D. Set up AWS Secrets Manager to store the EC2 SSH key. Create a new AWS Lambda function to create a new SSH key and to call AWS Systems Manager Session Manager to set the SSH key on the EC2 instance. Configure Secrets Manager to use the Lambda function for automatic rotation once daily. Instruct the engineers to fetch the SSH key from Secrets Manager when they connect through any SSH client."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C because it meets the given requirements:\n\n- No EC2 instance can use the same SSH key: By using EC2 Instance Connect, a unique one-time SSH key is generated for each connection, ensuring that no instance uses the same SSH key.\n- All connections must be logged in AWS CloudTrail: Connections made using EC2 Instance Connect are logged in AWS CloudTrail, providing the required logging functionality.\n\nEC2 Instance Connect allows users to connect to their EC2 instances using SSH without the need to manage any SSH keys. It generates a temporary SSH key for each connection, which is both secure and can be easily logged in CloudTrail.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option involves using AWS Secrets Manager to store SSH keys, which goes against the requirement of no EC2 instance using the same SSH key. Additionally, managing individual SSH keys for hundreds of instances would be time-consuming and complex.\n\nB. This option involves using AWS Systems Manager to set a new unique SSH key for each instance. While this would meet the requirement of no instance using the same SSH key, it does not address the logging requirement, as Systems Manager actions are not automatically logged in CloudTrail.\n\nD. This option uses a combination of AWS Secrets Manager and AWS Lambda to manage SSH keys. While it would meet the requirement of no instance using the same SSH key, it is more complex than the EC2 Instance Connect solution in C, and it doesn't directly address the logging requirement."
  },
  "322": {
    "question": "A company is migrating mobile banking applications to run on Amazon EC2 instances in a VPC. Backend service applications run in an on-premises data center. The data center has an AWS Direct Connect connection into AWS. The applications that run in the VPC need to resolve DNS requests to an on-premises Active Directory domain that runs in the data center.Which solution will meet these requirements with the LEAST administrative overhead?",
    "choices": [
      "A. Provision a set of EC2 instances across two Availability Zones in the VPC as caching DNS servers to resolve DNS queries from the application servers within the VPC.",
      "B. Provision an Amazon Route 53 private hosted zone. Configure NS records that point to on-premises DNS servers.",
      "C. Create DNS endpoints by using Amazon Route 53 Resolver. Add conditional forwarding rules to resolve DNS namespaces between the on-premises data center and the VPC.",
      "D. Provision a new Active Directory domain controller in the VPC with a bidirectional trust between this new domain and the on-premises Active Directory domain."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C, \"Create DNS endpoints by using Amazon Route 53 Resolver. Add conditional forwarding rules to resolve DNS namespaces between the on-premises data center and the VPC.\" This is the solution with the least administrative overhead for the given requirements.\n\nAmazon Route 53 Resolver is a managed service that provides DNS resolution between resources in your VPC and your on-premises network. It allows you to easily set up DNS resolution between your VPC and the on-premises Active Directory domain without the need to manage additional DNS infrastructure.\n\nThe key aspects of this solution are:\n\n- DNS Endpoints: Route 53 Resolver allows you to create inbound and outbound endpoints within your VPC to handle DNS queries between the VPC and the on-premises environment.\n- Conditional Forwarding Rules: You can configure conditional forwarding rules to resolve specific domain namespaces (such as the on-premises Active Directory domain) to the on-premises DNS servers. This ensures seamless DNS resolution for the applications running in the VPC.\n\nThis approach is the most efficient and least administratively burdensome solution, as it leverages managed AWS services to handle the DNS resolution requirements without the need to provision and manage additional DNS infrastructure.\n\n2. Explanations of the incorrect choices:\n\nA. \"Provision a set of EC2 instances across two Availability Zones in the VPC as caching DNS servers to resolve DNS queries from the application servers within the VPC.\"\nThis option would require additional infrastructure management and overhead, as you would need to provision, configure, and maintain the EC2 instances acting as DNS servers. It does not leverage the managed services provided by AWS to handle the DNS resolution requirements.\n\nB. \"Provision an Amazon Route 53 private hosted zone. Configure NS records that point to on-premises DNS servers.\"\nThis option is not suitable, as private hosted zones in Route 53 are intended for DNS records within AWS. They do not provide the functionality to resolve DNS queries to an on-premises Active Directory domain.\n\nD. \"Provision a new Active Directory domain controller in the VPC with a bidirectional trust between this new domain and the on-premises Active Directory domain.\"\nThis option would introduce significant administrative overhead, as it requires setting up and maintaining a"
  },
  "323": {
    "question": "A company needs to establish a connection from its on-premises data center to AWS. The company needs to connect all of its VPCs that are located in different AWS Regions with transitive routing capabilities between VPC networks. The company also must reduce network outbound traffic costs, increase bandwidth throughput, and provide a consistent network experience for end users.Which solution will meet these requirements?",
    "choices": [
      "A. Create an AWS Site-to-Site VPN connection between the on-premises data center and a new central VPC. Create VPC peering connections that initiate from the central VPC to all other VPCs.",
      "B. Create an AWS Direct Connect connection between the on-premises data center and AWS. Provision a transit VIF, and connect it to a Direct Connect gateway. Connect the Direct Connect gateway to all the other VPCs by using a transit gateway in each Region.",
      "C. Create an AWS Site-to-Site VPN connection between the on-premises data center and a new central VPUse a transit gateway with dynamic routing. Connect the transit gateway to all other VPCs.",
      "D. Create an AWS Direct Connect connection between the on-premises data center and AWS. Establish an AWS Site-to-Site VPN connection between all VPCs in each Region. Create VPC peering connections that initiate from the central VPC to all other VPCs."
    ],
    "answer": "B",
    "explanation": "1. Explanation of why the correct answer (B) is right:\n\nThe correct answer is B, which involves using AWS Direct Connect to connect the on-premises data center to AWS, and then using a Transit VIF (Virtual Interface) and a Direct Connect Gateway to connect the VPCs across different AWS regions.\n\nThis solution meets the requirements in the following ways:\n\n- The Direct Connect connection provides higher bandwidth and more consistent network performance compared to a Site-to-Site VPN, which helps meet the requirement of increasing bandwidth throughput and providing a consistent network experience.\n- The Transit VIF and Direct Connect Gateway allow for transitive routing between the VPCs, enabling connectivity between all the VPCs in different regions.\n- The use of the Direct Connect Gateway helps reduce network outbound traffic costs compared to using VPC peering or a Transit Gateway, as the traffic between VPCs is routed within the AWS network.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option uses VPC peering, which does not provide transitive routing capabilities between the VPCs. It also does not address the requirement of reducing network outbound traffic costs.\n\nC. This option uses a Site-to-Site VPN connection, which has lower bandwidth and a less consistent network experience compared to Direct Connect. It also does not address the requirement of reducing network outbound traffic costs.\n\nD. This option combines a Site-to-Site VPN connection with VPC peering, which has the same limitations as the previous two options. It also does not address the requirement of reducing network outbound traffic costs."
  },
  "324": {
    "question": "A company is migrating a legacy application from an on-premises data center to AWS. The application uses MongoDB as a key-value database. According to the company's technical guidelines, all Amazon EC2 instances must be hosted in a private subnet without an internet connection. In addition, all connectivity between applications and databases must be encrypted. The database must be able to scale based on demand.Which solution will meet these requirements?",
    "choices": [
      "A. Create new Amazon DocumentDB (with MongoDB compatibility) tables for the application with Provisioned IOPS volumes. Use the instance endpoint to connect to Amazon DocumentDB.",
      "B. Create new Amazon DynamoDB tables for the application with on-demand capacity. Use a gateway VPC endpoint for DynamoDB to connect to the DynamoDB tables.",
      "C. Create new Amazon DynamoDB tables for the application with on-demand capacity. Use an interface VPC endpoint for DynamoDB to connect to the DynamoDB tables.",
      "D. Create new Amazon DocumentDB (with MongoDB compatibility) tables for the application with Provisioned IOPS volumes. Use the cluster endpoint to connect to Amazon DocumentDB."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\nThe correct answer is B, which is to create new Amazon DynamoDB tables for the application with on-demand capacity and use a gateway VPC endpoint for DynamoDB to connect to the DynamoDB tables.\n\nThis is the best solution because:\n- DynamoDB is a fully managed NoSQL database that can automatically scale based on demand, which meets the requirement of the database being able to scale based on demand.\n- The application will be hosted in a private subnet without an internet connection, so a gateway VPC endpoint for DynamoDB is the appropriate choice to connect to the DynamoDB tables.\n- DynamoDB automatically encrypts data at rest and in transit, which meets the requirement of all connectivity between applications and databases being encrypted.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This is incorrect because Amazon DocumentDB uses Provisioned IOPS volumes, which are not automatically scalable based on demand.\n\nC. This is incorrect because interface VPC endpoints are used for services that have a regional endpoint, like Amazon S3 or Amazon SNS. DynamoDB has a global endpoint, so a gateway VPC endpoint should be used.\n\nD. This is incorrect because, similar to option A, Amazon DocumentDB uses Provisioned IOPS volumes, which are not automatically scalable based on demand."
  },
  "325": {
    "question": "A company is running an application on Amazon EC2 instances in the AWS Cloud. The application is using a MongoDB database with a replica set as its data tier. The MongoDB database is installed on systems in the company\u2019s on-premises data center and is accessible through an AWS Direct Connect connection to the data center environment.A solutions architect must migrate the on-premises MongoDB database to Amazon DocumentDB (with MongoDB compatibility).Which strategy should the solutions architect choose to perform this migration?",
    "choices": [
      "A. Create a fleet of EC2 instances. Install MongoDB Community Edition on the EC2 instances, and create a database. Configure continuous synchronous replication with the database that is running in the on-premises data center.",
      "B. Create an AWS Database Migration Service (AWS DMS) replication instance. Create a source endpoint for the on-premises MongoDB database by using change data capture (CDC). Create a target endpoint for the Amazon DocumentDB database. Create and run a DMS migration task.",
      "C. Create a data migration pipeline by using AWS Data Pipeline. Define data nodes for the on-premises MongoDB database and the Amazon DocumentDB database. Create a scheduled task to run the data pipeline.",
      "D. Create a source endpoint for the on-premises MongoDB database by using AWS Glue crawlers. Configure continuous asynchronous replication between the MongoDB database and the Amazon DocumentDB database."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B, which is to use AWS Database Migration Service (AWS DMS) to migrate the on-premises MongoDB database to Amazon DocumentDB.\n\nThis is the best strategy because:\n- AWS DMS is a managed service that can perform database migrations, including from on-premises MongoDB to Amazon DocumentDB.\n- AWS DMS supports change data capture (CDC), which allows it to replicate changes from the source MongoDB database to the target Amazon DocumentDB database in real-time. This helps minimize downtime and data loss during the migration.\n- AWS DMS simplifies the migration process by handling the creation of the source and target database endpoints, as well as the execution of the migration task itself.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This approach of installing MongoDB Community Edition on EC2 instances and configuring replication with the on-premises database is not recommended, as it requires additional manual setup and management of the EC2 instances and MongoDB replication. It does not leverage the managed services provided by AWS for database migrations.\n\nC. Using AWS Data Pipeline to create a data migration pipeline is not the best approach for this scenario. Data Pipeline is more suitable for periodic or scheduled data transfers, not real-time database migrations. It also does not provide the same level of automation and management features as AWS DMS.\n\nD. Creating a source endpoint using AWS Glue crawlers and configuring asynchronous replication is not the recommended approach. Asynchronous replication may result in data loss, and Glue crawlers are more suitable for data discovery and catalog management, not for real-time database migrations."
  },
  "326": {
    "question": "A company is rearchitecting its applications to run on AWS. The company\u2019s infrastructure includes multiple Amazon EC2 instances. The company's development team needs different levels of access. The company wants to implement a policy that requires all Windows EC2 instances to be joined to an Active Directory domain on AWS. The company also wants to implement enhanced security processes such as multi-factor authentication (MFA). The company wants to use managed AWS services wherever possible.Which solution will meet these requirements?",
    "choices": [
      "A. Create an AWS Directory Service for Microsoft Active Directory implementation. Launch an Amazon Workspace. Connect to and use the Workspace for domain security configuration tasks.",
      "B. Create an AWS Directory Service for Microsoft Active Directory implementation. Launch an EC2 instance. Connect to and use the EC2 instance for domain security configuration tasks.",
      "C. Create an AWS Directory Service Simple AD implementation. Launch an EC2 instance. Connect to and use the EC2 instance for domain security configuration tasks.",
      "D. Create an AWS Directory Service Simple AD implementation. Launch an Amazon Workspace. Connect to and use the Workspace for domain security configuration tasks."
    ],
    "answer": "B",
    "explanation": "1. Explanation of why the correct answer (B) is right:\n\nThe correct answer is B, which is to create an AWS Directory Service for Microsoft Active Directory implementation and launch an EC2 instance to connect to and use for domain security configuration tasks.\n\nThis is the right solution because:\n\n- The question states that the company wants to implement a policy that requires all Windows EC2 instances to be joined to an Active Directory domain on AWS. Creating an AWS Directory Service for Microsoft Active Directory fulfills this requirement.\n- The question also states that the company wants to use managed AWS services wherever possible. Launching an EC2 instance to perform the domain security configuration tasks is a better option than using an Amazon Workspace, which is not recommended for anything beyond minimal scale (5+ Workspaces) according to AWS best practices.\n- Using an EC2 instance provides a more robust and centralized approach to AD administration compared to using a Workspace.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Create an AWS Directory Service for Microsoft Active Directory implementation. Launch an Amazon Workspace. Connect to and use the Workspace for domain security configuration tasks.\n- This is not the best solution because AWS recommends against using Workspaces for anything beyond minimal scale (5+ Workspaces) for AD administration tasks. An EC2 instance is a more robust and scalable option.\n\nC. Create an AWS Directory Service Simple AD implementation. Launch an EC2 instance. Connect to and use the EC2 instance for domain security configuration tasks.\n- Simple AD is a directory service based on Samba 4, and it does not provide the full set of Active Directory features required for the company's needs, such as support for multi-factor authentication (MFA).\n\nD. Create an AWS Directory Service Simple AD implementation. Launch an Amazon Workspace. Connect to and use the Workspace for domain security configuration tasks.\n- Similar to choice A, this is not the best solution because of the limitations of using Workspaces for AD administration tasks. Additionally, Simple AD does not provide the full Active Directory features required."
  },
  "327": {
    "question": "A financial services company runs a complex, multi-tier application on Amazon EC2 instances and AWS Lambda functions. The application stores temporary data in Amazon S3. The S3 objects are valid for only 45 minutes and are deleted after 24 hours.The company deploys each version of the application by launching an AWS CloudFormation stack. The stack creates all resources that are required to run the application. When the company deploys and validates a new application version, the company deletes the CloudFormation stack of the old version.The company recently tried to delete the CloudFormation stack of an old application version, but the operation failed. An analysis shows that CloudFormation failed to delete an existing S3 bucket. A solutions architect needs to resolve this issue without making major changes to the application's architecture.Which solution meets these requirements?",
    "choices": [
      "A. Implement a Lambda function that deletes all files from a given S3 bucket. Integrate this Lambda function as a custom resource into the CloudFormation stack. Ensure that the custom resource has a DependsOn attribute that points to the S3 bucket's resource.",
      "B. Modify the CloudFormation template to provision an Amazon Elastic File System (Amazon EFS) file system to store the temporary files there instead of in Amazon S3. Configure the Lambda functions to run in the same VPC as the file system. Mount the file system to the EC2 instances and Lambda functions.",
      "C. Modify the CloudF ormation stack to create an S3 Lifecycle rule that expires all objects 45 minutes after creation. Add a DependsOn attribute that points to the S3 bucket\u2019s resource.",
      "D. Modify the CloudFormation stack to attach a DeletionPolicy attribute with a value of Delete to the S3 bucket."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\nThe correct answer is A because it addresses the issue of the CloudFormation stack failing to delete the existing S3 bucket due to the bucket not being empty. By implementing a Lambda function as a custom resource in the CloudFormation stack, the solution can ensure that the S3 bucket is properly cleaned up (i.e., all files are deleted) before the bucket itself is deleted. This allows the CloudFormation stack deletion to complete successfully without making major changes to the application's architecture.\n\n2. Explanations of why the incorrect choices are wrong:\nB. This option is incorrect because it requires making a significant change to the application's architecture by replacing Amazon S3 with Amazon EFS. The question specifically states that the solution should not involve major changes to the application's architecture.\n\nC. This option is incorrect because it relies on the application's existing logic to delete the objects after 24 hours. However, the question states that the CloudFormation stack deletion is failing due to the S3 bucket not being empty, which means the existing logic is not sufficient to address the issue.\n\nD. This option is incorrect because the DeletionPolicy attribute only controls what happens to the bucket itself, not its contents. If the bucket is not empty, the CloudFormation stack deletion will still fail, even with the DeletionPolicy set to \"Delete\"."
  },
  "328": {
    "question": "A company orchestrates a multi-account structure on AWS by using AWS Control Tower. The company is using AWS Organizations, AWS Config, and AWS Trusted Advisor. The company has a specific OU for development accounts that developers use to experiment on AWS. The company has hundreds of developers, and each developer has an individual development account.The company wants to optimize costs in these development accounts. Amazon EC2 instances and Amazon RDS instances in these accounts must be burstable. The company wants to disallow the use of other services that are not relevant.What should a solutions architect recommend to meet these requirements?",
    "choices": [
      "A. Create a custom SCP in AWS Organizations to allow the deployment of only burstable instances and to disallow services that are not relevant. Apply the SCP to the development OU.",
      "B. Create a custom detective control (guardrail) in AWS Control Tower. Configure the control (guardrail) to allow the deployment of only burstable instances and to disallow services that are not relevant. Apply the control (guardrail) to the development OU.",
      "C. Create a custom preventive control (guardrail) in AWS Control Tower. Configure the control (guardrail) to allow the deployment of only burstable instances and to disallow services that are not relevant. Apply the control (guardrail) to the development OU.",
      "D. Create an AWS Config rule in the AWS Control Tower account. Configure the AWS Config rule to allow the deployment of only burstable instances and to disallow services that are not relevant. Deploy the AWS Config rule to the development OU by using AWS CloudFormation StackSets."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C because the question specifically states that the company wants to \"disallow the use of other services that are not relevant\" and \"the company wants to optimize costs in these development accounts\". The use of a custom preventive control (guardrail) in AWS Control Tower is the appropriate solution to meet these requirements.\n\nPreventive controls in AWS Control Tower are designed to enforce specific policies and disallow actions that lead to policy violations. By creating a custom preventive control, the solutions architect can configure it to allow the deployment of only burstable instances (EC2 and RDS) and disallow the use of other services that are not relevant to the development accounts. This ensures that the cost optimization requirements are met, and the company's policies are enforced.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This is incorrect because using a custom SCP in AWS Organizations to control the development OU would create governance drift, which is not recommended when using AWS Control Tower. AWS Control Tower is designed to manage the entire account structure, and making changes to the OU directly in AWS Organizations can cause issues with the control tower's management capabilities.\n\nB. This is incorrect because there is no such thing as a \"custom detective control (guardrail)\" in AWS Control Tower. Detective controls in AWS Control Tower are designed to monitor and report on compliance, not to enforce specific policies.\n\nD. This is incorrect because creating an AWS Config rule in the AWS Control Tower account and deploying it to the development OU using CloudFormation StackSets is not the recommended approach. AWS Control Tower is designed to manage the entire account structure, and using a separate AWS Config rule may not be as effective in enforcing the specific policies required in this scenario."
  },
  "329": {
    "question": "A financial services company runs a complex, multi-tier application on Amazon EC2 instances and AWS Lambda functions. The application stores temporary data in Amazon S3. The S3 objects are valid for only 45 minutes and are deleted after 24 hours.The company deploys each version of the application by launching an AWS CloudFormation stack. The stack creates all resources that are required to run the application. When the company deploys and validates a new application version, the company deletes the CloudFormation stack of the old version.The company recently tried to delete the CloudFormation stack of an old application version, but the operation failed. An analysis shows that CloudFormation failed to delete an existing S3 bucket. A solutions architect needs to resolve this issue without making major changes to the application's architecture.Which solution meets these requirements?",
    "choices": [
      "A. Implement a Lambda function that deletes all files from a given S3 bucket. Integrate this Lambda function as a custom resource into the CloudFormation stack. Ensure that the custom resource has a DependsOn attribute that points to the S3 bucket's resource.",
      "B. Modify the CloudFormation template to provision an Amazon Elastic File System (Amazon EFS) file system to store the temporary files there instead of in Amazon S3. Configure the Lambda functions to run in the same VPC as the file system. Mount the file system to the EC2 instances and Lambda functions.",
      "C. Modify the CloudF ormation stack to create an S3 Lifecycle rule that expires all objects 45 minutes after creation. Add a DependsOn attribute that points to the S3 bucket\u2019s resource.",
      "D. Modify the CloudFormation stack to attach a DeletionPolicy attribute with a value of Delete to the S3 bucket."
    ],
    "answer": "A",
    "explanation": "Explanation:\n\n1. Correct Answer: A\nThe correct answer is A because it provides a solution that meets the requirements without making major changes to the application's architecture.\n\nThe key points are:\n- The CloudFormation stack deletion is failing because the S3 bucket cannot be deleted due to it not being empty.\n- Implementing a Lambda function as a custom resource in the CloudFormation stack allows you to trigger the deletion of the S3 bucket contents before the stack is deleted.\n- The DependsOn attribute ensures that the custom resource Lambda function runs after the S3 bucket resource, ensuring the bucket is empty before deletion.\n\nThis solution allows the CloudFormation stack to be properly deleted without requiring changes to the application itself.\n\n2. Incorrect Answers:\n\nB. This solution involves modifying the application's architecture to use Amazon EFS instead of Amazon S3 for temporary file storage. This is considered a major change, which goes against the requirements.\n\nC. This solution modifies the CloudFormation stack to create an S3 Lifecycle rule to expire objects after 45 minutes. However, the question states that the objects are valid for only 45 minutes and are deleted after 24 hours. Modifying the application's behavior would be considered a major change.\n\nD. This solution adds a DeletionPolicy attribute with a value of \"Delete\" to the S3 bucket resource in the CloudFormation stack. While this would allow the bucket to be deleted, it does not address the issue of the bucket not being empty, which is the root cause of the CloudFormation stack deletion failure."
  },
  "330": {
    "question": "A company has developed a mobile game. The backend for the game runs on several virtual machines located in an on-premises data center. The business logic is exposed using a REST API with multiple functions. Player session data is stored in central file storage. Backend services use different API keys for throttling and to distinguish between live and test traffic.The load on the game backend varies throughout the day. During peak hours, the server capacity is not sufficient. There are also latency issues when fetching player session data. Management has asked a solutions architect to present a cloud architecture that can handle the game\u2019s varying load and provide low-latency data access. The API model should not be changed.Which solution meets these requirements?",
    "choices": [
      "A. Implement the REST API using a Network Load Balancer (NLB). Run the business logic on an Amazon EC2 instance behind the NLB. Store player session data in Amazon Aurora Serverless.",
      "B. Implement the REST API using an Application Load Balancer (ALB). Run the business logic in AWS Lambda. Store player session data in Amazon DynamoDB with on-demand capacity.",
      "C. Implement the REST API using Amazon API Gateway. Run the business logic in AWS Lambda. Store player session data in Amazon DynamoDB with on-demand capacity.",
      "D. Implement the REST API using AWS AppSync. Run the business logic in AWS Lambda. Store player session data in Amazon Aurora Serverless."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C. Implementing the REST API using Amazon API Gateway, running the business logic in AWS Lambda, and storing player session data in Amazon DynamoDB with on-demand capacity.\n\nThis solution meets the requirements of the problem statement:\n\n- Amazon API Gateway can handle the varying load on the game backend by automatically scaling to handle the traffic. It can also manage the API keys for throttling and distinguishing between live and test traffic.\n- AWS Lambda is a serverless compute service that can run the business logic without the need to manage any underlying infrastructure. This allows the solution to scale up and down based on the load, without worrying about server capacity.\n- Amazon DynamoDB is a highly scalable, low-latency NoSQL database that can store the player session data. The on-demand capacity mode allows DynamoDB to automatically scale up and down based on the traffic, providing the required low-latency data access.\n- This architecture does not require any changes to the existing API model, as API Gateway can seamlessly integrate with the existing REST API.\n\n2. Explanations of why the other choices are incorrect:\n\nA. Implement the REST API using a Network Load Balancer (NLB), run the business logic on an Amazon EC2 instance behind the NLB, and store player session data in Amazon Aurora Serverless.\n- This solution does not provide automatic scaling capabilities for the game backend. The EC2 instances would need to be manually scaled to handle the varying load, which may not be efficient.\n- Amazon Aurora Serverless is a good choice for the database, but it may not provide the low-latency data access required for the player session data.\n\nB. Implement the REST API using an Application Load Balancer (ALB), run the business logic in AWS Lambda, and store player session data in Amazon DynamoDB with on-demand capacity.\n- While this solution uses AWS Lambda for the business logic and DynamoDB for the player session data, which is similar to the correct answer, it does not use Amazon API Gateway to expose the REST API. The ALB may not be as efficient in handling the varying load and managing the API keys as API Gateway.\n\nD. Implement the REST API using AWS AppSync, run the business logic in AWS Lambda"
  },
  "331": {
    "question": "A company is migrating an application to the AWS Cloud. The application runs in an on-premises data center and writes thousands of images into a mounted NFS file system each night. After the company migrates the application, the company will host the application on an Amazon EC2 instance with a mounted Amazon Elastic File System (Amazon EFS) file system.The company has established an AWS Direct Connect connection to AWS. Before the migration cutover, a solutions architect must build a process that will replicate the newly created on-premises images to the EFS file system.What is the MOST operationally efficient way to replicate the images?",
    "choices": [
      "A. Configure a periodic process to run the aws s3 sync command from the on-premises file system to Amazon S3. Configure an AWS Lambda function to process event notifications from Amazon S3 and copy the images from Amazon S3 to the EFS file system.",
      "B. Deploy an AWS Storage Gateway file gateway with an NFS mount point. Mount the file gateway file system on the on-premises server. Configure a process to periodically copy the images to the mount point.",
      "C. Deploy an AWS DataSync agent to an on-premises server that has access to the NFS file system. Send data over the Direct Connect connection to an S3 bucket by using a public VIF. Configure an AWS Lambda function to process event notifications from Amazon S3 and copy the images from Amazon S3 to the EFS file system.",
      "D. Deploy an AWS DataSync agent to an on-premises server that has access to the NFS file system. Send data over the Direct Connect connection to an AWS PrivateLink interface VPC endpoint for Amazon EFS by using a private VIF. Configure a DataSync scheduled task to send the images to the EFS file system every 24 hours."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (Option D):\n\nThe most operationally efficient way to replicate the images from the on-premises NFS file system to the AWS EFS file system is to use the AWS DataSync service (Option D). This is the most efficient approach for the following reasons:\n\n- AWS DataSync can directly copy data from the on-premises NFS file system to the AWS EFS file system, without the need for an intermediate storage step like Amazon S3 (as in Option C).\n- DataSync can leverage the AWS Direct Connect connection to transfer data securely and efficiently between the on-premises environment and the AWS EFS file system.\n- DataSync supports a scheduled task feature, which can be configured to automatically copy the new images to EFS on a regular schedule (e.g., every 24 hours), providing an automated and reliable replication process.\n- This approach eliminates the need for additional components like AWS Lambda functions to manage the data transfer process, making it a more operationally efficient solution.\n\n2. Explanations of why the incorrect choices are wrong:\n\nOption A:\n- Using an S3 intermediate step is less efficient than a direct transfer from on-premises to EFS, as it adds an unnecessary step and requires additional processing (e.g., the AWS Lambda function).\n- Relying on S3 event notifications and a Lambda function to copy the images to EFS adds complexity and potential failure points to the solution.\n\nOption B:\n- Using an AWS Storage Gateway file gateway does not directly address the requirement of replicating data from the on-premises NFS file system to the AWS EFS file system.\n- The question specifically states that the company will host the application on an EC2 instance with a mounted EFS file system, so the Storage Gateway solution does not align with the target architecture.\n\nOption C:\n- While using AWS DataSync to transfer data from the on-premises NFS file system to Amazon S3 is a valid approach, it still requires an additional step of copying the data from S3 to the EFS file system, which is less efficient than the direct DataSync to EFS approach in Option D.\n- The use of a public VIF for the DataSync connection is less secure than the private VIF option provided in Option D."
  },
  "332": {
    "question": "A company recently migrated a web application from an on-premises data center to the AWS Cloud. The web application infrastructure consists of an Amazon CloudFront distribution that routes to an Application Load Balancer (ALB), with Amazon Elastic Container Service (Amazon ECS) to process requests. A recent security audit revealed that the web application is accessible by using both CloudFront and ALB endpoints. However, the company requires that the web application must be accessible only by using the CloudFront endpoint.Which solution will meet this requirement with the LEAST amount of effort?",
    "choices": [
      "A. Create a new security group and attach it to the CloudFront distribution. Update the ALB security group ingress to allow access only from the CloudFront security group.",
      "B. Update ALB security group ingress to allow access only from the com.amazonaws.global.cloudfront.origin-facing CloudFront managed prefix list.",
      "C. Create a com.amazonaws.region.elasticloadbalancing VPC interface endpoint for Elastic Load Balancing. Update the ALB scheme from internet-facing to internal.",
      "D. Extract CloudFront IPs from the AWS provided ip-ranges.json document. Update ALB security group ingress to allow access only from CloudFront IPs."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B because it directly addresses the requirement to allow access to the web application only through the CloudFront endpoint. By updating the ALB security group ingress to allow access only from the com.amazonaws.global.cloudfront.origin-facing CloudFront managed prefix list, you can ensure that only the CloudFront distribution can access the ALB, effectively preventing direct access to the ALB endpoint. This is the most straightforward solution that requires the least amount of effort.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This solution is incorrect because you cannot directly assign a security group to a CloudFront distribution. CloudFront is a global service, and its security is managed by AWS, not by user-defined security groups.\n\nC. This solution is incorrect because creating a VPC interface endpoint for Elastic Load Balancing and changing the ALB scheme to internal would not prevent direct access to the ALB endpoint. It would only restrict access to the ALB within the VPC, but the requirement is to restrict access from the internet, not within the VPC.\n\nD. This solution is incorrect because maintaining a list of CloudFront IPs and updating the ALB security group ingress accordingly would be a more complex and error-prone solution. The CloudFront managed prefix list (used in the correct answer) is a more robust and maintainable solution, as it is automatically updated by AWS."
  },
  "333": {
    "question": "A company hosts a community forum site using an Application Load Balancer (ALB) and a Docker application hosted in an Amazon ECS cluster. The site data is stored in Amazon RDS for MySQL and the container image is stored in ECR. The company needs to provide their customers with a disaster recovery SLA with an RTO of no more than 24 hours and RPO of no more than 8 hours.Which of the following solutions is the MOST cost-effective way to meet the requirements?",
    "choices": [
      "A. Use AWS CloudFormation to deploy identical ALB, EC2, ECS and RDS resources in two regions. Schedule RDS snapshots every 8 hours. Use RDS multi-region replication to update the secondary region's copy of the database. In the event of a failure, restore from the latest snapshot, and use an Amazon Route 53 DNS failover policy to automatically redirect customers to the ALB in the secondary region.",
      "B. Store the Docker image in ECR in two regions. Schedule RDS snapshots every 8 hours with snapshots copied to the secondary region. In the event of a failure, use AWS CloudFormation to deploy the ALB, EC2, ECS and RDS resources in the secondary region, restore from the latest snapshot, and update the DNS record to point to the ALB in the secondary region.",
      "C. Use AWS CloudFormation to deploy identical ALB, EC2, ECS, and RDS resources in a secondary region. Schedule hourly RDS MySQL backups to Amazon S3 and use cross-region replication to replicate data to a bucket in the secondary region. In the event of a failure, import the latest Docker image to Amazon ECR in the secondary region, deploy to the EC2 instance, restore the latest MySQL backup, and update the DNS record to point to the ALB in the secondary region.",
      "D. Deploy a pilot light environment in a secondary region with an ALB and a minimal resource EC2 deployment for Docker in an AWS Auto Scaling group with a scaling policy to increase instance size and number of nodes. Create a cross-region read replica of the RDS data. In the event of a failure, promote the replica to primary, and update the DNS record to point to the ALB in the secondary region."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (Option B):\n\nOption B is the most cost-effective solution to meet the given requirements of 24-hour RTO and 8-hour RPO.\n\nThe key points are:\n- Storing the Docker image in ECR in two regions ensures that the container image is readily available in the secondary region in case of a failure.\n- Scheduling RDS snapshots every 8 hours and copying them to the secondary region satisfies the 8-hour RPO requirement.\n- In the event of a failure, AWS CloudFormation can be used to quickly deploy the necessary ALB, EC2, ECS, and RDS resources in the secondary region, restoring from the latest snapshot. This meets the 24-hour RTO requirement.\n- Updating the DNS record to point to the ALB in the secondary region ensures automatic failover for customers.\n\nThis solution is more cost-effective than maintaining a fully provisioned environment in the secondary region (Option A) or using hourly backups and cross-region replication (Option C), as it only requires the necessary resources to be deployed on-demand during a disaster scenario.\n\n2. Explanation of why the incorrect choices are wrong:\n\nOption A:\n- Maintaining identical resources in two regions, even when not in use, is more expensive than the on-demand approach in Option B.\n- RDS multi-region replication may add additional complexity and cost compared to the simpler snapshot-based solution.\n\nOption C:\n- Hourly RDS backups to S3 and cross-region replication add more complexity and potential costs compared to the 8-hour snapshot-based approach in Option B.\n- Restoring the MySQL backup and deploying the Docker image in the secondary region may take longer than the CloudFormation-based approach in Option B, potentially exceeding the 24-hour RTO.\n\nOption D:\n- Maintaining a pilot light environment with a read replica adds more complexity and cost compared to the simple snapshot-based approach in Option B.\n- Promoting a read replica to primary may take longer than restoring from a snapshot, potentially exceeding the 24-hour RTO."
  },
  "334": {
    "question": "A company is migrating its infrastructure to the AWS Cloud. The company must comply with a variety of regulatory standards for different projects. The company needs a multi-account environment.A solutions architect needs to prepare the baseline infrastructure. The solution must provide a consistent baseline of management and security, but it must allow flexibility for different compliance requirements within various AWS accounts. The solution also needs to integrate with the existing on-premises Active Directory Federation Services (AD FS) server.Which solution meets these requirements with the LEAST amount of operational overhead?",
    "choices": [
      "A. Create an organization in AWS Organizations. Create a single SCP for least privilege access across all accounts. Create a single OU for all accounts. Configure an IAM identity provider for federation with the on-premises AD FS server. Configure a central logging account with a defined process for log generating services to send log events to the central account. Enable AWS Config in the central account with conformance packs for all accounts.",
      "B. Create an organization in AWS Organizations. Enable AWS Control Tower on the organization. Review included controls (guardrails) for SCPs. Check AWS Config for areas that require additions. Add OUs as necessary. Connect AWS IAM Identity Center (AWS Single Sign-On) to the on-premises AD FS server.",
      "C. Create an organization in AWS Organizations. Create SCPs for least privilege access. Create an OU structure, and use it to group AWS accounts. Connect AWS IAM Identity Center (AWS Single Sign-On) to the on-premises AD FS server. Configure a central logging account with a defined process for log generating services to send log events to the central account. Enable AWS Config in the central account with aggregators and conformance packs.",
      "D. Create an organization in AWS Organizations. Enable AWS Control Tower on the organization. Review included controls (guardrails) for SCPs. Check AWS Config for areas that require additions. Configure an IAM identity provider for federation with the on-premises AD FS server."
    ],
    "answer": "B",
    "explanation": "1. Explanation of why the correct answer (B) is right:\n\nThe correct answer is B because it meets the requirements with the least amount of operational overhead.\n\nKey points:\n\n- The question requires a solution that provides a consistent baseline of management and security, allows flexibility for different compliance requirements, and integrates with the existing on-premises Active Directory Federation Services (AD FS) server.\n- Option B recommends using AWS Control Tower, which automates the creation of a well-architected, multi-account environment using best-practice blueprints. This reduces the operational overhead compared to a manual setup (Option A) or a more complex configuration (Option C).\n- Option B also recommends using AWS IAM Identity Center to connect with the on-premises AD FS server for workforce authentication and authorization. This is the recommended approach for integrating with existing identity providers.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option requires more manual configuration and maintenance, increasing the operational overhead. It involves creating an organization, SCPs, OUs, configuring an IAM identity provider, setting up a central logging account, and enabling AWS Config, which adds complexity and ongoing management tasks.\n\nC. While this option includes valuable components like central logging and AWS Config, it adds complexity and management overhead that exceeds the \"least amount of operational overhead\" requirement. Control Tower (Option B) can automate the setup and management of these components.\n\nD. This option does not leverage the automation and centralized management features of Control Tower, leading to more manual effort compared to Option B. It also does not mention the use of IAM Identity Center for integrating with the on-premises AD FS server."
  },
  "335": {
    "question": "A company wants to migrate its on-premises data center to the AWS Cloud. This includes thousands of virtualized Linux and Microsoft Windows servers, SAN storage, Java and PHP applications with MySQL, and Oracle databases. There are many dependent services hosted either in the same data center or externally. The technical documentation is incomplete and outdated. A solutions architect needs to understand the current environment and estimate the cloud resource costs after the migration.Which tools or services should the solutions architect use to plan the cloud migration? (Choose three.)",
    "choices": [
      "A. AWS Application Discovery Service",
      "B. AWS SMS",
      "C. AWS X-Ray",
      "D. AWS Cloud Adoption Readiness Tool (CART)",
      "E. Amazon Inspector",
      "F. AWS Migration Hub"
    ],
    "answer": "ADF",
    "explanation": "Explanation:\n\n1. Correct Answer: ADF\n\nA. AWS Application Discovery Service - This service is used to gather information about on-premises servers and applications, which is crucial for planning a cloud migration. It can collect server configuration, performance data, and dependency mapping, which are essential for the migration planning process.\n\nD. AWS Cloud Adoption Readiness Tool (CART) - This tool helps assess an organization's readiness for cloud adoption by evaluating factors like people, process, and technology. It provides a detailed report that can guide the migration planning and execution.\n\nF. AWS Migration Hub - This service provides a single place to track the progress of application migrations to AWS across different migration tools. It helps centralize the migration planning and execution, making it easier to monitor and manage the overall process.\n\n2. Incorrect Choices:\n\nB. AWS SMS (Server Migration Service) - This service has been discontinued by AWS as of March 31, 2022. AWS now recommends using alternative solutions like AWS Application Migration Service (MGN) for server migrations.\n\nC. AWS X-Ray - This service is used for distributed application tracing and debugging, which is not directly related to the migration planning process described in the question.\n\nE. Amazon Inspector - This service is used for security and compliance assessment of EC2 instances, which is not a primary requirement for the migration planning process mentioned in the question."
  },
  "336": {
    "question": "An online gaming company needs to optimize the cost of its workloads on AWS. The company uses a dedicated account to host the production environment for its online gaming application and an analytics application.Amazon EC2 instances host the gaming application and must always be available. The EC2 instances run all year. The analytics application uses data that is stored in Amazon S3. The analytics application can be interrupted and resumed without issue.Which solution will meet these requirements MOST cost-effectively?",
    "choices": [
      "A. Purchase an EC2 Instance Savings Plan for the online gaming application instances. Use On-Demand Instances for the analytics application.",
      "B. Purchase an EC2 Instance Savings Plan for the online gaming application instances. Use Spot Instances for the analytics application.",
      "C. Use Spot Instances for the online gaming application and the analytics application. Set up a catalog in AWS Service Catalog to provision services at a discount.",
      "D. Use On-Demand Instances for the online gaming application. Use Spot Instances for the analytics application. Set up a catalog in AWS Service Catalog to provision services at a discount."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B because it meets the cost-effectiveness requirement while also addressing the different needs of the two applications.\n\nFor the online gaming application, the instances must always be available, so purchasing an EC2 Instance Savings Plan is the most cost-effective solution. This provides a discounted hourly rate for a 1-year or 3-year commitment, which is suitable for the always-on requirement of the gaming application.\n\nFor the analytics application, which can be interrupted and resumed without issue, using Spot Instances is the most cost-effective solution. Spot Instances allow you to take advantage of unused EC2 capacity at a significant discount (up to 90% off the On-Demand price), which is ideal for the analytics application that can handle interruptions.\n\n2. Explanations of why the other choices are incorrect:\n\nA. Using On-Demand Instances for the analytics application is not the most cost-effective solution, as Spot Instances would provide a much deeper discount for the interruptible analytics workload.\n\nC. Using Spot Instances for the online gaming application is not recommended, as the instances must always be available. Spot Instances can be interrupted at any time, which would not meet the requirements of the gaming application.\n\nD. Using On-Demand Instances for the online gaming application is not the most cost-effective solution, as the EC2 Instance Savings Plan would provide a better discount for the always-on instances.\n\nIn summary, the correct answer (B) leverages the most cost-effective solutions for each application's requirements, making it the optimal choice."
  },
  "337": {
    "question": "A company runs applications in hundreds of production AWS accounts. The company uses AWS Organizations with all features enabled and has a centralized backup operation that uses AWS Backup.The company is concerned about ransomware attacks. To address this concern, the company has created a new policy that all backups must be resilient to breaches of privileged-user credentials in any production account.Which combination of steps will meet this new requirement? (Choose three.)",
    "choices": [
      "A. Implement cross-account backup with AWS Backup vaults in designated non-production accounts.",
      "B. Add an SCP that restricts the modification of AWS Backup vaults.",
      "C. Implement AWS Backup Vault Lock in compliance mode.C. Implement least privilege access for the IAM service role that is assigned to AWS Backup.",
      "D. Configure the backup frequency, lifecycle, and retention period to ensure that at least one backup always exists in the cold tier.",
      "E. Configure AWS Backup to write all backups to an Amazon S3 bucket in a designated non-production account. Ensure that the S3 bucket has S3 Object Lock enabled."
    ],
    "answer": "ABC",
    "explanation": "Correct Answer: ABC\n\nExplanation:\n\n1. The correct answer is ABC because:\n\nA. Implementing cross-account backup with AWS Backup vaults in designated non-production accounts ensures that the backups are stored in a separate account, making it more difficult for an attacker with privileged access in the production accounts to delete or tamper with the backups.\n\nB. Adding an SCP that restricts the modification of AWS Backup vaults provides an additional layer of protection by preventing unauthorized users, even those with privileged access, from modifying the backup vaults.\n\nC. Implementing AWS Backup Vault Lock in compliance mode makes the backups immutable, preventing any user, including the root user, from deleting or modifying the backups until the retention period is complete. This is a critical measure to protect against ransomware attacks targeting privileged credentials.\n\n2. Explanation of why the incorrect choices are wrong:\n\nD. Configuring the backup frequency, lifecycle, and retention period to ensure that at least one backup always exists in the cold tier is a good practice, but it does not provide the same level of protection as the other measures mentioned in the correct answer.\n\nE. Configuring AWS Backup to write all backups to an Amazon S3 bucket in a designated non-production account and enabling S3 Object Lock is a good practice, but it does not provide the same level of protection as the measures in the correct answer. Specifically, it does not address the risk of privileged credential compromise in the production accounts.\n\nC2. Implementing least privilege access for the IAM service role that is assigned to AWS Backup is a good practice, but it is not as effective as the measures in the correct answer for protecting against ransomware attacks involving privileged credential compromise. Least privilege can be bypassed by an attacker with sufficient permissions, whereas Vault Lock and SCP-based protection are more robust."
  },
  "338": {
    "question": "A company needs to aggregate Amazon CloudWatch logs from its AWS accounts into one central logging account. The collected logs must remain in the AWS Region of creation. The central logging account will then process the logs, normalize the logs into standard output format, and stream the output logs to a security tool for more processing.A solutions architect must design a solution that can handle a large volume of logging data that needs to be ingested. Less logging will occur outside normal business hours than during normal business hours. The logging solution must scale with the anticipated load. The solutions architect has decided to use an AWS Control Tower design to handle the multi-account logging process.Which combination of steps should the solutions architect take to meet the requirements? (Choose three.)",
    "choices": [
      "A. Create a destination Amazon Kinesis data stream in the central logging account.",
      "B. Create a destination Amazon Simple Queue Service (Amazon SQS) queue in the central logging account.",
      "C. Create an IAM role that grants Amazon CloudWatch Logs the permission to add data to the Amazon Kinesis data stream. Create a trust policy. Specify the trust policy in the IAM role. In each member account, create a subscription filter for each log group to send data to the Kinesis data stream.",
      "D. Create an IAM role that grants Amazon CloudWatch Logs the permission to add data to the Amazon Simple Queue Service (Amazon SQS) queue. Create a trust policy. Specify the trust policy in the IAM role. In each member account, create a single subscription filter for all log groups to send data to the SQS queue.",
      "E. Create an AWS Lambda function. Program the Lambda function to normalize the logs in the central logging account and to write the logs to the security tool.",
      "F. Create an AWS Lambda function. Program the Lambda function to normalize the logs in the member accounts and to write the logs to the security tool."
    ],
    "answer": "ACE",
    "explanation": "1. Explanation of the correct answer (ACE):\n\nA. Create a destination Amazon Kinesis data stream in the central logging account.\nThis is the correct choice because CloudWatch Logs can directly send logs to a Kinesis data stream, which meets the requirement of keeping the logs in the same AWS Region as created. Kinesis can handle the large volume of logging data that needs to be ingested.\n\nC. Create an IAM role that grants Amazon CloudWatch Logs the permission to add data to the Amazon Kinesis data stream. Create a trust policy. Specify the trust policy in the IAM role. In each member account, create a subscription filter for each log group to send data to the Kinesis data stream.\nThis is the correct choice because it sets up the necessary IAM role and subscription filters in the member accounts to send the logs to the central Kinesis data stream.\n\nE. Create an AWS Lambda function. Program the Lambda function to normalize the logs in the central logging account and to write the logs to the security tool.\nThis is the correct choice because it uses a Lambda function in the central logging account to process and normalize the logs before sending them to the security tool, as required by the question.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. Create a destination Amazon Simple Queue Service (Amazon SQS) queue in the central logging account.\nThis is incorrect because CloudWatch Logs cannot directly send logs to an SQS queue. SQS is not a supported destination for CloudWatch Logs subscriptions.\n\nD. Create an IAM role that grants Amazon CloudWatch Logs the permission to add data to the Amazon Simple Queue Service (Amazon SQS) queue. Create a trust policy. Specify the trust policy in the IAM role. In each member account, create a single subscription filter for all log groups to send data to the SQS queue.\nThis is incorrect for the same reason as choice B, as SQS is not a supported destination for CloudWatch Logs subscriptions.\n\nF. Create an AWS Lambda function. Program the Lambda function to normalize the logs in the member accounts and to write the logs to the security tool.\nThis is incorrect because the question states that the logs must remain in the AWS Region of creation, and processing the logs in the member accounts would go against this"
  },
  "339": {
    "question": "A company is migrating a legacy application from an on-premises data center to AWS. The application consists of a single application server and a Microsoft SQL Server database server. Each server is deployed on a VMware VM that consumes 500 TB of data across multiple attached volumes.The company has established a 10 Gbps AWS Direct Connect connection from the closest AWS Region to its on-premises data center. The Direct Connect connection is not currently in use by other services.Which combination of steps should a solutions architect take to migrate the application with the LEAST amount of downtime? (Choose two.)",
    "choices": [
      "A. Use an AWS Server Migration Service (AWS SMS) replication job to migrate the database server VM to AWS.",
      "B. Use VM Import/Export to import the application server VM.",
      "C. Export the VM images to an AWS Snowball Edge Storage Optimized device.",
      "D. Use an AWS Server Migration Service (AWS SMS) replication job to migrate the application server VM to AWS.",
      "E. Use an AWS Database Migration Service (AWS DMS) replication instance to migrate the database to an Amazon RDS DB instance."
    ],
    "answer": "DE",
    "explanation": "1. Explanation of the correct answer (DE):\n\nThe correct answer is DE, which suggests using AWS Server Migration Service (AWS SMS) to migrate the application server VM to AWS, and using AWS Database Migration Service (AWS DMS) to migrate the database to an Amazon RDS DB instance.\n\nThis approach is the best for achieving the least amount of downtime because:\n\n- AWS SMS allows for incremental replication of the application server VM to AWS, minimizing downtime during the final cutover.\n- AWS DMS can perform a continuous data replication from the on-premises SQL Server database to the Amazon RDS instance, again minimizing downtime during the final switchover.\n\n2. Explanations of the incorrect choices:\n\nA. Incorrect. Using AWS SMS to migrate the database server VM is not the best approach, as the database size (500 TB) exceeds the maximum size supported by Amazon RDS (16 TB). This would lead to significant downtime during the migration.\n\nB. Incorrect. Using VM Import/Export to import the application server VM is not the best approach, as it does not provide the benefits of incremental replication that AWS SMS offers, leading to longer downtime during the migration.\n\nC. Incorrect. Exporting the VM images to an AWS Snowball Edge Storage Optimized device and then importing them to AWS is not the best approach, as it would still require a significant amount of downtime during the final data transfer and VM import process.\n\nE. Incorrect. Using AWS DMS to migrate the database to an Amazon RDS DB instance is correct, but it should be paired with AWS SMS to migrate the application server VM, not the database server VM, due to the database size limitations."
  },
  "340": {
    "question": "A company operates a fleet of servers on premises and operates a fleet of Amazon EC2 instances in its organization in AWS Organizations. The company's AWS accounts contain hundreds of VPCs. The company wants to connect its AWS accounts to its on-premises network. AWS Site-to-Site VPN connections are already established to a single AWS account. The company wants to control which VPCs can communicate with other VPCs.Which combination of steps will achieve this level of control with the LEAST operational effort? (Choose three.)",
    "choices": [
      "A. Create a transit gateway in an AWS account. Share the transit gateway across accounts by using AWS Resource Access Manager (AWS RAM).",
      "B. Configure attachments to all VPCs and VPNs.",
      "C. Setup transit gateway route tables. Associate the VPCs and VPNs with the route tables.",
      "D. Configure VPC peering between the VPCs.",
      "E. Configure attachments between the VPCs and VPNs.",
      "F. Setup route tables on the VPCs and VPNs."
    ],
    "answer": "ABC",
    "explanation": "Explanation of the correct answer (ABC):\n\n1. A - Create a transit gateway in an AWS account. Share the transit gateway across accounts by using AWS Resource Access Manager (AWS RAM):\n   - A transit gateway acts as a central hub to connect multiple VPCs and on-premises networks, allowing controlled communication between them.\n   - By creating a transit gateway and sharing it across accounts using AWS RAM, the company can centrally manage and control the connectivity between its multiple VPCs in different accounts.\n\n2. B - Configure attachments to the relevant VPCs and VPNs:\n   - After creating the shared transit gateway, the company must configure attachments between the transit gateway and the relevant VPCs and VPNs.\n   - This allows the transit gateway to connect to the VPCs and VPNs, enabling communication between them.\n\n3. C - Set up transit gateway route tables and associate the VPCs and VPNs with the route tables:\n   - Transit gateway route tables control the routing of traffic between the attached VPCs and VPNs.\n   - By configuring the route tables and associating the relevant VPCs and VPNs, the company can control which VPCs can communicate with each other through the transit gateway.\n\nExplanations of why the incorrect choices are wrong:\n\nD - Configure VPC peering between the VPCs:\n   - VPC peering is not scalable for hundreds of VPCs, as it requires configuring individual peering connections between each pair of VPCs.\n   - The transit gateway approach is more efficient and scalable for managing connectivity between a large number of VPCs.\n\nE - Configure attachments between the VPCs and VPNs:\n   - This option is incorrect because the attachments should be configured between the VPCs/VPNs and the transit gateway, not directly between the VPCs and VPNs.\n   - The transit gateway acts as the central hub, managing the connectivity between the VPCs and VPNs.\n\nF - Set up route tables on the VPCs and VPNs:\n   - This option is incorrect because the route tables should be configured on the transit gateway, not on the individual VPCs and VPNs.\n   - The"
  },
  "341": {
    "question": "A company needs to optimize the cost of its application on AWS. The application uses AWS Lambda functions and Amazon Elastic Container Service (Amazon ECS) containers that run on AWS Fargate. The application is write-heavy and stores data in an Amazon Aurora MySQL database.The load on the application is not consistent. The application experiences long periods of no usage, followed by sudden and significant increases and decreases in traffic. The database runs on a memory optimized DB instance that cannot handle the load.A solutions architect must design a solution that can scale to handle the changes in traffic.Which solution will meet these requirements MOST cost-effectively?",
    "choices": [
      "A. Add additional read replicas to the database. Purchase Instance Savings Plans and RDS Reserved Instances.",
      "B. Migrate the database to an Aurora DB cluster that has multiple writer instances. Purchase Instance Savings Plans.",
      "C. Migrate the database to an Aurora global database. Purchase Compute Savings Plans and RDS Reserved instances.",
      "D. Migrate the database to Aurora Serverless v1. Purchase Compute Savings Plans."
    ],
    "answer": "D",
    "explanation": "1. Clear explanation of why the correct answer (D) is right:\n\nThe correct answer is D. Migrate the database to Aurora Serverless v1 and purchase Compute Savings Plans.\n\nAurora Serverless v1 is the most cost-effective solution for this scenario because it can automatically scale the database capacity up and down based on the application's usage patterns. This is ideal for the given scenario, where the application experiences long periods of no usage followed by sudden and significant increases and decreases in traffic. Aurora Serverless v1 can handle these fluctuations in load without the need for manual scaling or provisioning of database resources.\n\nAdditionally, purchasing Compute Savings Plans can provide further cost savings for the Lambda functions and Fargate containers, which makes this the most cost-effective solution overall.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nA. Add additional read replicas to the database and purchase Instance Savings Plans and RDS Reserved Instances:\nThis solution does not address the issue of the database being unable to handle the load during peak traffic periods. Adding read replicas only helps with read-heavy workloads, but the question states that the application is write-heavy. Additionally, while Instance Savings Plans and RDS Reserved Instances can provide some cost savings, they do not offer the same level of scalability and cost-effectiveness as Aurora Serverless v1.\n\nB. Migrate the database to an Aurora DB cluster with multiple writer instances and purchase Instance Savings Plans:\nThis solution can handle increased traffic, but it still requires manual provisioning and scaling of database resources. It does not provide the same level of automatic scaling and cost optimization as Aurora Serverless v1.\n\nC. Migrate the database to an Aurora global database and purchase Compute Savings Plans and RDS Reserved Instances:\nAurora global database is designed for low-latency global access, but it may not be the most cost-effective solution for the given scenario, which is focused on handling fluctuations in traffic. The question does not mention a requirement for low-latency global access, so Aurora global database may be an overkill. Additionally, RDS Reserved Instances do not provide the same level of cost optimization as Compute Savings Plans for the Lambda functions and Fargate containers."
  },
  "342": {
    "question": "A company migrated an application to the AWS Cloud. The application runs on two Amazon EC2 instances behind an Application Load Balancer (ALB).Application data is stored in a MySQL database that runs on an additional EC2 instance. The application's use of the database is read-heavy.The application loads static content from Amazon Elastic Block Store (Amazon EBS) volumes that are attached to each EC2 instance. The static content is updated frequently and must be copied to each EBS volume.The load on the application changes throughout the day. During peak hours, the application cannot handle all the incoming requests. Trace data shows that the database cannot handle the read load during peak hours.Which solution will improve the reliability of the application?",
    "choices": [
      "A. Migrate the application to a set of AWS Lambda functions. Set the Lambda functions as targets for the ALB. Create a new single EBS volume for the static content. Configure the Lambda functions to read from the new EBS volume. Migrate the database to an Amazon RDS for MySQL Multi-AZ DB cluster.",
      "B. Migrate the application to a set of AWS Step Functions state machines. Set the state machines as targets for the ALCreate an Amazon Elastic File System (Amazon EFS) file system for the static content. Configure the state machines to read from the EFS file system. Migrate the database to Amazon Aurora MySQL Serverless v2 with a reader DB instance.",
      "C. Containerize the application. Migrate the application to an Amazon Elastic Container Service (Amazon ECS) cluster. Use the AWS Fargate launch type for the tasks that host the application. Create a new single EBS volume for the static content. Mount the new EBS volume on the ECS cluster. Configure AWS Application Auto Scaling on the ECS cluster. Set the ECS service as a target for the ALB. Migrate the database to an Amazon RDS for MySQL Multi-AZ DB cluster.",
      "D. Containerize the application. Migrate the application to an Amazon Elastic Container Service (Amazon ECS) cluster. Use the AWS Fargate launch type for the tasks that host the application. Create an Amazon Elastic File System (Amazon EFS) file system for the static content. Mount the EFS file system to each container. Configure AWS Application Auto Scaling on the ECS cluster. Set the ECS service as a target for the ALB. Migrate the database to Amazon Aurora MySQL Serverless v2 with a reader DB instance."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (D):\n\nThe correct answer is D because it provides the most comprehensive and scalable solution to improve the reliability of the application:\n\n- Containerizing the application and migrating it to Amazon ECS with the Fargate launch type allows for easy scaling and automatic provisioning of resources based on demand. This helps address the issue of the application not being able to handle all incoming requests during peak hours.\n\n- Using an Amazon EFS file system for the static content ensures that the data is accessible from multiple containers, eliminating the need to copy it to each individual EC2 instance. This improves the reliability and consistency of the static content delivery.\n\n- Configuring AWS Application Auto Scaling on the ECS cluster allows for automatic scaling based on metrics like CPU utilization, ensuring that the application can handle increased load during peak hours.\n\n- Migrating the database to Amazon Aurora MySQL Serverless v2 with a reader DB instance provides a highly scalable and reliable database solution. The Serverless v2 offering can automatically scale the database based on demand, and the reader instance can handle the read-heavy workload, addressing the issue with the database not being able to handle the read load during peak hours.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This solution is not optimal because:\n- Migrating to AWS Lambda functions may not be the best fit for a read-heavy application that requires a database.\n- Migrating the database to an RDS Multi-AZ cluster may not provide the same level of automatic scaling and performance as Aurora Serverless v2.\n\nB. This solution is not optimal because:\n- Migrating to AWS Step Functions may not be the best fit for a read-heavy application that requires a database.\n- Using an Amazon EFS file system for the static content is a good idea, but it does not address the scalability issues of the application and database.\n\nC. This solution is not optimal because:\n- While containerizing the application and using AWS Fargate is a good idea, the use of a single EBS volume for static content may not provide the same level of reliability and consistency as an Amazon EFS file system.\n- Migrating the database to an RDS Multi-AZ cluster may not provide the same level of automatic scaling and performance"
  },
  "343": {
    "question": "A solutions architect wants to make sure that only AWS users or roles with suitable permissions can access a new Amazon API Gateway endpoint. The solutions architect wants an end-to-end view of each request to analyze the latency of the request and create service maps.How can the solutions architect design the API Gateway access control and perform request inspections?",
    "choices": [
      "A. For the API Gateway method, set the authorization to AWS_IAM. Then, give the IAM user or role execute-api:Invoke permission on the REST API resource. Enable the API caller to sign requests with AWS Signature when accessing the endpoint. Use AWS X-Ray to trace and analyze user requests to API Gateway.",
      "B. For the API Gateway resource, set CORS to enabled and only return the company's domain in Access-Control-Allow-Origin headers. Then, give the IAM user or role execute-api:Invoke permission on the REST API resource. Use Amazon CloudWatch to trace and analyze user requests to API Gateway.",
      "C. Create an AWS Lambda function as the custom authorizer, ask the API client to pass the key and secret when making the call, and then use Lambda to validate the key/secret pair against the IAM system. Use AWS X-Ray to trace and analyze user requests to API Gateway.",
      "D. Create a client certificate for API Gateway. Distribute the certificate to the AWS users and roles that need to access the endpoint. Enable the API caller to pass the client certificate when accessing the endpoint. Use Amazon CloudWatch to trace and analyze user requests to API Gateway."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\n\nThe correct answer is A because it meets all the requirements specified in the question:\n\n- For access control, setting the API Gateway method authorization to AWS_IAM and granting the IAM user or role the execute-api:Invoke permission on the REST API resource ensures that only authorized AWS users or roles can access the API Gateway endpoint.\n- Enabling the API caller to sign requests with AWS Signature adds an additional layer of security, ensuring that only authorized parties can access the resource.\n- Using AWS X-Ray to trace and analyze user requests to API Gateway provides an end-to-end view of each request, allowing the solutions architect to analyze the latency of the requests and create service maps, as required.\n\n2. Explanations of the incorrect choices:\n\nB. This option is incorrect because:\n- Setting CORS to enabled and returning the company's domain in the Access-Control-Allow-Origin headers does not provide the required access control mechanism based on IAM permissions.\n- Using Amazon CloudWatch to trace and analyze user requests to API Gateway does not provide the same level of end-to-end visibility and analysis capabilities as AWS X-Ray.\n\nC. This option is incorrect because:\n- Creating a custom authorizer using an AWS Lambda function is more complex than necessary when AWS_IAM authorization is already suitable for the given requirements.\n- Asking the API client to pass a key and secret for validation is an additional layer of complexity that is not required when using AWS_IAM for access control.\n\nD. This option is incorrect because:\n- Creating and distributing client certificates for API Gateway access is a more complex solution compared to using AWS_IAM for access control.\n- Using Amazon CloudWatch to trace and analyze user requests to API Gateway does not provide the same level of end-to-end visibility and analysis capabilities as AWS X-Ray."
  },
  "344": {
    "question": "A company is using AWS CodePipeline for the CI/CD of an application to an Amazon EC2 Auto Scaling group. All AWS resources are defined in AWS CloudFormation templates. The application artifacts are stored in an Amazon S3 bucket and deployed to the Auto Scaling group using instance user data scripts. As the application has become more complex, recent resource changes in the CloudFormation templates have caused unplanned downtime.How should a solutions architect improve the CI/CD pipeline to reduce the likelihood that changes in the templates will cause downtime?",
    "choices": [
      "A. Adapt the deployment scripts to detect and report CloudFormation error conditions when performing deployments. Write test plans for a testing team to run in a non-production environment before approving the change for production.",
      "B. Implement automated testing using AWS CodeBuild in a test environment. Use CloudFormation change sets to evaluate changes before deployment. Use AWS CodeDeploy to leverage blue/green deployment patterns to allow evaluations and the ability to revert changes, if needed.",
      "C. Use plugins for the integrated development environment (IDE) to check the templates for errors, and use the AWS CLI to validate that the templates are correct. Adapt the deployment code to check for error conditions and generate notifications on errors. Deploy to a test environment and run a manual test plan before approving the change for production.",
      "D. Use AWS CodeDeploy and a blue/green deployment pattern with CloudFormation to replace the user data deployment scripts. Have the operators log in to running instances and go through a manual test plan to verify the application is running as expected."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (Option B):\n\nOption B is the correct answer because it addresses the key issues in the current CI/CD pipeline and provides a comprehensive solution to reduce the likelihood of unplanned downtime due to changes in the CloudFormation templates.\n\nThe key elements of the solution in Option B are:\n\na. Automated testing using AWS CodeBuild: This allows for thorough testing of the application and infrastructure changes in a non-production environment before deployment, helping to identify and address issues early in the process.\n\nb. CloudFormation change sets: This feature allows the solutions architect to evaluate the impact of proposed changes before deploying them, reducing the risk of unexpected issues during deployment.\n\nc. Blue/green deployment with AWS CodeDeploy: This deployment pattern enables the introduction of new changes to a separate environment (the \"blue\" environment) while maintaining the existing production environment (the \"green\" environment). This allows for evaluation of the new changes and the ability to easily revert to the previous version if needed, minimizing downtime.\n\n2. Explanations of why the other options are incorrect:\n\na. Option A: This option focuses on adapting the deployment scripts to detect and report CloudFormation errors, and manual testing in a non-production environment. While this approach can help identify some issues, it does not provide the comprehensive solution of automated testing, change impact evaluation, and the blue/green deployment pattern, which are more effective in reducing the likelihood of unplanned downtime.\n\nb. Option C: This option suggests using IDE plugins and the AWS CLI to validate CloudFormation templates, as well as adapting the deployment code to check for errors and generate notifications. While these steps can help, they do not provide the same level of automated testing and deployment capabilities as Option B, which is more likely to reduce the risk of unplanned downtime.\n\nc. Option D: This option suggests using AWS CodeDeploy and a blue/green deployment pattern with CloudFormation, but it also includes a manual test plan by operators, which introduces the risk of human error and is less effective than the automated testing and evaluation approach in Option B."
  },
  "345": {
    "question": "A North American company with headquarters on the East Coast is deploying a new web application running on Amazon EC2 in the us-east-1 Region. The application should dynamically scale to meet user demand and maintain resiliency. Additionally, the application must have disaster recovery capabilities in an active-passive configuration with the us-west-1 Region.Which steps should a solutions architect take after creating a VPC in the us-east-1 Region?",
    "choices": [
      "A. Create a VPC in the us-west-1 Region. Use inter-Region VPC peering to connect both VPCs. Deploy an Application Load Balancer (ALB) spanning multiple Availability Zones (AZs) to the VPC in the us-east-1 Region. Deploy EC2 instances across multiple AZs in each Region as part of an Auto Scaling group spanning both VPCs and served by the ALB.",
      "B. Deploy an Application Load Balancer (ALB) spanning multiple Availability Zones (AZs) to the VPC in the us-east-1 Region. Deploy EC2 instances across multiple AZs as part of an Auto Scaling group served by the ALDeploy the same solution to the us-west-1 Region. Create an Amazon Route 53 record set with a failover routing policy and health checks enabled to provide high availability across both Regions.",
      "C. Create a VPC in the us-west-1 Region. Use inter-Region VPC peering to connect both VPCs. Deploy an Application Load Balancer (ALB) that spans both VPCs. Deploy EC2 instances across multiple Availability Zones as part of an Auto Scaling group in each VPC served by the ALB. Create an Amazon Route 53 record that points to the ALB.",
      "D. Deploy an Application Load Balancer (ALB) spanning multiple Availability Zones (AZs) to the VPC in the us-east-1 Region. Deploy EC2 instances across multiple AZs as part of an Auto Scaling group served by the ALB. Deploy the same solution to the us-west-1 Region. Create separate Amazon Route 53 records in each Region that point to the ALB in the Region. Use Route 53 health checks to provide high availability across both Regions."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B because it outlines the right steps to achieve the required disaster recovery capabilities in an active-passive configuration across the us-east-1 and us-west-1 regions.\n\nThe key steps are:\n\n- Deploy an Application Load Balancer (ALB) spanning multiple Availability Zones (AZs) in the us-east-1 region.\n- Deploy EC2 instances across multiple AZs as part of an Auto Scaling group, served by the ALB in the us-east-1 region.\n- Deploy the same solution (ALB and Auto Scaling group) in the us-west-1 region.\n- Create an Amazon Route 53 record set with a failover routing policy and health checks enabled to provide high availability across both regions.\n\nThis setup ensures that the application can dynamically scale to meet user demand in the primary us-east-1 region. If the us-east-1 region becomes unavailable, Route 53 health checks will detect this and automatically failover to the secondary us-west-1 region, providing disaster recovery capabilities.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option is incorrect because it requires inter-region VPC peering, which is not necessary for the disaster recovery setup described in the question. Also, an ALB cannot span multiple VPCs in different regions.\n\nC. This option is incorrect because an ALB cannot span VPCs in different regions. Additionally, the question specifically states that the disaster recovery setup should be in an active-passive configuration, which is not achieved by this option.\n\nD. This option is similar to the correct answer (B), but it's slightly different in the way it describes the Route 53 setup. While both options are valid, B is more concise and aligns better with the requirements stated in the question."
  },
  "346": {
    "question": "A company has a legacy application that runs on multiple NET Framework components. The components share the same Microsoft SQL Server database and communicate with each other asynchronously by using Microsoft Message Queueing (MSMQ).The company is starting a migration to containerized .NET Core components and wants to refactor the application to run on AWS. The .NET Core components require complex orchestration. The company must have full control over networking and host configuration. The application's database model is strongly relational.Which solution will meet these requirements?",
    "choices": [
      "A. Host the INET Core components on AWS App Runner. Host the database on Amazon RDS for SQL Server. Use Amazon EventBiridge for asynchronous messaging.",
      "B. Host the .NET Core components on Amazon Elastic Container Service (Amazon ECS) with the AWS Fargate launch type. Host the database on Amazon DynamoDUse Amazon Simple Notification Service (Amazon SNS) for asynchronous messaging.",
      "C. Host the .NET Core components on AWS Elastic Beanstalk. Host the database on Amazon Aurora PostgreSQL Serverless v2. Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) for asynchronous messaging.",
      "D. Host the NET Core components on Amazon Elastic Container Service (Amazon ECS) with the Amazon EC2 launch type. Host the database on Amazon Aurora MySQL Serverless v2. Use Amazon Simple Queue Service (Amazon SQS) for asynchronous messaging."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (D):\n\nThe correct answer is D because it best meets the requirements outlined in the question.\n\n- Hosting the .NET Core components on Amazon ECS with the EC2 launch type provides the company with full control over networking and host configuration, as required.\n- Amazon Aurora MySQL Serverless v2 can support the strongly relational database model, which aligns with the requirement of the application's database model.\n- Amazon SQS can be used as an alternative to MSMQ for asynchronous messaging, meeting the need for asynchronous communication between the components.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option is incorrect because:\n- Amazon App Runner is a managed service that may not provide the level of control over networking and host configuration that the company requires.\n- Amazon EventBridge is not a direct replacement for MSMQ and may not be the best fit for the asynchronous messaging requirements.\n\nB. This option is incorrect because:\n- Amazon DynamoDB is a NoSQL database, which may not be the best fit for the strongly relational database model required by the application.\n- Amazon SNS is a pub/sub messaging service and may not be the best replacement for the point-to-point messaging provided by MSMQ.\n\nC. This option is incorrect because:\n- AWS Elastic Beanstalk is a managed service that may not provide the level of control over networking and host configuration that the company requires.\n- Amazon Aurora PostgreSQL Serverless v2 is not an existing service, and the question specifically mentions the application's database model is strongly relational.\n- Amazon MSK (Managed Streaming for Apache Kafka) may not be the best replacement for the asynchronous messaging provided by MSMQ."
  },
  "347": {
    "question": "A solutions architect has launched multiple Amazon EC2 instances in a placement group within a single Availability Zone. Because of additional load on the system, the solutions architect attempts to add new instances to the placement group. However, the solutions architect receives an insufficient capacity error.What should the solutions architect do to troubleshoot this issue?",
    "choices": [
      "A. Use a spread placement group. Set a minimum of eight instances for each Availability Zone.",
      "B. Stop and start all the instances in the placement group. Try the launch again.",
      "C. Create a new placement group. Merge the new placement group with the original placement group.",
      "D. Launch the additional instances as Dedicated Hosts in the placement groups."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B - Stop and start all the instances in the placement group, then try the launch again.\n\nWhen you try to add new instances to an existing placement group, you may encounter an \"insufficient capacity\" error if the underlying hardware does not have enough available resources to accommodate the additional instances. Stopping and starting all the existing instances in the placement group can help resolve this issue by migrating the instances to hardware with available capacity, making room for the new instances to be launched.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Use a spread placement group. Set a minimum of eight instances for each Availability Zone.\nThis is incorrect because a spread placement group is used to distribute instances across different hardware, which is not the issue here. The problem is a lack of capacity in the existing placement group, not the type of placement group used.\n\nC. Create a new placement group. Merge the new placement group with the original placement group.\nThis is incorrect because creating a new placement group and merging it with the original one does not address the root cause of the insufficient capacity issue. The new placement group would likely face the same capacity constraints.\n\nD. Launch the additional instances as Dedicated Hosts in the placement groups.\nThis is incorrect because Dedicated Hosts are a different EC2 instance type that provides a physical server dedicated to the user's use. Launching instances as Dedicated Hosts would not solve the insufficient capacity issue in the existing placement group."
  },
  "348": {
    "question": "A company has used infrastructure as code (IaC) to provision a set of two Amazon EC2 instances. The instances have remained the same for several years.The company's business has grown rapidly in the past few months. In response, the company\u2019s operations team has implemented an Auto Scaling group to manage the sudden increases in traffic. Company policy requires a monthly installation of security updates on all operating systems that are running.The most recent security update required a reboot. As a result, the Auto Scaling group terminated the instances and replaced them with new, unpatched instances.Which combination of steps should a solutions architect recommend to avoid a recurrence of this issue? (Choose two.)",
    "choices": [
      "A. Modify the Auto Scaling group by setting the Update policy to target the oldest launch configuration for replacement.",
      "B. Create a new Auto Scaling group before the next patch maintenance. During the maintenance window, patch both groups and reboot the instances.",
      "C. Create an Elastic Load Balancer in front of the Auto Scaling group. Configure monitoring to ensure that target group health checks return healthy after the Auto Scaling group replaces the terminated instances.",
      "D. Create automation scripts to patch an AMI, update the launch configuration, and invoke an Auto Scaling instance refresh.",
      "E. Create an Elastic Load Balancer in front of the Auto Scaling group. Configure termination protection on the instances."
    ],
    "answer": "AD",
    "explanation": "1. Clear explanation of the correct answer:\n\nThe correct answer is A and D.\n\nA. Modify the Auto Scaling group by setting the Update policy to target the oldest launch configuration for replacement.\nThis is a valid option, as it ensures that the Auto Scaling group will replace the instances with the oldest launch configuration first. This can help mitigate the issue where the Auto Scaling group is terminating the patched instances and replacing them with new, unpatched instances.\n\nD. Create automation scripts to patch an AMI, update the launch configuration, and invoke an Auto Scaling instance refresh.\nThis is the key step to address the issue. By creating automation scripts to patch the AMI, update the launch configuration, and then invoke an Auto Scaling instance refresh, the company can ensure that all new instances provisioned by the Auto Scaling group are based on the patched AMI and launch configuration. This will prevent the issue of new, unpatched instances being launched after the security updates and reboots.\n\n2. Explanations of why each incorrect choice is wrong:\n\nB. Create a new Auto Scaling group before the next patch maintenance. During the maintenance window, patch both groups and reboot the instances.\nThis is not the optimal solution, as it would still result in instances being terminated and replaced with new, unpatched instances during the reboot process. The issue would still persist, as the Auto Scaling group would not be aware of the patched instances.\n\nC. Create an Elastic Load Balancer in front of the Auto Scaling group. Configure monitoring to ensure that target group health checks return healthy after the Auto Scaling group replaces the terminated instances.\nAdding an Elastic Load Balancer is unnecessary in this scenario, as the issue is not related to load balancing or traffic management. The problem is with the Auto Scaling group terminating the patched instances and replacing them with new, unpatched instances.\n\nE. Create an Elastic Load Balancer in front of the Auto Scaling group. Configure termination protection on the instances.\nSimilar to the previous choice, adding an Elastic Load Balancer is not necessary. Additionally, configuring termination protection on the instances would prevent the Auto Scaling group from replacing instances, which is counter to the purpose of the Auto Scaling group."
  },
  "349": {
    "question": "A team of data scientists is using Amazon SageMaker instances and SageMaker APIs to train machine learning (ML) models. The SageMaker instances are deployed in a VPC that does not have access to or from the internet. Datasets for ML model training are stored in an Amazon S3 bucket. Interface VPC endpoints provide access to Amazon S3 and the SageMaker APIs.Occasionally, the data scientists require access to the Python Package Index (PyPI) repository to update Python packages that they use as part of their workflow. A solutions architect must provide access to the PyPI repository while ensuring that the SageMaker instances remain isolated from the internet.Which solution will meet these requirements?",
    "choices": [
      "A. Create an AWS CodeCommit repository for each package that the data scientists need to access. Configure code synchronization between the PyPI repository and the CodeCommit repository. Create a VPC endpoint for CodeCommit.",
      "B. Create a NAT gateway in the VPC. Configure VPC routes to allow access to the internet with a network ACL that allows access to only the PyPI repository endpoint.",
      "C. Create a NAT instance in the VPConfigure VPC routes to allow access to the internet. Configure SageMaker notebook instance firewall rules that allow access to only the PyPI repository endpoint.",
      "D. Create an AWS CodeArtifact domain and repository. Add an external connection for public:pypi to the CodeArtifact repository. Configure the Python client to use the CodeArtifact repository. Create a VPC endpoint for CodeArtifact."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (D):\n\nThe correct answer is option D, which involves using AWS CodeArtifact to provide access to the Python Package Index (PyPI) repository while keeping the SageMaker instances isolated from the internet.\n\nThe key advantages of this approach are:\n\n- CodeArtifact allows you to create a private package repository that can proxy access to public repositories like PyPI. This way, the SageMaker instances can access the required packages without directly connecting to the internet.\n- By creating a VPC endpoint for CodeArtifact, the data scientists can access the private package repository from within the VPC, maintaining the isolation of the SageMaker instances.\n- The external connection to the public PyPI repository is configured within CodeArtifact, so the data scientists can continue to use the standard Python package management tools (like pip) without any additional configuration.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option involves creating an AWS CodeCommit repository for each package, which is not a scalable or efficient solution, especially when dealing with a large number of packages from PyPI.\n\nB. Using a NAT gateway would still allow internet access from the VPC, which goes against the requirement of keeping the SageMaker instances isolated from the internet. Additionally, configuring a network ACL to allow access only to the PyPI repository endpoint is a more complex and less-managed solution compared to using CodeArtifact.\n\nC. Similar to option B, using a NAT instance would still allow internet access from the VPC, which is not a suitable solution for maintaining isolation. Configuring firewall rules on the SageMaker notebook instances to allow access only to the PyPI repository endpoint is also a more complex and less-managed solution compared to using CodeArtifact."
  },
  "350": {
    "question": "A solutions architect works for a government agency that has strict disaster recovery requirements. All Amazon Elastic Block Store (Amazon EBS) snapshots are required to be saved in at least two additional AWS Regions. The agency also is required to maintain the lowest possible operational overhead.Which solution meets these requirements?",
    "choices": [
      "A. Configure a policy in Amazon Data Lifecycle Manager (Amazon DLM) to run once daily to copy the EBS snapshots to the additional Regions.",
      "B. Use Amazon EventBridge to schedule an AWS Lambda function to copy the EBS snapshots to the additional Regions.",
      "C. Setup AWS Backup to create the EBS snapshots. Configure Amazon S3 Cross-Region Replication to copy the EBS snapshots to the additional Regions.",
      "D. Schedule Amazon EC2 Image Builder to run once daily to create an AMI and copy the AMI to the additional Regions."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\n\nThe correct answer is A, which is to configure a policy in Amazon Data Lifecycle Manager (Amazon DLM) to run once daily and copy the EBS snapshots to the additional Regions. This solution meets the requirements of:\n\n- Saving all EBS snapshots in at least two additional AWS Regions for disaster recovery.\n- Maintaining the lowest possible operational overhead by automating the snapshot copy process using DLM.\n\nDLM is a native AWS service that can be used to automate the creation, retention, and copying of EBS snapshots. By creating a DLM policy to run on a daily schedule, the solution can automatically copy the EBS snapshots to the additional Regions without the need for manual intervention, thereby meeting the low operational overhead requirement.\n\n2. Explanations of the incorrect choices:\n\nB. Using Amazon EventBridge and a Lambda function to copy the EBS snapshots is a valid approach, but it requires more operational overhead compared to the DLM solution. The developer would need to manage the Lambda function, its scheduling, and the copy process, which adds more complexity.\n\nC. Setting up AWS Backup to create the EBS snapshots and then using Amazon S3 Cross-Region Replication (CRR) to copy the snapshots is not a correct solution. EBS snapshots are stored in Amazon S3, but they cannot be accessed directly through the S3 console or API. Therefore, S3 CRR cannot be used to copy the EBS snapshots to additional Regions.\n\nD. Scheduling Amazon EC2 Image Builder to create an AMI and copy it to additional Regions is not the correct solution. This approach creates a machine image (AMI) instead of managing EBS snapshots, which is not the requirement in the question."
  },
  "351": {
    "question": "A company has a project that is launching Amazon EC2 instances that are larger than required. The project's account cannot be part of the company's organization in AWS Organizations due to policy restrictions to keep this activity outside of corporate IT. The company wants to allow only the launch of t3.small EC2 instances by developers in the project's account. These EC2 instances must be restricted to the us-east-2 Region.What should a solutions architect do to meet these requirements?",
    "choices": [
      "A. Create a new developer account. Move all EC2 instances, users, and assets into us-east-2. Add the account to the company's organization in AWS Organizations. Enforce a tagging policy that denotes Region affinity.",
      "B. Create an SCP that denies the launch of all EC2 instances except t3.small EC2 instances in us-east-2. Attach the SCP to the project's account.",
      "C. Create and purchase a t3.small EC2 Reserved Instance for each developer in us-east-2. Assign each developer a specific EC2 instance with their name as the tag.",
      "D. Create an IAM policy than allows the launch of only t3.small EC2 instances in us-east-2. Attach the policy to the roles and groups that the developers use in the project's account."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (D):\n\nThe correct answer is D, which is to create an IAM policy that allows the launch of only t3.small EC2 instances in the us-east-2 region, and attach this policy to the roles and groups used by the developers in the project's account.\n\nThis is the appropriate solution because the question states that the project's account cannot be part of the company's AWS Organizations due to policy restrictions. Since the account cannot be part of the organization, the use of Service Control Policies (SCPs) mentioned in option B is not possible.\n\nAdditionally, the question requires the restriction of EC2 instances to the t3.small size and the us-east-2 region. Attaching an IAM policy directly to the roles and groups used by the developers is the best way to achieve this, as it allows for fine-grained control over the actions permitted within the project's account.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option involves moving all resources to a new developer account and adding it to the company's AWS Organizations. However, the question states that the project's account cannot be part of the organization, so this solution is not viable.\n\nB. This option suggests using an SCP to deny the launch of all EC2 instances except t3.small in us-east-2. As mentioned earlier, the use of SCPs is not possible since the project's account cannot be part of the AWS Organizations.\n\nC. This option involves purchasing t3.small EC2 Reserved Instances for each developer and assigning them to specific instances. While this could work, it is a more complex and resource-intensive solution compared to the IAM policy approach in option D. Additionally, it does not provide the same level of flexibility and control as the IAM policy solution."
  },
  "352": {
    "question": "A scientific company needs to process text and image data from an Amazon S3 bucket. The data is collected from several radar stations during a live, time-critical phase of a deep space mission. The radar stations upload the data to the source S3 bucket. The data is prefixed by radar station identification number.The company created a destination S3 bucket in a second account. Data must be copied from the source S3 bucket to the destination S3 bucket to meet a compliance objective. This replication occurs through the use of an S3 replication rule to cover all objects in the source S3 bucket.One specific radar station is identified as having the most accurate data. Data replication at this radar station must be monitored for completion within 30 minutes after the radar station uploads the objects to the source S3 bucket.What should a solutions architect do to meet these requirements?",
    "choices": [
      "A. Setup an AWS DataSync agent to replicate the prefixed data from the source S3 bucket to the destination S3 bucket. Select to use all available bandwidth on the task, and monitor the task to ensure that itis in the TRANSFERRING status. Create an Amazon EventBridge rule to initiate an alert if this status changes.",
      "B. In the second account, create another S3 bucket to receive data from the radar station with the most accurate data. Set up a new replication rule for this new S3 bucket to separate the replication from the other radar stations. Monitor the maximum replication time to the destination. Create an Amazon EventBridge rule to initiate an alert when the time exceeds the desired threshold.",
      "C. Enable Amazon S3 Transfer Acceleration on the source S3 bucket, and configure the radar station with the most accurate data to use the new endpoint. Monitor the S3 destination bucket's TotalRequestLatency metric. Create an Amazon EventBridge rule to initiate an alert if this status changes.",
      "D. Create a new S3 replication rule on the source S3 bucket that filters for the keys that use the prefix of the radar station with the most accurate data. Enable S3 Replication Time Control (S3 RTC). Monitor the maximum replication time to the destination. Create an Amazon EventBridge rule to initiate an alert when the time exceeds the desired threshold."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (D):\n\nThe correct answer is D. This is the best solution to meet the given requirements.\n\nExplanation:\n- The question states that data replication at the radar station with the most accurate data must be monitored for completion within 30 minutes.\n- Option D suggests creating a new S3 replication rule on the source S3 bucket that filters for the keys (prefixes) of the radar station with the most accurate data.\n- This allows you to monitor the replication time for just the data from the critical radar station, separate from the replication of data from the other radar stations.\n- By enabling S3 Replication Time Control (S3 RTC), you can set a replication time objective of 30 minutes, and Amazon S3 will provide metrics and alerts if the replication time exceeds this threshold.\n- This solution allows you to meet the specific monitoring and time-critical requirements for the most accurate radar station data, while still replicating all data from the source S3 bucket to the destination S3 bucket.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This solution uses AWS DataSync, which is not the most appropriate choice for this scenario. S3 Replication is a more native and optimized solution for moving data between S3 buckets. Additionally, the question specifically states the need to monitor the replication time for the most accurate radar station, which cannot be achieved with a generic DataSync task.\n\nB. This solution creates an additional S3 bucket in the second account to receive data from the radar station with the most accurate data. This adds unnecessary complexity and overhead, as the requirement can be met by setting up a replication rule on the existing source S3 bucket, as suggested in the correct answer.\n\nC. Enabling S3 Transfer Acceleration can improve upload/download performance, but it does not provide the specific monitoring and alerting capabilities required for the time-critical replication of the most accurate radar station data. The question specifically asks for monitoring the replication time, which is better addressed by S3 Replication Time Control (S3 RTC) in option D."
  },
  "353": {
    "question": "A company wants to migrate its on-premises data center to the AWS Cloud. This includes thousands of virtualized Linux and Microsoft Windows servers, SAN storage, Java and PHP applications with MySQL, and Oracle databases. There are many dependent services hosted either in the same data center or externally. The technical documentation is incomplete and outdated. A solutions architect needs to understand the current environment and estimate the cloud resource costs after the migration.Which tools or services should the solutions architect use to plan the cloud migration? (Choose three.)",
    "choices": [
      "A. AWS Application Discovery Service",
      "B. AWS SMS",
      "C. AWS X-Ray",
      "D. AWS Cloud Adoption Readiness Tool (CART)",
      "E. Amazon Inspector",
      "F. AWS Migration Hub"
    ],
    "answer": "ADF",
    "explanation": "Explanation of the correct answer (ADF):\n\n1. AWS Application Discovery Service (A): This service helps the solutions architect assess the current on-premises environment by collecting data about your servers, their configurations, and interdependencies. This information is crucial for planning the cloud migration and estimating the required cloud resources.\n\n2. AWS Cloud Adoption Readiness Tool (CART) (D): This tool evaluates an organization's readiness for cloud adoption across various dimensions, such as organization, people, process, and technology. The solutions architect can use the insights from CART to create a plan for the cloud migration and ensure a smooth transition.\n\n3. AWS Migration Hub (F): This service provides a single place to track the progress of application migrations to AWS. The solutions architect can use Migration Hub to monitor the migration process, identify any issues, and coordinate the overall migration effort.\n\nExplanation of the incorrect choices:\n\nB. AWS SMS (Server Migration Service) is no longer recommended by AWS. It has been discontinued as of March 31, 2022, and AWS now recommends using alternative solutions like AWS Application Migration Service (AWS MGN) for server migrations.\n\nC. AWS X-Ray is a service for analyzing and debugging distributed applications. It is not directly related to the planning and estimation of a cloud migration.\n\nE. Amazon Inspector is a security assessment service that helps identify potential vulnerabilities and compliance issues in the AWS environment. While it can be useful during the migration process, it does not directly address the planning and estimation of the cloud migration."
  },
  "354": {
    "question": "A solutions architect is reviewing an application's resilience before launch. The application runs on an Amazon EC2 instance that is deployed in a private subnet of a VPC. The EC2 instance is provisioned by an Auto Scaling group that has a minimum capacity of 1 and a maximum capacity of 1. The application stores data on an Amazon RDS for MySQL DB instance. The VPC has subnets configured in three Availability Zones and is configured with a single NAT gateway.The solutions architect needs to recommend a solution to ensure that the application will operate across multiple Availability Zones.Which solution will meet this requirement?",
    "choices": [
      "A. Deploy an additional NAT gateway in the other Availability Zones. Update the route tables with appropriate routes. Modify the RDS for MySQL DB instance to a Multi-AZ configuration. Configure the Auto Scaling group to launch the instances across Availability Zones. Set the minimum capacity and maximum capacity of the Auto Scaling group to 3.",
      "B. Replace the NAT gateway with a virtual private gateway. Replace the RDS for MySQL DB instance with an Amazon Aurora MySQL DB cluster. Configure the Auto Scaling group to launch instances across all subnets in the VPC. Set the minimum capacity and maximum capacity of the Auto Scaling group to 3.",
      "C. Replace the NAT gateway with a NAT instance. Migrate the RDS for MySQL DB instance to an RDS for PostgreSQL DB instance. Launch a new EC2 instance in the other Availability Zones.",
      "D. Deploy an additional NAT gateway in the other Availability Zones. Update the route tables with appropriate routes. Modify the RDS for MySQL DB instance to turn on automatic backups and retain the backups for 7 days. Configure the Auto Scaling group to launch instances across all subnets in the VPC. Keep the minimum capacity and the maximum capacity of the Auto Scaling group at 1."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\n\nThe correct answer is A because it addresses the key requirements of the question:\n\n- Deploying an additional NAT gateway in the other Availability Zones ensures that the application can still access the internet and external resources even if one NAT gateway fails, providing high availability.\n- Updating the route tables with appropriate routes ensures that the EC2 instances can reach the RDS instance and other resources across Availability Zones.\n- Modifying the RDS for MySQL DB instance to a Multi-AZ configuration ensures that the database remains highly available and can withstand the failure of a single Availability Zone.\n- Configuring the Auto Scaling group to launch instances across Availability Zones ensures that the application can run on EC2 instances in multiple Availability Zones, providing resilience against the failure of a single Availability Zone.\n- Setting the minimum and maximum capacity of the Auto Scaling group to 3 ensures that there are at least 3 EC2 instances running, providing redundancy and fault tolerance.\n\n2. Explanations for the incorrect choices:\n\nB. This option is incorrect because:\n- Replacing the NAT gateway with a virtual private gateway would not provide the necessary internet connectivity for the EC2 instances to access external resources.\n- Replacing the RDS for MySQL DB instance with an Amazon Aurora MySQL DB cluster is unnecessary, as the requirement can be met by configuring the existing RDS instance for Multi-AZ.\n- Configuring the Auto Scaling group to launch instances across all subnets in the VPC is not required, as the instances can be launched across Availability Zones.\n\nC. This option is incorrect because:\n- Replacing the NAT gateway with a NAT instance would introduce a single point of failure, as a NAT instance is a less reliable solution than a NAT gateway.\n- Migrating the RDS for MySQL DB instance to an RDS for PostgreSQL DB instance is unnecessary, as the requirement can be met by configuring the existing RDS instance for Multi-AZ.\n- Launching a new EC2 instance in the other Availability Zones is not sufficient, as the Auto Scaling group should be configured to launch instances across Availability Zones.\n\nD. This option is incorrect because:\n-"
  },
  "355": {
    "question": "Accompany is deploying a new cluster for big data analytics on AWS. The cluster will run across many Linux Amazon EC2 instances that are spread across multiple Availability Zones.All of the nodes in the cluster must have read and write access to common underlying file storage. The file storage must be highly available, must be resilient, must be compatible with the Portable Operating System Interface (POSIX), and must accommodate high levels of throughput.Which storage solution will meet these requirements?",
    "choices": [
      "A. Provision an AWS Storage Gateway file gateway NFS file share that is attached to an Amazon S3 bucket. Mount the NFS file share on each EC2 instance in the cluster.",
      "B. Provision a new Amazon Elastic File System (Amazon EFS) file system that uses General Purpose performance mode. Mount the EFS file system on each EC2 instance in the cluster.",
      "C. Provision a new Amazon Elastic Block Store (Amazon EBS) volume that uses the io2 volume type. Attach the EBS volume to all of the EC2 instances in the cluster.",
      "D. Provision a new Amazon Elastic File System (Amazon EFS) file system that uses Max I/O performance mode. Mount the EFS file system on each EC2 instance in the cluster."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (D):\n\nThe correct answer is option D, which is to provision a new Amazon Elastic File System (Amazon EFS) file system that uses the Max I/O performance mode. This is the best choice because:\n\n- The question states that the file storage must accommodate high levels of throughput, which is a key feature of the Max I/O performance mode in Amazon EFS. Max I/O mode is designed for workloads that require high throughput and IOPS, such as big data analytics.\n- The file storage must also be highly available and resilient, which EFS provides by replicating data across multiple Availability Zones.\n- EFS is compatible with the POSIX file system interface, meeting the requirement for POSIX compatibility.\n- Mounting the EFS file system on each EC2 instance in the cluster will provide the necessary read and write access to the common underlying file storage.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Provisioning an AWS Storage Gateway file gateway NFS file share attached to an S3 bucket is not the best choice because:\n   - S3 does not provide POSIX compatibility, which is a requirement in the question.\n   - The NFS file share may not be able to accommodate the high levels of throughput required for big data analytics.\n\nB. Provisioning an Amazon EFS file system in General Purpose performance mode is not the best choice because:\n   - General Purpose mode is better suited for latency-sensitive use cases, not high-throughput workloads like big data analytics.\n   - To achieve the maximum 250,000 read IOPS, the file system would need to use Elastic throughput, which is only available in General Purpose mode, not Max I/O mode.\n\nC. Provisioning an Amazon EBS volume with the io2 volume type is not the best choice because:\n   - EBS volumes are block-based storage, not file-based storage, and may not provide the same level of POSIX compatibility and high availability as a file system like EFS.\n   - EBS volumes are typically attached to a single EC2 instance, whereas the question requires the storage to be accessible to all nodes in the cluster."
  },
  "356": {
    "question": "A company is planning to migrate to the AWS Cloud. The company hosts many applications on Windows servers and Linux servers. Some of the servers are physical, and some of the servers are virtual. The company uses several types of databases in its on-premises environment. The company does not have an accurate inventory of its on-premises servers and applications.The company wants to rightsize its resources during migration. A solutions architect needs to obtain information about the network connections and the application relationships. The solutions architect must assess the company\u2019s current environment and develop a migration plan.Which solution will provide the solutions architect with the required information to develop the migration plan?",
    "choices": [
      "A. Use Migration Evaluator to request an evaluation of the environment from AWS. Use the AWS Application Discovery Service Agentless Collector to import the details into a Migration Evaluator Quick Insights report.",
      "B. Use AWS Migration Hub and install the AWS Application Discovery Agent on the servers. Deploy the Migration Hub Strategy Recommendations application data collector. Generate a report by using Migration Hub Strategy Recommendations.",
      "C. Use AWS Migration Hub and run the AWS Application Discovery Service Agentless Collector on the servers. Group the servers and databases by using AWS Application Migration Service. Generate a report by using Migration Hub Strategy Recommendations.",
      "D. Use the AWS Migration Hub import tool to load the details of the company\u2019s on-premises environment. Generate a report by using Migration Hub Strategy Recommendations."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (Option B):\n\nThe correct answer is Option B because it uses the AWS Application Discovery Agent to collect the required information about the company's on-premises environment. The question states that the company has a mix of physical and virtual servers, and the solutions architect needs to obtain information about the network connections and application relationships. The AWS Application Discovery Agent is designed to capture this type of data, including system configuration, performance, running processes, and network connection details. By installing the agent on the servers, the solutions architect can gather the necessary information to develop the migration plan.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option uses the AWS Application Discovery Service Agentless Collector, which is only suitable for collecting data from on-premises virtual machines. Since the company has a mix of physical and virtual servers, the agentless collector would not be able to gather the required information from the physical servers.\n\nC. This option also uses the AWS Application Discovery Service Agentless Collector, which is not suitable for the given scenario.\n\nD. This option suggests using the AWS Migration Hub import tool to load the details of the company's on-premises environment. However, the question states that the company does not have an accurate inventory of its on-premises servers and applications, so importing this data directly would not be possible."
  },
  "357": {
    "question": "A financial services company sells its software-as-a-service (SaaS) platform for application compliance to large global banks. The SaaS platform runs on AWS and uses multiple AWS accounts that are managed in an organization in AWS Organizations. The SaaS platform uses many AWS resources globally.For regulatory compliance, all API calls to AWS resources must be audited, tracked for changes, and stored in a durable and secure data store.Which solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      "A. Create a new AWS CloudTrail trail. Use an existing Amazon S3 bucket in the organization's management account to store the logs. Deploy the trail to all AWS Regions. Enable MFA delete and encryption on the S3 bucket.",
      "B. Create a new AWS CloudTrail trail in each member account of the organization. Create new Amazon S3 buckets to store the logs. Deploy the trail to all AWS Regions. Enable MFA delete and encryption on the S3 buckets.",
      "C. Create a new AWS CloudTrail trail in the organization's management account. Create a new Amazon S3 bucket with versioning turned on to store the logs. Deploy the trail for all accounts in the organization. Enable MFA delete and encryption on the S3 bucket.",
      "D. Create a new AWS CloudTrail trail in the organization's management account. Create a new Amazon S3 bucket to store the logs. Configure Amazon Simple Notification Service (Amazon SNS) to send log-file delivery notifications to an external management system that will track the logs. Enable MFA delete and encryption on the S3 bucket."
    ],
    "answer": "C",
    "explanation": "1. Clear explanation of why the correct answer (C) is right:\n\nThe correct answer is C because it meets all the requirements with the least operational overhead:\n\n- It creates a new CloudTrail trail in the organization's management account, which acts as a central hub for logging and auditing across the entire organization.\n- It creates a new S3 bucket in the management account to store the logs, reducing the operational overhead of managing multiple buckets across different accounts.\n- Enabling versioning on the S3 bucket ensures that old log versions are not automatically deleted, providing an additional layer of compliance and auditability.\n- Enabling MFA delete and encryption on the S3 bucket provides the required durability and security for the log data.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nA. This option creates a single CloudTrail trail and uses an existing S3 bucket in the management account. While this reduces some operational overhead, it does not provide the same level of compliance and auditability as using a versioned S3 bucket.\n\nB. This option creates a CloudTrail trail in each member account and uses new S3 buckets in those accounts. This increases the operational overhead of managing multiple trails and buckets across the organization.\n\nD. This option creates a CloudTrail trail in the management account and uses a new S3 bucket to store the logs. It also configures Amazon SNS to send log-file delivery notifications to an external management system. While this provides additional notification capabilities, it does not offer the same level of compliance and auditability as using a versioned S3 bucket."
  },
  "358": {
    "question": "A company is deploying a distributed in-memory database on a fleet of Amazon EC2 instances. The fleet consists of a primary node and eight worker nodes. The primary node is responsible for monitoring cluster health, accepting user requests, distributing user requests to worker nodes, and sending an aggregate response back to a client. Worker nodes communicate with each other to replicate data partitions.The company requires the lowest possible networking latency to achieve maximum performance.Which solution will meet these requirements?",
    "choices": [
      "A. Launch memory optimized EC2 instances in a partition placement group.",
      "B. Launch compute optimized EC2 instances in a partition placement group.",
      "C. Launch memory optimized EC2 instances in a cluster placement group.",
      "D. Launch compute optimized EC2 instances in a spread placement group."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C. Launch memory-optimized EC2 instances in a cluster placement group.\n\nThis solution is the best fit for the given requirements because:\n\n- Cluster placement groups (CPGs) are designed to place related resources (such as the nodes of a distributed in-memory database) in the same Availability Zone (AZ). This reduces network latency and improves communication efficiency between the nodes.\n- Memory-optimized instances are the most suitable instance type for an in-memory database, as they provide high-performance memory and compute resources.\n- Placing the memory-optimized instances in a CPG ensures that the worker nodes can communicate with each other with the lowest possible networking latency, which is critical for the replication of data partitions.\n\n2. Explanations of the incorrect choices:\n\nA. Launch memory-optimized EC2 instances in a partition placement group:\n- Partition placement groups are designed to spread instances across different racks, which can help with fault tolerance, but they do not optimize for low networking latency between instances.\n\nB. Launch compute-optimized EC2 instances in a partition placement group:\n- Compute-optimized instances may not be the best fit for an in-memory database, as they are optimized for CPU-intensive workloads rather than memory-intensive ones.\n- Partition placement groups, as mentioned in choice A, do not optimize for low networking latency between instances.\n\nD. Launch compute-optimized EC2 instances in a spread placement group:\n- Compute-optimized instances may not be the best fit for an in-memory database.\n- Spread placement groups are designed to spread instances across different underlying hardware to improve fault tolerance, but they do not optimize for low networking latency between instances."
  },
  "359": {
    "question": "A company maintains information on premises in approximately 1 million.csv files that are hosted on a VM. The data initially is 10 TB in size and grows at a rate of 1 TB each week. The company needs to automate backups of the data to the AWS Cloud.Backups of the data must occur daily. The company needs a solution that applies custom filters to back up only a subset of the data that is located in designated source directories. The company has set up an AWS Direct Connect connection.Which solution will meet the backup requirements with the LEAST operational overhead?",
    "choices": [
      "A. Use the Amazon S3 CopyObject API operation with multipart upload to copy the existing data to Amazon S3. Use the CopyObject API operation to replicate new data to Amazon S3 daily.",
      "B. Create a backup plan in AWS Backup to back up the data to Amazon S3. Schedule the backup plan to run daily.",
      "C. Install the AWS DataSync agent as a VM that runs on the on-premises hypervisor. Configure a DataSync task to replicate the data to Amazon S3 daily.",
      "D. Use an AWS Snowball Edge device for the initial backup. Use AWS DataSync for incremental backups to Amazon S3 daily."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\nThe correct answer is C. Install the AWS DataSync agent as a VM that runs on the on-premises hypervisor. Configure a DataSync task to replicate the data to Amazon S3 daily.\n\nThis solution is the best fit for the given requirements because:\n\n- AWS DataSync can apply custom filters to back up only a subset of the data located in designated source directories, which meets the company's requirement.\n- DataSync can be scheduled to run daily, providing the required frequency of backups.\n- DataSync is designed to handle large datasets and a high volume of small files (like the 1 million .csv files), which is well-suited for the company's data.\n- By running the DataSync agent as a VM on the on-premises hypervisor, the solution integrates seamlessly with the existing infrastructure and leverages the AWS Direct Connect connection for efficient data transfer.\n- DataSync is a fully managed service, reducing the operational overhead for the company.\n\n2. Explanations of the incorrect choices:\n\nA. Use the Amazon S3 CopyObject API operation with multipart upload to copy the existing data to Amazon S3. Use the CopyObject API operation to replicate new data to Amazon S3 daily.\n- This solution does not provide a way to apply custom filters to the data, which is a key requirement.\n- Manually managing the CopyObject API operations would result in higher operational overhead compared to a managed service like DataSync.\n\nB. Create a backup plan in AWS Backup to back up the data to Amazon S3. Schedule the backup plan to run daily.\n- AWS Backup does not back up data directly to Amazon S3. Instead, it stores backups in AWS Backup Vaults, which does not meet the requirement of backing up data to Amazon S3.\n- AWS Backup is primarily designed for managing backups of AWS resources, not for large on-premises datasets with custom filtering requirements.\n\nD. Use an AWS Snowball Edge device for the initial backup. Use AWS DataSync for incremental backups to Amazon S3 daily.\n- This solution introduces additional complexity and operational overhead by requiring the use of a Snowball Edge device for the initial backup.\n- The requirement can be met more efficiently by using DataSync alone"
  },
  "360": {
    "question": "A financial services company has an asset management product that thousands of customers use around the world. The customers provide feedback about the product through surveys. The company is building a new analytical solution that runs on Amazon EMR to analyze the data from these surveys. The following user personas need to access the analytical solution to perform different actions:\u2022\tAdministrator: Provisions the EMR cluster for the analytics team based on the team\u2019s requirements\u2022\tData engineer: Runs ETL scripts to process, transform, and enrich the datasets\u2022\tData analyst: Runs SQL and Hive queries on the dataA solutions architect must ensure that all the user personas have least privilege access to only the resources that they need. The user personas must be able to launch only applications that are approved and authorized. The solution also must ensure tagging for all resources that the user personas create.Which solution will meet these requirements?",
    "choices": [
      "A. Create IAM roles for each user persona. Attach identity-based policies to define which actions the user who assumes the role can perform. Create an AWS Config rule to check for noncompliant resources. Configure the rule to notify the administrator to remediate the noncompliant resources.",
      "B. Setup Kerberos-based authentication for EMR clusters upon launch. Specify a Kerberos security configuration along with cluster-specific Kerberos options.",
      "C. Use AWS Service Catalog to control the Amazon EMR versions available for deployment, the cluster configuration, and the permissions for each user persona.",
      "D. Launch the EMR cluster by using AWS CloudFormation, Attach resource-based policies to the EMR cluster during cluster creation. Create an AWS. Config rule to check for noncompliant clusters and noncompliant Amazon S3 buckets. Configure the rule to notify the administrator to remediate the noncompliant resources."
    ],
    "answer": "C",
    "explanation": "1. Clear explanation of why the correct answer (C) is right:\n\nThe correct answer is C - Use AWS Service Catalog to control the Amazon EMR versions available for deployment, the cluster configuration, and the permissions for each user persona.\n\nAWS Service Catalog is the best solution to meet the given requirements because:\n\n- It allows the organization to define and manage approved Amazon EMR versions, cluster configurations, and permissions for different user personas (administrator, data engineer, data analyst) in a centralized manner.\n- It enforces the principle of least privilege by restricting users to only the approved resources and actions that are authorized for their roles.\n- It provides a way to control the deployment of Amazon EMR resources and ensure that all created resources are compliant with the organization's policies, including mandatory tagging.\n- It simplifies the management of access control and compliance compared to approaches like creating IAM roles and using AWS Config rules (as in option A).\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nA. Create IAM roles for each user persona and use AWS Config rules:\n- This approach relies on manual creation and management of IAM roles and policies, which can be more complex to maintain than the centralized control provided by AWS Service Catalog.\n- The use of AWS Config rules adds an extra layer of management and potential lag in remediation, whereas AWS Service Catalog enforces compliance directly during deployment.\n\nB. Setup Kerberos-based authentication for EMR clusters:\n- Kerberos-based authentication is a valid security mechanism, but it does not address the requirements around controlling the available EMR versions, cluster configurations, and permissions for different user personas.\n\nD. Use AWS CloudFormation and resource-based policies:\n- While AWS CloudFormation can be used to provision the EMR cluster and attach resource-based policies, it does not provide the same level of centralized control and enforcement of policies as AWS Service Catalog.\n- The use of AWS Config rules, as in option A, adds an extra layer of management and potential lag in remediation."
  },
  "361": {
    "question": "A software as a service (SaaS) company uses AWS to host a service that is powered by AWS PrivateLink. The service consists of proprietary software that runs on three Amazon EC2 instances behind a Network Load Balancer (NLB). The instances are in private subnets in multiple Availability Zones in the eu-west-2 Region. All the company's customers are in eu-west-2.However, the company now acquires a new customer in the us-east-1 Region. The company creates a new VPC and new subnets in us-east-1. The company establishes inter-Region VPC peering between the VPCs in the two Regions.The company wants to give the new customer access to the SaaS service, but the company does not want to immediately deploy new EC2 resources in us-east-1.Which solution will meet these requirements?",
    "choices": [
      "A. Configure a PrivateLink endpoint service in us-east-1 to use the existing NLB that is in eu-west-2. Grant specific AWS accounts access to connect to the SaaS service.",
      "B. Create an NLB in us-east-1. Create an IP target group that uses the IP addresses of the company's instances in eu-west-2 that host the SaaS service. Configure a PrivateLink endpoint service that uses the NLB that is in us-east-1. Grant specific AWS accounts access to connect to the SaaS service.",
      "C. Create an Application Load Balancer (ALB) in front of the EC2 instances in eu-west-2. Create an NLB in us-east-1. Associate the NLB that is in us-east-1 with an ALB target group that uses the ALB that is in eu-west-2. Configure a PrivateLink endpoint service that uses the NLB that is in us-east-1. Grant specific AWS accounts access to connect to the SaaS service.",
      "D. Use AWS Resource Access Manager (AWS RAM) to share the EC2 instances that are in eu-west-2. In us-east-1, create an NLB and an instance target group that includes the shared EC2 instances from eu-west-2. Configure a PrivateLink endpoint service that uses the NLB that is in us-east-1. Grant specific AWS accounts access to connect to the SaaS service."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\n\nThe correct answer is A because it meets the requirements specified in the question:\n\n- Configuring a PrivateLink endpoint service in us-east-1 to use the existing NLB that is in eu-west-2 allows the new customer in us-east-1 to access the SaaS service without deploying new EC2 resources in us-east-1. This aligns with the requirement of not immediately deploying new EC2 resources in us-east-1.\n\n- Granting specific AWS accounts access to connect to the SaaS service ensures that only authorized users can access the service, which addresses the security requirement.\n\n2. Explanations of why each incorrect choice is wrong:\n\nB. This option is incorrect because an NLB in us-east-1 cannot directly target the EC2 instances in eu-west-2. NLBs only support targets within the same VPC or in a peered VPC in the same AWS Region. Creating an IP target group in us-east-1 that references the instances in eu-west-2 is not a valid configuration.\n\nC. This option is incorrect because it introduces an unnecessary Application Load Balancer (ALB) in front of the EC2 instances in eu-west-2. The requirement is to provide access to the SaaS service without deploying new resources in us-east-1, and using an ALB in eu-west-2 does not align with this requirement.\n\nD. This option is incorrect because sharing the EC2 instances in eu-west-2 using AWS Resource Access Manager (RAM) does not address the requirement of not deploying new resources in us-east-1. The NLB and instance target group in us-east-1 would still require new resources to be created in that region."
  },
  "362": {
    "question": "A company needs to monitor a growing number of Amazon S3 buckets across two AWS Regions. The company also needs to track the percentage of objects that are encrypted in Amazon S3. The company needs a dashboard to display this information for internal compliance teams.Which solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      "A. Create a new 3 Storage Lens dashboard in each Region to track bucket and encryption metrics. Aggregate data from both Region dashboards into a single dashboard in Amazon QuickSight for the compliance teams.",
      "B. Deploy an AWS Lambda function in each Region to list the number of buckets and the encryption status of objects. Store this data in Amazon S3. Use Amazon Athena queries to display the data on a custom dashboard in Amazon QuickSight for the compliance teams.",
      "C. Use the S3 Storage Lens default dashboard to track bucket and encryption metrics. Give the compliance teams access to the dashboard directly in the S3 console.",
      "D. Create an Amazon EventBridge rule to detect AWS CloudTrail events for S3 object creation. Configure the rule to invoke an AWS Lambda function to record encryption metrics in Amazon DynamoDB. Use Amazon QuickSight to display the metrics in a dashboard for the compliance teams."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C because it leverages the pre-built S3 Storage Lens dashboard, which provides the required functionality with the least operational overhead. S3 Storage Lens is a feature that provides detailed analytics and recommendations for your S3 environment, including metrics on object encryption. The compliance teams can access the default S3 Storage Lens dashboard directly in the S3 console, without the need to set up and manage additional resources like Lambda functions, Athena queries, or custom QuickSight dashboards.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option introduces more operational overhead by requiring the creation of multiple dashboards (one in each region) and then aggregating the data into a single QuickSight dashboard. This adds complexity and management overhead compared to the built-in S3 Storage Lens dashboard.\n\nB. This option requires deploying and managing AWS Lambda functions in each region, storing data in S3, and using Athena queries to display the metrics. This is a more complex solution with higher operational overhead compared to the pre-built S3 Storage Lens dashboard.\n\nD. This option uses EventBridge and Lambda to record encryption metrics in DynamoDB, and then requires building a custom QuickSight dashboard. This is a more complex solution with higher operational overhead compared to the pre-built S3 Storage Lens dashboard."
  },
  "363": {
    "question": "A company\u2019s CISO has asked a solutions architect to re-engineer the company's current CI/CD practices to make sure patch deployments to its application can happen as quickly as possible with minimal downtime if vulnerabilities are discovered. The company must also be able to quickly roll back a change in case of errors.The web application is deployed in a fleet of Amazon EC2 instances behind an Application Load Balancer. The company is currently using GitHub to host the application source code, and has configured an AWS CodeBuild project to build the application. The company also intends to use AWS CodePipeline to trigger builds from GitHub commits using the existing CodeBuild project.What CI/CD configuration meets all of the requirements?",
    "choices": [
      "A. Configure CodePipeline with a deploy stage using AWS CodeDeploy configured for in-place deployment. Monitor the newly deployed code, and, if there are any issues, push another code update",
      "B. Configure CodePipeline with a deploy stage using AWS CodeDeploy configured for blue/green deployments. Monitor the newly deployed code, and, if there are any issues, trigger a manual rollback using CodeDeploy.",
      "C. Configure CodePipeline with a deploy stage using AWS CloudFormation to create a pipeline for test and production stacks. Monitor the newly deployed code, and, if there are any issues, push another code update.",
      "D. Configure the CodePipeline with a deploy stage using AWS OpsWorks and in-place deployments. Monitor the newly deployed code, and, if there are any issues, push another code update."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\nThe correct answer is B, which is to configure CodePipeline with a deploy stage using AWS CodeDeploy configured for blue/green deployments. This approach aligns well with the requirements outlined in the question:\n\n- Quick patch deployments with minimal downtime: Blue/green deployments allow for a new version of the application to be deployed in parallel with the current production version (the \"blue\" environment). This enables quick deployments of patches or updates without disrupting the current production environment.\n\n- Ability to quickly roll back: If issues are detected with the new deployment, the application can be quickly rolled back to the previous \"green\" environment, allowing for a fast recovery.\n\n- Monitoring and manual rollbacks: The question mentions monitoring the newly deployed code, and if there are any issues, triggering a manual rollback using CodeDeploy, which is supported by the blue/green deployment approach.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. In-place deployment with CodeDeploy: This approach does not provide the ability to quickly roll back to a previous version, as it involves directly updating the production environment. This does not meet the requirement for a fast rollback process.\n\nC. CloudFormation for test and production stacks: While this approach can provide a pipeline for deployments, it does not specifically address the need for quick patch deployments and the ability to easily roll back. The question requires a solution that focuses on the deployment process and the ability to manage updates and rollbacks.\n\nD. OpsWorks with in-place deployments: Similar to choice A, this approach involves directly updating the production environment, which does not meet the requirement for a fast rollback process.\n\nIn summary, the blue/green deployment approach using CodeDeploy, as described in choice B, is the most appropriate solution to meet the requirements outlined in the question, as it provides the necessary speed, reliability, and safety for critical application updates and patches."
  },
  "364": {
    "question": "A company is managing many AWS accounts by using an organization in AWS Organizations. Different business units in the company run applications on Amazon EC2 instances. All the EC2 instances must have a BusinessUnit tag so that the company can track the cost for each business unit.A recent audit revealed that some instances were missing this tag. The company manually added the missing tag to the instances.What should a solutions architect do to enforce the tagging requirement in the future?",
    "choices": [
      "A. Enable tag policies in the organization. Create a tag policy for the BusinessUnit tag. Ensure that compliance with tag key capitalization is turned off. Implement the tag policy for the ec2:instance resource type. Attach the tag policy to the root of the organization.",
      "B. Enable tag policies in the organization. Create a tag policy for the BusinessUnit tag. Ensure that compliance with tag key capitalization is turned on. Implement the tag policy for the ec2:instance resource type. Attach the tag policy to the organization's management account.",
      "C. Create an SCP and attach the SCP to the root of the organization. Include the following statement in the SCP:",
      "D. Create an SCP and attach the SCP to the organization\u2019s management account. Include the following statement in the SCP:"
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C, which suggests creating an SCP (Service Control Policy) and attaching it to the root of the organization. This is the right approach because SCPs are used to enforce organization-wide restrictions and requirements, such as the tagging requirement for EC2 instances.\n\nBy creating an SCP and attaching it to the root of the organization, the tagging requirement will be applied to all member accounts, ensuring that the \"BusinessUnit\" tag is applied to all new EC2 instances created in the future. This proactive approach is better than relying on manual intervention or tag policies, which may not be as effective in enforcing the tagging requirement across the entire organization.\n\n2. Explanations of why the other choices are incorrect:\n\nA. This option is incorrect because it suggests using tag policies instead of SCPs. Tag policies are used to manage and enforce tagging requirements, but they are not as effective as SCPs in preventing the creation of resources that do not comply with the tagging requirements.\n\nB. This option is incorrect because it suggests attaching the tag policy to the organization's management account instead of the root. The management account is responsible for managing the organization, but the root is the appropriate level to apply organization-wide policies, such as the tagging requirement.\n\nD. This option is incorrect because it suggests creating an SCP and attaching it to the organization's management account instead of the root. As mentioned in the explanation for choice C, the root is the appropriate level to apply organization-wide policies."
  },
  "365": {
    "question": "A company is running a workload that consists of thousands of Amazon EC2 instances. The workload is running in a VPC that contains several public subnets and private subnets. The public subnets have a route for 0.0.0.0/0 to an existing internet gateway. The private subnets have a route for 0.0.0.0/0 to an existing NAT gateway.A solutions architect needs to migrate the entire fleet of EC2 instances to use IPv6. The EC2 instances that are in private subnets must not be accessible from the public internet.What should the solutions architect do to meet these requirements?",
    "choices": [
      "A. Update the existing VPC, and associate a custom IPv6 CIDR block with the VPC and all subnets. Update all the VPC route tables, and add a route for ::/0 to the internet gateway.",
      "B. Update the existing VPC, and associate an Amazon-provided IPv6 CIDR block with the VPC and all subnets. Update the VPC route tables for all private subnets, and add a route for ::/0 to the NAT gateway.",
      "C. Update the existing VPC, and associate an Amazon-provided IPv6 CIDR block with the VPC and all subnets. Create an egress-only internet gateway. Update the VPC route tables for all private subnets, and add a route for ::/0 to the egress-only internet gateway.",
      "D. Update the existing VPC, and associate a custom IPV6 CIDR block with the VPC and all subnets. Create a new NAT gateway, and enable IPV6 support. Update the VPC route tables for all private subnets, and add a route for ::/0 to the IPv6-enabled NAT gateway."
    ],
    "answer": "C",
    "explanation": "1. Clear Explanation of the Correct Answer (C):\n\nThe correct answer is C because it addresses the key requirements stated in the question:\n\n- Associating an Amazon-provided IPv6 CIDR block with the VPC and all subnets ensures that the EC2 instances can use IPv6 without additional configuration.\n- Creating an egress-only internet gateway allows traffic from the private subnets to exit the VPC to the public internet, but prevents incoming traffic (ingress) from the public internet from reaching the private subnets. This meets the requirement that the EC2 instances in private subnets must not be accessible from the public internet.\n- Updating the VPC route tables for all private subnets and adding a route for ::/0 to the egress-only internet gateway ensures that IPv6 traffic destined for addresses outside the VPC can exit through the egress-only internet gateway.\n\n2. Explanations of why the Incorrect Choices are Wrong:\n\nA. This option is incorrect because it associates a custom IPv6 CIDR block with the VPC and all subnets, which is not necessary and adds an additional step. Additionally, it uses an internet gateway for the private subnets, which would allow inbound traffic from the public internet, violating the requirement.\n\nB. This option is incorrect because it uses a NAT gateway for the private subnets, which is not necessary for IPv6 traffic. IPv6 traffic should use an egress-only internet gateway instead, as it provides the required one-way outbound access without allowing inbound access from the public internet.\n\nD. This option is incorrect because it requires creating a new NAT gateway and enabling IPv6 support, which is unnecessary. The egress-only internet gateway in option C provides the required functionality without the additional complexity."
  },
  "366": {
    "question": "A company is planning to migrate an application from on premises to the AWS Cloud. The company will begin the migration by moving the application\u2019s underlying data storage to AWS. The application data is stored on a shared file system on premises, and the application servers connect to the shared file system through SMB.A solutions architect must implement a solution that uses an Amazon S3 bucket for shared storage. Until the application is fully migrated and code is rewritten to use native Amazon S3 APIs, the application must continue to have access to the data through SMB. The solutions architect must migrate the application data to AWS to its new location while still allowing the on-premises application to access the data.Which solution will meet these requirements?",
    "choices": [
      "A. Create a new Amazon FSx for Windows File Server file system. Configure AWS DataSync with one location for the on-premises file share and one location for the new Amazon FSx file system. Create a new DataSync task to copy the data from the on-premises file share location to the Amazon FSx file system.",
      "B. Create an S3 bucket for the application. Copy the data from the on-premises storage to the S3 bucket.",
      "C. Deploy an AWS Server Migration Service (AWS SMS) VM to the on-premises environment. Use AWS SMS to migrate the file storage server from on premises to an Amazon EC2 instance.",
      "D. Create an S3 bucket for the application. Deploy a new AWS Storage Gateway file gateway on an on-premises VM. Create a new file share that stores data in the S3 bucket and is associated with the file gateway. Copy the data from the on-premises storage to the new file gateway endpoint."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (Option D):\n\nOption D is the correct answer because it meets all the requirements stated in the question:\n\n- It creates an S3 bucket as the new shared storage location for the application's data.\n- It deploys an AWS Storage Gateway file gateway on an on-premises VM, which allows the on-premises application to continue accessing the data through SMB, even after the data has been migrated to the S3 bucket.\n- It creates a new file share that stores data in the S3 bucket and is associated with the file gateway, providing the bridge between the on-premises file system and the S3 bucket.\n- It allows the data to be copied from the on-premises storage to the new file gateway endpoint, effectively migrating the data to the AWS cloud while still maintaining access for the on-premises application.\n\n2. Explanations of why the other options are incorrect:\n\nA. This option is incorrect because it involves using Amazon FSx for Windows File Server, which does not meet the requirement of using an S3 bucket for shared storage.\n\nB. This option is incorrect because it only copies the data from the on-premises storage to the S3 bucket, without providing a way for the on-premises application to continue accessing the data through SMB.\n\nC. This option is incorrect because it involves migrating the file storage server from on-premises to an Amazon EC2 instance, which does not meet the requirement of using an S3 bucket for shared storage.\n\nIn summary, Option D is the correct answer because it fully addresses all the requirements stated in the question, including using an S3 bucket for shared storage and maintaining SMB access for the on-premises application during the migration process."
  },
  "367": {
    "question": "A large payroll company recently merged with a small staffing company. The unified company now has multiple business units, each with its own existing AWS account.A solutions architect must ensure that the company can centrally manage the billing and access policies for all the AWS accounts. The solutions architect configures AWS Organizations by sending an invitation to all member accounts of the company from a centralized management account.What should the solutions architect do next to meet these requirements?",
    "choices": [
      "A. Create the OrganizationAccountAccess IAM group in each member account. Include the necessary IAM roles for each administrator.",
      "B. Create the OrganizationAccountAccessPolicy IAM policy in each member account. Connect the member accounts to the management account by using cross-account access.",
      "C. Create the OrganizationAccountAccessRole IAM role in each member account. Grant permission to the management account to assume the IAM role.",
      "D. Create the OrganizationAccountAccessRole IAM role in the management account. Attach the AdministratorAccess AWS managed policy to the IAM role. Assign the IAM role to the administrators in each member account."
    ],
    "answer": "C",
    "explanation": "1. Clear explanation of why the correct answer (C) is right:\n\nThe correct answer is C, \"Create the OrganizationAccountAccessRole IAM role in each member account. Grant permission to the management account to assume the IAM role.\"\n\nThis is the correct approach because it allows the solutions architect to centrally manage the billing and access policies for all the AWS accounts. By creating an IAM role in each member account and granting the management account permission to assume that role, the administrators in the management account can perform necessary actions across all the accounts. This aligns with AWS best practices for Organizations and simplifies the management and auditing of various accounts, ensuring a standardized role exists across all accounts for consistent access control.\n\n2. Brief explanations of why the incorrect choices are wrong:\n\nA. \"Create the OrganizationAccountAccess IAM group in each member account. Include the necessary IAM roles for each administrator.\" This is incorrect because creating an IAM group and roles in each member account does not provide centralized management from the management account.\n\nB. \"Create the OrganizationAccountAccessPolicy IAM policy in each member account. Connect the member accounts to the management account by using cross-account access.\" This is incorrect because creating an IAM policy and using cross-account access does not provide the level of centralized management required, as it still requires configuration in each member account.\n\nD. \"Create the OrganizationAccountAccessRole IAM role in the management account. Attach the AdministratorAccess AWS managed policy to the IAM role. Assign the IAM role to the administrators in each member account.\" This is incorrect because creating the role in the management account and assigning it to administrators in each member account does not provide a standardized, centralized approach to managing access across all accounts."
  },
  "368": {
    "question": "A company has application services that have been containerized and deployed on multiple Amazon EC2 instances with public IPs. An Apache Kafka cluster has been deployed to the EC2 instances. A PostgreSQL database has been migrated to Amazon RDS for PostgreSQL. The company expects a significant increase of orders on its platform when a new version of its flagship product is released.What changes to the current architecture will reduce operational overhead and support the product release?",
    "choices": [
      "A. Create an EC2 Auto Scaling group behind an Application Load Balancer. Create additional read replicas for the DB instance. Create Amazon Kinesis data streams and configure the application services to use the data streams. Store and serve static content directly from Amazon S3.",
      "B. Create an EC2 Auto Scaling group behind an Application Load Balancer. Deploy the DB instance in Multi-AZ mode and enable storage auto scaling. Create Amazon Kinesis data streams and configure the application services to use the data streams. Store and serve static content directly from Amazon S3.",
      "C. Deploy the application on a Kubernetes cluster created on the EC2 instances behind an Application Load Balancer. Deploy the DB instance in Multi-AZ mode and enable storage auto scaling. Create an Amazon Managed Streaming for Apache Kafka cluster and configure the application services to use the cluster. Store static content in Amazon S3 behind an Amazon CloudFront distribution.",
      "D. Deploy the application on Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate and enable auto scaling behind an Application Load Balancer. Create additional read replicas for the DB instance. Create an Amazon Managed Streaming for Apache Kafka cluster and configure the application services to use the cluster. Store static content in Amazon S3 behind an Amazon CloudFront distribution."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (Option D):\n\nOption D is the correct answer because it provides a highly scalable, managed, and serverless-like architecture that can effectively handle the expected increase in orders for the company's platform. \n\nKey points:\n\n- Deploying the application on Amazon EKS with AWS Fargate eliminates the need for managing the underlying EC2 instances, reducing operational overhead. Fargate automatically scales the compute resources based on the application's needs.\n- Auto-scaling behind an Application Load Balancer ensures the application can scale up to handle increased traffic during the product release.\n- Creating additional read replicas for the database instance improves availability and distributes the read load, reducing the impact on the primary instance.\n- Using an Amazon Managed Streaming for Apache Kafka cluster provides a managed and scalable message streaming service, simplifying operations.\n- Storing static content in Amazon S3 behind an Amazon CloudFront distribution offloads the content delivery from the application, improving performance and reducing the load on the application services.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option lacks the benefits of a managed Kubernetes service (like EKS) and the serverless compute provided by Fargate. Additionally, the use of Amazon Kinesis data streams may be an overkill in this scenario, as the Managed Kafka service in option D is more appropriate.\n\nB. This option does not include the benefits of a managed Kubernetes service and Fargate, which can provide a more efficient and scalable platform compared to managing the EC2 instances directly.\n\nC. While this option leverages Kubernetes and managed services, it does not include the serverless compute benefits of Fargate, which can further reduce operational overhead. Additionally, the use of an Amazon Managed Streaming for Apache Kafka cluster is more appropriate than creating a custom Apache Kafka cluster on EC2 instances."
  },
  "369": {
    "question": "A company hosts a VPN in an on-premises data center. Employees currently connect to the VPN to access files in their Windows home directories. Recently, there has been a large growth in the number of employees who work remotely. As a result, bandwidth usage for connections into the data center has begun to reach 100% during business hours.The company must design a solution on AWS that will support the growth of the company's remote workforce, reduce the bandwidth usage for connections into the data center, and reduce operational overhead.Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose two.)",
    "choices": [
      "A. Create an AWS Storage Gateway Volume Gateway. Mount a volume from the Volume Gateway to the on-premises file server.",
      "B. Migrate the home directories to Amazon FSx for Windows File Server.",
      "C. Migrate the home directories to Amazon FSx for Lustre.",
      "D. Migrate remote users to AWS Client VPN.",
      "E. Create an AWS Direct Connect connection from the on-premises data center to AWS."
    ],
    "answer": "BD",
    "explanation": "1. Explanation of the correct answer (B and D):\n\nB. Migrate the home directories to Amazon FSx for Windows File Server:\nThis is the correct choice because migrating the home directories to Amazon FSx for Windows File Server allows remote employees to access their files directly from the cloud, without the need to connect through a VPN or to an on-premises file server. This reduces the bandwidth usage and operational overhead associated with the on-premises file server and VPN connection.\n\nD. Migrate remote users to AWS Client VPN:\nThis is also a correct choice because migrating remote users to AWS Client VPN provides a secure and scalable way for employees to access company resources and files from anywhere. This solution reduces the need for employees to connect through a VPN or to an on-premises file server, which in turn reduces bandwidth usage and operational overhead.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Create an AWS Storage Gateway Volume Gateway. Mount a volume from the Volume Gateway to the on-premises file server:\nThis option does not reduce the bandwidth usage or operational overhead, as the remote employees would still need to connect to the on-premises file server through the VPN.\n\nC. Migrate the home directories to Amazon FSx for Lustre:\nAmazon FSx for Lustre is designed for high-performance, computing-intensive workloads, and may not be the best fit for storing home directories, which are typically more focused on file storage and access.\n\nE. Create an AWS Direct Connect connection from the on-premises data center to AWS:\nWhile an AWS Direct Connect connection can help reduce bandwidth usage, it does not address the need to reduce operational overhead, as the on-premises file server and VPN would still need to be maintained."
  },
  "370": {
    "question": "A company has multiple AWS accounts. The company recently had a security audit that revealed many unencrypted Amazon Elastic Block Store (Amazon EBS) volumes attached to Amazon EC2 instances.A solutions architect must encrypt the unencrypted volumes and ensure that unencrypted volumes will be detected automatically in the future. Additionally, the company wants a solution that can centrally manage multiple AWS accounts with a focus on compliance and security.Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
    "choices": [
      "A. Create an organization in AWS Organizations. Set up AWS Control Tower, and turn on the strongly recommended controls (guardrails). Join all accounts to the organization. Categorize the AWS accounts into OUs.",
      "B. Use the AWS CLI to list all the unencrypted volumes in all the AWS accounts. Run a script to encrypt all the unencrypted volumes in place.",
      "C. Create a snapshot of each unencrypted volume. Create a new encrypted volume from the unencrypted snapshot. Detach the existing volume, and replace it with the encrypted volume.",
      "D. Create an organization in AWS Organizations. Set up AWS Control Tower, and turn on the mandatory controls (guardrails). Join all accounts to the organization. Categorize the AWS accounts into OUs.",
      "E. Turn on AWS CloudTrail. Configure an Amazon EventBridge rule to detect and automatically encrypt unencrypted volumes."
    ],
    "answer": "AC",
    "explanation": "1. Explanation of the correct answer:\n\nA and C are the correct choices because they address the key requirements of the scenario:\n\nA) Creating an AWS Organization with AWS Control Tower allows for centralized management and enforcement of security and compliance policies across multiple AWS accounts. The strongly recommended controls in Control Tower include the ability to detect unencrypted Amazon EBS volumes attached to EC2 instances, ensuring that this issue is automatically identified and addressed in the future.\n\nC) Replacing unencrypted volumes with encrypted ones is the most efficient way to encrypt the existing unencrypted volumes. This involves creating snapshots of the unencrypted volumes, creating new encrypted volumes from the snapshots, and then detaching the unencrypted volumes and attaching the new encrypted ones. This approach ensures that the data remains intact while the volumes are encrypted.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB) Using the AWS CLI to list and encrypt the unencrypted volumes is a manual and error-prone approach. It does not provide the centralized management and automated detection of unencrypted volumes that the organization requires.\n\nD) Turning on mandatory controls (guardrails) in AWS Control Tower is not the appropriate choice, as the question specifically mentions \"strongly recommended controls\" as the requirement.\n\nE) Configuring an Amazon EventBridge rule to detect and automatically encrypt unencrypted volumes does not address the need for centralized management of multiple AWS accounts. It also does not provide a solution for encrypting the existing unencrypted volumes."
  },
  "371": {
    "question": "A company hosts an intranet web application on Amazon EC2 instances behind an Application Load Balancer (ALB). Currently, users authenticate to the application against an internal user database.The company needs to authenticate users to the application by using an existing AWS Directory Service for Microsoft Active Directory directory. All users with accounts in the directory must have access to the application.Which solution will meet these requirements?",
    "choices": [
      "A. Create a new app client in the directory. Create a listener rule for the ALB. Specify the authenticate-oidc action for the listener rule. Configure the listener rule with the appropriate issuer, client ID and secret, and endpoint details for the Active Directory service. Configure the new app client with the callback URL that the ALB provides.",
      "B. Configure an Amazon Cognito user pool. Configure the user pool with a federated identity provider (ldP) that has metadata from the directory. Create an app client. Associate the app client with the user pool. Create a listener rule for the ALSpecify the authenticate-cognito action for the listener rule. Configure the listener rule to use the user pool and app client.",
      "C. Add the directory as a new IAM identity provider (ldP). Create a new IAM role that has an entity type of SAML 2.0 federation. Configure a role policy that allows access to the ALB. Configure the new role as the default authenticated user role for the ldP. Create a listener rule for the ALB. Specify the authenticate-oidc action for the listener rule.",
      "D. Enable AWS IAM Identity Center (AWS Single Sign-On). Configure the directory as an external identity provider (ldP) that uses SAML. Use the automatic provisioning method. Create a new IAM role that has an entity type of SAML 2.0 federation. Configure a role policy that allows access to the ALB. Attach the new role to all groups. Create a listener rule for the ALB. Specify the authenticate-cognito action for the listener rule."
    ],
    "answer": "B",
    "explanation": "1. Clear explanation of why the correct answer (B) is right:\n\nThe correct answer is B because it aligns best with the requirements stated in the question:\n\n- The company needs to authenticate users to the application by using an existing AWS Directory Service for Microsoft Active Directory directory.\n- All users with accounts in the directory must have access to the application.\n\nOption B proposes configuring an Amazon Cognito user pool and using it as the authentication mechanism for the Application Load Balancer (ALB). This allows the company to leverage the existing Active Directory as a federated identity provider (IdP) for the Cognito user pool. \n\nBy configuring the Cognito user pool with the metadata from the Active Directory, the application can authenticate users against the existing directory. The creation of an app client and associating it with the user pool enables the ALB to use Cognito for user authentication.\n\nThe \"authenticate-cognito\" action for the ALB listener rule then integrates the Cognito user pool authentication with the ALB, providing a seamless authentication experience for users.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nA. This option proposes using the \"authenticate-oidc\" action for the ALB listener rule, which requires configuring an OpenID Connect (OIDC) provider. This is not the most appropriate solution, as the requirement is to use the existing Active Directory as the authentication source, not an OIDC provider.\n\nC. This option suggests adding the directory as an IAM identity provider and creating an IAM role for SAML 2.0 federation. While this could work, it is a more complex solution compared to the Cognito-based approach in option B. It also does not directly integrate the Active Directory as the authentication source for the application.\n\nD. This option involves enabling AWS IAM Identity Center (AWS SSO) and configuring the directory as an external SAML identity provider. While this could provide single sign-on access to AWS resources, it is not directly relevant to the requirement of authenticating users to the intranet web application hosted on EC2 instances behind the ALB."
  },
  "372": {
    "question": "A company has a website that serves many visitors. The company deploys a backend service for the website in a primary AWS Region and a disaster recovery (DR) Region.A single Amazon CloudFront distribution is deployed for the website. The company creates an Amazon Route 53 record set with health checks and a failover routing policy for the primary Region\u2019s backend service. The company configures the Route 53 record set as an origin for the CloudFront distribution. The company configures another record set that points to the backend service's endpoint in the DR Region as a secondary failover record type. The TTL for both record sets is 60 seconds.Currently, failover takes more than 1 minute. A solutions architect must design a solution that will provide the fastest failover time.Which solution will achieve this goal?",
    "choices": [
      "A. Deploy an additional CloudFront distribution. Create a new Route 53 failover record set with health checks for both CloudFront distributions.",
      "B. Set the TTL to 4 second for the existing Route 53 record sets that are used for the backend service in each Region.",
      "C. Create new record sets for the backend services by using a latency routing policy. Use the record sets as an origin in the CloudFront distribution.",
      "D. Create a CloudFront origin group that includes two origins, one for each backend service Region. Configure origin failover as a cache behavior for the CloudFront distribution."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (D):\n\nThe correct answer is D. Create a CloudFront origin group that includes two origins, one for each backend service Region. Configure origin failover as a cache behavior for the CloudFront distribution.\n\nThis solution achieves the fastest failover time because CloudFront's origin failover feature automatically switches to the secondary origin when the primary origin becomes unavailable. This process is much faster than the health check-based failover implemented with Route 53, which can take more than 1 minute.\n\nWhen the CloudFront distribution receives a request, it first attempts to fetch the content from the primary origin. If the primary origin is unavailable, CloudFront will immediately failover to the secondary origin, providing a faster response time for the end-user.\n\n2. Explanations of the incorrect choices:\n\nA. Deploy an additional CloudFront distribution. Create a new Route 53 failover record set with health checks for both CloudFront distributions.\nThis solution is unnecessarily complex and does not provide any significant improvement in failover time compared to the correct solution (option D). Using multiple CloudFront distributions and Route 53 failover record sets still relies on health checks, which can take more than 1 minute to detect a failure.\n\nB. Set the TTL to 4 seconds for the existing Route 53 record sets that are used for the backend service in each Region.\nReducing the TTL of the Route 53 record sets would not significantly improve the failover time, as the primary limitation is the health check-based failover mechanism, not the caching of the DNS records.\n\nC. Create new record sets for the backend services by using a latency routing policy. Use the record sets as an origin in the CloudFront distribution.\nThis solution may help distribute traffic between the two origins, but it does not address the main issue of improving the failover time. The latency routing policy does not provide the same immediate failover capabilities as CloudFront's origin failover feature."
  },
  "373": {
    "question": "A company is using multiple AWS accounts and has multiple DevOps teams running production and non-production workloads in these accounts. The company would like to centrally-restrict access to some of the AWS services that the DevOps teams do not use. The company decided to use AWS Organizations and successfully invited all AWS accounts into the Organization. They would like to allow access to services that are currently in-use and deny a few specific services. Also they would like to administer multiple accounts together as a single unit.What combination of steps should the solutions architect take to satisfy these requirements? (Choose three.)",
    "choices": [
      "A. Use a Deny list strategy.",
      "B. Review the Access Advisor in AWS IAM to determine services recently used",
      "C. Review the AWS Trusted Advisor report to determine services recently used.",
      "D. Remove the default FullAWSAccess SCP.",
      "E. Define organizational units (OUs) and place the member accounts in the OUs.",
      "F. Remove the default DenyAWSAccess SCP."
    ],
    "answer": "ABE",
    "explanation": "1. Clear explanation of the correct answer (ABE):\n\nA. Use a Deny list strategy:\nThis is the correct approach as it allows the company to explicitly deny access to specific AWS services that the DevOps teams do not use, while allowing access to all other services that are currently in use. This \"deny list\" strategy is more secure and granular compared to allowing everything by default and then trying to restrict access.\n\nB. Review the Access Advisor in AWS IAM to determine services recently used:\nThis is also a correct step. The Access Advisor provides insights into the AWS services that have been accessed by IAM users, groups, and roles. This information is crucial to identify the services that are actively used by the DevOps teams, which can then be allowed access to, while the unused services can be denied.\n\nE. Define organizational units (OUs) and place the member accounts in the OUs:\nThis is the third correct step. Organizing the member accounts into OUs within the AWS Organization allows the company to apply Service Control Policies (SCPs) at the OU level. This makes it easier to manage permissions and restrictions across multiple accounts as a single unit, as required by the company.\n\n2. Explanations of why the incorrect choices are wrong:\n\nC. Review the AWS Trusted Advisor report to determine services recently used:\nThis is incorrect because the Trusted Advisor report provides a more general assessment of the AWS environment, and does not specifically focus on the usage of individual AWS services. The Access Advisor provides more detailed and relevant information for this use case.\n\nD. Remove the default FullAWSAccess SCP:\nThis is incorrect because the FullAWSAccess SCP grants full access to all AWS services and actions. Removing it without a carefully crafted replacement policy can lead to unintended access restrictions, which is the opposite of the company's requirements to allow access to services that are currently in use.\n\nF. Remove the default DenyAWSAccess SCP:\nThis is incorrect because the DenyAWSAccess SCP denies access to all AWS services and actions. Removing it would grant full access to all services, which is not the company's goal. The company wants to selectively deny access to specific services, not grant full access."
  },
  "374": {
    "question": "A live-events company is designing a scaling solution for its ticket application on AWS. The application has high peaks of utilization during sale events. Each sale event is a one-time event that is scheduled. The application runs on Amazon EC2 instances that are in an Auto Scaling group. The application uses PostgreSQL for the database layer.The company needs a scaling solution to maximize availability during the sale events.Which solution will meet these requirements?",
    "choices": [
      "A. Use a predictive scaling policy for the EC2 instances. Host the database on an Amazon Aurora PostgreSQL Serverless v2 Multi-AZ DB instance with automatically scaling read replicas. Create an AWS Step Functions state machine to run parallel AWS Lambda functions to pre-warm the database before a sale event. Create an Amazon EventBridge rule to invoke the state machine.",
      "B. Use a scheduled scaling policy for the EC2 instances. Host the database on an Amazon RDS for PostgreSQL Mulli-AZ DB instance with automatically scaling read replicas. Create an Amazon EventBridge rule that invokes an AWS Lambda function to create a larger read replica before a sale event. Fail over to the larger read replica. Create another EventBridge rule that invokes another Lambda function to scale down the read replica after the sale event.",
      "C. Use a predictive scaling policy for the EC2 instances. Host the database on an Amazon RDS for PostgreSQL MultiAZ DB instance with automatically scaling read replicas. Create an AWS Step Functions state machine to run parallel AWS Lambda functions to pre-warm the database before a sale event. Create an Amazon EventBridge rule to invoke the state machine.",
      "D. Use a scheduled scaling policy for the EC2 instances. Host the database on an Amazon Aurora PostgreSQL Multi-AZ DB cluster. Create an Amazon EventBridge rule that invokes an AWS Lambda function to create a larger Aurora Replica before a sale event. Fail over to the larger Aurora Replica. Create another EventBridge rule that invokes another Lambda function to scale down the Aurora Replica after the sale event."
    ],
    "answer": "D",
    "explanation": "1. Explanation of why the correct answer (D) is right:\n\nThe correct answer (D) is the best solution to meet the requirements of the live-events company's ticket application. Here's why:\n\n- It uses a scheduled scaling policy for the EC2 instances, which is ideal for handling predictable, high-traffic sale events.\n- It hosts the database on an Amazon Aurora PostgreSQL Multi-AZ DB cluster, which provides high-performance, reliable, and scalable database capabilities.\n- It creates an Amazon EventBridge rule that invokes an AWS Lambda function to create a larger Aurora Replica before a sale event. This allows the database to scale up and handle the increased traffic during the sale event.\n- After the sale event, another EventBridge rule invokes a Lambda function to scale down the Aurora Replica, ensuring efficient resource utilization.\n- The use of a larger Aurora Replica during the event and scaling down afterward aligns the database capacity with the fluctuating demand.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This solution uses a predictive scaling policy for the EC2 instances, which may not be as effective as a scheduled scaling policy for the known, predictable sale events. Additionally, using Amazon Aurora PostgreSQL Serverless v2 for the database may not provide the same level of performance and reliability as the managed Amazon Aurora PostgreSQL solution.\n\nB. This solution uses a scheduled scaling policy for the EC2 instances, which is appropriate. However, it hosts the database on an Amazon RDS for PostgreSQL Multi-AZ DB instance, which may not offer the same level of performance and scalability as the Amazon Aurora PostgreSQL solution.\n\nC. This solution uses a predictive scaling policy for the EC2 instances, which may not be as effective as a scheduled scaling policy for the known, predictable sale events. Additionally, it hosts the database on an Amazon RDS for PostgreSQL Multi-AZ DB instance, which may not offer the same level of performance and scalability as the Amazon Aurora PostgreSQL solution."
  },
  "375": {
    "question": "A company runs an intranet application on premises. The company wants to configure a cloud backup of the application. The company has selected AWS Elastic Disaster Recovery for this solution.The company requires that replication traffic does not travel through the public internet. The application also must not be accessible from the internet. The company does not want this solution to consume all available network bandwidth because other applications require bandwidth.Which combination of steps will meet these requirements? (Choose three.)",
    "choices": [
      "A. Create a VPC that has at least two private subnets, two NAT gateways, and a virtual private gateway.",
      "B. Create a VPC that has at least two public subnets, a virtual private gateway, and an internet gateway.",
      "C. Create an AWS Site-to-Site VPN connection between the on-premises network and the target AWS network.",
      "D. Create an AWS Direct Connect connection and a Direct Connect gateway between the on-premises network and the target AWS network.",
      "E. During configuration of the replication servers, select the option to use private IP addresses for data replication.",
      "F. During configuration of the launch settings for the target servers, select the option to ensure that the Recovery instance\u2019s private IP address matches the source server's private IP address."
    ],
    "answer": "ADE",
    "explanation": "1. Explanation of the correct answer (ADE):\n\nA. Create a VPC that has at least two private subnets, two NAT gateways, and a virtual private gateway.\nThis option is correct because it ensures that the replication traffic does not travel through the public internet. The private subnets and the virtual private gateway allow for secure, private connectivity between the on-premises network and the AWS network.\n\nD. Create an AWS Direct Connect connection and a Direct Connect gateway between the on-premises network and the target AWS network.\nThis option is correct because it provides a dedicated, high-bandwidth, and lower-latency connection to AWS, ensuring that the replication traffic does not consume all available internet bandwidth. Direct Connect also ensures that the traffic does not traverse the public internet.\n\nE. During configuration of the replication servers, select the option to use private IP addresses for data replication.\nThis option is correct because it ensures that the replication occurs over private connectivity rather than the public internet, aligning with the security requirement.\n\n2. Explanation of why the incorrect choices are wrong:\n\nB. Create a VPC that has at least two public subnets, a virtual private gateway, and an internet gateway.\nThis option is incorrect because it would allow the application to be accessible from the internet, which goes against the requirement that the application must not be accessible from the internet.\n\nC. Create an AWS Site-to-Site VPN connection between the on-premises network and the target AWS network.\nThis option is incorrect because the site-to-site VPN connection still uses the public internet, which does not meet the requirement that the replication traffic should not travel through the public internet.\n\nF. During configuration of the launch settings for the target servers, select the option to ensure that the Recovery instance's private IP address matches the source server's private IP address.\nThis option is incorrect because it does not address the requirement of ensuring that the replication traffic does not travel through the public internet or consume all available network bandwidth."
  },
  "376": {
    "question": "A company that provides image storage services wants to deploy a customer-facing solution to AWS. Millions of individual customers will use the solution. The solution will receive batches of large image files, resize the files, and store the files in an Amazon S3 bucket for up to 6 months.The solution must handle significant variance in demand. The solution must also be reliable at enterprise scale and have the ability to rerun processing jobs in the event of failure.Which solution will meet these requirements MOST cost-effectively?",
    "choices": [
      "A. Use AWS Step Functions to process the S3 event that occurs when a user stores an image. Run an AWS Lambda function that resizes the image in place and replaces the original file in the S3 bucket. Create an S3 Lifecycle expiration policy to expire all stored images after 6 months.",
      "B. Use Amazon EventBridge to process the S3 event that occurs when a user uploads an image. Run an AWS Lambda function that resizes the image in place and replaces the original file in the S3 bucket. Create an S3 Lifecycle expiration policy to expire all stored images after 6 months.",
      "C. Use S3 Event Notifications to invoke an AWS Lambda function when a user stores an image. Use the Lambda function to resize the image in place and to store the original file in the S3 bucket. Create an S3 Lifecycle policy to move all stored images to S3 Standard-Infrequent Access (S3 Standard-IA) after 6 months.",
      "D. Use Amazon Simple Queue Service (Amazon SQS) to process the S3 event that occurs when a user stores an image. Run an AWS Lambda function that resizes the image and stores the resized file in an S3 bucket that uses S3 Standard-Infrequent Access (S3 Standard-IA). Create an S3 Lifecycle policy to move all stored images to S3 Glacier Deep Archive after 6 months."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (D):\n\nThe correct answer is option D, which uses Amazon Simple Queue Service (Amazon SQS) to process the S3 event that occurs when a user stores an image. This solution is the most cost-effective for the given requirements:\n\n- Amazon SQS provides a reliable and scalable queue service, which can handle the significant variance in demand and ensure reliable processing of the large image files at enterprise scale.\n- By using AWS Lambda to resize the images and store the resized files in an S3 bucket that uses S3 Standard-Infrequent Access (S3 Standard-IA), the solution can effectively manage the cost of storage for the processed images.\n- The S3 Lifecycle policy to move all stored images to S3 Glacier Deep Archive after 6 months aligns with the requirement to store the images for up to 6 months, further optimizing costs.\n- The use of SQS allows for the ability to rerun processing jobs in the event of failure, ensuring the reliability of the solution.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option uses AWS Step Functions, which is not a supported event destination for S3 events. Additionally, resizing the images in place and replacing the original files may not be the most cost-effective approach, as it does not leverage colder storage tiers for long-term storage.\n\nB. This option uses Amazon EventBridge, which is a good choice, but it does not provide the ability to rerun processing jobs in the event of failure, as the SQS solution in option D does.\n\nC. This option uses S3 Event Notifications to invoke a Lambda function, which is a valid approach. However, the S3 Lifecycle policy to move the images to S3 Standard-Infrequent Access (S3 Standard-IA) after 6 months does not align with the requirement to store the images for up to 6 months. The solution in option D is more cost-effective by using S3 Glacier Deep Archive for long-term storage."
  },
  "377": {
    "question": "A company has an organization in AWS Organizations that includes a separate AWS account for each of the company\u2019s departments. Application teams from different departments develop and deploy solutions independently.The company wants to reduce compute costs and manage costs appropriately across departments. The company also wants to improve visibility into billing for individual departments. The company does not want to lose operational flexibility when the company selects compute resources.Which solution will meet these requirements?",
    "choices": [
      "A. Use AWS Budgets for each department. Use Tag Editor to apply tags to appropriate resources. Purchase EC2 Instance Savings Plans.",
      "B. Configure AWS Organizations to use consolidated billing. Implement a tagging strategy that identifies departments. Use SCPs to apply tags to appropriate resources. Purchase EC2 Instance Savings Plans.",
      "C. Configure AWS Organizations to use consolidated billing. Implement a tagging strategy that identifies departments. Use Tag Editor to apply tags to appropriate resources. Purchase Compute Savings Plans.",
      "D. Use AWS Budgets for each department. Use SCPs to apply tags to appropriate resources. Purchase Compute Savings Plans."
    ],
    "answer": "C",
    "explanation": "1. Clear explanation of why the correct answer (C) is right:\n\nThe correct answer is C because it addresses all the key requirements mentioned in the question:\n\n- Configure AWS Organizations to use consolidated billing: This provides a centralized view and management of costs across the different departments, improving visibility into billing for individual departments.\n- Implement a tagging strategy that identifies departments: This enables accurate cost allocation and tracking at the department level.\n- Use Tag Editor to apply tags to appropriate resources: Tag Editor simplifies the process of applying the necessary tags to resources, ensuring consistent tagging across the organization.\n- Purchase Compute Savings Plans: Compute Savings Plans offer cost savings on a broader range of compute services (EC2, Fargate, Lambda) compared to EC2 Instance Savings Plans, providing flexibility in compute resource selection.\n\n2. Brief explanations of why the incorrect choices are wrong:\n\nA. This option lacks the consolidated billing feature provided by AWS Organizations, limiting cost visibility and potential discounts across departments.\n\nB. SCPs (Service Control Policies) in AWS Organizations are primarily used for compliance enforcement, not for applying tags to resources. The question specifically mentions using tags to improve visibility into billing, which is not addressed by this option.\n\nD. This option misses the benefits of consolidated billing through AWS Organizations, which is crucial for improving cost visibility and management across the different departments."
  },
  "378": {
    "question": "A company has a web application that securely uploads pictures and videos to an Amazon S3 bucket. The company requires that only authenticated users are allowed to post content. The application generates a presigned URL that is used to upload objects through a browser interface. Most users are reporting slow upload times for objects larger than 100 MB.What can a solutions architect do to improve the performance of these uploads while ensuring only authenticated users are allowed to post content?",
    "choices": [
      "A. Set up an Amazon API Gateway with an edge-optimized API endpoint that has a resource as an S3 service proxy. Configure the PUT method for this resource to expose the S3 PutObject operation. Secure the API Gateway using a COGNITO_USER_POOLS authorizer. Have the browser interface use API Gateway instead of the presigned URL to upload objects.",
      "B. Set up an Amazon API Gateway with a regional API endpoint that has a resource as an S3 service proxy. Configure the PUT method for this resource to expose the S3 PutObject operation. Secure the API Gateway using an AWS Lambda authorizer. Have the browser interface use API Gateway instead of the presigned URL to upload objects.",
      "C. Enable an S3 Transfer Acceleration endpoint on the S3 bucket. Use the endpoint when generating the presigned URL. Have the browser interface upload the objects to this URL using the S3 multipart upload API.",
      "D. Configure an Amazon CloudFront distribution for the destination S3 bucket. Enable PUT and POST methods for the CloudFront cache behavior. Update the CloudFront origin to use an origin access identity (OAI). Give the OAI user 3: PutObject permissions in the bucket policy. Have the browser interface upload objects using the CloudFront distribution."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C, which recommends enabling S3 Transfer Acceleration and using the accelerated endpoint when generating the presigned URL. This is the best solution to address the performance issues for uploading large files (over 100 MB) while maintaining secure access for authenticated users.\n\nS3 Transfer Acceleration leverages Amazon's global network and edge locations to optimize the data transfer speed for uploads to S3. By using the accelerated endpoint in the presigned URL, the browser interface can take advantage of this feature, resulting in significantly faster upload times for large objects.\n\nAdditionally, the use of presigned URLs ensures that only authenticated users can upload content to the S3 bucket, as the presigned URL is generated on the server-side and grants temporary access to the S3 bucket.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option suggests using an Amazon API Gateway with an edge-optimized API endpoint as a proxy for S3. While this approach can provide authentication and authorization, the API Gateway has a payload size limit of 10 MB, which is not suitable for handling large files (over 100 MB).\n\nB. Similar to option A, this option also uses an Amazon API Gateway, but with a regional API endpoint. Again, the API Gateway's payload size limit of 10 MB makes it unsuitable for handling large file uploads.\n\nD. This option proposes using an Amazon CloudFront distribution to handle the uploads. While CloudFront can support PUT and POST methods, it is primarily designed for content distribution and caching, not for optimizing large file uploads. CloudFront does not provide the same level of acceleration as S3 Transfer Acceleration, which is specifically designed to improve upload performance for large objects."
  },
  "379": {
    "question": "A company has multiple AWS accounts. The company recently had a security audit that revealed many unencrypted Amazon Elastic Block Store (Amazon EBS) volumes attached to Amazon EC2 instances.A solutions architect must encrypt the unencrypted volumes and ensure that unencrypted volumes will be detected automatically in the future. Additionally, the company wants a solution that can centrally manage multiple AWS accounts with a focus on compliance and security.Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
    "choices": [
      "A. Create an organization in AWS Organizations. Set up AWS Control Tower, and turn on the strongly recommended controls (guardrails). Join all accounts to the organization. Categorize the AWS accounts into OUs.",
      "B. Use the AWS CLI to list all the unencrypted volumes in all the AWS accounts. Run a script to encrypt all the unencrypted volumes in place.",
      "C. Create a snapshot of each unencrypted volume. Create a new encrypted volume from the unencrypted snapshot. Detach the existing volume, and replace it with the encrypted volume.",
      "D. Create an organization in AWS Organizations. Set up AWS Control Tower, and turn on the mandatory controls (guardrails). Join all accounts to the organization. Categorize the AWS accounts into OUs.",
      "E. Turn on AWS CloudTrail. Configure an Amazon EventBridge rule to detect and automatically encrypt unencrypted volumes."
    ],
    "answer": "AC",
    "explanation": "1. Explanation of the correct answer (AC):\n\nA: This is the correct choice because setting up an AWS Organization with AWS Control Tower allows the solutions architect to centrally manage multiple AWS accounts, enforce compliance, and apply security best practices across the organization. The \"strongly recommended controls\" in AWS Control Tower include a guardrail to detect whether Amazon EBS volumes attached to Amazon EC2 instances are encrypted, which aligns with the requirement to automatically detect unencrypted volumes.\n\nC: This is the correct choice because it outlines the best approach to encrypt the existing unencrypted EBS volumes. By creating snapshots of the unencrypted volumes, creating new encrypted volumes from the snapshots, and then replacing the original unencrypted volumes, the architect can efficiently encrypt all the volumes without data loss.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB: This choice is incorrect because using the AWS CLI to list and encrypt all the unencrypted volumes across multiple accounts is not a scalable or centralized solution. It would require the architect to manually run scripts in each account, which does not meet the requirement for a centralized management solution.\n\nD: This choice is incorrect because it mentions turning on the \"mandatory controls\" in AWS Control Tower, which are less comprehensive than the \"strongly recommended controls\" mentioned in the correct choice (A). The strongly recommended controls include the necessary guardrail to detect unencrypted EBS volumes.\n\nE: This choice is incorrect because it only addresses the detection of unencrypted volumes, but does not provide a solution for centrally managing multiple AWS accounts or encrypting the existing unencrypted volumes. While using Amazon EventBridge to detect unencrypted volumes could be a useful additional step, it is not a complete solution on its own."
  },
  "380": {
    "question": "A company has a solution that analyzes weather data from thousands of weather stations. The weather stations send the data over an Amazon API Gateway REST API that has an AWS Lambda function integration. The Lambda function calls a third-party service for data pre-processing. The third-party service gets overloaded and fails the pre-processing, causing a loss of data.A solutions architect must improve the resiliency of the solution. The solutions architect must ensure that no data is lost and that data can be processed later if failures occur.What should the solutions architect do to meet these requirements?",
    "choices": [
      "A. Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure the queue as the dead-letter queue for the API.",
      "B. Create two Amazon Simple Queue Service (Amazon SQS) queues: a primary queue and a secondary queue. Configure the secondary queue as the dead-letter queue for the primary queue. Update the API to use a new integration to the primary queue. Configure the Lambda function as the invocation target for the primary queue.",
      "C. Create two Amazon EventBridge event buses: a primary event bus and a secondary event bus. Update the API to use a new integration to the primary event bus. Configure an EventBridge rule to react to all events on the primary event bus. Specify the Lambda function as the target of the rule. Configure the secondary event bus as the failure destination for the Lambda function.",
      "D. Create a custom Amazon EventBridge event bus. Configure the event bus as the failure destination for the Lambda function."
    ],
    "answer": "B",
    "explanation": "1. Explanation of why the correct answer (B) is right:\n\nThe correct answer is B because it uses two Amazon SQS queues to ensure that no data is lost and that data can be processed later in case of failures. The primary queue acts as the initial landing point for data from the API Gateway, and the secondary queue serves as a dead-letter queue, capturing data that could not be processed due to third-party service failures or other issues. This setup maintains data integrity and allows for later processing, effectively improving the solution's resiliency.\n\n2. Explanations of why each incorrect choice is wrong:\n\nA. This option uses a single Amazon SQS queue as the dead-letter queue for the API Gateway. While this can help capture failed messages, it does not provide a primary queue for successful messages, which is a key requirement to ensure no data is lost.\n\nC. This option uses Amazon EventBridge, which is not the most suitable choice for the given scenario. API Gateway is designed to work with SQS queues, and using EventBridge as the target would require additional integration and processing steps, which could add complexity and reduce the overall resilience of the solution.\n\nD. Similar to option C, this option uses a custom Amazon EventBridge event bus as the failure destination for the Lambda function. Again, this is not the most suitable choice, as SQS queues are better suited for the requirements of maintaining data integrity and allowing for later processing."
  },
  "381": {
    "question": "A company built an ecommerce website on AWS using a three-tier web architecture. The application is Java-based and composed of an Amazon CloudFront distribution, an Apache web server layer of Amazon EC2 instances in an Auto Scaling group, and a backend Amazon Aurora MySQL database.Last month, during a promotional sales event, users reported errors and timeouts while adding items to their shopping carts. The operations team recovered the logs created by the web servers and reviewed Aurora DB cluster performance metrics. Some of the web servers were terminated before logs could be collected and the Aurora metrics were not sufficient for query performance analysis.Which combination of steps must the solutions architect take to improve application performance visibility during peak traffic events? (Choose three.)",
    "choices": [
      "A. Configure the Aurora MySQL DB cluster to publish slow query and error logs to Amazon CloudWatch Logs.",
      "B. Implement the AWS X-Ray SDK to trace incoming HTTP requests on the EC2 instances and implement tracing of SQL queries with the X-Ray SDK for Java.",
      "C. Configure the Aurora MySQL DB cluster to stream slow query and error logs to Amazon Kinesis.",
      "D. Install and configure an Amazon CloudWatch Logs agent on the EC2 instances to send the Apache logs to CloudWatch Logs.",
      "E. Enable and configure AWS CloudTrail to collect and analyze application activity from Amazon EC2 and Aurora",
      "F. Enable Aurora MySQL DB cluster performance benchmarking and publish the stream to AWS X-Ray."
    ],
    "answer": "ABD",
    "explanation": "1. Explanation of the correct answer (ABD):\n\nA. Configure the Aurora MySQL DB cluster to publish slow query and error logs to Amazon CloudWatch Logs:\nThis is the correct choice because it allows the operations team to analyze the database performance issues by accessing the slow query and error logs from CloudWatch Logs. This helps identify any problematic queries that might be contributing to the application's performance problems during peak traffic events.\n\nB. Implement the AWS X-Ray SDK to trace incoming HTTP requests on the EC2 instances and implement tracing of SQL queries with the X-Ray SDK for Java:\nThis is the correct choice because it enables the operations team to trace the incoming HTTP requests on the EC2 instances and the corresponding SQL queries executed by the application. This provides a comprehensive view of the application's performance, including the web server layer and the database layer, which is essential for diagnosing and resolving the performance bottlenecks.\n\nD. Install and configure an Amazon CloudWatch Logs agent on the EC2 instances to send the Apache logs to CloudWatch Logs:\nThis is the correct choice because it allows the operations team to collect and analyze the Apache logs from the EC2 instances. The Apache logs can provide valuable information about the web server's performance, such as error messages, request latency, and other relevant metrics that can help identify issues during peak traffic events.\n\n2. Explanations of why the incorrect choices are wrong:\n\nC. Configure the Aurora MySQL DB cluster to stream slow query and error logs to Amazon Kinesis:\nThis is incorrect because Kinesis is used for real-time data streaming, while the requirement is to analyze the slow query and error logs for the application's performance issues. Streaming the logs to Kinesis may not provide the same level of analysis and visibility as publishing them to CloudWatch Logs.\n\nE. Enable and configure AWS CloudTrail to collect and analyze application activity from Amazon EC2 and Aurora:\nThis is incorrect because CloudTrail is primarily used for logging and monitoring AWS API calls, which is not directly relevant to the performance issues faced by the application. While CloudTrail can provide some information about the application's activities, it does not offer the same level of visibility and analysis as the combination of CloudWatch Logs, X-Ray, and Apache logs.\n\nF. Enable Aurora MySQL DB cluster performance"
  },
  "382": {
    "question": "A company that provisions job boards for a seasonal workforce is seeing an increase in traffic and usage. The backend services run on a pair of Amazon EC2 instances behind an Application Load Balancer with Amazon DynamoDB as the datastore. Application read and write traffic is slow during peak seasons.Which option provides a scalable application architecture to handle peak seasons with the LEAST development effort?",
    "choices": [
      "A. Migrate the backend services to AWS Lambda. Increase the read and write capacity of DynamoDB.",
      "B. Migrate the backend services to AWS Lambda. Configure DynamoDB to use global tables.",
      "C. Use Auto Scaling groups for the backend services. Use DynamoDB auto scaling.",
      "D. Use Auto Scaling groups for the backend services. Use Amazon Simple Queue Service (Amazon SQS) and an AWS Lambda function to write to DynamoDB."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C, \"Use Auto Scaling groups for the backend services. Use DynamoDB auto scaling.\"\n\nThis option provides the most scalable application architecture to handle peak seasons with the least development effort. By using Auto Scaling groups, the backend services can automatically scale up or down based on the demand, ensuring the application can handle increased traffic during peak seasons. Additionally, configuring DynamoDB to use auto scaling allows the database to scale its read and write capacity dynamically, which can improve performance during periods of high traffic. This approach requires the least development effort compared to the other options, as it leverages AWS managed services to handle the scaling and provisioning of resources.\n\n2. Explanations of the incorrect answers:\n\nA. \"Migrate the backend services to AWS Lambda. Increase the read and write capacity of DynamoDB.\"\nThis option would require more development effort to migrate the backend services to AWS Lambda, which may not be necessary if the current EC2-based architecture can be scaled effectively. Additionally, simply increasing the read and write capacity of DynamoDB may not be the most scalable solution, as it requires manual intervention to adjust the capacity as needed.\n\nB. \"Migrate the backend services to AWS Lambda. Configure DynamoDB to use global tables.\"\nSimilar to option A, migrating the backend services to AWS Lambda would require more development effort. While configuring DynamoDB to use global tables can provide additional scalability and redundancy, it may not be the most appropriate solution if the main concern is handling peak season traffic, which can be addressed more effectively using Auto Scaling groups and DynamoDB auto scaling.\n\nD. \"Use Auto Scaling groups for the backend services. Use Amazon Simple Queue Service (Amazon SQS) and an AWS Lambda function to write to DynamoDB.\"\nThis option introduces additional complexity by using SQS and a Lambda function to write to DynamoDB. While this approach can provide some benefits, such as decoupling the writing process, it may not be the most efficient solution for the given scenario, which focuses on handling peak season traffic. The use of Auto Scaling groups and DynamoDB auto scaling in option C is a more straightforward and effective solution."
  },
  "383": {
    "question": "A company is using AWS Organizations to manage multiple AWS accounts. For security purposes, the company requires the creation of an Amazon Simple Notification Service (Amazon SNS) topic that enables integration with a third-party alerting system in all the Organizations member accounts.A solutions architect used an AWS CloudFormation template to create the SNS topic and stack sets to automate the deployment of CloudFormation stacks. Trusted access has been enabled in Organizations.What should the solutions architect do to deploy the CloudFormation StackSets in all AWS accounts?",
    "choices": [
      "A. Create a stack set in the Organizations member accounts. Use service-managed permissions. Set deployment options to deploy to an organization. Use CloudFormation StackSets drift detection.",
      "B. Create stacks in the Organizations member accounts. Use self-service permissions. Set deployment options to deploy to an organization. Enable the CloudFormation StackSets automatic deployment.",
      "C. Create a stack set in the Organizations management account. Use service-managed permissions. Set deployment options to deploy to the organization. Enable CloudFormation StackSets automatic deployment.",
      "D. Create stacks in the Organizations management account. Use service-managed permissions. Set deployment options to deploy to the organization. Enable CloudFormation StackSets drift detection."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\nThe correct answer is C because it aligns best with the requirements and the capabilities of AWS Organizations and CloudFormation StackSets.\n\nThe key points are:\n- The solution architect needs to deploy the SNS topic across all AWS accounts managed by AWS Organizations.\n- Using CloudFormation StackSets is the recommended approach to achieve this in an automated and centralized manner.\n- Creating the stack set in the Organizations management account allows the deployment to be managed from a single location.\n- Choosing \"service-managed permissions\" enables the CloudFormation service to manage the necessary permissions to deploy the stack set across all member accounts.\n- Setting the deployment options to \"deploy to the organization\" ensures the stack set is deployed to all member accounts.\n- Enabling the \"CloudFormation StackSets automatic deployment\" feature makes the deployment fully automated, so any new member accounts will automatically receive the stack set.\n\n2. Explanations of the incorrect choices:\n\nA. This option is incorrect because it suggests creating the stack set in the member accounts, rather than the management account. This would require managing the stack set deployment in multiple locations, which is not the most efficient approach.\n\nB. This option is incorrect because it suggests using \"self-service permissions\" rather than \"service-managed permissions\". Self-service permissions require the solutions architect to manually manage the necessary permissions, which goes against the requirement of a centralized and automated deployment.\n\nD. This option is incorrect because it suggests creating individual stacks in the management account, rather than a single stack set. This would require managing multiple stacks, which is less efficient than a single stack set deployment. Additionally, enabling \"CloudFormation StackSets drift detection\" is not a requirement in this case, and should not be a primary focus."
  },
  "384": {
    "question": "A company provides a software as a service (SaaS) application that runs in the AWS Cloud. The application runs on Amazon EC2 instances behind a Network Load Balancer (NLB). The instances are in an Auto Scaling group and are distributed across three Availability Zones in a single AWS Region.The company is deploying the application into additional Regions. The company must provide static IP addresses for the application to customers so that the customers can add the IP addresses to allow lists. The solution must automatically route customers to the Region that is geographically closest to them.Which solution will meet these requirements?",
    "choices": [
      "A. Create an Amazon CloudFront distribution. Create a CloudFront origin group. Add the NLB for each additional Region to the origin group. Provide customers with the IP address ranges of the distribution\u2019s edge locations.",
      "B. Create an AWS Global Accelerator standard accelerator. Create a standard accelerator endpoint for the NLB in each additional Region. Provide customers with the Global Accelerator IP address.",
      "C. Create an Amazon CloudFront distribution. Create a custom origin for the NLB in each additional Region. Provide customers with the IP address ranges of the distribution\u2019s edge locations.",
      "D. Create an AWS Global Accelerator custom routing accelerator. Create a listener for the custom routing accelerator. Add the IP address and ports for the NLB in each additional Region. Provide customers with the Global Accelerator IP address."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B, which is to create an AWS Global Accelerator standard accelerator. This solution meets the requirements in the following ways:\n\n- Global Accelerator provides static IP addresses that can be shared with customers for allow listing, as required.\n- The standard accelerator automatically routes traffic to the nearest healthy endpoint (NLB) in the closest AWS Region, providing the geographically closest experience for customers, as required.\n- Global Accelerator supports Network Load Balancers (NLBs) as endpoints, which is the load balancing mechanism used in the existing architecture.\n\n2. Explanations of the incorrect choices:\n\nA. This choice is incorrect because CloudFront does not work with Network Load Balancers (NLBs), and it cannot provide static IP addresses for customers. CloudFront edge locations do not have fixed IP addresses, so providing a list of IP address ranges would not meet the requirements.\n\nC. This choice is incorrect for the same reasons as A. CloudFront cannot work with NLBs, and it cannot provide static IP addresses for customers.\n\nD. This choice is incorrect because a custom routing accelerator in Global Accelerator does not automatically route traffic to the nearest healthy endpoint. Instead, it requires the client to specify the endpoint and does not provide the geographically closest experience for customers."
  },
  "385": {
    "question": "A company is running multiple workloads in the AWS Cloud. The company has separate units for software development. The company uses AWS Organizations and federation with SAML to give permissions to developers to manage resources in their AWS accounts. The development units each deploy their production workloads into a common production account.Recently, an incident occurred in the production account in which members of a development unit terminated an EC2 instance that belonged to a different development unit. A solutions architect must create a solution that prevents a similar incident from happening in the future. The solution also must allow developers the possibility to manage the instances used for their workloads.Which strategy will meet these requirements?",
    "choices": [
      "A. Create separate OUs in AWS Organizations for each development unit. Assign the created OUs to the company AWS accounts. Create separate SCP with a deny action and a StringNotEquals condition for the DevelopmentUnit resource tag that matches the development unit name. Assign the SCP to the corresponding OU.",
      "B. Pass an attribute for DevelopmentUnit as an AWS Security Token Service (AWS STS) session tag during SAML federation. Update the IAM policy for the developers\u2019 assumed IAM role with a deny action and a StringNotEquals condition for the DevelopmentUnit resource tag and aws:PrincipalTag/DevelopmentUnit.",
      "C. Pass an attribute for DevelopmentUnit as an AWS Security Token Service (AWS STS) session tag during SAML federation. Create an SCP with an allow action and a StringEquals condition for the DevelopmentUnit resource tag and aws:PrincipalTag/DevelopmentUnit. Assign the SCP to the root OU.",
      "D. Create separate IAM policies for each development unit. For every IAM policy, add an allow action and a StringEquals condition for the DevelopmentUnit resource tag and the development unit name. During SAML federation, use AWS Security Token Service (AWS STS) to assign the IAM policy and match the development unit name to the assumed IAM role."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B because it provides the best strategy to prevent developers from managing resources in another development unit's account, while still allowing them to manage their own instances. \n\nBy passing an attribute for \"DevelopmentUnit\" as an AWS Security Token Service (AWS STS) session tag during SAML federation, the IAM policies assigned to the developers' assumed IAM roles can be filtered based on their own development unit name. This ensures that developers can only manage resources within their own development unit's resources.\n\nThe IAM policy is updated with a deny action and a StringNotEquals condition for the \"DevelopmentUnit\" resource tag and \"aws:PrincipalTag/DevelopmentUnit\". This effectively prevents developers from managing resources in another development unit's account, as their assumed IAM role will not allow them to do so.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This strategy of creating separate OUs and SCPs in AWS Organizations is not optimal, as it is more suitable for broader permission boundaries and might not offer the same granularity as STS session tags and IAM policies.\n\nC. This strategy of creating an SCP with an allow action and a StringEquals condition for the \"DevelopmentUnit\" resource tag and \"aws:PrincipalTag/DevelopmentUnit\" is not sufficient, as it does not actively prevent developers from managing resources in another development unit's account. It only allows access to resources with a matching \"DevelopmentUnit\" tag.\n\nD. This strategy of creating separate IAM policies for each development unit and using AWS STS to assign the IAM policy during SAML federation is not as effective as the correct answer (B). It does not provide the same level of granularity and control as using STS session tags and a deny action in the IAM policy."
  },
  "386": {
    "question": "A company provides a software as a service (SaaS) application that runs in the AWS Cloud. The application runs on Amazon EC2 instances behind a Network Load Balancer (NLB). The instances are in an Auto Scaling group and are distributed across three Availability Zones in a single AWS Region.The company is deploying the application into additional Regions. The company must provide static IP addresses for the application to customers so that the customers can add the IP addresses to allow lists. The solution must automatically route customers to the Region that is geographically closest to them.Which solution will meet these requirements?",
    "choices": [
      "A. Create an Amazon CloudFront distribution. Create a CloudFront origin group. Add the NLB for each additional Region to the origin group. Provide customers with the IP address ranges of the distribution\u2019s edge locations.",
      "B. Create an AWS Global Accelerator standard accelerator. Create a standard accelerator endpoint for the NLB in each additional Region. Provide customers with the Global Accelerator IP address.",
      "C. Create an Amazon CloudFront distribution. Create a custom origin for the NLB in each additional Region. Provide customers with the IP address ranges of the distribution\u2019s edge locations.",
      "D. Create an AWS Global Accelerator custom routing accelerator. Create a listener for the custom routing accelerator. Add the IP address and ports for the NLB in each additional Region. Provide customers with the Global Accelerator IP address."
    ],
    "answer": "B",
    "explanation": "Explanation of the correct answer:\n\nThe correct answer is B. Creating an AWS Global Accelerator standard accelerator is the best solution to meet the given requirements.\n\n1. The key requirements are:\n   - Provide static IP addresses for the application to customers\n   - Automatically route customers to the AWS Region that is geographically closest to them\n\n2. A standard accelerator in AWS Global Accelerator automatically routes traffic to the nearest healthy endpoint, which aligns with the requirement of routing customers to the closest Region. Global Accelerator also provides a static IP address that customers can use, satisfying the first requirement.\n\nExplanation of why the incorrect answers are wrong:\n\nA. Creating an Amazon CloudFront distribution is not the best solution because CloudFront does not support Network Load Balancers (NLBs) as origins, and it does not provide static IP addresses that customers can use.\n\nC. Similarly, creating a custom CloudFront origin for the NLBs in each Region is not a suitable solution, as CloudFront does not support NLBs, and the IP address ranges of the CloudFront edge locations would change over time, making it difficult to maintain a static IP address for customers.\n\nD. Creating an AWS Global Accelerator custom routing accelerator is not the best solution here, as custom routing accelerators do not automatically route traffic to the nearest healthy endpoint. Instead, they require you to manage the routing rules manually, which does not align with the requirement of automatically routing customers to the closest Region.\n\nIn summary, the correct answer is B because an AWS Global Accelerator standard accelerator can automatically route traffic to the nearest healthy endpoint (the NLB in the closest Region) and provide a static IP address that customers can use, meeting all the stated requirements."
  },
  "387": {
    "question": "A company deploys a new web application. As part of the setup, the company configures AWS WAF to log to Amazon S3 through Amazon Kinesis Data Firehose. The company develops an Amazon Athena query that runs once daily to return AWS WAF log data from the previous 24 hours. The volume of daily logs is constant. However, over time, the same query is taking more time to run.A solutions architect needs to design a solution to prevent the query time from continuing to increase. The solution must minimize operational overhead.Which solution will meet these requirements?",
    "choices": [
      "A. Create an AWS Lambda function that consolidates each day's AWS WAF logs into one log file.",
      "B. Reduce the amount of data scanned by configuring AWS WAF to send logs to a different S3 bucket each day.",
      "C. Update the Kinesis Data Firehose configuration to partition the data in Amazon S3 by date and time. Create external tables for Amazon Redshift. Configure Amazon Redshift Spectrum to query the data source.",
      "D. Modify the Kinesis Data Firehose configuration and Athena table definition to partition the data by date and time. Change the Athena query to view the relevant partitions."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (D):\n\nThe correct answer is D, \"Modify the Kinesis Data Firehose configuration and Athena table definition to partition the data by date and time. Change the Athena query to view the relevant partitions.\"\n\nThis is the best solution because:\n\n- Partitioning the data in Amazon S3 by date and time will allow Athena to only query the relevant partitions, reducing the amount of data scanned and improving query performance.\n- Configuring the Kinesis Data Firehose to partition the data as it is ingested will ensure the data is organized correctly in S3, without requiring any additional data processing steps.\n- Modifying the Athena table definition to reflect the partitioning scheme and updating the query to target the relevant partitions will complete the optimization.\n- This solution minimizes operational overhead by automating the partitioning process and requiring only configuration changes, rather than developing additional custom code (like a Lambda function).\n\n2. Explanations of why the other choices are incorrect:\n\nA. \"Create an AWS Lambda function that consolidates each day's AWS WAF logs into one log file.\"\n- This solution requires additional operational overhead of managing and maintaining a Lambda function, which is not desired.\n- Consolidating the logs into a single file may not provide the same performance benefits as partitioning the data.\n\nB. \"Reduce the amount of data scanned by configuring AWS WAF to send logs to a different S3 bucket each day.\"\n- This solution does not actually reduce the overall data volume or improve query performance. It simply moves the data to a different location, which does not solve the core issue.\n- Maintaining multiple S3 buckets and coordinating the Athena queries across them would introduce additional operational complexity.\n\nC. \"Update the Kinesis Data Firehose configuration to partition the data in Amazon S3 by date and time. Create external tables for Amazon Redshift. Configure Amazon Redshift Spectrum to query the data source.\"\n- This solution introduces additional complexity by bringing in Amazon Redshift, which is not necessary to solve the given problem.\n- Configuring Redshift Spectrum to query the partitioned data in S3 may provide some performance benefits, but it also"
  },
  "388": {
    "question": "A company is developing a web application that runs on Amazon EC2 instances in an Auto Scaling group behind a public-facing Application Load Balancer (ALB). Only users from a specific country are allowed to access the application. The company needs the ability to log the access requests that have been blocked. The solution should require the least possible maintenance.Which solution meets these requirements?",
    "choices": [
      "A. Create an IPSet containing a list of IP ranges that belong to the specified country. Create an AWS WAF web ACL. Configure a rule to block any requests that do not originate from an IP range in the IPSet. Associate the rule with the web ACL. Associate the web ACL with the ALB.",
      "B. Create an AWS WAF web ACL. Configure a rule to block any requests that do not originate from the specified country. Associate the rule with the web ACL. Associate the web ACL with the ALB.",
      "C. Configure AWS Shield to block any requests that do not originate from the specified country. Associate AWS Shield with the ALB.",
      "D. Create a security group rule that allows ports 80 and 443 from IP ranges that belong to the specified country. Associate the security group with the ALB."
    ],
    "answer": "B",
    "explanation": "Explanation:\n\n1. Correct Answer: B\nThe correct answer is B because it meets all the requirements stated in the question:\n- It uses AWS WAF (Web Application Firewall) to block requests from countries other than the specified one.\n- AWS WAF has built-in geo-matching capabilities, allowing you to easily configure a rule to block requests based on the country of origin. This eliminates the need to manually manage IP ranges, as required in option A.\n- AWS WAF automatically logs the access requests that have been blocked, meeting the requirement for logging blocked requests.\n- This solution requires the least possible maintenance, as AWS WAF handles the updates to the country-based rules automatically.\n\n2. Explanations for incorrect choices:\n\nA. This solution is incorrect because it requires more maintenance than the correct answer (option B). While it does provide logging of blocked requests, it requires the manual management of IP ranges in an IPSet, which can be more complex and error-prone than the geo-matching capabilities of AWS WAF.\n\nC. This solution is incorrect because AWS Shield is primarily designed to defend against DDoS attacks and does not offer the granular geo-blocking capabilities required in this scenario. AWS Shield does not provide the level of control and logging needed to meet the requirements.\n\nD. This solution is incorrect because it uses security group rules, which are not the best approach for country-based filtering. Security groups are designed to control network-level access, whereas the requirement here is to filter based on the country of origin, which is better handled by a solution like AWS WAF."
  },
  "389": {
    "question": "A company is migrating an application from on-premises infrastructure to the AWS Cloud. During migration design meetings, the company expressed concerns about the availability and recovery options for its legacy Windows file server. The file server contains sensitive business-critical data that cannot be recreated in the event of data corruption or data loss. According to compliance requirements, the data must not travel across the public internet. The company wants to move to AWS managed services where possible.The company decides to store the data in an Amazon FSx for Windows File Server file system. A solutions architect must design a solution that copies the data to another AWS Region for disaster recovery (DR) purposes.Which solution will meet these requirements?",
    "choices": [
      "A. Create a destination Amazon S3 bucket in the DR Region. Establish connectivity between the FSx for Windows File Server file system in the primary Region and the S3 bucket in the DR Region by using Amazon FSx File Gateway. Configure the S3 bucket as a continuous backup source in FSx File Gateway.",
      "B. Create an FSx for Windows File Server file system in the DR Region. Establish connectivity between the VPC the primary Region and the VPC in the DR Region by using AWS Site-to-Site VPN. Configure AWS DataSync to communicate by using VPN endpoints.",
      "C. Create an FSx for Windows File Server file system in the DR Region. Establish connectivity between the VPC in the primary Region and the VPC in the DR Region by using VPC peering. Configure AWS DataSync to communicate by using interface VPC endpoints with AWS PrivateLink.",
      "D. Create an FSx for Windows File Server file system in the DR Region. Establish connectivity between the VPC in the primary Region and the VPC in the DR Region by using AWS Transit Gateway in each Region. Use AWS Transfer Family to copy files between the FSx for Windows File Server file system in the primary Region and the FSx for Windows File Server file system in the DR Region over the private AWS backbone network."
    ],
    "answer": "C",
    "explanation": "1. Clear explanation of why the correct answer (C) is right:\n\nThe correct answer is C because it best meets all the given requirements:\n\n- It creates an FSx for Windows File Server file system in the DR Region, which allows for disaster recovery in another AWS Region.\n- It establishes connectivity between the VPC in the primary Region and the VPC in the DR Region using VPC peering, which allows for private communication between the two VPCs without traversing the public internet.\n- It configures AWS DataSync to communicate between the two FSx file systems using interface VPC endpoints with AWS PrivateLink, which ensures the data transfer happens over the private AWS network and not the public internet.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nA. This option uses Amazon S3 as the DR solution, which would require the data to travel across the public internet, violating the requirement that the data must not traverse the public internet.\n\nB. This option uses AWS Site-to-Site VPN to connect the VPCs, which is not necessary when the VPCs are in the same AWS account and different regions. VPC peering is a more suitable and cost-effective option.\n\nD. This option uses AWS Transit Gateway, which is not required for a simple VPC-to-VPC connection. VPC peering is a more straightforward and cost-effective solution to connect the two VPCs."
  },
  "390": {
    "question": "A company is currently in the design phase of an application that will need an RPO of less than 5 minutes and an RTO of less than 10 minutes. The solutions architecture team is forecasting that the database will store approximately 10 TB of data. As part of the design, they are looking for a database solution that will provide the company with the ability to fail over to a secondary Region.Which solution will meet these business requirements at the LOWEST cost?",
    "choices": [
      "A. Deploy an Amazon Aurora DB cluster and take snapshots of the cluster every 5 minutes. Once a snapshot is complete, copy the snapshot to a secondary Region to serve as a backup in the event of a failure.",
      "B. Deploy an Amazon RDS instance with a cross-Region read replica in a secondary Region. In the event of a failure, promote the read replica to become the primary.",
      "C. Deploy an Amazon Aurora DB cluster in the primary Region and another in a secondary Region. Use AWS DMS to keep the secondary Region in sync.",
      "D. Deploy an Amazon RDS instance with a read replica in the same Region. In the event of a failure, promote the read replica to become the primary."
    ],
    "answer": "B",
    "explanation": "1. Clear explanation of why the correct answer (B) is right:\n\nThe correct answer is option B, which is to deploy an Amazon RDS instance with a cross-Region read replica in a secondary Region. This solution meets the business requirements at the lowest cost:\n\n- RPO of less than 5 minutes: Cross-Region read replicas continuously replicate data from the primary RDS instance to the secondary Region, providing a near-real-time RPO of less than 5 minutes.\n- RTO of less than 10 minutes: In the event of a failure, the read replica in the secondary Region can be promoted to become the primary, typically within minutes, meeting the RTO requirement.\n- Ability to fail over to a secondary Region: The cross-Region read replica provides the ability to fail over to the secondary Region in the event of a failure in the primary Region.\n- Lower cost: Maintaining a read replica in a secondary Region is generally less expensive than maintaining a full Aurora DB cluster in a secondary Region (as in option C).\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nA. Deploy an Amazon Aurora DB cluster and take snapshots: This solution does not meet the RTO requirement of less than 10 minutes, as restoring from a snapshot can take longer than that.\n\nC. Deploy an Aurora DB cluster in the primary and secondary Regions, and use AWS DMS to keep them in sync: This solution is more expensive than the correct answer (option B), as it requires maintaining two full Aurora DB clusters instead of just a read replica.\n\nD. Deploy an RDS instance with a read replica in the same Region: This solution does not provide the ability to fail over to a secondary Region, which is a key requirement in the question."
  },
  "391": {
    "question": "A financial company needs to create a separate AWS account for a new digital wallet application. The company uses AWS Organizations to manage its accounts. A solutions architect uses the IAM user Support1 from the management account to create a new member account with [email\u00a0protected] as the email address.What should the solutions architect do to create IAM users in the new member account?",
    "choices": [
      "A. Sign in to the AWS Management Console with AWS account root user credentials by using the 64-character password from the initial AWS Organizations email sent to [email\u00a0protected]. Set up the IAM users as required.",
      "B. From the management account, switch roles to assume the OrganizationAccountAccessRole role with the account ID of the new member account. Set up the IAM users as required.",
      "C. Go to the AWS Management Console sign-in page. Choose \u201cSign in using root account credentials.\u201d Sign in in by using the email address finance [email\u00a0protected] and the management account's root password. Set up the IAM users as required.",
      "D. Go to the AWS Management Console sign-in page. Sign in by using the account ID of the new member account and the Support1 IAM credentials. Set up the IAM users as required."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B. From the management account, the solutions architect should switch roles to assume the OrganizationAccountAccessRole with the account ID of the new member account. This allows the architect to set up the IAM users in the new member account without needing to use the root user credentials.\n\nAssuming the OrganizationAccountAccessRole provides the necessary permissions to access the new member account and perform actions on its behalf, without exposing the management account's root user credentials. This is a more secure and recommended approach compared to using the root user credentials directly.\n\n2. Explanations of the incorrect choices:\n\nA. This is incorrect because using the root user credentials from the initial AWS Organizations email is not a recommended practice. Root user credentials should be closely guarded and used only for critical account management tasks.\n\nC. This is incorrect because the solutions architect should not use the root user credentials from the management account to access the new member account. Using root user credentials directly introduces unnecessary security risks and is not the recommended approach.\n\nD. This is incorrect because the solutions architect should not use the Support1 IAM user credentials to directly access the new member account. The recommended approach is to assume the OrganizationAccountAccessRole, which provides the necessary permissions without exposing the IAM user credentials."
  },
  "392": {
    "question": "A car rental company has built a serverless REST API to provide data to its mobile app. The app consists of an Amazon API Gateway API with a Regional endpoint, AWS Lambda functions, and an Amazon Aurora MySQL Serverless DB cluster. The company recently opened the API to mobile apps of partners. A significant increase in the number of requests resulted, causing sporadic database memory errors.Analysis of the API traffic indicates that clients are making multiple HTTP GET requests for the same queries in a short period of time. Traffic is concentrated during business hours, with spikes around holidays and other events.The company needs to improve its ability to support the additional usage while minimizing the increase in costs associated with the solution.Which strategy meets these requirements?",
    "choices": [
      "A. Convert the API Gateway Regional endpoint to an edge-optimized endpoint. Enable caching in the production stage.",
      "B. Implement an Amazon ElastiCache for Redis cache to store the results of the database calls. Modify the Lambda functions to use the cache.",
      "C. Modify the Aurora Serverless DB cluster configuration to increase the maximum amount of available memory.",
      "D. Enable throttling in the API Gateway production stage. Set the rate and burst values to limit the incoming calls."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B, \"Implement an Amazon ElastiCache for Redis cache to store the results of the database calls. Modify the Lambda functions to use the cache.\"\n\nThis is the best strategy to meet the requirements of supporting the additional usage while minimizing the increase in costs. Here's why:\n\n- The problem indicates that clients are making multiple HTTP GET requests for the same queries in a short period of time, which is causing the database to experience memory errors.\n- Implementing an Amazon ElastiCache for Redis cache allows the application to store the results of the database calls in-memory, reducing the number of direct database queries and alleviating the load on the Aurora Serverless DB cluster.\n- Modifying the Lambda functions to use the cache before querying the database ensures that frequently accessed data is served from the high-performance Redis cache, rather than the database, significantly reducing the database read load.\n- This caching approach is more cost-effective than the other options, as it directly addresses the root cause of the problem (high database read load) without requiring additional infrastructure like edge-optimized endpoints or increased database capacity.\n\n2. Explanations of the incorrect choices:\n\nA. Convert the API Gateway Regional endpoint to an edge-optimized endpoint. Enable caching in the production stage.\n- While this approach can improve the overall response time for clients by caching the responses at the edge, it does not directly address the database memory errors caused by the high read load. The caching would occur at the API Gateway level, not the database level, and would not reduce the number of database queries.\n\nC. Modify the Aurora Serverless DB cluster configuration to increase the maximum amount of available memory.\n- Increasing the database capacity may temporarily alleviate the memory errors, but it does not address the root cause of the problem, which is the high volume of identical queries. This solution would result in a permanent increase in costs, without guaranteeing that the problem will not resurface in the future as the traffic continues to grow.\n\nD. Enable throttling in the API Gateway production stage. Set the rate and burst values to limit the incoming calls.\n- Throttling the incoming requests may help to reduce the load on the database, but it comes at the cost of a poor user experience for the clients"
  },
  "393": {
    "question": "A company is migrating an on-premises application and a MySQL database to AWS. The application processes highly sensitive data, and new data is constantly updated in the database. The data must not be transferred over the internet. The company also must encrypt the data in transit and at rest.The database is 5 TB in size. The company already has created the database schema in an Amazon RDS for MySQL DB instance. The company has set up a 1 Gbps AWS Direct Connect connection to AWS. The company also has set up a public VIF and a private VIF. A solutions architect needs to design a solution that will migrate the data to AWS with the least possible downtime.Which solution will meet these requirements?",
    "choices": [
      "A. Perform a database backup. Copy the backup files to an AWS Snowball Edge Storage Optimized device. Import the backup to Amazon S3. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3) for encryption at rest. Use TLS for encryption in transit. Import the data from Amazon S3 to the DB instance.",
      "B. Use AWS Database Migration Service (AWS DMS) to migrate the data to AWS. Create a DMS replication instance in a private subnet. Create VPC endpoints for AWS DMS. Configure a DMS task to copy data from the on-premises database to the DB instance by using full load plus change data capture (CDC). Use the AWS Key Management Service (AWS KMS) default key for encryption at rest. Use TLS for encryption in transit.",
      "C. Perform a database backup. Use AWS DataSync to transfer the backup files to Amazon S3. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3) for encryption at rest. Use TLS for encryption in transit. Import the data from Amazon S3 to the DB instance.",
      "D. Use Amazon S3 File Gateway. Set up a private connection to Amazon S3 by using AWS PrivateLink. Perform a database backup. Copy the backup files to Amazon S3. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3) for encryption at rest. Use TLS for encryption in transit. Import the data from Amazon S3 to the DB instance."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B because it best meets the requirements outlined in the question:\n\n- The company needs to migrate a 5 TB MySQL database to AWS while ensuring the data is not transferred over the internet and is encrypted in transit and at rest.\n- Using AWS Database Migration Service (AWS DMS) is the most appropriate solution, as it allows for a direct migration from the on-premises database to the Amazon RDS for MySQL DB instance, without the need to first transfer the data to Amazon S3.\n- By creating a DMS replication instance in a private subnet and using VPC endpoints, the data will be transferred over the private AWS Direct Connect connection, ensuring it does not traverse the public internet.\n- For encryption, AWS DMS supports encryption at rest using the AWS Key Management Service (AWS KMS) default key, as well as encryption in transit using TLS.\n- This approach minimizes the number of steps involved in the data migration process, reducing potential downtime.\n\n2. Explanations of the incorrect choices:\n\nA. This option involves first backing up the database, copying the backup files to an AWS Snowball Edge device, importing the data to Amazon S3, and then importing it to the Amazon RDS DB instance. This approach is more complex and introduces an additional step (the Snowball Edge transfer) that is not necessary. Additionally, the data would still need to be copied from Amazon S3 to the DB instance, which could lead to increased downtime.\n\nC. This option is similar to option A, but it uses AWS DataSync instead of Snowball Edge to transfer the backup files to Amazon S3. While this approach may be simpler than option A, it still requires an additional step of transferring the data from Amazon S3 to the DB instance, which could lead to increased downtime.\n\nD. This option involves using the Amazon S3 File Gateway to transfer the database backup files to Amazon S3 and then importing the data to the DB instance. While this approach leverages the private AWS PrivateLink connection, it still requires an additional step of transferring the data from Amazon S3 to the DB instance, which could lead to increased downtime.\n\nIn summary, option B, using AWS Database Migration Service (AWS DMS), is the most direct and efficient solution"
  },
  "394": {
    "question": "Accompany is deploying a new cluster for big data analytics on AWS. The cluster will run across many Linux Amazon EC2 instances that are spread across multiple Availability Zones.All of the nodes in the cluster must have read and write access to common underlying file storage. The file storage must be highly available, must be resilient, must be compatible with the Portable Operating System Interface (POSIX), and must accommodate high levels of throughput.Which storage solution will meet these requirements?",
    "choices": [
      "A. Provision an AWS Storage Gateway file gateway NFS file share that is attached to an Amazon S3 bucket. Mount the NFS file share on each EC2 instance in the cluster.",
      "B. Provision a new Amazon Elastic File System (Amazon EFS) file system that uses General Purpose performance mode. Mount the EFS file system on each EC2 instance in the cluster.",
      "C. Provision a new Amazon Elastic Block Store (Amazon EBS) volume that uses the io2 volume type. Attach the EBS volume to all of the EC2 instances in the cluster.",
      "D. Provision a new Amazon Elastic File System (Amazon EFS) file system that uses Max I/O performance mode. Mount the EFS file system on each EC2 instance in the cluster."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (D):\n\nThe correct answer is D. Provision a new Amazon Elastic File System (Amazon EFS) file system that uses Max I/O performance mode. Mount the EFS file system on each EC2 instance in the cluster.\n\nEFS with Max I/O performance mode is the best choice for this use case because:\n\n- It provides high throughput and IOPS to support the data analytics workload, which requires parallel operations from many EC2 instances.\n- EFS is highly available and resilient, as it is a fully managed and highly scalable NFS file system that can span multiple Availability Zones.\n- EFS is compatible with POSIX, ensuring compatibility with the Linux EC2 instances.\n- The Max I/O performance mode is optimized for workloads that need the highest possible aggregate throughput and IOPS, such as data analytics.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Provisioning an AWS Storage Gateway file gateway NFS file share attached to an S3 bucket is not the best choice because:\n- S3 does not provide the POSIX compatibility required by the workload.\n- The file gateway approach may not provide the high throughput and IOPS required for the data analytics workload.\n\nB. Provisioning an EFS file system with General Purpose performance mode is not the best choice because:\n- While General Purpose mode is suitable for latency-sensitive workloads, it may not provide the highest possible throughput and IOPS required for this data analytics workload.\n- To achieve the maximum 250,000 read IOPS, the file system would need to use Elastic throughput, which is only available for file systems in General Purpose mode.\n\nC. Provisioning an Amazon EBS volume with the io2 volume type is not the best choice because:\n- EBS volumes are block-based storage, not file-based storage, and may not provide the POSIX compatibility required by the workload.\n- EBS volumes are designed for individual EC2 instances, and may not provide the high availability and resilience required for the cluster spread across multiple Availability Zones."
  },
  "395": {
    "question": "A company hosts a software as a service (SaaS) solution on AWS. The solution has an Amazon API Gateway API that serves an HTTPS endpoint. The API uses AWS Lambda functions for compute. The Lambda functions store data in an Amazon Aurora Serverless v1 database.The company used the AWS Serverless Application Model (AWS SAM) to deploy the solution. The solution extends across multiple Availability Zones and has no disaster recovery (DR) plan.A solutions architect must design a DR strategy that can recover the solution in another AWS Region. The solution has an RTO of 5 minutes and an RPO of 1 minute.What should the solutions architect do to meet these requirements?",
    "choices": [
      "A. Create a read replica of the Aurora Serverless v1 database in the target Region. Use AWS SAM to create a runbook to deploy the solution to the target Region. Promote the read replica to primary in case of disaster.",
      "B. Change the Aurora Serverless v1 database to a standard Aurora MySQL global database that extends across the source Region and the target Region. Use AWS SAM to create a runbook to deploy the solution to the target Region.",
      "C. Create an Aurora Serverless v1 DB cluster that has multiple writer instances in the target Region. Launch the solution in the target Region. Configure the two Regional solutions to work in an active-passive configuration.",
      "D. Change the Aurora Serverless v1 database to a standard Aurora MySQL global database that extends across the source Region and the target Region. Launch the solution in the target Region. Configure the two Regional solutions to work in an active-passive configuration."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (D):\n\nThe correct answer is D because it addresses the key requirements of the problem statement:\n\n- RTO of 5 minutes and RPO of 1 minute\n- Ability to recover the solution in another AWS Region\n\nBy converting the Aurora Serverless v1 database to a standard Aurora MySQL global database that extends across the source and target regions, the solution can leverage Aurora's global database capabilities to provide fast recovery and near real-time data replication to meet the RTO and RPO requirements.\n\nAdditionally, launching the solution in the target region and configuring the two regional solutions to work in an active-passive configuration ensures that the recovery process can be automated and streamlined, meeting the 5-minute RTO requirement.\n\n2. Explanations of why the other choices are incorrect:\n\nA. This option is incorrect because Aurora Serverless v1 does not support read replicas, which are necessary for the disaster recovery setup. Additionally, promoting a read replica to primary may not be fast enough to meet the 5-minute RTO requirement.\n\nB. This option is incorrect because it does not address the need for an active-passive configuration between the source and target regions. While a global database can provide the necessary data replication, it does not automatically configure the solution to work in an active-passive mode, which is required to meet the recovery time objective.\n\nC. This option is incorrect because it introduces additional complexity by requiring the two regional solutions to be configured in an active-passive mode. This may not be as efficient or cost-effective as the active-passive configuration between the two regions, as described in the correct answer."
  },
  "396": {
    "question": "A company owns a chain of travel agencies and is running an application in the AWS Cloud. Company employees use the application to search for information about travel destinations. Destination content is updated four times each year.Two fixed Amazon EC2 instances serve the application. The company uses an Amazon Route 53 public hosted zone with a multivalue record of travel.example.com that returns the Elastic IP addresses for the EC2 instances. The application uses Amazon DynamoDB as its primary data store. The company uses a self-hosted Redis instance as a caching solution.During content updates, the load on the EC2 instances and the caching solution increases drastically. This increased load has led to downtime on several occasions. A solutions architect must update the application so that the application is highly available and can handle the load that is generated by the content updates.Which solution will meet these requirements?",
    "choices": [
      "A. Set up DynamoDB Accelerator (DAX) as in-memory cache. Update the application to use DAX. Create an Auto Scaling group for the EC2 instances. Create an Application Load Balancer (ALB). Set the Auto Scaling group as a target for the ALB. Update the Route 53 record to use a simple routing policy that targets the ALB's DNS alias. Configure scheduled scaling for the EC2 instances before the content updates.",
      "B. Set up Amazon ElastiCache for Redis. Update the application to use ElastiCache. Create an Auto Scaling group for the EC2 instances. Create an Amazon CloudFront distribution, and set the Auto Scaling group as an origin for the distribution. Update the Route 53 record to use a simple routing policy that targets the CloudFront distribution\u2019s DNS alias. Manually scale up EC2 instances before the content updates.",
      "C. Set up Amazon ElastiCache for Memcached. Update the application to use ElastiCache. Create an Auto Scaling group for the EC2 instances. Create an Application Load Balancer (ALB). Set the Auto Scaling group as a target for the ALB. Update the Route 53 record to use a simple routing policy that targets the ALB's DNS alias. Configure scheduled scaling for the application before the content updates.",
      "D. Set up DynamoDB Accelerator (DAX) as in-memory cache. Update the application to use DAX. Create an Auto Scaling group for the EC2 instances. Create an Amazon CloudFront distribution, and set the Auto Scaling group as an origin for the distribution. Update the Route 53 record to use a simple routing policy that targets the CloudFront distribution's DNS alias. Manually scale up EC2 instances before the content updates."
    ],
    "answer": "A",
    "explanation": "1. Clear explanation of why the correct answer (A) is right:\n\nThe correct answer is A because it addresses all the key requirements mentioned in the question:\n\n- Sets up DynamoDB Accelerator (DAX) as an in-memory cache to handle the increased load during content updates. This is better than using a generic caching solution like ElastiCache, as DAX is specifically optimized for DynamoDB.\n- Creates an Auto Scaling group for the EC2 instances to automatically scale up and down based on the load. This ensures high availability and the ability to handle the increased load.\n- Sets up an Application Load Balancer (ALB) to distribute the traffic across the Auto Scaling group. This provides a highly available and scalable solution.\n- Updates the Route 53 record to use a simple routing policy that targets the ALB's DNS alias. This ensures that the application can be accessed through a single, highly available endpoint.\n- Configures scheduled scaling for the EC2 instances before the content updates to proactively handle the expected increase in load.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nB. This option uses ElastiCache for Redis, which is a good caching solution, but not as tightly integrated with DynamoDB as DAX. Additionally, using a CloudFront distribution as the origin is not the optimal solution for handling the increased database load and scaling the EC2 instances.\n\nC. This option uses ElastiCache for Memcached, which is not as feature-rich as Redis and lacks the DynamoDB integration provided by DAX.\n\nD. This option uses DAX for caching, which is a good choice. However, it relies on manually scaling up the EC2 instances before the content updates, which is not as scalable and automated as the solution in option A."
  },
  "397": {
    "question": "A company needs to store and process image data that will be uploaded from mobile devices using a custom mobile app. Usage peaks between 8 AM and 5 PM on weekdays, with thousands of uploads per minute. The app is rarely used at any other time. A user is notified when image processing is complete.Which combination of actions should a solutions architect take to ensure image processing can scale to handle the load? (Choose three.)",
    "choices": [
      "A. Upload files from the mobile software directly to Amazon S3. Use S3 event notifications to create a message in an Amazon MQ queue.",
      "B. Upload files from the mobile software directly to Amazon S3. Use S3 event notifications to create a message in an Amazon Simple Queue Service (Amazon SQS) standard queue.",
      "C. Invoke an AWS Lambda function to perform image processing when a message is available in the queue.",
      "D. Invoke an S3 Batch Operations job to perform image processing when a message is available in the queue.",
      "E. Send a push notification to the mobile app by using Amazon Simple Notification Service (Amazon SNS) when processing is complete.",
      "F. Send a push notification to the mobile app by using Amazon Simple Email Service (Amazon SES) when processing is complete."
    ],
    "answer": "BCE",
    "explanation": "1. Explanation of the correct answer (BCE):\n\nB. Upload files from the mobile software directly to Amazon S3. Use S3 event notifications to create a message in an Amazon Simple Queue Service (Amazon SQS) standard queue.\n- This leverages the scalability and durability of SQS to handle the large and unpredictable load of image uploads during peak hours.\n- S3 event notifications will reliably trigger messages in the SQS queue for each uploaded image.\n\nC. Invoke an AWS Lambda function to perform image processing when a message is available in the queue.\n- Using Lambda allows for on-demand, scalable image processing that can handle the high upload rates.\n- Lambda can be triggered by messages in the SQS queue, allowing for parallel processing of the images.\n\nE. Send a push notification to the mobile app by using Amazon Simple Notification Service (Amazon SNS) when processing is complete.\n- SNS provides a reliable way to send push notifications to the mobile app, notifying users when their image processing is complete.\n- This provides a good user experience by keeping the users informed about the status of their image uploads.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Amazon MQ: While viable for durable messaging, it's less scalable and cost-effective compared to SQS for this use case involving high peak loads.\n\nD. S3 Batch Operations: Designed for batch processing of large datasets, not real-time processing of individual image uploads.\n\nF. Amazon SES: Primarily for email delivery, not push notifications to mobile apps. It would not be the appropriate service to notify users about image processing completion."
  },
  "398": {
    "question": "A company is building an application on AWS. The application sends logs to an Amazon OpenSearch Service cluster for analysis. All data must be stored within a VPC.Some of the company\u2019s developers work from home. Other developers work from three different company office locations. The developers need to access OpenSearch Service to analyze and visualize logs directly from their local development machines.Which solution will meet these requirements?",
    "choices": [
      "A. Configure and set up an AWS Client VPN endpoint. Associate the Client VPN endpoint with a subnet in the VPC. Configure a Client VPN self-service portal. Instruct the developers to connect by using the client for Client VPN.",
      "B. Create a transit gateway, and connect it to the VPC. Create an AWS Site-to-Site VPN. Create an attachment to the transit gateway. Instruct the developers to connect by using an OpenVPN client.",
      "C. Create a transit gateway, and connect it to the VPOrder an AWS Direct Connect connection. Set up a public VIF on the Direct Connect connection. Associate the public VIF with the transit gateway. Instruct the developers to connect to the Direct Connect connection.",
      "D. Create and configure a bastion host in a public subnet of the VPC. Configure the bastion host security group to allow SSH access from the company CIDR ranges. Instruct the developers to connect by using SSH."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\nThe correct answer is A because it provides the most suitable solution to meet the given requirements. The key points are:\n\n- The application sends logs to an Amazon OpenSearch Service cluster, and all data must be stored within a VPC.\n- The developers, both working from home and from company office locations, need to access the OpenSearch Service directly from their local development machines.\n\nBy configuring an AWS Client VPN endpoint and associating it with a subnet in the VPC, the developers can securely connect to the VPC from their local machines, regardless of their location (home or office). The Client VPN self-service portal allows the developers to easily connect using the Client VPN client, providing them with direct access to the OpenSearch Service within the VPC.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. Site-to-Site VPN: This solution is designed for connecting entire networks, not individual devices. It requires VPN hardware/software at each office location, which is not a suitable approach for individual developer access.\n\nC. Direct Connect: This is primarily for high-bandwidth, low-latency connections between on-premises networks and AWS, not for individual developer access. It does not directly address the requirement for developers to access the OpenSearch Service from their local machines.\n\nD. Bastion Host: While this solution provides access to the VPC, it introduces a potential security risk by exposing a public-facing host. Additionally, it requires the developers to learn and use SSH, which may not be the most convenient or user-friendly option for direct access to the OpenSearch Service."
  },
  "399": {
    "question": "A company wants to migrate its website from an on-premises data center onto AWS. At the same time, it wants to migrate the website to a containerized microservice-based architecture to improve the availability and cost efficiency. The company\u2019s security policy states that privileges and network permissions must be configured according to best practice, using least privilege.A solutions architect must create a containerized architecture that meets the security requirements and has deployed the application to an Amazon ECS cluster.What steps are required after the deployment to meet the requirements? (Choose two.)",
    "choices": [
      "A. Create tasks using the bridge network mode.",
      "B. Create tasks using the awsvpc network mode.",
      "C. Apply security groups to Amazon EC2 instances, and use IAM roles for EC2 instances to access other resources.",
      "D. Apply security groups to the tasks, and pass IAM credentials into the container at launch time to access other resources.",
      "E. Apply security groups to the tasks, and use IAM roles for tasks to access other resources."
    ],
    "answer": "BE",
    "explanation": "1. Explanation of the correct answer (BE):\n\nB. Create tasks using the awsvpc network mode.\nThis is the correct choice because the awsvpc network mode allows each task to have its own elastic network interface (ENI) within the VPC, enabling the application of security groups directly to the tasks. This helps to meet the security requirement of configuring privileges and network permissions according to the principle of least privilege.\n\nE. Apply security groups to the tasks, and use IAM roles for tasks to access other resources.\nThis is the correct choice because using IAM roles for tasks allows the containers to access other AWS resources (such as databases, S3 buckets, etc.) with the appropriate permissions, without needing to store and manage credentials within the containers. This also aligns with the security requirement of using least privilege.\n\n2. Explanations of the incorrect choices:\n\nA. Create tasks using the bridge network mode.\nThis is incorrect because the bridge network mode does not provide the same level of network isolation and security control as the awsvpc network mode. In the bridge mode, all tasks share the same ENI, making it more difficult to apply security groups and enforce least privilege.\n\nC. Apply security groups to Amazon EC2 instances, and use IAM roles for EC2 instances to access other resources.\nThis is incorrect because it does not address the requirement of applying security groups and IAM roles at the task level. Applying security groups and IAM roles to the underlying EC2 instances may not provide the desired level of granular control and security for the individual tasks.\n\nD. Apply security groups to the tasks, and pass IAM credentials into the container at launch time to access other resources.\nThis is incorrect because storing IAM credentials within the containers is not considered a best practice for security. Instead, it is recommended to use IAM roles for tasks to provide the necessary permissions without embedding credentials in the containers."
  },
  "400": {
    "question": "A company is running a serverless application that consists of several AWS Lambda functions and Amazon DynamoDB tables. The company has created new functionality that requires the Lambda functions to access an Amazon Neptune DB cluster. The Neptune DB cluster is located in three subnets in a VPC.Which of the possible solutions will allow the Lambda functions to access the Neptune DB cluster and DynamoDB tables? (Choose two.)",
    "choices": [
      "A. Create three public subnets in the Neptune VPC, and route traffic through an internet gateway. Host the Lambda functions in the three new public subnets.",
      "B. Create three private subnets in the Neptune VPC, and route internet traffic through a NAT gateway. Host the Lambda functions in the three new private subnets.",
      "C. Host the Lambda functions outside the VPUpdate the Neptune security group to allow access from the IP ranges of the Lambda functions.",
      "D. Host the Lambda functions outside the VPC. Create a VPC endpoint for the Neptune database, and have the Lambda functions access Neptune over the VPC endpoint.",
      "E. Create three private subnets in the Neptune VPC. Host the Lambda functions in the three new isolated subnets. Create a VPC endpoint for DynamoDB, and route DynamoDB traffic to the VPC endpoint."
    ],
    "answer": "BE",
    "explanation": "1. Clear explanation of the correct answer (B and E):\n\nB. Create three private subnets in the Neptune VPC, and route internet traffic through a NAT gateway. Host the Lambda functions in the three new private subnets.\nThis option allows the Lambda functions to access the Neptune DB cluster within the VPC by placing them in the same private subnets as the Neptune DB. The NAT gateway is required to allow the Lambda functions in the private subnets to access the internet (e.g., to access DynamoDB).\n\nE. Create three private subnets in the Neptune VPC. Host the Lambda functions in the three new isolated subnets. Create a VPC endpoint for DynamoDB, and route DynamoDB traffic to the VPC endpoint.\nThis option allows the Lambda functions to access DynamoDB through a VPC endpoint, without needing to route traffic through the public internet. By placing the Lambda functions in the same VPC as the Neptune DB, they can access both the Neptune DB and the DynamoDB VPC endpoint.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Create three public subnets in the Neptune VPC, and route traffic through an internet gateway. Host the Lambda functions in the three new public subnets.\nThis is incorrect because placing the Lambda functions in public subnets would expose them to the internet, which is not a secure or recommended practice. The Lambda functions should be in private subnets to access the Neptune DB cluster.\n\nC. Host the Lambda functions outside the VPC. Update the Neptune security group to allow access from the IP ranges of the Lambda functions.\nThis is incorrect because hosting the Lambda functions outside the VPC would prevent them from accessing the Neptune DB cluster directly. The Lambda functions need to be within the same VPC as the Neptune DB cluster to access it.\n\nD. Host the Lambda functions outside the VPC. Create a VPC endpoint for the Neptune database, and have the Lambda functions access Neptune over the VPC endpoint.\nThis is incorrect because, as mentioned in the community discussion, Amazon Neptune does not currently support VPC endpoints. Without VPC endpoint support, the Lambda functions outside the VPC cannot access the Neptune DB cluster."
  },
  "401": {
    "question": "A company is using AWS Control Tower to manage AWS accounts in an organization in AWS Organizations. The company has an OU that contains accounts. The company must prevent any new or existing Amazon EC2 instances in the OU's accounts from gaining a public IP address.Which solution will meet these requirements?",
    "choices": [
      "A. Configure all instances in each account in the OU to use AWS Systems Manager. Use a Systems Manager Automation runbook to prevent public IP addresses from being attached to the instances.",
      "B. Implement the AWS Control Tower proactive control to check whether instances in the OU's accounts have a public IP address. Set the AssociatePublicIpAddress property to False. Attach the proactive control to the OU.",
      "C. Create an SCP that prevents the launch of instances that have a public IP address. Additionally, configure the SCP to prevent the attachment of a public IP address to existing instances. Attach the SCP to the OU.",
      "D. Create an AWS Config custom rule that detects instances that have a public IP address. Configure a remediation action that uses an AWS Lambda function to detach the public IP addresses from the instances."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C, which recommends creating an SCP (Service Control Policy) that prevents the launch of instances with a public IP address and prevents the attachment of a public IP address to existing instances. This is the best solution because:\n\n- SCPs are a powerful tool in AWS Organizations that allow you to centrally manage and enforce policies across multiple AWS accounts within an Organization.\n- By creating and attaching an SCP to the OU (Organizational Unit) that contains the accounts, you can effectively restrict the ability to launch instances with public IP addresses or attach public IP addresses to existing instances.\n- This approach is more comprehensive than the other options, as it covers both new instance launches and existing instances, ensuring that the requirement of preventing public IP addresses is met.\n- SCPs are a native AWS Organizations feature and do not involve modifying or potentially drifting the state of AWS Control Tower configurations, which is a risk with the other options.\n\n2. Explanations of why the other choices are incorrect:\n\nA. This option uses AWS Systems Manager Automation, which is a valid approach. However, it only addresses the issue for new instances, not existing instances in the OU. Additionally, it requires more complex configuration and maintenance compared to the SCP solution.\n\nB. This option uses AWS Control Tower's proactive controls, which are designed to check resources during creation or update. However, as mentioned in the community discussions, proactive controls may not affect requests made directly through the AWS CLI, AWS API, or AWS Console, meaning they may not fully enforce the requirement.\n\nD. This option uses an AWS Config custom rule to detect instances with public IP addresses and a remediation action using AWS Lambda to detach the public IP addresses. While this is a valid solution, it is more complex and reactive compared to the SCP approach in option C. It also requires additional resource provisioning and maintenance.\n\nIn summary, the correct answer (C) is the most straightforward and comprehensive solution that leverages the native capabilities of AWS Organizations and SCPs to enforce the requirement of preventing public IP addresses on EC2 instances within the specified OU."
  },
  "402": {
    "question": "A company is creating a solution that can move 400 employees into a remote working environment in the event of an unexpected disaster. The user desktops have a mix of Windows and Linux operating systems. Multiple types of software, such as web browsers and mail clients, are installed on each desktop.A solutions architect needs to implement a solution that can be integrated with the company\u2019s on-premises Active Directory to allow employees to use their existing identity credentials. The solution must provide multifactor authentication (MFA) and must replicate the user experience from the existing desktops.Which solution will meet these requirements?",
    "choices": [
      "A. Use Amazon WorkSpaces for the cloud desktop service. Set up a VPN connection to the on-premises network. Create an AD Connector, and connect to the on-premises Active Directory. Activate MFA for Amazon WorkSpaces by using the AWS Management Console.",
      "B. Use Amazon AppStream 2.0 as an application streaming service. Configure Desktop View for the employees. Set up a VPN connection to the on-premises network. Set up Active Directory Federation Services (AD FS) on premises. Connect the VPC network to AD FS through the VPN connection.",
      "C. Use Amazon WorkSpaces for the cloud desktop service. Set up a VPN connection to the on-premises network. Create an AD Connector, and connect to the on-premises Active Directory. Configure a RADIUS server for MFA.",
      "D. Use Amazon AppStream 2.0 as an application streaming service. Set up Active Directory Federation Services on premises. Configure MFA to grant users access on AppStream 2.0."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C because it meets all the requirements specified in the question:\n\n- It uses Amazon WorkSpaces for the cloud desktop service, which can replicate the user experience from the existing desktops.\n- It sets up a VPN connection to the on-premises network, allowing integration with the company's on-premises Active Directory.\n- It creates an AD Connector to connect to the on-premises Active Directory, allowing employees to use their existing identity credentials.\n- It configures a RADIUS server for multi-factor authentication (MFA), which is a requirement for the solution.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option uses Amazon WorkSpaces but does not mention the configuration of a RADIUS server for MFA, which is a required feature.\n\nB. This option uses Amazon AppStream 2.0 instead of Amazon WorkSpaces, which does not meet the requirement of replicating the user experience from the existing desktops.\n\nD. This option uses Amazon AppStream 2.0 and configures Active Directory Federation Services (AD FS) on-premises, but does not mention the setup of a RADIUS server for MFA, which is a requirement."
  },
  "403": {
    "question": "A company has deployed an Amazon Connect contact center. Contact center agents are reporting large numbers of computer-generated calls. The company is concerned about the cost and productivity effects of these calls. The company wants a solution that will allow agents to flag the call as spam and automatically block the numbers from going to an agent in the future.What is the MOST operationally efficient solution to meet these requirements?",
    "choices": [
      "A. Customize the Contact Control Panel (CCP) by adding a flag call button that will invoke an AWS Lambda function that calls the UpdateContactAttributes API. Use an Amazon DynamoDB table to store the spam numbers. Modify the contact flows to look for the updated attribute and to use a Lambda function to read and write to the DynamoDB table.",
      "B. Use a Contact Lens for Amazon Connect rule that will look for spam calls. Use an Amazon DynamoDB table to store the spam numbers. Modify the contact flows to look for the rule and to invoke an AWS Lambda function to read and write to the DynamoDB table.",
      "C. Use an Amazon DynamoDB table to store the spam numbers. Create a quick connect that the agents can transfer the spam call to from the Contact Control Panel (CCP). Modify the quick connect contact flow to invoke an AWS Lambda function to write to the DynamoDB table.",
      "D. Modify the initial contact flow to ask for caller input. If the agent does not receive input, the agent should mark the caller as spam. Use an Amazon DynamoDB table to store the spam numbers. Use an AWS Lambda function to read and write to the DynamoDB table."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\n\nThe correct answer is A because it provides the most operationally efficient solution to meet the given requirements. The key aspects of this solution are:\n\n- Customizing the Contact Control Panel (CCP) to add a \"flag call\" button that will invoke an AWS Lambda function. This allows agents to easily mark a call as spam.\n- Using an Amazon DynamoDB table to store the spam numbers. This provides a scalable and efficient way to manage the list of blocked numbers.\n- Modifying the contact flows to look for the \"spam\" attribute and use a Lambda function to read and write to the DynamoDB table. This ensures that the spam numbers are automatically blocked from reaching agents in the future.\n\nThis solution is the most operationally efficient as it provides a seamless integration between the agent workflow, the spam number management, and the contact flow logic. It minimizes the manual effort required by agents and administrators, making the overall process more scalable and efficient.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. This solution uses a Contact Lens for Amazon Connect rule to identify spam calls, which may not be as flexible or customizable as the agent-driven approach in the correct answer. Additionally, the process of modifying the contact flows to invoke a Lambda function to read and write to the DynamoDB table is still required, making it less efficient than the correct answer.\n\nC. This solution uses a quick connect to transfer the spam call, which does not provide the same level of automation and integration as the correct answer. The process of creating a quick connect and modifying the contact flow to invoke a Lambda function to write to the DynamoDB table is also less efficient than the approach in the correct answer.\n\nD. This solution requires the caller to provide input, which may not be a reliable method for identifying spam calls. Additionally, the process of using a Lambda function to read and write to the DynamoDB table is still required, making it less efficient than the correct answer, which provides a more seamless integration between the agent workflow and the spam number management."
  },
  "404": {
    "question": "A company has mounted sensors to collect information about environmental parameters such as humidity and light throughout all the company's factories. The company needs to stream and analyze the data in the AWS Cloud in real time. If any of the parameters fall out of acceptable ranges, the factory operations team must receive a notification immediately.Which solution will meet these requirements?",
    "choices": [
      "A. Stream the data to an Amazon Kinesis Data Firehose delivery stream. Use AWS Step Functions to consume and analyze the data in the Kinesis Data Firehose delivery stream. Use Amazon Simple Notification Service (Amazon SNS) to notify the operations team.",
      "B. Stream the data to an Amazon Managed Streaming for Apache Kafka (Amazon MSK) cluster. Set up a trigger in Amazon MSK to invoke an AWS Fargate task to analyze the data. Use Amazon Simple Email Service (Amazon SES) to notify the operations team.",
      "C. Stream the data to an Amazon Kinesis data stream. Create an AWS Lambda function to consume the Kinesis data stream and to analyze the data. Use Amazon Simple Notification Service (Amazon SNS) to notify the operations team.",
      "D. Stream the data to an Amazon Kinesis Data Analytics application. Use an automatically scaled and containerized service in Amazon Elastic Container Service (Amazon ECS) to consume and analyze the data. Use Amazon Simple Email Service (Amazon SES) to notify the operations team."
    ],
    "answer": "C",
    "explanation": "1. Clear explanation of why the correct answer (C) is right:\n\nThe correct answer is C because it best meets the requirements of the problem statement:\n\n- Streaming the data to an Amazon Kinesis data stream allows for real-time data processing, which is a key requirement.\n- Creating an AWS Lambda function to consume the Kinesis data stream and analyze the data is an appropriate solution for real-time data analysis.\n- Using Amazon Simple Notification Service (Amazon SNS) to notify the operations team immediately when parameters fall out of acceptable ranges is the correct notification mechanism, as specified in the requirements.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nA. This option uses Kinesis Data Firehose, which is not a real-time solution. It also uses Step Functions, which may be overkill for this use case.\n\nB. This option uses Amazon MSK, which is a valid choice, but it also introduces an unnecessary trigger and the use of Amazon SES for notification, which is not the preferred service for this use case.\n\nD. This option uses Kinesis Data Analytics, which is a valid choice, but it also introduces an unnecessary containerized service in Amazon ECS and the use of Amazon SES for notification, which is not the preferred service for this use case."
  },
  "405": {
    "question": "A company is preparing to deploy an Amazon Elastic Kubernetes Service (Amazon EKS) cluster for a workload. The company expects the cluster to support an unpredictable number of stateless pods. Many of the pods will be created during a short time period as the workload automatically scales the number of replicas that the workload uses.Which solution will MAXIMIZE node resilience?",
    "choices": [
      "A. Use a separate launch template to deploy the EKS control plane into a second cluster that is separate from the workload node groups.",
      "B. Update the workload node groups. Use a smaller number of node groups and larger instances in the node groups.",
      "C. Configure the Kubernetes Cluster Autoscaler to ensure that the compute capacity of the workload node groups stays underprovisioned.",
      "D. Configure the workload to use topology spread constraints that are based on Availability Zone."
    ],
    "answer": "D",
    "explanation": "1. Why the correct answer (D) is right:\n\nThe correct answer is option D, which is to configure the workload to use topology spread constraints that are based on Availability Zone. This solution will maximize node resilience because it ensures that the pods are distributed across different Availability Zones (AZs) within the Amazon EKS cluster. This way, if one AZ experiences an outage or high resource utilization, the other AZs can continue to handle the workload, minimizing the impact on the overall application.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Use a separate launch template to deploy the EKS control plane into a second cluster that is separate from the workload node groups:\nThis solution does not address the resilience of the nodes, as it focuses on the control plane rather than the workload node groups. Separating the control plane from the workload node groups does not necessarily improve node resilience.\n\nB. Update the workload node groups. Use a smaller number of node groups and larger instances in the node groups:\nWhile using larger instances can provide more resources per node, it does not guarantee improved node resilience. If the workload experiences a sudden increase in demand, a smaller number of larger node groups may not be able to scale quickly enough to handle the increased load, leading to potential service disruptions.\n\nC. Configure the Kubernetes Cluster Autoscaler to ensure that the compute capacity of the workload node groups stays underprovisioned:\nWhile this approach can help maintain a buffer of available resources, it does not directly address the resilience of the nodes themselves. An underprovisioned cluster may still be vulnerable to issues in a single Availability Zone, as the remaining resources may not be sufficient to handle the entire workload."
  },
  "406": {
    "question": "A company needs to implement a disaster recovery (DR) plan for a web application. The application runs in a single AWS Region.The application uses microservices that run in containers. The containers are hosted on AWS Fargate in Amazon Elastic Container Service (Amazon ECS). The application has an Amazon RDS for MySQL DB instance as its data layer and uses Amazon Route 53 for DNS resolution. An Amazon CloudWatch alarm invokes an Amazon EventBridge rule if the application experiences a failure.A solutions architect must design a DR solution to provide application recovery to a separate Region. The solution must minimize the time that is necessary to recover from a failure.Which solution will meet these requirements?",
    "choices": [
      "A. Setup a second ECS cluster and ECS service on Fargate in the separate Region. Create an AWS Lambda function to perform the following actions: take a snapshot of the RDS DB instance, copy the snapshot to the separate Region, create a new RDS DB instance from the snapshot, and update Route 53 to route traffic to the second ECS cluster. Update the EventBridge rule to add a target that will invoke the Lambda function.",
      "B. Create an AWS Lambda function that creates a second ECS cluster and ECS service in the separate Region. Configure the Lambda function to perform the following actions: take a snapshot of the RDS DB instance, copy the snapshot to the separate Region, create a new RDS DB instance from the snapshot, and update Route 53 to route traffic to the second ECS cluster. Update the EventBridge rule to add a target that will invoke the Lambda function.",
      "C. Setup a second ECS cluster and ECS service on Fargate in the separate Region. Create a cross-Region read replica of the RDS DB instance in the separate Region. Create an AWS Lambda function to promote the read replica to the primary database. Configure the Lambda function to update Route 53 to route traffic to the second ECS cluster. Update the EventBridge rule to add a target that will invoke the Lambda function.",
      "D. Setup a second ECS cluster and ECS service on Fargate in the separate Region. Take a snapshot of the RDS DB instance. Convert the snapshot to an Amazon DynamoDB global table. Create an AWS Lambda function to update Route 53 to route traffic to the second ECS cluster. Update the EventBridge rule to add a target that will invoke the Lambda function."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C because it provides the most suitable and efficient disaster recovery (DR) solution for the given requirements.\n\nKey points:\n\n- Setting up a second ECS cluster and ECS service in a separate AWS Region ensures that the application infrastructure is available in the secondary Region, allowing for rapid failover.\n- Creating a cross-Region read replica of the RDS DB instance in the separate Region provides an up-to-date copy of the data, which can be quickly promoted to the primary database in case of a failure.\n- The AWS Lambda function is used to automate the failover process, including promoting the read replica to the primary database and updating Route 53 to route traffic to the secondary Region. This minimizes the time required for recovery and reduces manual intervention.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This solution relies on creating a snapshot of the RDS DB instance and copying it to the separate Region. This approach is slower and less efficient than using a read replica, as it requires creating a new RDS instance from the snapshot, which can take longer to become fully synchronized.\n\nB. Similar to option A, this solution also relies on creating a snapshot of the RDS DB instance and copying it to the separate Region. The additional step of creating a new ECS cluster and service in the separate Region is not necessary, as option C already includes this step.\n\nD. Converting the RDS DB snapshot to an Amazon DynamoDB global table is not a straightforward or recommended approach for this use case. DynamoDB is a NoSQL database, and it may not be a direct replacement for the MySQL database used in the application. Additionally, this approach would require significant application changes to use DynamoDB instead of the existing RDS DB instance."
  },
  "407": {
    "question": "A company has AWS accounts that are in an organization in AWS Organizations. The company wants to track Amazon EC2 usage as a metric. The company\u2019s architecture team must receive a daily alert if the EC2 usage is more than 10% higher the average EC2 usage from the last 30 days.Which solution will meet these requirements?",
    "choices": [
      "A. Configure AWS Budgets in the organization's management account. Specify a usage type of EC2 running hours. Specify a daily period. Set the budget amount to be 10% more than the reported average usage for the last 30 days from AWS Cost Explorer. Configure an alert to notify the architecture team if the usage threshold is met",
      "B. Configure AWS Cost Anomaly Detection in the organization's management account. Configure a monitor type of AWS Service. Apply a filter of Amazon EC2. Configure an alert subscription to notify the architecture team if the usage is 10% more than the average usage for the last 30 days.",
      "C. Enable AWS Trusted Advisor in the organization's management account. Configure a cost optimization advisory alert to notify the architecture team if the EC2 usage is 10% more than the reported average usage for the last 30 days.",
      "D. Configure Amazon Detective in the organization's management account. Configure an EC2 usage anomaly alert to notify the architecture team if Detective identifies a usage anomaly of more than 10%."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\n\nThe correct answer is A. Configure AWS Budgets in the organization's management account.\n\nThis solution meets all the requirements:\n\n- It tracks EC2 usage over time by using the \"usage type\" of EC2 running hours.\n- It calculates the average usage for the last 30 days using AWS Cost Explorer data.\n- It sets a budget amount that is 10% more than the reported average usage for the last 30 days.\n- It configures an alert to notify the architecture team if the usage threshold is met.\n\nAWS Budgets allows you to set custom budgets and receive alerts when your costs or usage exceed the budgeted amount. This makes it the ideal choice for the given scenario, where the requirement is to monitor EC2 usage and trigger an alert when it exceeds a specific threshold.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. Configure AWS Cost Anomaly Detection: This option is incorrect because Cost Anomaly Detection is primarily focused on detecting unexpected cost increases, not specifically monitoring usage metrics like EC2 running hours.\n\nC. Enable AWS Trusted Advisor: This option is incorrect because Trusted Advisor is a service that provides recommendations to optimize your AWS environment, but it does not provide the specific monitoring and alerting functionality required in the scenario.\n\nD. Configure Amazon Detective: This option is incorrect because Amazon Detective is a security and investigation service that analyzes data from multiple AWS services to identify potential security issues. It does not provide the usage monitoring and alerting capabilities needed in this scenario.\n\nIn summary, the correct answer is A because it allows the organization to set a custom budget for EC2 usage, calculate the average usage for the last 30 days, and receive an alert when the usage exceeds the defined threshold, which aligns with the requirements of the question."
  },
  "408": {
    "question": "An e-commerce company is revamping its IT infrastructure and is planning to use AWS services. The company\u2019s CIO has asked a solutions architect to design a simple, highly available, and loosely coupled order processing application. The application is responsible for receiving and processing orders before storing them in an Amazon DynamoDB table. The application has a sporadic traffic pattern and should be able to scale during marketing campaigns to process the orders with minimal delays.Which of the following is the MOST reliable approach to meet the requirements?",
    "choices": [
      "A. Receive the orders in an Amazon EC2-hosted database and use EC2 instances to process them.",
      "B. Receive the orders in an Amazon SQS queue and invoke an AWS Lambda function to process them.",
      "C. Receive the orders using the AWS Step Functions program and launch an Amazon ECS container to process them.",
      "D. Receive the orders in Amazon Kinesis Data Streams and use Amazon EC2 instances to process them."
    ],
    "answer": "B",
    "explanation": "1. Explanation of why the correct answer (option B) is the right approach:\n\nOption B is the most reliable approach to meet the requirements. By using an Amazon SQS queue to receive the orders and an AWS Lambda function to process them, the solution provides the following benefits:\n\n- Loose coupling: The use of SQS queue and Lambda function decouples the order processing from the storage (Amazon DynamoDB), allowing the components to be scaled and maintained independently.\n- Scalability: AWS Lambda can automatically scale up or down based on the incoming request load, ensuring that orders are processed quickly even during peak traffic periods, such as marketing campaigns.\n- High availability: The SQS queue acts as a buffer, storing orders temporarily until they can be processed. This ensures that no orders are lost due to system failures or temporary spikes in traffic.\n- Serverless architecture: The serverless nature of AWS Lambda eliminates the need to manage and scale EC2 instances, simplifying the infrastructure and reducing operational overhead.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Receive the orders in an Amazon EC2-hosted database and use EC2 instances to process them:\n   - This approach requires manual scaling and management of the EC2 instances, which may not be able to handle the sporadic traffic patterns effectively.\n   - The tight coupling between the order processing and the database storage makes the architecture less flexible and more complex to maintain.\n\nC. Receive the orders using the AWS Step Functions program and launch an Amazon ECS container to process them:\n   - While Step Functions can provide workflow orchestration, it adds an extra layer of complexity that may not be necessary for this simple order processing application.\n   - Launching Amazon ECS containers requires more infrastructure management compared to the serverless approach of using Lambda functions.\n\nD. Receive the orders in Amazon Kinesis Data Streams and use Amazon EC2 instances to process them:\n   - Kinesis Data Streams is more suitable for real-time data processing, while the order processing requirement does not seem to have strict real-time constraints.\n   - Similar to option A, the use of EC2 instances introduces the need for manual scaling and management, which may not be the most reliable approach for this use case."
  },
  "409": {
    "question": "A company is deploying AWS Lambda functions that access an Amazon RDS for PostgreSQL database. The company needs to launch the Lambda functions in a QA environment and in a production environment.The company must not expose credentials within application code and must rotate passwords automatically.Which solution will meet these requirements?",
    "choices": [
      "A. Store the database credentials for both environments in AWS Systems Manager Parameter Store. Encrypt the credentials by using an AWS Key Management Service (AWS KMS) key. Within the application code of the Lambda functions, pull the credentials from the Parameter Store parameter by using the AWS SDK for Python (Boto3). Add a role to the Lambda functions to provide access to the Parameter Store parameter.",
      "B. Store the database credentials for both environments in AWS Secrets Manager with distinct key entry for the QA environment and the production environment. Turn on rotation. Provide a reference to the Secrets Manager key as an environment variable for the Lambda functions.",
      "C. Store the database credentials for both environments in AWS Key Management Service (AWS KMS). Turn on rotation. Provide a reference to the credentials that are stored in AWS KMS as an environment variable for the Lambda functions.",
      "D. Create separate S3 buckets for the QA environment and the production environment. Turn on server-side encryption with AWS KMS keys (SSE-KMS) for the S3 buckets. Use an object naming pattern that gives each Lambda function\u2019s application code the ability to pull the correct credentials for the function's corresponding environment. Grant each Lambda function's execution role access to Amazon S3."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B because it meets all the requirements specified in the question:\n\n- It stores the database credentials for both the QA and production environments in AWS Secrets Manager, which provides a secure way to store and manage sensitive information like passwords.\n- AWS Secrets Manager allows you to create distinct secret entries for the QA and production environments, ensuring that the credentials are isolated and not exposed across environments.\n- Secrets Manager also provides automatic password rotation, which addresses the requirement of rotating passwords automatically without the need to update the application code.\n- By providing a reference to the Secrets Manager key as an environment variable for the Lambda functions, the application code does not need to directly handle the credentials, which meets the requirement of not exposing credentials within the application code.\n\n2. Explanations of the incorrect answers:\n\nA. This option uses AWS Systems Manager Parameter Store to store the credentials, which lacks the built-in support for automatic rotation of secrets that Secrets Manager provides.\n\nC. Storing the credentials in AWS KMS is not the best solution, as KMS is primarily designed for managing cryptographic keys and does not provide the same level of functionality for storing and rotating secrets as Secrets Manager.\n\nD. Using separate S3 buckets to store the credentials is not the most appropriate solution, as it would require additional management and custom logic within the application code to retrieve the correct credentials for each environment.\n\nIn summary, the correct answer is B because it leverages the features of AWS Secrets Manager, which allows for secure storage, automatic rotation, and seamless integration with the Lambda functions, without the need to expose the credentials within the application code."
  },
  "410": {
    "question": "A company is using AWS Control Tower to manage AWS accounts in an organization in AWS Organizations. The company has an OU that contains accounts. The company must prevent any new or existing Amazon EC2 instances in the OU's accounts from gaining a public IP address.Which solution will meet these requirements?",
    "choices": [
      "A. Configure all instances in each account in the OU to use AWS Systems Manager. Use a Systems Manager Automation runbook to prevent public IP addresses from being attached to the instances.",
      "B. Implement the AWS Control Tower proactive control to check whether instances in the OU's accounts have a public IP address. Set the AssociatePublicIpAddress property to False. Attach the proactive control to the OU.",
      "C. Create an SCP that prevents the launch of instances that have a public IP address. Additionally, configure the SCP to prevent the attachment of a public IP address to existing instances. Attach the SCP to the OU.",
      "D. Create an AWS Config custom rule that detects instances that have a public IP address. Configure a remediation action that uses an AWS Lambda function to detach the public IP addresses from the instances."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is Option C: Create an SCP that prevents the launch of instances that have a public IP address. Additionally, configure the SCP to prevent the attachment of a public IP address to existing instances. Attach the SCP to the OU.\n\nThis solution is effective because:\n- Service Control Policies (SCPs) in AWS Organizations allow you to impose restrictions on the actions that can be performed in the accounts within an Organizational Unit (OU).\n- By creating an SCP that prohibits the launch of instances with a public IP address and the attachment of a public IP address to existing instances, you can effectively enforce the requirement of preventing any new or existing EC2 instances in the OU's accounts from gaining a public IP address.\n- Attaching the SCP to the OU ensures that the policy is applied to all the accounts within that OU.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This solution using AWS Systems Manager is incorrect because:\n   - It does not directly prevent the assignment of public IP addresses to EC2 instances. Instead, it relies on a Systems Manager Automation runbook to manage the instances, which may not be as effective as a direct policy-based approach.\n\nB. This solution using the AWS Control Tower proactive control is incorrect because:\n   - The AWS Control Tower proactive controls are implemented using AWS CloudFormation, which means they may not be able to effectively control actions performed directly through the AWS CLI, AWS API, or the AWS console.\n   - Modifying the existing proactive controls created by AWS Control Tower could result in a state drift, which is not recommended by AWS.\n\nD. This solution using an AWS Config custom rule is incorrect because:\n   - It relies on a detection and remediation approach, rather than a preventive approach. This means that instances could still be launched with a public IP address, and the remediation action would need to be executed to detach the public IP address.\n   - The remediation action using an AWS Lambda function may not be as reliable or scalable as a policy-based approach."
  },
  "411": {
    "question": "A company is deploying a third-party web application on AWS. The application is packaged as a Docker image. The company has deployed the Docker image as an AWS Fargate service in Amazon Elastic Container Service (Amazon ECS). An Application Load Balancer (ALB) directs traffic to the application.The company needs to give only a specific list of users the ability to access the application from the internet. The company cannot change the application and cannot integrate the application with an identity provider. All users must be authenticated through multi-factor authentication (MFA).Which solution will meet these requirements?",
    "choices": [
      "A. Create a user pool in Amazon Cognito. Configure the pool for the application. Populate the pool with the required users. Configure the pool to require MFConfigure a listener rule on the ALB to require authentication through the Amazon Cognito hosted UI.",
      "B. Configure the users in AWS Identity and Access Management (IAM). Attach a resource policy to the Fargate service to require users to use MFA. Configure a listener rule on the ALB to require authentication through IAM.",
      "C. Configure the users in AWS Identity and Access Management (IAM). Enable AWS IAM Identity Center (AWS Single Sign-On). Configure resource protection for the ALB. Create a resource protection rule to require users to use MFA.",
      "D. Create a user pool in AWS Amplify. Configure the pool for the application. Populate the pool with the required users. Configure the pool to require MFA. Configure a listener rule on the ALB to require authentication through the Amplify hosted UI."
    ],
    "answer": "A",
    "explanation": "1. Clear explanation of why the correct answer (A) is right:\n\nThe correct answer is A because it fully addresses the given requirements:\n\n- The application is deployed as a Docker image on AWS Fargate, so it cannot be integrated with an external identity provider.\n- The company needs to provide access to a specific list of users and require multi-factor authentication (MFA) for all users.\n- By creating an Amazon Cognito user pool and configuring the Application Load Balancer (ALB) to require authentication through the Cognito hosted UI, the solution meets all the requirements:\n  - The Cognito user pool allows the company to define and manage the list of authorized users.\n  - Configuring the user pool to require MFA ensures that all users must provide an additional authentication factor beyond just their username and password.\n  - The ALB listener rule that requires authentication through the Cognito hosted UI ensures that all users are properly authenticated before accessing the application.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nB. This solution using IAM users and a resource policy on the Fargate service would not provide the required MFA functionality, as IAM does not natively support MFA for user authentication.\n\nC. This solution using IAM Identity Center (AWS SSO) would also not provide the required MFA functionality, as IAM Identity Center does not have a built-in MFA requirement option.\n\nD. This solution using AWS Amplify is incorrect because Amplify is a front-end development framework, not a user authentication service like Cognito. Amplify does not have the necessary capabilities to manage a user pool and enforce MFA."
  },
  "412": {
    "question": "A solutions architect is preparing to deploy a new security tool into several previously unused AWS Regions. The solutions architect will deploy the tool by using an AWS CloudFormation stack set. The stack set's template contains an IAM role that has a custom name. Upon creation of the stack set, no stack instances are created successfully.What should the solutions architect do to deploy the stacks successfully?",
    "choices": [
      "A. Enable the new Regions in all relevant accounts. Specify the CAPABILITY_NAMED_IAM capability during the creation of the stack set.",
      "B. Use the Service Quotas console to request a quota increase for the number of CloudFormation stacks in each new Region in all relevant accounts. Specify the CAPABILITY_IAM capability during the creation of the stack set.",
      "C. Specify the CAPABILITY_NAMED_IAM capability and the SELF_MANAGED permissions model during the creation of the stack set.",
      "D. Specify an administration role ARN and the CAPABILITY_IAM capability during the creation of the stack set."
    ],
    "answer": "A",
    "explanation": "1. Clear explanation of why the correct answer (A) is right:\n\nThe correct answer is A because:\n\n- The question states that the solutions architect is deploying the stack set to \"several previously unused AWS Regions.\" This means the new Regions need to be enabled in the relevant accounts before the stack set can be deployed successfully.\n- The CloudFormation stack set template contains an IAM role with a custom name. This requires the CAPABILITY_NAMED_IAM capability to be specified during the stack set creation, as the template includes resources that can affect permissions in the AWS account.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB is incorrect because:\n- Requesting a quota increase for the number of CloudFormation stacks is not the issue here. The problem is related to the new Regions not being enabled and the need to specify the CAPABILITY_NAMED_IAM capability.\n\nC is incorrect because:\n- While specifying the CAPABILITY_NAMED_IAM capability is correct, the SELF_MANAGED permissions model is not necessary. The question does not indicate that the solutions architect needs to create the IAM roles required by the StackSet.\n\nD is incorrect because:\n- Specifying an administration role ARN and the CAPABILITY_IAM capability is not the correct solution. The issue is specifically related to the new Regions not being enabled and the need for the CAPABILITY_NAMED_IAM capability."
  },
  "413": {
    "question": "A company hosts a software as a service (SaaS) solution on AWS. The solution has an Amazon API Gateway API that serves an HTTPS endpoint. The API uses AWS Lambda functions for compute. The Lambda functions store data in an Amazon Aurora Serverless v1 database.The company used the AWS Serverless Application Model (AWS SAM) to deploy the solution. The solution extends across multiple Availability Zones and has no disaster recovery (DR) plan.A solutions architect must design a DR strategy that can recover the solution in another AWS Region. The solution has an RTO of 5 minutes and an RPO of 1 minute.What should the solutions architect do to meet these requirements?",
    "choices": [
      "A. Create a read replica of the Aurora Serverless v1 database in the target Region. Use AWS SAM to create a runbook to deploy the solution to the target Region. Promote the read replica to primary in case of disaster.",
      "B. Change the Aurora Serverless v1 database to a standard Aurora MySQL global database that extends across the source Region and the target Region. Use AWS SAM to create a runbook to deploy the solution to the target Region.",
      "C. Create an Aurora Serverless v1 DB cluster that has multiple writer instances in the target Region. Launch the solution in the target Region. Configure the two Regional solutions to work in an active-passive configuration.",
      "D. Change the Aurora Serverless v1 database to a standard Aurora MySQL global database that extends across the source Region and the target Region. Launch the solution in the target Region. Configure the two Regional solutions to work in an active-passive configuration."
    ],
    "answer": "D",
    "explanation": "1. Clear explanation of the correct answer (D):\n\nThe correct answer is D because it addresses the key requirements of the problem:\n\n- RTO of 5 minutes and RPO of 1 minute\n- Ability to recover the solution in another AWS Region\n\nChanging the Aurora Serverless v1 database to a standard Aurora MySQL global database that extends across the source and target Regions, and then launching the solution in the target Region, allows for fast recovery and near-real-time data replication. The active-passive configuration ensures that the secondary Region can take over in case of a disaster, meeting the strict RTO and RPO requirements.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Incorrect because Aurora Serverless v1 does not support read replicas, so creating a read replica in the target Region is not possible. Additionally, promoting a read replica to primary in case of disaster would not meet the 5-minute RTO requirement.\n\nB. Incorrect because while the solution can be deployed to the target Region using AWS SAM, the RTO and RPO requirements would not be met. The runbook deployment alone does not provide the necessary data replication and automatic failover capabilities.\n\nC. Incorrect because Aurora Serverless v1 does not support multiple writer instances across Regions. The active-passive configuration would also introduce additional complexity and manual intervention, which would not meet the strict RTO requirement."
  },
  "414": {
    "question": "A retail company is mounting IoT sensors in all of its stores worldwide. During the manufacturing of each sensor, the company\u2019s private certificate authority (CA) issues an X.509 certificate that contains a unique serial number. The company then deploys each certificate to its respective sensor.A solutions architect needs to give the sensors the ability to send data to AWS after they are installed. Sensors must not be able to send data to AWS until they are installed.Which solution will meet these requirements?",
    "choices": [
      "A. Create an AWS Lambda function that can validate the serial number. Create an AWS IoT Core provisioning template. Include the SerialNumber parameter in the Parameters section. Add the Lambda function as a pre-provisioning hook. During manufacturing, call the RegisterThing API operation and specify the template and parameters.",
      "B. Create an AWS Step Functions state machine that can validate the serial number. Create an AWS IoT Core provisioning template. Include the SerialNumber parameter in the Parameters section. Specify the Step Functions state machine to validate parameters. Call the StartThingRegistrationTask API operation during installation.",
      "C. Create an AWS Lambda function that can validate the serial number. Create an AWS IoT Core provisioning template. Include the SerialNumber parameter in the Parameters section. Add the Lambda function as a pre-provisioning hook. Register the CA with AWS IoT Core, specify the provisioning template, and set the allow-auto-registration parameter.",
      "D. Create an AWS IoT Core provisioning template. Include the SerialNumber parameter in the Parameters section. Include parameter validation in the template. Provision a claim certificate and a private key for each device that uses the CA. Grant AWS IoT Core service permissions to update AWS IoT things during provisioning."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C because it aligns best with the requirements stated in the question. The key requirements are:\n- Sensors must not be able to send data to AWS until they are installed.\n- The solution should be able to validate the serial number of each sensor.\n\nOption C mentions creating an AWS Lambda function to validate the serial number, and then using that Lambda function as a pre-provisioning hook in the AWS IoT Core provisioning template. This allows the company to control the provisioning process and ensure that the sensors can only communicate with AWS IoT Core after they have been validated and installed.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option uses a Lambda function to validate the serial number, which is correct. However, it mentions using the RegisterThing API operation, which would allow the sensors to be registered with AWS IoT Core before installation, violating the requirement that sensors must not be able to send data to AWS until they are installed.\n\nB. This option uses an AWS Step Functions state machine to validate the serial number, which is a valid approach. However, it mentions using the StartThingRegistrationTask API operation, which would also allow the sensors to be registered with AWS IoT Core before installation, violating the requirement.\n\nD. This option does not mention any validation of the serial number, which is a key requirement. It also mentions granting AWS IoT Core service permissions to update AWS IoT things during provisioning, which could potentially allow sensors to be provisioned before installation."
  },
  "415": {
    "question": "A startup company recently migrated a large ecommerce website to AWS. The website has experienced a 70% increase in sales. Software engineers are using a private GitHub repository to manage code. The DevOps team is using Jenkins for builds and unit testing. The engineers need to receive notifications for bad builds and zero downtime during deployments. The engineers also need to ensure any changes to production are seamless for users and can be rolled back in the event of a major issue.The software engineers have decided to use AWS CodePipeline to manage their build and deployment process.Which solution will meet these requirements?",
    "choices": [
      "A. Use GitHub websockets to trigger the CodePipeline pipeline. Use the Jenkins plugin for AWS CodeBuild to conduct unit testing. Send alerts to an Amazon SNS topic for any bad builds. Deploy in an in-place, all-at-once deployment configuration using AWS CodeDeploy.",
      "B. Use GitHub webhooks to trigger the CodePipeline pipeline. Use the Jenkins plugin for AWS CodeBuild to conduct unit testing. Send alerts to an Amazon SNS topic for any bad builds. Deploy in a blue/green deployment using AWS CodeDeploy.",
      "C. Use GitHub websockets to trigger the CodePipeline pipeline. Use AWS X-Ray for unit testing and static code analysis. Send alerts to an Amazon SNS topic for any bad builds. Deploy in a blue/green deployment using AWS CodeDeploy.",
      "D. Use GitHub webhooks to trigger the CodePipeline pipeline. Use AWS X-Ray for unit testing and static code analysis. Send alerts to an Amazon SNS topic for any bad builds. Deploy in an in-place, all-at-once deployment configuration using AWS CodeDeploy."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B because it best meets the requirements stated in the question:\n\n- Use GitHub webhooks to trigger the CodePipeline pipeline: This ensures that the pipeline is automatically triggered whenever changes are pushed to the GitHub repository, providing a smooth integration between the code repository and the deployment pipeline.\n\n- Use the Jenkins plugin for AWS CodeBuild to conduct unit testing: Since the question states that the DevOps team is already using Jenkins for builds and unit testing, integrating Jenkins with AWS CodeBuild through the plugin is the most logical choice to leverage the existing setup.\n\n- Send alerts to an Amazon SNS topic for any bad builds: Sending alerts to an SNS topic ensures that the software engineers receive notifications about any failed builds, allowing them to take immediate action.\n\n- Deploy in a blue/green deployment using AWS CodeDeploy: This deployment strategy enables zero-downtime deployments and seamless rollbacks in the event of a major issue, as required by the question.\n\n2. Explanations of the incorrect choices:\n\nA. This solution uses GitHub websockets instead of webhooks, which is less common and may not be as reliable. Additionally, it uses an in-place, all-at-once deployment configuration with AWS CodeDeploy, which does not provide the zero-downtime and rollback capabilities required.\n\nC. This solution uses GitHub websockets instead of webhooks, and it uses AWS X-Ray for unit testing and static code analysis, which is not the most appropriate choice for the given requirement of using Jenkins for unit testing.\n\nD. This solution uses GitHub webhooks to trigger the pipeline, which is correct. However, it uses an in-place, all-at-once deployment configuration with AWS CodeDeploy, which does not meet the requirement for zero-downtime deployments and seamless rollbacks."
  },
  "416": {
    "question": "A software as a service (SaaS) company has developed a multi-tenant environment. The company uses Amazon DynamoDB tables that the tenants share for the storage layer. The company uses AWS Lambda functions for the application services.The company wants to offer a tiered subscription model that is based on resource consumption by each tenant. Each tenant is identified by a unique tenant ID that is sent as part of each request to the Lambda functions. The company has created an AWS Cost and Usage Report (AWS CUR) in an AWS account. The company wants to allocate the DynamoDB costs to each tenant to match that tenant's resource consumption.Which solution will provide a granular view of the DynamoDB cost for each tenant with the LEAST operational effort?",
    "choices": [
      "A. Associate a new tag that is named tenant ID with each table in DynamoDB. Activate the tag as a cost allocation tag in the AWS Billing and Cost Management console. Deploy new Lambda function code to log the tenant ID in Amazon CloudWatch Logs. Use the AWS CUR to separate DynamoDB consumption cost for each tenant ID.",
      "B. Configure the Lambda functions to log the tenant ID and the number of RCUs and WCUs consumed from DynamoDB for each transaction to Amazon CloudWatch Logs. Deploy another Lambda function to calculate the tenant costs by using the logged capacity units and the overall DynamoDB cost from the AWS Cost Explorer API. Create an Amazon EventBridge rule to invoke the calculation Lambda function on a schedule.",
      "C. Create a new partition key that associates DynamoDB items with individual tenants. Deploy a Lambda function to populate the new column as part of each transaction. Deploy another Lambda function to calculate the tenant costs by using Amazon Athena to calculate the number of tenant items from DynamoDB and the overall DynamoDB cost from the AWS CUR. Create an Amazon EventBridge rule to invoke the calculation Lambda function on a schedule.",
      "D. Deploy a Lambda function to log the tenant ID, the size of each response, and the duration of the transaction call as custom metrics to Amazon CloudWatch Logs. Use CloudWatch Logs Insights to query the custom metrics for each tenant. Use AWS Pricing Calculator to obtain the overall DynamoDB costs and to calculate the tenant costs."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (Option B):\n\nOption B is the correct answer because it provides a granular view of the DynamoDB cost for each tenant with the least operational effort. Here's the explanation:\n\n- The Lambda functions are already logging the tenant ID and the number of RCUs and WCUs consumed from DynamoDB for each transaction to Amazon CloudWatch Logs. This provides the necessary data to calculate the tenant-specific DynamoDB costs.\n- The solution then deploys an additional Lambda function to calculate the tenant costs by using the logged capacity units and the overall DynamoDB cost from the AWS Cost Explorer API. This leverages existing data sources and minimizes additional operational overhead.\n- The use of an Amazon EventBridge rule to invoke the calculation Lambda function on a schedule ensures that the cost allocation is performed automatically, reducing manual effort.\n\nOverall, this solution utilizes the existing data sources (CloudWatch Logs and AWS Cost Explorer) and automates the cost calculation process, making it the least operationally intensive approach among the provided options.\n\n2. Explanations of why the other options are incorrect:\n\nOption A:\n- While tagging the DynamoDB tables with the tenant ID and using the AWS Billing and Cost Management console can provide a granular view of the costs, it requires additional manual effort to update the code and deploy the changes across the Lambda functions.\n- Logging the tenant ID in Amazon CloudWatch Logs is an additional step that increases the operational complexity compared to Option B.\n\nOption C:\n- Creating a new partition key to associate DynamoDB items with individual tenants requires modifying the data model and application logic, which increases the overall complexity and operational effort.\n- Deploying an additional Lambda function to populate the new column and another one to calculate the tenant costs using Amazon Athena introduces more moving parts, making the solution more complex to manage.\n\nOption D:\n- Logging the tenant ID, response size, and transaction duration as custom metrics to Amazon CloudWatch Logs requires additional instrumentation and code changes in the Lambda functions.\n- Using CloudWatch Logs Insights and the AWS Pricing Calculator to derive the tenant-specific DynamoDB costs is a more manual and less automated approach compared to Option B.\n\nIn summary, Option B provides the most granular view of the DynamoDB costs per tenant with"
  },
  "417": {
    "question": "A company has an application that stores data in a single Amazon S3 bucket. The company must keep all data for 1 year. The company\u2019s security team is concerned that an attacker could gain access to the AWS account through leaked long-term credentials.Which solution will ensure that existing and future objects in the S3 bucket are protected?",
    "choices": [
      "A. Create a new AWS account that is accessible only to the security team through an assumed role. Create an S3 bucket in the new account. Enable S3 Versioning and S3 Object Lock. Configure a default retention period of 1 year. Set up replication from the existing S3 bucket to the new S3 bucket. Create an S3 Batch Replication job to copy all existing data.",
      "B. Use the s3-bucket-versioning-enabled AWS Config managed rule. Configure an automatic remediation action that uses an AWS Lambda function to enable S3 Versioning and MFA Delete on noncompliant resources. Add an S3 Lifecycle rule to delete objects after 1 year.",
      "C. Explicitly deny bucket creation from all users and roles except for an AWS Service Catalog launch constraint role. Define a Service Catalog product for the creation of the S3 bucket to force S3 Versioning and MFA Delete to be enabled. Authorize users to launch the product when they need to create an S3 bucket.",
      "D. Enable Amazon GuardDuty with the S3 protection feature for the account and the AWS Region. Add an S3 Lifecycle rule to delete objects after 1 year."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the Correct Answer (A):\nThe correct answer is A because it provides the most comprehensive and effective solution to protect the existing and future objects in the S3 bucket. This approach has the following key advantages:\n\na. Isolation: By creating a new AWS account accessible only to the security team, it isolates the sensitive data from the main account, reducing the risk of unauthorized access even if the original account's credentials are compromised.\n\nb. S3 Versioning and Object Lock: Enabling S3 Versioning and S3 Object Lock on the new S3 bucket ensures that existing and future objects are protected against deletion or modification, even in the event of an attack.\n\nc. Replication: Setting up replication from the existing S3 bucket to the new S3 bucket, along with the S3 Batch Replication job, ensures that all existing data is securely copied to the new, protected environment.\n\nd. Retention: Configuring a default retention period of 1 year on the new S3 bucket satisfies the company's data retention requirement.\n\n2. Explanations of Incorrect Choices:\n\na. Choice B (Incorrect):\n   - Relying solely on AWS Config and automatic remediation may not be effective if an attacker gains access to the account and disables or modifies these configurations.\n   - The S3 Lifecycle rule to delete objects after 1 year does not provide the same level of protection as S3 Versioning and Object Lock.\n\nb. Choice C (Incorrect):\n   - Controlling bucket creation is important, but it does not address the protection of existing data or objects in the current bucket.\n   - Restricting bucket creation through Service Catalog may be cumbersome and limit the flexibility for the company to manage its S3 resources.\n\nc. Choice D (Incorrect):\n   - Amazon GuardDuty is a threat detection service and does not provide the same level of data protection as S3 Versioning and Object Lock.\n   - The S3 Lifecycle rule to delete objects after 1 year does not offer the same level of control and protection as the solution in Option A.\n\nIn summary, the correct answer (A) provides the most comprehensive and effective solution to"
  },
  "418": {
    "question": "A company needs to improve the security of its web-based application on AWS. The application uses Amazon CloudFront with two custom origins. The first custom origin routes requests to an Amazon API Gateway HTTP API. The second custom origin routes traffic to an Application Load Balancer (ALB). The application integrates with an OpenID Connect (OIDC) identity provider (IdP) for user management.A security audit shows that a JSON Web Token (JWT) authorizer provides access to the API. The security audit also shows that the ALB accepts requests from unauthenticated users.A solutions architect must design a solution to ensure that all backend services respond to only authenticated users.Which solution will meet this requirement?",
    "choices": [
      "A. Configure the ALB to enforce authentication and authorization by integrating the ALB with the IdP. Allow only authenticated users to access the backend services.",
      "B. Modify the CloudFront configuration to use signed URLs. Implement a permissive signing policy that allows any request to access the backend services.",
      "C. Create an AWS WAF web ACL that filters out unauthenticated requests at the ALB level. Allow only authenticated traffic to reach the backend services.",
      "D. Enable AWS CloudTrail to log all requests that come to the ALB. Create an AWS Lambda function to analyze the logs and block any requests that come from unauthenticated users."
    ],
    "answer": "A",
    "explanation": "1. Explanation of why the correct answer (A) is right:\n\nThe correct answer is A, which is to configure the ALB to enforce authentication and authorization by integrating the ALB with the OIDC IdP. This solution meets the requirement of ensuring that all backend services respond to only authenticated users for the following reasons:\n\n- It enforces authentication at the load balancer level, which is the correct place to handle this requirement. The ALB can be configured to integrate with the OIDC IdP to validate the user's identity and authorization before forwarding the request to the backend services.\n- It prevents unauthenticated requests from reaching the backend services. The ALB will reject any requests from users who are not authenticated, ensuring that only authenticated users can access the backend services.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. Modify the CloudFront configuration to use signed URLs: This would not solve the issue, as signed URLs only provide access control at the CloudFront level, not the backend services. Unauthenticated users could still access the backend services through the ALB.\n\nC. Create an AWS WAF web ACL to filter out unauthenticated requests at the ALB level: While this could work, it is a less optimal solution compared to directly integrating the ALB with the OIDC IdP. Using AWS WAF adds an additional layer of complexity and management overhead.\n\nD. Enable AWS CloudTrail to log requests and use a Lambda function to analyze and block unauthenticated requests: This is a reactive and complex solution, as it involves additional components (CloudTrail, Lambda) and does not provide the same level of control and security as the recommended solution in option A."
  },
  "419": {
    "question": "A company creates an AWS Control Tower landing zone to manage and govern a multi-account AWS environment. The company's security team will deploy preventive controls and detective controls to monitor AWS services across all the accounts. The security team needs a centralized view of the security state of all the accounts.Which solution will meet these requirements?",
    "choices": [
      "A. From the AWS Control Tower management account, use AWS CloudFormation StackSets to deploy an AWS Config conformance pack to all accounts in the organization.",
      "B. Enable Amazon Detective for the organization in AWS Organizations. Designate one AWS account as the delegated administrator for Detective.",
      "C. From the AWS Control Tower management account, deploy an AWS CloudFormation stack set that uses the automatic deployment option to enable Amazon Detective for the organization.",
      "D. Enable AWS Security Hub for the organization in AWS Organizations. Designate one AWS account as the delegated administrator for Security Hub."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (Option D):\n\nThe correct answer is Option D: Enable AWS Security Hub for the organization in AWS Organizations and designate one AWS account as the delegated administrator for Security Hub.\n\nThis solution meets the requirements because:\n\n- AWS Security Hub provides a centralized view of the security state across all accounts in the organization. It aggregates security findings from multiple AWS services and third-party security products, giving the security team a unified view of the security posture.\n\n- By enabling Security Hub at the organization level and designating a delegated administrator account, the security team can centrally manage and configure Security Hub across all member accounts in the organization. This allows for a consistent security monitoring and reporting experience.\n\n- The delegated administrator account can be used to centrally manage Security Hub configurations, security standards, and security controls across the entire organization.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option uses AWS Config conformance packs, which are useful for deploying and managing configurations across accounts, but they do not provide the centralized security monitoring and reporting capabilities required in the question.\n\nB. Amazon Detective is a security service that analyzes security data to identify potential threats, but it does not provide the centralized security view and management capabilities required in the question.\n\nC. This option also uses Amazon Detective, which does not meet the requirement for a centralized security monitoring and reporting solution.\n\nIn summary, Option D is the correct choice because it specifically addresses the requirement for a centralized view of the security state across all accounts, which can be achieved through the use of AWS Security Hub and the delegated administrator model."
  },
  "420": {
    "question": "A company that develops consumer electronics with offices in Europe and Asia has 60 TB of software images stored on premises in Europe. The company wants to transfer the images to an Amazon S3 bucket in the ap-northeast-1 Region. New software images are created daily and must be encrypted in transit. The company needs a solution that does not require custom development to automatically transfer all existing and new software images to Amazon S3.What is the next step in the transfer process?",
    "choices": [
      "A. Deploy an AWS DataSync agent and configure a task to transfer the images to the S3 bucket.",
      "B. Configure Amazon Kinesis Data Firehose to transfer the images using S3 Transfer Acceleration.",
      "C. Use an AWS Snowball device to transfer the images with the S3 bucket as the target.",
      "D. Transfer the images over a Site-to-Site VPN connection using the S3 API with multipart upload."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\n\nThe correct answer is A. Deploy an AWS DataSync agent and configure a task to transfer the images to the S3 bucket.\n\nAWS DataSync is a managed data transfer service that simplifies, automates, and accelerates moving data between on-premises storage systems and AWS storage services, including Amazon S3. It is the ideal solution for the given scenario for the following reasons:\n\n- DataSync can transfer large datasets, like the 60 TB of software images, without requiring any custom development.\n- It supports encryption in transit, which meets the company's requirement to have the images encrypted during transfer.\n- DataSync is designed for efficient, high-speed data transfers, achieving speeds of up to 10 Gbps, which is much faster than manual methods like the S3 API with multipart upload.\n- DataSync can handle the ongoing transfer of new software images created daily, automatically and without the need for additional maintenance.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. Configure Amazon Kinesis Data Firehose to transfer the images using S3 Transfer Acceleration:\nKinesis Data Firehose is a data ingestion service, not a data transfer service. It is typically used for streaming data, not for bulk data transfers like the 60 TB of software images. Additionally, Kinesis Data Firehose would require custom development, which the company wants to avoid.\n\nC. Use an AWS Snowball device to transfer the images with the S3 bucket as the target:\nAWS Snowball is a physical data transfer device, which is not suitable for the ongoing daily transfer of new software images. Snowball is better suited for one-time, large-scale data migrations, not for a continuous data transfer scenario.\n\nD. Transfer the images over a Site-to-Site VPN connection using the S3 API with multipart upload:\nWhile this approach would work, it would be significantly slower than using AWS DataSync. The S3 API with multipart upload requires more manual effort and is not optimized for large-scale, high-speed data transfers like DataSync is."
  },
  "421": {
    "question": "A company has a web application that uses Amazon API Gateway. AWS Lambda, and Amazon DynamoDB. A recent marketing campaign has increased demand. Monitoring software reports that many requests have significantly longer response times than before the marketing campaign.A solutions architect enabled Amazon CloudWatch Logs for API Gateway and noticed that errors are occurring on 20% of the requests. In CloudWatch, the Lambda function Throttles metric represents 1% of the requests and the Errors metric represents 10% of the requests. Application logs indicate that, when errors occur, there is a call to DynamoDB.What change should the solutions architect make to improve the current response times as the web application becomes more popular?",
    "choices": [
      "A. Increase the concurrency limit of the Lambda function.",
      "B. Implement DynamoDB auto scaling on the table.",
      "C. Increase the API Gateway throttle limit.",
      "D. Re-create the DynamoDB table with a better-partitioned primary index."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B. Implement DynamoDB auto scaling on the table.\n\nThe key issue here is that the application is experiencing errors and throttling on the DynamoDB table, which is causing longer response times. The CloudWatch metrics show that 20% of the requests are encountering errors, and 10% of the requests are seeing DynamoDB errors.\n\nImplementing DynamoDB auto scaling on the table will automatically adjust the provisioned read and write capacity based on the actual usage patterns. This will ensure that the table has enough capacity to handle the increased traffic from the marketing campaign, preventing throttling and errors on the DynamoDB side. Auto scaling will dynamically scale the table's capacity up and down as needed, maintaining optimal performance and responsiveness.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Increase the concurrency limit of the Lambda function:\nThis option does not address the root cause of the issue, which is the DynamoDB table being overwhelmed by the increased traffic. Increasing the Lambda concurrency limit may help with handling more concurrent requests, but it does not solve the underlying problem of DynamoDB throttling and errors.\n\nC. Increase the API Gateway throttle limit:\nIncreasing the API Gateway throttle limit may help in the short term, but it does not address the underlying issue with the DynamoDB table. The errors and throttling are happening at the DynamoDB level, so increasing the API Gateway throttle limit would only temporarily mask the problem without solving it.\n\nD. Re-create the DynamoDB table with a better-partitioned primary index:\nWhile re-partitioning the DynamoDB table may help in some cases, it is not the right solution here. The issue is not related to the table's partitioning, but rather the inability of the provisioned capacity to handle the increased traffic. Implementing DynamoDB auto scaling is a more appropriate solution to dynamically adjust the table's capacity based on the actual usage patterns."
  },
  "422": {
    "question": "A company has an application that has a web frontend. The application runs in the company's on-premises data center and requires access to file storage for critical data. The application runs on three Linux VMs for redundancy. The architecture includes a load balancer with HTTP request-based routing.The company needs to migrate the application to AWS as quickly as possible. The architecture on AWS must be highly available.Which solution will meet these requirements with the FEWEST changes to the architecture?",
    "choices": [
      "A. Migrate the application to Amazon Elastic Container Service (Amazon ECS) containers that use the Fargate launch type in three Availability Zones. Use Amazon S3 to provide file storage for all three containers. Use a Network Load Balancer to direct traffic to the containers.",
      "B. Migrate the application to Amazon EC2 instances in three Availability Zones. Use Amazon Elastic File System (Amazon EFS) for file storage. Mount the file storage on all three EC2 instances. Use an Application Load Balancer to direct traffic to the EC2 instances.",
      "C. Migrate the application to Amazon Elastic Kubernetes Service (Amazon EKS) containers that use the Fargate launch type in three Availability Zones. Use Amazon FSx for Lustre to provide file storage for all three containers. Use a Network Load Balancer to direct traffic to the containers.",
      "D. Migrate the application to Amazon EC2 instances in three AWS Regions. Use Amazon Elastic Block Store (Amazon EBS) for file storage. Enable Cross-Region Replication (CRR) for all three EC2 instances. Use an Application Load Balancer to direct traffic to the EC2 instances."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\nThe correct answer is option B, which involves migrating the application to Amazon EC2 instances in three Availability Zones, using Amazon Elastic File System (Amazon EFS) for file storage, and using an Application Load Balancer to direct traffic to the EC2 instances.\n\nThis solution meets the requirements with the fewest changes to the existing architecture:\n\n- Migrating the application to EC2 instances in three Availability Zones closely matches the existing on-premises setup, where the application runs on three Linux VMs for redundancy.\n- Using Amazon EFS for file storage provides a scalable and highly available file system that can be mounted on all three EC2 instances, mirroring the existing on-premises file storage access.\n- Replacing the existing load balancer with an AWS Application Load Balancer provides similar HTTP request-based routing capabilities, with the added benefits of improved performance and scalability.\n\nThis solution requires the least amount of changes to the existing application architecture, making it the most straightforward migration path to AWS while still meeting the high availability requirement.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This solution uses Amazon ECS with Fargate, which would require more changes to the application architecture, as the application would need to be containerized. Additionally, using Amazon S3 for file storage would require significant changes to the application's file access logic.\n\nC. This solution uses Amazon EKS with Fargate, which would require even more changes to the application architecture, as the application would need to be containerized and adapted to run on a Kubernetes-based platform. Additionally, using Amazon FSx for Lustre for file storage may not be the most straightforward solution, as it is optimized for high-performance computing workloads.\n\nD. This solution uses Amazon EC2 instances in multiple AWS Regions, which introduces additional complexity and latency for file access, as the instances would need to access the replicated data across Regions. Additionally, using Amazon EBS for file storage would not provide the same level of scalability and availability as Amazon EFS."
  },
  "423": {
    "question": "A company is planning to migrate an on-premises data center to AWS. The company currently hosts the data center on Linux-based VMware VMs. A solutions architect must collect information about network dependencies between the VMs. The information must be in the form of a diagram that details host IP addresses, hostnames, and network connection information.Which solution will meet these requirements?",
    "choices": [
      "A. Use AWS Application Discovery Service. Select an AWS Migration Hub home AWS Region. Install the AWS Application Discovery Agent on the on-premises servers for data collection. Grant permissions to Application Discovery Service to use the Migration Hub network diagrams.",
      "B. Use the AWS Application Discovery Service Agentless Collector for server data collection. Export the network diagrams from the AWS Migration Hub in .png format.",
      "C. Install the AWS Application Migration Service agent on the on-premises servers for data collection. Use AWS Migration Hub data in Workload Discovery on AWS to generate network diagrams.",
      "D. Install the AWS Application Migration Service agent on the on-premises servers for data collection. Export data from AWS Migration Hub in .csv format into an Amazon CloudWatch dashboard to generate network diagrams."
    ],
    "answer": "A",
    "explanation": "1. Clear explanation of why the correct answer (A) is right:\n\nThe correct answer is A because it meets all the requirements specified in the question:\n- It uses the AWS Application Discovery Service, which is designed for discovering and mapping on-premises infrastructure, including network dependencies between systems.\n- It requires installing the AWS Application Discovery Agent on the on-premises servers, which collects detailed information about network dependencies, including host IP addresses, hostnames, and network connection information.\n- It involves selecting an AWS Migration Hub home region, which ensures the discovery process can collect data from the on-premises environment and import it into the Migration Hub for visualization.\n- It requires granting permissions to the Application Discovery Service to use the Migration Hub network diagrams, which allows the service to create a visual representation of the network dependencies, as requested in the question.\n\n2. Brief explanations of why the incorrect choices are wrong:\n\nB. This option uses the AWS Application Discovery Service Agentless Collector, which can collect server data but cannot detect network dependencies between VMs. The question specifically requires information about network dependencies, which the Agentless Collector cannot provide.\n\nC. This option involves using the AWS Application Migration Service agent for data collection, which is not the appropriate service for the given requirements. The question asks for network dependency information, which the Application Migration Service does not provide.\n\nD. This option also uses the AWS Application Migration Service agent for data collection, which is not the right solution for the given requirements. Additionally, exporting data to a CloudWatch dashboard would not generate the required network diagrams, as requested in the question."
  },
  "424": {
    "question": "A company runs a software-as-a-service (SaaS) application on AWS. The application consists of AWS Lambda functions and an Amazon RDS for MySQL Multi-AZ database. During market events, the application has a much higher workload than normal. Users notice slow response times during the peak periods because of many database connections. The company needs to improve the scalable performance and availability of the database.Which solution meets these requirements?",
    "choices": [
      "A. Create an Amazon CloudWatch alarm action that triggers a Lambda function to add an Amazon RDS for MySQL read replica when resource utilization hits a threshold.",
      "B. Migrate the database to Amazon Aurora, and add a read replica. Add a database connection pool outside of the Lambda handler function.",
      "C. Migrate the database to Amazon Aurora, and add a read replica. Use Amazon Route 53 weighted records.",
      "D. Migrate the database to Amazon Aurora, and add an Aurora Replica. Configure Amazon RDS Proxy to manage database connection pools."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (D):\n\nThe correct answer is D - Migrate the database to Amazon Aurora, and add an Aurora Replica. Configure Amazon RDS Proxy to manage database connection pools.\n\nThis solution meets the requirements of the problem statement:\n\n- Migrating to Amazon Aurora provides a more scalable and highly available database solution compared to the existing Amazon RDS for MySQL setup.\n- Adding an Aurora Replica allows for offloading read-heavy workloads from the primary Aurora instance, improving the overall performance and availability of the database.\n- Configuring Amazon RDS Proxy to manage the database connection pools is a crucial component. RDS Proxy can handle connection management efficiently, preventing the database from being overwhelmed by many concurrent connections, which was causing the slow response times during peak periods.\n\n2. Explanations of why the other choices are incorrect:\n\nA. This solution, which involves creating a CloudWatch alarm to trigger a Lambda function to add a read replica, does not address the core issue of connection management. Adding a read replica alone may not be sufficient to handle the increased workload during peak periods.\n\nB. Migrating to Amazon Aurora and adding a read replica is a good step, but using a database connection pool outside of the Lambda handler function may not be as efficient as using Amazon RDS Proxy, which is designed specifically for connection management.\n\nC. Migrating to Amazon Aurora and adding a read replica is a good step, but using Amazon Route 53 weighted records is not the best solution to handle the connection management issue. Route 53 is more suitable for load balancing traffic across multiple endpoints, not for managing database connections."
  },
  "425": {
    "question": "A company is planning to migrate an application from on premises to the AWS Cloud. The company will begin the migration by moving the application\u2019s underlying data storage to AWS. The application data is stored on a shared file system on premises, and the application servers connect to the shared file system through SMB.A solutions architect must implement a solution that uses an Amazon S3 bucket for shared storage. Until the application is fully migrated and code is rewritten to use native Amazon S3 APIs, the application must continue to have access to the data through SMB. The solutions architect must migrate the application data to AWS to its new location while still allowing the on-premises application to access the data.Which solution will meet these requirements?",
    "choices": [
      "A. Create a new Amazon FSx for Windows File Server file system. Configure AWS DataSync with one location for the on-premises file share and one location for the new Amazon FSx file system. Create a new DataSync task to copy the data from the on-premises file share location to the Amazon FSx file system.",
      "B. Create an S3 bucket for the application. Copy the data from the on-premises storage to the S3 bucket.",
      "C. Deploy an AWS Server Migration Service (AWS SMS) VM to the on-premises environment. Use AWS SMS to migrate the file storage server from on premises to an Amazon EC2 instance.",
      "D. Create an S3 bucket for the application. Deploy a new AWS Storage Gateway file gateway on an on-premises VM. Create a new file share that stores data in the S3 bucket and is associated with the file gateway. Copy the data from the on-premises storage to the new file gateway endpoint."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (Option D):\n\nOption D is the correct answer because it meets all the requirements specified in the question:\n\n- It creates an S3 bucket for the application's data storage, as required.\n- It deploys an AWS Storage Gateway file gateway on an on-premises VM, which allows the on-premises application to continue accessing the data through SMB, even after the data is migrated to the S3 bucket.\n- It creates a new file share that stores data in the S3 bucket and is associated with the file gateway, providing a bridge between the on-premises file system and the S3 bucket.\n- It allows the data to be copied from the on-premises storage to the new file gateway endpoint, effectively migrating the data to the AWS cloud.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option uses an Amazon FSx for Windows File Server file system, which does not meet the requirement of using an S3 bucket for shared storage.\n\nB. This option simply copies the data from the on-premises storage to an S3 bucket, but it does not provide a way for the on-premises application to continue accessing the data through SMB, as required.\n\nC. This option uses AWS Server Migration Service (SMS) to migrate the file storage server from on-premises to an Amazon EC2 instance, but it does not provide a way for the on-premises application to continue accessing the data through SMB."
  },
  "426": {
    "question": "A global company has a mobile app that displays ticket barcodes. Customers use the tickets on the mobile app to attend live events. Event scanners read the ticket barcodes and call a backend API to validate the barcode data against data in a database. After the barcode is scanned, the backend logic writes to the database's single table to mark the barcode as used.The company needs to deploy the app on AWS with a DNS name of api.example.com. The company will host the database in three AWS Regions around the world.Which solution will meet these requirements with the LOWEST latency?",
    "choices": [
      "A. Host the database on Amazon Aurora global database clusters. Host the backend on three Amazon Elastic Container Service (Amazon ECS) clusters that are in the same Regions as the database. Create an accelerator in AWS Global Accelerator to route requests to the nearest ECS cluster. Create an Amazon Route 53 record that maps api.example.com to the accelerator endpoint",
      "B. Host the database on Amazon Aurora global database clusters. Host the backend on three Amazon Elastic Kubernetes Service (Amazon EKS) clusters that are in the same Regions as the database. Create an Amazon CloudFront distribution with the three clusters as origins. Route requests to the nearest EKS cluster. Create an Amazon Route 53 record that maps api.example.com to the CloudFront distribution.",
      "C. Host the database on Amazon DynamoDB global tables. Create an Amazon CloudFront distribution. Associate the CloudFront distribution with a CloudFront function that contains the backend logic to validate the barcodes. Create an Amazon Route 53 record that maps api.example.com to the CloudFront distribution.",
      "D. Host the database on Amazon DynamoDB global tables. Create an Amazon CloudFront distribution. Associate the CloudFront distribution with a Lambda@Edge function that contains the backend logic to validate the barcodes. Create an Amazon Route 53 record that maps api.example.com to the CloudFront distribution."
    ],
    "answer": "D",
    "explanation": "1. Explanation of why the correct answer (D) is right:\n\nThe correct answer is D because it leverages the strengths of AWS services to minimize latency for the use case described in the question.\n\nThe key aspects are:\n\n- Hosting the database on Amazon DynamoDB global tables: DynamoDB global tables provide a multi-Region, multi-master database solution, allowing low-latency access to the database from any of the Regions where it is replicated.\n\n- Using a CloudFront distribution with a Lambda@Edge function: The Lambda@Edge function can be used to implement the backend logic for validating the ticket barcodes. This allows the logic to be executed closer to the end-users, reducing latency, as the CloudFront distribution can route requests to the nearest edge location.\n\n- Mapping api.example.com to the CloudFront distribution using Amazon Route 53: This provides a single, global DNS endpoint for the application, with the CloudFront distribution and Lambda@Edge function handling the request routing and backend logic execution.\n\nThis solution minimizes latency by:\n- Storing the data in a globally distributed database (DynamoDB global tables)\n- Executing the backend logic at the edge, close to the end-users (Lambda@Edge)\n- Providing a single, low-latency DNS endpoint (Route 53 + CloudFront)\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This solution uses Aurora global database clusters and deploys the backend on ECS clusters. While Aurora global databases provide low-latency read access, the write operations (marking barcodes as used) would still be centralized in a single Region, which could introduce higher latency for some users.\n\nB. This solution uses Aurora global database clusters and deploys the backend on EKS clusters. Similar to choice A, the centralized write operations could introduce higher latency. Additionally, using CloudFront as the routing mechanism may not be the optimal choice, as CloudFront is primarily designed for content delivery, not low-latency API requests.\n\nC. This solution uses DynamoDB global tables and a CloudFront distribution with a CloudFront Function. CloudFront Functions are limited to modifying the request and response headers, whereas the use case requires more complex backend logic to validate the ticket"
  },
  "427": {
    "question": "A medical company is running a REST API on a set of Amazon EC2 instances. The EC2 instances run in an Auto Scaling group behind an Application Load Balancer (ALB). The ALB runs in three public subnets, and the EC2 instances run in three private subnets. The company has deployed an Amazon CloudFront distribution that has the ALB as the only origin.Which solution should a solutions architect recommend to enhance the origin security?",
    "choices": [
      "A. Store a random string in AWS Secrets Manager. Create an AWS Lambda function for automatic secret rotation. Configure CloudFront to inject the random string as a custom HTTP header for the origin request. Create an AWS WAF web ACL rule with a string match rule for the custom header. Associate the web ACL with the ALB.",
      "B. Create an AWS WAF web ACL rule with an IP match condition of the CloudFront service IP address ranges. Associate the web ACL with the ALMove the ALB into the three private subnets.",
      "C. Store a random string in AWS Systems Manager Parameter Store. Configure Parameter Store automatic rotation for the string. Configure CloudFront to inject the random string as a custom HTTP header for the origin request. Inspect the value of the custom HTTP header, and block access in the ALB.",
      "D. Configure AWS Shield Advanced Create a security group policy to allow connections from CloudFront service IP address ranges. Add the policy to AWS Shield Advanced, and attach the policy to the ALB."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\n\nThe correct answer is A because it provides the most comprehensive security solution to enhance the origin security of the medical company's REST API.\n\nThe key elements of this solution are:\n\nA. Storing a random string in AWS Secrets Manager: This ensures the sensitive string is securely stored and managed, with the ability to automatically rotate the secret periodically to enhance security.\n\nB. Creating an AWS Lambda function for automatic secret rotation: This automates the process of updating the secret, reducing the risk of manual errors or delays in secret rotation.\n\nC. Configuring CloudFront to inject the random string as a custom HTTP header for the origin request: This adds an additional layer of authentication, requiring the origin (ALB) to validate the presence and value of the custom header before allowing access.\n\nD. Creating an AWS WAF web ACL rule with a string match rule for the custom header: This inspects the incoming requests to the ALB and blocks any requests that do not contain the expected custom header value, effectively preventing unauthorized direct access to the origin.\n\nE. Associating the web ACL with the ALB: This ensures the security rules are applied at the edge of the network, protecting the origin from malicious traffic.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. This solution is incorrect because moving the ALB to private subnets would make the CloudFront traffic unreachable, defeating the purpose of using CloudFront as the origin.\n\nC. This solution is incorrect because using AWS Systems Manager Parameter Store instead of Secrets Manager does not provide the same level of security and automatic rotation capabilities.\n\nD. This solution is incorrect because it does not provide the same level of granular control and origin validation as the solution in A. Relying solely on AWS Shield Advanced and a security group policy is not as robust as the multi-layered approach in A."
  },
  "428": {
    "question": "To abide by industry regulations, a solutions architect must design a solution that will store a company's critical data in multiple public AWS Regions, including in the United States, where the company's headquarters is located. The solutions architect is required to provide access to the data stored in AWS to the company\u2019s global WAN network. The security team mandates that no traffic accessing this data should traverse the public internet.How should the solutions architect design a highly available solution that meets the requirements and is cost-effective?",
    "choices": [
      "A. Establish AWS Direct Connect connections from the company headquarters to all AWS Regions in use. Use the company WAN to send traffic over to the headquarters and then to the respective DX connection to access the data.",
      "B. Establish two AWS Direct Connect connections from the company headquarters to an AWS Region. Use the company WAN to send traffic over a DX connection. Use inter-region VPC peering to access the data in other AWS Regions.",
      "C. Establish two AWS Direct Connect connections from the company headquarters to an AWS Region. Use the company WAN to send traffic over a DX connection. Use an AWS transit VPC solution to access data in other AWS Regions.",
      "D. Establish two AWS Direct Connect connections from the company headquarters to an AWS Region. Use the company WAN to send traffic over a DX connection. Use Direct Connect Gateway to access data in other AWS Regions."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (D):\n\nThe correct answer is option D, which involves establishing two AWS Direct Connect connections from the company's headquarters to an AWS Region, using the company's WAN to send traffic over the Direct Connect connections, and then leveraging the AWS Direct Connect Gateway to access data in other AWS Regions.\n\nThis solution is the most cost-effective and meets the given requirements:\n\n- Establishes redundant Direct Connect connections for high availability, as required.\n- Uses the company's WAN to send traffic, ensuring that no traffic accesses the data via the public internet, as per the security team's mandate.\n- Utilizes the Direct Connect Gateway to provide access to the data stored in multiple AWS Regions, without the need for complex inter-region VPC peering or a transit VPC solution.\n\n2. Explanation of the incorrect choices:\n\nA. This option is not cost-effective, as it requires establishing Direct Connect connections from the headquarters to all the AWS Regions in use, which can be more expensive than the solution in option D.\n\nB. This option uses inter-region VPC peering to access data in other AWS Regions, which can be more complex to set up and manage than the Direct Connect Gateway solution in option D.\n\nC. This option uses a transit VPC solution to access data in other AWS Regions, which can be more complex and potentially more expensive than the Direct Connect Gateway solution in option D."
  },
  "429": {
    "question": "A company has developed an application that is running Windows Server on VMware vSphere VMs that the company hosts on premises. The application data is stored in a proprietary format that must be read through the application. The company manually provisioned the servers and the application.As part of its disaster recovery plan, the company wants the ability to host its application on AWS temporarily if the company's on-premises environment becomes unavailable. The company wants the application to return to on-premises hosting after a disaster recovery event is complete. The RPO is 5 minutes.Which solution meets these requirements with the LEAST amount of operational overhead?",
    "choices": [
      "A. Configure AWS DataSync. Replicate the data to Amazon Elastic Block Store (Amazon EBS) volumes. When the on-premises environment is unavailable, use AWS CloudFormation templates to provision Amazon EC2 instances and attach the EBS volumes.",
      "B. Configure AWS Elastic Disaster Recovery. Replicate the data to replication Amazon EC2 instances that are attached to Amazon Elastic Block Store (Amazon EBS) volumes. When the on-premises environment is unavailable, use Elastic Disaster Recovery to launch EC2 instances that use the replicated volumes.",
      "C. Provision an AWS Storage Gateway file gateway. Replicate the data to an Amazon S3 bucket. When the on-premises environment is unavailable, use AWS Backup to restore the data to Amazon Elastic Block Store (Amazon EBS) volumes and launch Amazon EC2 instances from these EBS volumes.",
      "D. Provision an Amazon FSx for Windows File Server file system on AWS. Replicate the data to the file system. When the on-premises environment is unavailable, use AWS CloudFormation templates to provision Amazon EC2 instances and use AWS::CloudFormation::Init commands to mount the Amazon FSx file shares."
    ],
    "answer": "B",
    "explanation": "1. Explanation of why the correct answer (B) is right:\n\nThe correct answer is B, which is to configure AWS Elastic Disaster Recovery. This solution best meets the requirements with the least amount of operational overhead for the following reasons:\n\n- AWS Elastic Disaster Recovery provides continuous data replication of the on-premises VMware vSphere VMs to Amazon EC2 instances attached to Amazon EBS volumes. This ensures the required RPO of 5 minutes is met.\n- When the on-premises environment becomes unavailable, Elastic Disaster Recovery can automatically launch the EC2 instances using the replicated volumes, streamlining the recovery process and reducing manual intervention.\n- The service handles the entire recovery process, including the provisioning of EC2 instances and attachment of EBS volumes, without the need for the company to configure additional services or scripts. This results in lower operational overhead compared to the other options.\n- The solution also supports the requirement of the application returning to on-premises hosting after the disaster recovery event is complete, as Elastic Disaster Recovery can facilitate the failback process.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Configure AWS DataSync. This option is incorrect because DataSync lacks comprehensive disaster recovery capabilities and still requires manual provisioning of resources on AWS, which increases the operational overhead.\n\nC. Provision an AWS Storage Gateway file gateway. This option is incorrect because it introduces additional complexity by involving AWS Backup for data restoration and does not support the required 5-minute RPO.\n\nD. Provision an Amazon FSx for Windows File Server file system on AWS. This option is incorrect because it does not provide an automated disaster recovery solution for VMware vSphere VMs, which would increase the operational overhead for the company."
  },
  "430": {
    "question": "A company runs a highly available data collection application on Amazon EC2 in the eu-north-1 Region. The application collects data from end-user devices and writes records to an Amazon Kinesis data stream and a set of AWS Lambda functions that process the records. The company persists the output of the record processing to an Amazon S3 bucket in eu-north-1. The company uses the data in the S3 bucket as a data source for Amazon Athena.The company wants to increase its global presence. A solutions architect must launch the data collection capabilities in the sa-east-1 and ap-northeast-1 Regions. The solutions architect deploys the application, the Kinesis data stream, and the Lambda functions in the two new Regions. The solutions architect keeps the S3 bucket in eu-north-1 to meet a requirement to centralize the data analysis.During testing of the new setup, the solutions architect notices a significant lag on the arrival of data from the new Regions to the S3 bucket.Which solution will improve this lag time the MOST?",
    "choices": [
      "A. In each of the two new Regions, set up the Lambda functions to run in a VPC. Set up an S3 gateway endpoint in that VPC.",
      "B. Turn on S3 Transfer Acceleration on the S3 bucket in eu-north-1. Change the application to use the new S3 accelerated endpoint when the application uploads data to the S3 bucket.",
      "C. Create an S3 bucket in each of the two new Regions. Set the application in each new Region to upload to its respective S3 bucket. Set up S3 Cross-Region Replication to replicate data to the S3 bucket in eu-north-1.",
      "D. Increase the memory requirements of the Lambda functions to ensure that they have multiple cores available. Use the multipart upload feature when the application uploads data to Amazon S3 from Lambda."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\nThe correct answer is C because it addresses the issue of lag time by setting up an S3 bucket in each of the two new Regions (sa-east-1 and ap-northeast-1) and using S3 Cross-Region Replication to replicate the data to the central S3 bucket in eu-north-1. This solution will improve the lag time the most by reducing the distance and network latency between the data collection points and the local S3 buckets, while still maintaining the requirement to centralize the data analysis in the eu-north-1 Region.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option is incorrect because setting up an S3 gateway endpoint in a VPC does not address the issue of lag time. The data would still need to be transferred from the new Regions to the central S3 bucket in eu-north-1, which is the source of the lag.\n\nB. This option is incorrect because S3 Transfer Acceleration is not yet supported in the eu-north-1 Region. The question states that the solutions architect must keep the S3 bucket in eu-north-1, and Transfer Acceleration is not available in that Region, so this solution would not be applicable.\n\nD. This option is incorrect because increasing the memory and using multipart upload on the Lambda functions does not address the root cause of the lag, which is the distance and network latency between the data collection points in the new Regions and the central S3 bucket in eu-north-1. Optimizing the Lambda functions would not significantly improve the lag time in this scenario."
  },
  "431": {
    "question": "A company provides a centralized Amazon EC2 application hosted in a single shared VPC. The centralized application must be accessible from client applications running in the VPCs of other business units. The centralized application front end is configured with a Network Load Balancer (NLB) for scalability.Up to 10 business unit VPCs will need to be connected to the shared VPC. Some of the business unit VPC CIDR blocks overlap with the shared VPC, and some overlap with each other Network connectivity to the centralized application in the shared VPC should be allowed from authorized business unit VPCs only.Which network configuration should a solutions architect use to provide connectivity from the client applications in the business unit VPCs to the centralized application in the shared VPC?",
    "choices": [
      "A. Create an AWS Transit Gateway. Attach the shared VPC and the authorized business unit VPCs to the transit gateway. Create a single transit gateway route table and associate it with all of the attached VPCs. Allow automatic propagation of routes from the attachments into the route table. Configure VPC routing tables to send traffic to the transit gateway.",
      "B. Create a VPC endpoint service using the centralized application NLB and enable the option to require endpoint acceptance. Create a VPC endpoint in each of the business unit VPCs using the service name of the endpoint service. Accept authorized endpoint requests from the endpoint service console.",
      "C. Create a VPC peering connection from each business unit VPC to the shared VPAccept the VPC peering connections from the shared VPC console. Configure VPC routing tables to send traffic to the VPC peering connection.",
      "D. Configure a virtual private gateway for the shared VPC and create customer gateways for each of the authorized business unit VPCs. Establish a Site-to-Site VPN connection from the business unit VPCs to the shared VPC. Configure VPC routing tables to send traffic to the VPN connection."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B, which is to create a VPC endpoint service using the centralized application's Network Load Balancer (NLB) and enable the \"require endpoint acceptance\" option. This approach addresses the key requirements in the scenario:\n\n- Overlapping CIDR blocks between the shared VPC and the business unit VPCs: By using a VPC endpoint service, you can securely connect the client applications in the business unit VPCs to the centralized application in the shared VPC, without the need to manage overlapping IP ranges.\n- Controlled access from authorized business unit VPCs: The \"require endpoint acceptance\" option allows the centralized VPC owner to selectively approve the VPC endpoint requests from the authorized business unit VPCs, ensuring that only the intended VPCs can access the centralized application.\n- Scalability: The centralized application is configured with a Network Load Balancer (NLB), which provides scalability and high availability for the application.\n\n2. Explanations of the incorrect choices:\n\nA. AWS Transit Gateway:\n- While Transit Gateway can connect multiple VPCs, it does not provide a mechanism to control access or handle overlapping CIDR blocks between VPCs, which is a requirement in this scenario.\n\nC. VPC Peering:\n- VPC peering does not support overlapping CIDR blocks between VPCs, which is a requirement in this scenario.\n- Managing multiple VPC peering connections can become complex and difficult to maintain as the number of VPCs increases.\n\nD. Site-to-Site VPN:\n- Site-to-Site VPN does not provide a way to control access from the authorized business unit VPCs to the centralized application in the shared VPC.\n- It also does not address the issue of overlapping CIDR blocks between the VPCs.\n\nIn summary, the VPC endpoint service with the \"require endpoint acceptance\" option (choice B) is the most suitable solution as it addresses all the key requirements in the scenario, including handling overlapping CIDR blocks and providing controlled access from authorized business unit VPCs to the centralized application."
  },
  "432": {
    "question": "A company wants to migrate its website to AWS. The website uses microservices and runs on containers that are deployed in an on-premises, self-managed Kubernetes cluster. All the manifests that define the deployments for the containers in the Kubernetes deployment are in source control.All data for the website is stored in a PostgreSQL database. An open source container image repository runs alongside the on-premises environment.A solutions architect needs to determine the architecture that the company will use for the website on AWS.Which solution will meet these requirements with the LEAST effort to migrate?",
    "choices": [
      "A. Create an AWS App Runner service. Connect the App Runner service to the open source container image repository. Deploy the manifests from on premises to the App Runner service. Create an Amazon RDS for PostgreSQL database.",
      "B. Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster that has managed node groups. Copy the application containers to a new Amazon Elastic Container Registry (Amazon ECR) repository. Deploy the manifests from on premises to the EKS cluster. Create an Amazon Aurora PostgreSQL DB cluster.",
      "C. Create an Amazon Elastic Container Service (Amazon ECS) cluster that has an Amazon EC2 capacity pool. Copy the application containers to a new Amazon Elastic Container Registry (Amazon ECR) repository. Register each container image as a new task definition. Configure ECS services for each task definition to match the original Kubernetes deployments. Create an Amazon Aurora PostgreSQL DB cluster.",
      "D. Rebuild the on-premises Kubernetes cluster by hosting the cluster on Amazon EC2 instances. Migrate the open source container image repository to the EC2 instances. Deploy the manifests from on premises to the new cluster on AWS. Deploy an open source PostgreSQL database on the new cluster."
    ],
    "answer": "B",
    "explanation": "1. Explanation of why the correct answer (B) is right:\n\nOption B is the best choice because it leverages the managed Amazon EKS service to migrate the existing Kubernetes-based deployment to AWS. This approach requires the least effort compared to the other options:\n\n- It allows you to seamlessly migrate the existing Kubernetes deployment to a managed EKS cluster, which takes care of the control plane management.\n- The managed node groups in EKS simplify the migration by automatically handling the worker node provisioning and scaling.\n- Copying the application containers to a new Amazon ECR repository is a straightforward process, and deploying the manifests from on-premises to the EKS cluster can be easily automated.\n- Choosing Amazon Aurora PostgreSQL DB cluster over a self-managed PostgreSQL database on the cluster provides a fully managed, highly available, and scalable database solution.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Create an AWS App Runner service:\n- App Runner is a serverless container service, which may not provide the same level of control and flexibility as a managed Kubernetes service like EKS.\n- Connecting to an external container image repository may introduce additional complexity and maintenance overhead.\n- Deploying the manifests from on-premises to the App Runner service may require more effort than directly deploying to an EKS cluster.\n\nC. Create an Amazon Elastic Container Service (ECS) cluster:\n- ECS is a container orchestration service, which may not provide the same level of familiarity and compatibility as a Kubernetes-based deployment.\n- Registering each container image as a new task definition and configuring ECS services to match the original Kubernetes deployments may require more effort than directly deploying to an EKS cluster.\n\nD. Rebuild the on-premises Kubernetes cluster by hosting it on AWS EC2 instances:\n- This option represents a \"lift and shift\" approach, which may not take full advantage of the managed services and features provided by AWS.\n- Migrating the open source container image repository to the EC2 instances and maintaining the self-managed Kubernetes cluster may require more effort than the managed EKS approach.\n- Deploying an open source PostgreSQL database on the cluster may not provide the same level of high availability an"
  },
  "433": {
    "question": "A company uses a mobile app on AWS to run online contests. The company selects a winner at random at the end of each contest. The contests run for variable lengths of time. The company does not need to retain any data from a contest after the contest is finished.The company uses custom code that is hosted on Amazon EC2 instances to process the contest data and select a winner. The EC2 instances run behind an Application Load Balancer and store contest entries on Amazon RDS DB instances. The company must design a new architecture to reduce the cost of running the contests.Which solution will meet these requirements MOST cost-effectively?",
    "choices": [
      "A. Migrate storage of the contest entries to Amazon DynamoDB. Create a DynamoDB Accelerator (DAX) cluster. Rewrite the code to run as Amazon Elastic Container Service (Amazon ECS) containers that use the Fargate launch type. At the end of the contest, delete the DynamoDB table.",
      "B. Migrate the storage of the contest entries to Amazon Redshift. Rewrite the code as AWS Lambda functions. At the end of the contest, delete the Redshift cluster.",
      "C. Add an Amazon ElastiCache for Redis cluster in front of the RDS DB instances to cache the contest entries. Rewrite the code to run as Amazon Elastic Container Service (Amazon ECS) containers that use the Fargate launch type. Set the ElastiCache TTL attribute on each entry to expire each entry at the end of the contest.",
      "D. Migrate the storage of the contest entries to Amazon DynamoDB. Rewrite the code as AWS Lambda functions. Set the DynamoDB TTL attribute on each entry to expire each entry at the end of the contest."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (Option D):\n\nOption D is the most cost-effective solution because it leverages the most appropriate AWS services for the given requirements:\n\n- Migrating the storage of the contest entries to Amazon DynamoDB eliminates the need for managing and scaling RDS DB instances, which can be more expensive. DynamoDB is a fully managed NoSQL database with pay-per-use pricing, making it a more cost-effective option for data storage with variable contest durations.\n- Rewriting the code as AWS Lambda functions removes the need for managing and scaling EC2 instances, which can incur significant costs. Lambda functions are charged based on the actual compute time used, making them a more cost-effective choice for short-duration, event-driven workloads like selecting contest winners.\n- Using the DynamoDB TTL (Time-to-Live) attribute to automatically expire the contest entries at the end of each contest eliminates the need for manual data cleanup, further reducing operational costs.\n\n2. Explanations of why the incorrect choices are wrong:\n\nOption A:\n- While migrating to DynamoDB is a good idea, the addition of a DynamoDB Accelerator (DAX) cluster is unnecessary for this use case, as the contest entries are not expected to have high read/write traffic that would require the caching capabilities of DAX.\n- Rewriting the code to run as Amazon ECS containers using the Fargate launch type is an unnecessary complication, as Lambda functions would be a more cost-effective choice for this short-duration, event-driven workload.\n\nOption B:\n- Migrating the storage to Amazon Redshift, a data warehouse service, is not the most appropriate choice for this use case, as Redshift is designed for analytical processing of large datasets, not for storing and retrieving individual contest entries.\n- Rewriting the code as AWS Lambda functions is a good idea, but the use of Redshift instead of DynamoDB makes this solution less cost-effective.\n\nOption C:\n- Adding an Amazon ElastiCache for Redis cluster to cache the contest entries is an unnecessary complication, as the contest entries are not expected to have high read/write traffic that would benefit from caching.\n- Rewriting the code to run as Amazon ECS containers using"
  },
  "434": {
    "question": "A company has implemented a new security requirement. According to the new requirement, the company must scan all traffic from corporate AWS instances in the company's VPC for violations of the company's security policies. As a result of these scans, the company can block access to and from specific IP addresses.To meet the new requirement, the company deploys a set of Amazon EC2 instances in private subnets to serve as transparent proxies. The company installs approved proxy server software on these EC2 instances. The company modifies the route tables on all subnets to use the corresponding EC2 instances with proxy software as the default route. The company also creates security groups that are compliant with the security policies and assigns these security groups to the EC2 instances.Despite these configurations, the traffic of the EC2 instances in their private subnets is not being properly forwarded to the internet.What should a solutions architect do to resolve this issue?",
    "choices": [
      "A. Disable source/destination checks on the EC2 instances that run the proxy software.",
      "B. Add a rule to the security group that is assigned to the proxy EC2 instances to allow all traffic between instances that have this security group. Assign this security group to all EC2 instances in the VPC.",
      "C. Change the VPCs DHCP options set. Set the DNS server options to point to the addresses of the proxy EC2 instances.",
      "D. Assign one additional elastic network interface to each proxy EC2 instance. Ensure that one of these network interfaces has a route to the private subnets. Ensure that the other network interface has a route to the internet."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\n\nThe correct answer is A. Disable source/destination checks on the EC2 instances that run the proxy software.\n\nIn an Amazon VPC, the source/destination check is a security feature that ensures that an EC2 instance cannot be used as a network gateway or router to forward traffic between resources. By default, this check is enabled on all EC2 instances.\n\nWhen you want to use an EC2 instance as a transparent proxy or network appliance to forward traffic between resources, you need to disable the source/destination check on that instance. This allows the instance to receive and forward traffic that is not destined for itself, which is necessary for the proxy functionality to work properly.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. Add a rule to the security group that is assigned to the proxy EC2 instances to allow all traffic between instances that have this security group. Assign this security group to all EC2 instances in the VPC.\n- This would not resolve the issue, as the problem is related to the source/destination check, which is a separate network configuration from the security group rules.\n\nC. Change the VPCs DHCP options set. Set the DNS server options to point to the addresses of the proxy EC2 instances.\n- Changing the DHCP options set would not affect the proxy instances' ability to forward traffic, as the issue is not related to DNS or DHCP configuration.\n\nD. Assign one additional elastic network interface to each proxy EC2 instance. Ensure that one of these network interfaces has a route to the private subnets. Ensure that the other network interface has a route to the internet.\n- While this option may be necessary in some cases to provide the proxy instances with connectivity to both the private subnets and the internet, it does not directly address the source/destination check issue, which is the root cause of the problem described in the question."
  },
  "435": {
    "question": "A company is running its solution on AWS in a manually created VPC. The company is using AWS CloudFormation to provision other parts of the infrastructure. According to a new requirement, the company must manage all infrastructure in an automatic way.What should the company do to meet this new requirement with the LEAST effort?",
    "choices": [
      "A. Create a new AWS Cloud Development Kit (AWS CDK) stack that strictly provisions the existing VPC resources and configuration. Use AWS CDK to import the VPC into the stack and to manage the VPC.",
      "B. Create a CloudFormation stack set that creates the VPC. Use the stack set to import the VPC into the stack.",
      "C. Create a new CloudFormation template that strictly provisions the existing VPC resources and configuration. From the CloudFormation console, create a new stack by importing the Existing resources.",
      "D. Create a new CloudFormation template that creates the VPC. Use the AWS Serverless Application Model (AWS SAM) CLI to import the VPC."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C because it provides the simplest and most straightforward way to manage the existing VPC infrastructure in an automatic way with the least effort. By creating a new CloudFormation template that strictly provisions the existing VPC resources and configuration, the company can then import the existing VPC resources into the new CloudFormation stack. This allows the company to manage the VPC infrastructure through CloudFormation, which meets the new requirement of managing all infrastructure in an automatic way.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option is incorrect because creating a new AWS CDK stack to manage the existing VPC would require significant effort to migrate the entire infrastructure from CloudFormation to CDK. This would be a significant change and would not be the least effort approach.\n\nB. This option is incorrect because CloudFormation stack sets are designed for deploying stacks across multiple accounts and regions, not for importing existing resources into a single VPC or stack. Using a stack set to import the VPC would not be the correct approach.\n\nD. This option is incorrect because the AWS Serverless Application Model (AWS SAM) CLI is used for working with serverless applications, not for importing existing VPC resources into a CloudFormation stack. It would not be the appropriate tool for this scenario."
  },
  "436": {
    "question": "A company has developed a new release of a popular video game and wants to make it available for public download. The new release package is approximately 5 GB in size. The company provides downloads for existing releases from a Linux-based, publicly facing FTP site hosted in an on-premises data center. The company expects the new release will be downloaded by users worldwide. The company wants a solution that provides improved download performance and low transfer costs, regardless of a user's location.",
    "choices": [
      "A. Store the game files on Amazon EBS volumes mounted on Amazon EC2 instances within an Auto Scaling group. Configure an FTP service on the EC2 instances. Use an Application Load Balancer in front of the Auto Scaling group. Publish the game download URL for users to download the package.",
      "B. Store the game files on Amazon EFS volumes that are attached to Amazon EC2 instances within an Auto Scaling group. Configure an FTP service on each of the EC2 instances. Use an Application Load Balancer in front of the Auto Scaling group. Publish the game download URL for users to download the package.",
      "C. Configure Amazon Route 53 and an Amazon S3 bucket for website hosting. Upload the game files to the S3 bucket. Use Amazon CloudFront for the website. Publish the game download URL for users to download the package.",
      "D. Configure Amazon Route 53 and an Amazon S3 bucket for website hosting. Upload the game files to the S3 bucket. Set Requester Pays for the S3 bucket. Publish the game download URL for users to download the package."
    ],
    "answer": "C",
    "explanation": "1. Clear explanation of why the correct answer (C) is right:\n\nThe correct answer is C because it provides the best solution for the company's requirements of improved download performance and low transfer costs for users worldwide.\n\nBy using Amazon S3 for storage and Amazon CloudFront as the content delivery network (CDN), the solution has the following advantages:\n\n- Amazon S3 provides highly scalable, durable, and secure storage for the 5GB game files, with minimal storage costs.\n- Amazon CloudFront, AWS's global CDN, can cache the game files at edge locations around the world, reducing latency and providing faster downloads for users globally.\n- The combination of S3 and CloudFront offers a cost-effective solution, as CloudFront has low data transfer costs and can efficiently serve the game files from the nearest edge location.\n\n2. Brief explanations of why the incorrect choices are wrong:\n\nA. Storing the files on Amazon EBS volumes and using EC2 instances with an FTP service is not the best solution, as EBS volumes are not designed for global file distribution, and the on-premises FTP service would not provide the same level of performance and scale as a CDN.\n\nB. Using Amazon EFS instead of S3 is also not optimal, as EFS is primarily designed for shared file storage among EC2 instances, not for global public file distribution. The FTP service on each EC2 instance would still not match the performance and scalability of a CDN.\n\nD. Setting \"Requester Pays\" on the S3 bucket is unnecessary and would shift the data transfer costs to the end-users, which goes against the requirement of low transfer costs for the users."
  },
  "437": {
    "question": "An ecommerce company runs an application on AWS. The application has an Amazon API Gateway API that invokes an AWS Lambda function. The data is stored in an Amazon RDS for PostgreSQL DB instance.During the company\u2019s most recent flash sale, a sudden increase in API calls negatively affected the application's performance. A solutions architect reviewed the Amazon CloudWatch metrics during that time and noticed a significant increase in Lambda invocations and database connections. The CPU utilization also was high on the DB instance.What should the solutions architect recommend to optimize the application's performance?",
    "choices": [
      "A. Increase the memory of the Lambda function. Modify the Lambda function to close the database connections when the data is retrieved.",
      "B. Add an Amazon ElastiCache for Redis cluster to store the frequently accessed data from the RDS database.",
      "C. Create an RDS proxy by using the Lambda console. Modify the Lambda function to use the proxy endpoint.",
      "D. Modify the Lambda function to connect to the database outside of the function's handler. Check for an existing database connection before creating a new connection."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C: Create an RDS proxy by using the Lambda console. Modify the Lambda function to use the proxy endpoint.\n\nThe key reasons why this is the right recommendation:\n\n- The issue is related to the sudden increase in database connections during the flash sale, which led to high CPU utilization on the RDS database instance.\n- Using an RDS proxy can help optimize the database connections by managing the connection pool and reusing existing connections, reducing the overhead on the RDS instance.\n- Creating the RDS proxy directly from the Lambda console is a convenient way to set it up and integrate it with the Lambda function, without requiring additional configuration.\n- Modifying the Lambda function to use the proxy endpoint ensures that the application can leverage the benefits of the RDS proxy, such as improved connection management and performance.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Increase the memory of the Lambda function. Modify the Lambda function to close the database connections when the data is retrieved.\n- Increasing the memory of the Lambda function may not directly address the issue of high database connections and CPU utilization on the RDS instance.\n- Closing the database connections within the Lambda function handler does not solve the underlying problem of the sudden spike in database connections.\n\nB. Add an Amazon ElastiCache for Redis cluster to store the frequently accessed data from the RDS database.\n- Adding an ElastiCache for Redis cluster would introduce an additional layer of complexity to the architecture, and may not be the most appropriate solution for the specific issue described.\n- Caching frequently accessed data can help improve performance, but it does not address the root cause of the high database connection and CPU utilization on the RDS instance.\n\nD. Modify the Lambda function to connect to the database outside of the function's handler. Check for an existing database connection before creating a new connection.\n- This approach can help optimize the database connections, but it requires more manual effort and custom implementation compared to using an RDS proxy.\n- The RDS proxy (option C) provides a more comprehensive and managed solution for connection pooling and optimization, which may be a more effective approach in this scenario."
  },
  "438": {
    "question": "A retail company is operating its ecommerce application on AWS. The application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The company uses an Amazon RDS DB instance as the database backend. Amazon CloudFront is configured with one origin that points to the ALB. Static content is cached. Amazon Route 53 is used to host all public zones.After an update of the application, the ALB occasionally returns a 502 status code (Bad Gateway) error. The root cause is malformed HTTP headers that are returned to the ALB. The webpage returns successfully when a solutions architect reloads the webpage immediately after the error occurs.While the company is working on the problem, the solutions architect needs to provide a custom error page instead of the standard ALB error page to visitors.Which combination of steps will meet this requirement with the LEAST amount of operational overhead? (Choose two.)",
    "choices": [
      "A. Create an Amazon S3 bucket. Configure the S3 bucket to host a static webpage. Upload the custom error pages to Amazon S3.",
      "B. Create an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health check response Target.FailedHealthChecks is greater than 0. Configure the Lambda function to modify the forwarding rule at the ALB to point to a publicly accessible web server.",
      "C. Modify the existing Amazon Route 53 records by adding health checks. Configure a fallback target if the health check fails. Modify DNS records to point to a publicly accessible webpage.",
      "D. Create an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health check response Elb.InternalError is greater than 0. Configure the Lambda function to modify the forwarding rule at the ALB to point to a public accessible web server.",
      "E. Add a custom error response by configuring a CloudFront custom error page. Modify DNS records to point to a publicly accessible web page."
    ],
    "answer": "AE",
    "explanation": "1. Explanation of the correct answer (AE):\n\nA. Create an Amazon S3 bucket. Configure the S3 bucket to host a static webpage. Upload the custom error pages to Amazon S3.\nThis option allows you to quickly set up a custom error page hosted on Amazon S3. Since the issue is with malformed HTTP headers returned by the ALB, this provides a simple way to display a custom error page without modifying the ALB or other components.\n\nE. Add a custom error response by configuring a CloudFront custom error page. Modify DNS records to point to a publicly accessible web page.\nThis option uses Amazon CloudFront to configure a custom error page. CloudFront can be set up to display a custom error page when the origin (the ALB in this case) returns an error. This is a good alternative to option A, as it also provides a way to display a custom error page with minimal operational overhead.\n\nBoth options A and E meet the requirement of providing a custom error page with the least amount of operational overhead, as they do not require modifying the ALB, RDS, or other components.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. Create an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health check response Target.FailedHealthChecks is greater than 0. Configure the Lambda function to modify the forwarding rule at the ALB to point to a publicly accessible web server.\nThis option is overly complex and introduces additional components (CloudWatch, Lambda) that are not necessary to solve the problem. It also requires modifying the ALB, which may not be desirable or possible in the short term.\n\nC. Modify the existing Amazon Route 53 records by adding health checks. Configure a fallback target if the health check fails. Modify DNS records to point to a publicly accessible webpage.\nModifying the Route 53 records and configuring a fallback target is not directly related to providing a custom error page. This solution would not address the immediate need to display a custom error page when the ALB returns a 502 error.\n\nD. Create an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health check response Elb.InternalError is greater than 0. Configure the Lambda function to modify the"
  },
  "439": {
    "question": "A company wants to migrate an Amazon Aurora MySQL DB cluster from an existing AWS account to a new AWS account in the same AWS Region. Both accounts are members of the same organization in AWS Organizations.The company must minimize database service interruption before the company performs DNS cutover to the new database.Which migration strategy will meet this requirement? (Choose two.)",
    "choices": [
      "A. Take a snapshot of the existing Aurora database. Share the snapshot with the new AWS account. Create an Aurora DB cluster in the new account from the snapshot.",
      "B. Create an Aurora DB cluster in the new AWS account. Use AWS Database Migration Service (AWS DMS) to migrate data between the two Aurora DB clusters.",
      "C. Use AWS Backup to share an Aurora database backup from the existing AWS account to the new AWS account. Create an Aurora DB cluster in the new AWS account from the snapshot.",
      "D. Create an Aurora DB cluster in the new AWS account. Use AWS Application Migration Service to migrate data between the two Aurora DB clusters."
    ],
    "answer": "AB",
    "explanation": "1. Clear explanation of the correct answer (A and B):\n\nA. Take a snapshot of the existing Aurora database. Share the snapshot with the new AWS account. Create an Aurora DB cluster in the new account from the snapshot.\nThis option allows the company to create a new Aurora DB cluster in the new AWS account using a snapshot of the existing database. This approach minimizes downtime as the original database can continue to operate while the new cluster is being set up. The snapshot can be shared with the new account to ensure the data is migrated.\n\nB. Create an Aurora DB cluster in the new AWS account. Use AWS Database Migration Service (AWS DMS) to migrate data between the two Aurora DB clusters.\nThis option involves creating a new Aurora DB cluster in the new AWS account and then using AWS DMS to migrate the data from the existing cluster to the new one. AWS DMS is designed to perform live migrations with minimal downtime, making it a suitable choice for this requirement.\n\nThe combination of A and B provides a comprehensive migration strategy that meets the requirement of minimizing database service interruption before the DNS cutover.\n\n2. Explanations of why the incorrect choices are wrong:\n\nC. Use AWS Backup to share an Aurora database backup from the existing AWS account to the new AWS account. Create an Aurora DB cluster in the new AWS account from the snapshot.\nThis option is incorrect because it uses a backup instead of a snapshot. Backups typically take longer to restore and may not be as up-to-date as a snapshot, potentially leading to more downtime during the migration process.\n\nD. Create an Aurora DB cluster in the new AWS account. Use AWS Application Migration Service to migrate data between the two Aurora DB clusters.\nThis option is incorrect because AWS Application Migration Service is designed for migrating entire applications, not specifically for database migrations. AWS DMS is the recommended service for database migrations, as it is optimized for the task and can perform live migrations with minimal downtime."
  },
  "440": {
    "question": "A software as a service (SaaS) company provides a media software solution to customers. The solution is hosted on 50 VPCs across various AWS Regions and AWS accounts. One of the VPCs is designated as a management VPC. The compute resources in the VPCs work independently.The company has developed a new feature that requires all 50 VPCs to be able to communicate with each other. The new feature also requires one-way access from each customer's VPC to the company's management VPC. The management VPC hosts a compute resource that validates licenses for the media software solution.The number of VPCs that the company will use to host the solution will continue to increase as the solution grows.Which combination of steps will provide the required VPC connectivity with the LEAST operational overhead? (Choose two.)",
    "choices": [
      "A. Create a transit gateway. Attach all the company's VPCs and relevant subnets to the transit gateway.",
      "B. Create VPC peering connections between all the company's VPCs.",
      "C. Create a Network Load Balancer (NLB) that points to the compute resource for license validation. Create an AWS PrivateLink endpoint service that is available to each customer's VPAssociate the endpoint service with the NLB.",
      "D. Create a VPN appliance in each customer's VPC. Connect the company's management VPC to each customer's VPC by using AWS Site-to-Site VPN.",
      "E. Create a VPC peering connection between the company's management VPC and each customer's VPC."
    ],
    "answer": "AC",
    "explanation": "1. Explanation of the correct answer (A and C):\n\nA. Create a transit gateway. Attach all the company's VPCs and relevant subnets to the transit gateway.\nThis is the correct choice because a transit gateway provides a scalable and centralized way to manage VPC connectivity. By attaching all the company's VPCs to the transit gateway, you establish a mesh network where the VPCs can communicate with each other. This is more efficient and easier to manage than setting up individual VPC peering connections between all 50 VPCs, which would become increasingly complex as the number of VPCs grows.\n\nC. Create a Network Load Balancer (NLB) that points to the compute resource for license validation. Create an AWS PrivateLink endpoint service that is available to each customer's VPC. Associate the endpoint service with the NLB.\nThis is the correct choice because it provides a secure and scalable way for the customer's VPCs to access the license validation compute resource in the company's management VPC. AWS PrivateLink allows for one-way access from the customer VPCs to the management VPC, without the need for internet gateways, NAT gateways, or VPNs. This simplifies the network configuration and reduces operational overhead.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. Create VPC peering connections between all the company's VPCs.\nThis is incorrect because creating individual VPC peering connections between all 50 VPCs would become increasingly complex and operationally intensive as the number of VPCs grows. VPC peering is not a scalable solution for a large number of VPCs.\n\nD. Create a VPN appliance in each customer's VPC. Connect the company's management VPC to each customer's VPC by using AWS Site-to-Site VPN.\nThis is incorrect because it would require setting up and managing a VPN appliance in each customer's VPC, which adds unnecessary complexity and operational overhead. The use of AWS PrivateLink (as in the correct answer) is a simpler and more scalable solution for the one-way access requirement.\n\nE. Create a VPC peering connection between the company's management VPC and each customer's VPC."
  },
  "441": {
    "question": "A company has multiple lines of business (LOBs) that roll up to the parent company. The company has asked its solutions architect to develop a solution with the following requirements:\u2022\tProduce a single AWS invoice for all of the AWS accounts used by its LOBs.\u2022\tThe costs for each LOB account should be broken out on the invoice.\u2022\tProvide the ability to restrict services and features in the LOB accounts, as defined by the company's governance policy.\u2022\tEach LOB account should be delegated full administrator permissions, regardless of the governance policy.Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
    "choices": [
      "A. Use AWS Organizations to create an organization in the parent account for each LOB. Then invite each LOB account to the appropriate organization.",
      "B. Use AWS Organizations to create a single organization in the parent account. Then, invite each LOB's AWS account to join the organization.",
      "C. Implement service quotas to define the services and features that are permitted and apply the quotas to each LOB. as appropriate.",
      "D. Create an SCP that allows only approved services and features, then apply the policy to the LOB accounts.",
      "E. Enable consolidated billing in the parent account's billing console and link the LOB accounts."
    ],
    "answer": "BD",
    "explanation": "1. Explanation of the correct answer (B and D):\n\nB. Use AWS Organizations to create a single organization in the parent account. Then, invite each LOB's AWS account to join the organization.\n- This meets the requirement of producing a single AWS invoice for all the LOB accounts, as AWS Organizations enables consolidated billing.\n- Each LOB account will be part of the same organization, allowing the costs to be broken out on the invoice.\n\nD. Create an SCP that allows only approved services and features, then apply the policy to the LOB accounts.\n- This meets the requirement of restricting services and features in the LOB accounts, as defined by the company's governance policy.\n- SCPs (Service Control Policies) in AWS Organizations allow you to define the allowed or denied services and actions for the accounts in the organization.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Use AWS Organizations to create an organization in the parent account for each LOB. Then invite each LOB account to the appropriate organization.\n- This would result in multiple organizations, which does not meet the requirement of a single invoice for all the LOB accounts.\n\nC. Implement service quotas to define the services and features that are permitted and apply the quotas to each LOB as appropriate.\n- Service Quotas alone do not meet the requirement of providing a single invoice for all the LOB accounts. Service Quotas are more suitable for limiting usage within a single account.\n\nE. Enable consolidated billing in the parent account's billing console and link the LOB accounts.\n- This is incorrect because consolidated billing is already enabled by default when you create an AWS Organization, as mentioned in the community discussions."
  },
  "442": {
    "question": "A company has an application that has a web frontend. The application runs in the company's on-premises data center and requires access to file storage for critical data. The application runs on three Linux VMs for redundancy. The architecture includes a load balancer with HTTP request-based routing.The company needs to migrate the application to AWS as quickly as possible. The architecture on AWS must be highly available.Which solution will meet these requirements with the FEWEST changes to the architecture?",
    "choices": [
      "A. Migrate the application to Amazon Elastic Container Service (Amazon ECS) containers that use the Fargate launch type in three Availability Zones. Use Amazon S3 to provide file storage for all three containers. Use a Network Load Balancer to direct traffic to the containers.",
      "B. Migrate the application to Amazon EC2 instances in three Availability Zones. Use Amazon Elastic File System (Amazon EFS) for file storage. Mount the file storage on all three EC2 instances. Use an Application Load Balancer to direct traffic to the EC2 instances.",
      "C. Migrate the application to Amazon Elastic Kubernetes Service (Amazon EKS) containers that use the Fargate launch type in three Availability Zones. Use Amazon FSx for Lustre to provide file storage for all three containers. Use a Network Load Balancer to direct traffic to the containers.",
      "D. Migrate the application to Amazon EC2 instances in three AWS Regions. Use Amazon Elastic Block Store (Amazon EBS) for file storage. Enable Cross-Region Replication (CRR) for all three EC2 instances. Use an Application Load Balancer to direct traffic to the EC2 instances."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B because it meets the requirements with the fewest changes to the existing architecture:\n\n- Migrating the application to Amazon EC2 instances in three Availability Zones closely matches the existing on-premises deployment using Linux VMs. This ensures minimal changes to the application's architecture.\n- Using Amazon Elastic File System (Amazon EFS) for file storage provides a scalable and highly available file system that can be mounted on all three EC2 instances, similar to the on-premises file storage solution.\n- Utilizing an Application Load Balancer (ALB) to direct traffic to the EC2 instances is a straightforward replacement for the existing load balancer, maintaining the HTTP request-based routing functionality.\n\nThis solution requires the least amount of changes to the existing application architecture, making the migration to AWS quick and efficient.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This solution uses Amazon ECS containers with the Fargate launch type, which requires significant changes to the application's architecture. Additionally, using Amazon S3 for file storage instead of the existing file storage solution would require major modifications to the application.\n\nC. This solution uses Amazon EKS containers with the Fargate launch type, which is a significant architectural change from the existing Linux VMs. Additionally, using Amazon FSx for Lustre for file storage instead of the existing file storage solution would require substantial application changes.\n\nD. This solution uses Amazon EC2 instances in three AWS Regions, which is a more complex architecture than the existing on-premises deployment in three Availability Zones. Furthermore, using Amazon EBS for file storage and enabling Cross-Region Replication (CRR) would require significant changes to the application's file storage handling."
  },
  "443": {
    "question": "A flood monitoring agency has deployed more than 10,000 water-level monitoring sensors. Sensors send continuous data updates, and each update is less than 1 MB in size. The agency has a fleet of on-premises application servers. These servers receive updates from the sensors, convert the raw data into a human readable format, and write the results to an on-premises relational database server. Data analysts then use simple SQL queries to monitor the data.The agency wants to increase overall application availability and reduce the effort that is required to perform maintenance tasks. These maintenance tasks, which include updates and patches to the application servers, cause downtime. While an application server is down, data is lost from sensors because the remaining servers cannot handle the entire workload.The agency wants a solution that optimizes operational overhead and costs. A solutions architect recommends the use of AWS IoT Core to collect the sensor data.What else should the solutions architect recommend to meet these requirements?",
    "choices": [
      "A. Send the sensor data to Amazon Kinesis Data Firehose. Use an AWS Lambda function to read the Kinesis Data Firehose data, convert it to .csv format, and insert it into an Amazon Aurora MySQL DB instance. Instruct the data analysts to query the data directly from the DB instance.",
      "B. Send the sensor data to Amazon Kinesis Data Firehose. Use an AWS Lambda function to read the Kinesis Data Firehose data, convert it to Apache Parquet format, and save it to an Amazon S3 bucket. Instruct the data analysts to query the data by using Amazon Athena.",
      "C. Send the sensor data to an Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) application to convert the data to .csv format and store it in an Amazon S3 bucket. Import the data into an Amazon Aurora MySQL DB instance. Instruct the data analysts to query the data directly from the DB instance.",
      "D. Send the sensor data to an Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) application to convert the data to Apache Parquet format and store it in an Amazon S3 bucket. Instruct the data analysts to query the data by using Amazon Athena."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\nThe correct answer is B because it aligns best with the requirements and provides a scalable, serverless solution with minimal operational overhead.\n\nThe key points are:\n- Sending the sensor data to Amazon Kinesis Data Firehose ensures reliable and scalable data ingestion, as it can handle the high volume of continuous updates from the sensors.\n- Using an AWS Lambda function to read the Kinesis Data Firehose data and convert it to Apache Parquet format provides several benefits:\n  - Parquet is a columnar storage format optimized for analytics workloads, offering efficient compression and query performance.\n  - Lambda is a serverless compute service that automatically scales, reducing the need for manual server management and maintenance.\n- Storing the transformed data in an Amazon S3 bucket allows the data analysts to query the data using Amazon Athena, a serverless, interactive query service.\n- This solution addresses the agency's requirement to increase overall application availability and reduce the effort required for maintenance tasks, as it relies on fully managed, serverless services.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Incorrect because:\n- Storing the data in a .csv format in an Amazon Aurora MySQL DB instance does not provide the same performance and storage efficiency as the Parquet format in S3.\n- Querying the data directly from the DB instance may not be as scalable and cost-effective as using Athena to query the data in S3.\n\nC. Incorrect because:\n- Sending the sensor data to an Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) application adds unnecessary complexity, as Flink does not provide automatic data transformation capabilities.\n- The solution of converting the data to .csv format and storing it in an Amazon Aurora MySQL DB instance has the same limitations as option A.\n\nD. Incorrect because:\n- Sending the sensor data to an Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) application adds unnecessary complexity, as Flink does not provide automatic data transformation capabilities.\n- The solution of converting the data to Apache Parquet format and storing it in an Amazon S3 bucket is similar to the correct answer, but"
  },
  "444": {
    "question": "A public retail web application uses an Application Load Balancer (ALB) in front of Amazon EC2 instances running across multiple Availability Zones (AZs) in a Region backed by an Amazon RDS MySQL Multi-AZ deployment. Target group health checks are configured to use HTTP and pointed at the product catalog page. Auto Scaling is configured to maintain the web fleet size based on the ALB health check.Recently, the application experienced an outage. Auto Scaling continuously replaced the instances during the outage. A subsequent investigation determined that the web server metrics were within the normal range, but the database tier was experiencing high load, resulting in severely elevated query response times.Which of the following changes together would remediate these issues while improving monitoring capabilities for the availability and functionality of the entire application stack for future growth? (Choose two.)",
    "choices": [
      "A. Configure read replicas for Amazon RDS MySQL and use the single reader endpoint in the web application to reduce the load on the backend database tier.",
      "B. Configure the target group health check to point at a simple HTML page instead of a product catalog page and the Amazon Route 53 health check against the product page to evaluate full application functionality. Configure Amazon CloudWatch alarms to notify administrators when the site fails.",
      "C. Configure the target group health check to use a TCP check of the Amazon EC2 web server and the Amazon Route 53 health check against the product page to evaluate full application functionality. Configure Amazon CloudWatch alarms to notify administrators when the site fails.",
      "D. Configure an Amazon CloudWatch alarm for Amazon RDS with an action to recover a high-load, impaired RDS instance in the database tier.",
      "E. Configure an Amazon ElastiCache cluster and place it between the web application and RDS MySQL instances to reduce the load on the backend database tier."
    ],
    "answer": "BE",
    "explanation": "1. Explanation of the correct answer (BE):\n\nB. Configure the target group health check to point at a simple HTML page instead of a product catalog page and the Amazon Route 53 health check against the product page to evaluate full application functionality. Configure Amazon CloudWatch alarms to notify administrators when the site fails.\nThis is correct because the issue was that the database tier was experiencing high load, resulting in severely elevated query response times. By configuring the target group health check to point to a simple HTML page instead of the product catalog page, the health check will not put additional load on the database tier, allowing the Auto Scaling group to function properly.\n\nE. Configure an Amazon ElastiCache cluster and place it between the web application and RDS MySQL instances to reduce the load on the backend database tier.\nThis is also correct because by introducing an ElastiCache cluster, the web application can cache frequently accessed data, reducing the load on the RDS MySQL instances and improving the overall application performance.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Configure read replicas for Amazon RDS MySQL and use the single reader endpoint in the web application to reduce the load on the backend database tier.\nThis is incorrect because the question does not mention the need for read-only scenarios, and the issue was not related to the write load on the database, but rather the overall high load and elevated query response times.\n\nC. Configure the target group health check to use a TCP check of the Amazon EC2 web server and the Amazon Route 53 health check against the product page to evaluate full application functionality. Configure Amazon CloudWatch alarms to notify administrators when the site fails.\nThis is incorrect because a TCP check of the EC2 web servers alone does not ensure the full application functionality, as the issue was with the database tier. The Route 53 health check against the product page is a better approach, but it should be coupled with a simpler health check for the target group to avoid putting additional load on the database.\n\nD. Configure an Amazon CloudWatch alarm for Amazon RDS with an action to recover a high-load, impaired RDS instance in the database tier.\nThis is incorrect because while setting up a CloudWatch alarm for the RDS instance is a good idea, the action to recover the instance may not be the best solution. The root cause of"
  },
  "445": {
    "question": "A company has an on-premises data center and is using Kubernetes to develop a new solution on AWS. The company uses Amazon Elastic Kubernetes Service (Amazon EKS) clusters for its development and test environments.The EKS control plane and data plane for production workloads must reside on premises. The company needs an AWS managed solution for Kubernetes management.Which solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      "A. Install an AWS Outposts server in the on-premises data center. Deploy Amazon EKS by using a local cluster configuration on the Outposts server for the production workloads.",
      "B. Install Amazon EKS Anywhere on the company's hardware in the on-premises data center. Deploy the production workloads on an EKS Anywhere cluster.",
      "C. Install an AWS Outposts server in the on-premises data center. Deploy Amazon EKS by using an extended cluster configuration on the Outposts server for the production workloads.",
      "D. Install an AWS Outposts server in the on-premises data center. Install Amazon EKS Anywhere on the Outposts server. Deploy the production workloads on an EKS Anywhere cluster."
    ],
    "answer": "A",
    "explanation": "Clear explanation of why the correct answer (A) is right:\n\nThe correct answer is A. Install an AWS Outposts server in the on-premises data center and deploy Amazon EKS using a local cluster configuration on the Outposts server for the production workloads.\n\nThis solution meets the key requirements:\n1. The EKS control plane and data plane for production workloads reside on-premises, as specified in the question.\n2. Amazon EKS on Outposts is a fully managed AWS service, providing the least operational overhead compared to the other options.\n\nWith the local cluster configuration on Outposts, the entire Kubernetes infrastructure, including the control plane and data plane, runs on the Outposts hardware within the company's on-premises data center. This satisfies the requirement of having the production workloads on-premises.\n\nAdditionally, Amazon EKS on Outposts is a managed service, where AWS handles the provisioning, upgrading, and lifecycle management of the Kubernetes control plane. This provides the least operational overhead for the company, as they don't have to manage the underlying infrastructure and control plane themselves.\n\nBrief explanations of why each incorrect choice is wrong:\n\nB. Install Amazon EKS Anywhere on the company's hardware in the on-premises data center:\nThis is incorrect because EKS Anywhere is a self-managed solution, where the customer is responsible for managing the underlying infrastructure and control plane. This goes against the requirement of needing an AWS-managed solution for Kubernetes management.\n\nC. Install an AWS Outposts server in the on-premises data center and deploy Amazon EKS using an extended cluster configuration on the Outposts server:\nThis is incorrect because the extended cluster configuration of Amazon EKS on Outposts runs the Kubernetes control plane in an AWS Region, while the nodes are on the Outposts. This does not fully satisfy the requirement of having the control plane and data plane reside on-premises.\n\nD. Install an AWS Outposts server in the on-premises data center, install Amazon EKS Anywhere on the Outposts server, and deploy the production workloads on an EKS Anywhere cluster:\nThis is incorrect because it combines the management overhead of an Outposts server with"
  },
  "446": {
    "question": "A solutions architect is reviewing an application's resilience before launch. The application runs on an Amazon EC2 instance that is deployed in a private subnet of a VPC. The EC2 instance is provisioned by an Auto Scaling group that has a minimum capacity of 1 and a maximum capacity of 1. The application stores data on an Amazon RDS for MySQL DB instance. The VPC has subnets configured in three Availability Zones and is configured with a single NAT gateway.The solutions architect needs to recommend a solution to ensure that the application will operate across multiple Availability Zones.Which solution will meet this requirement?",
    "choices": [
      "A. Deploy an additional NAT gateway in the other Availability Zones. Update the route tables with appropriate routes. Modify the RDS for MySQL DB instance to a Multi-AZ configuration. Configure the Auto Scaling group to launch the instances across Availability Zones. Set the minimum capacity and maximum capacity of the Auto Scaling group to 3.",
      "B. Replace the NAT gateway with a virtual private gateway. Replace the RDS for MySQL DB instance with an Amazon Aurora MySQL DB cluster. Configure the Auto Scaling group to launch instances across all subnets in the VPC. Set the minimum capacity and maximum capacity of the Auto Scaling group to 3.",
      "C. Replace the NAT gateway with a NAT instance. Migrate the RDS for MySQL DB instance to an RDS for PostgreSQL DB instance. Launch a new EC2 instance in the other Availability Zones.",
      "D. Deploy an additional NAT gateway in the other Availability Zones. Update the route tables with appropriate routes. Modify the RDS for MySQL DB instance to turn on automatic backups and retain the backups for 7 days. Configure the Auto Scaling group to launch instances across all subnets in the VPC. Keep the minimum capacity and the maximum capacity of the Auto Scaling group at 1."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\n\nThe correct answer is A because it addresses the key requirements to ensure the application runs across multiple Availability Zones (AZs):\n\n- Deploy an additional NAT gateway in the other AZs: This provides redundancy and high availability for the NAT gateway, which is a critical component for the application in the private subnet to access the internet.\n- Update the route tables with appropriate routes: Ensures the traffic is routed correctly to the NAT gateways in the respective AZs.\n- Modify the RDS for MySQL DB instance to a Multi-AZ configuration: This provides high availability and failover support for the database, ensuring data resilience across AZs.\n- Configure the Auto Scaling group to launch instances across AZs: This distributes the application instances across multiple AZs, improving overall application resilience and availability.\n- Set the minimum capacity and maximum capacity of the Auto Scaling group to 3: This ensures there are at least 3 application instances running, with the ability to scale up to 3, providing redundancy and high availability.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. This solution is inappropriate because:\n- Replacing the NAT gateway with a virtual private gateway would not provide the required internet access for the private subnet.\n- Replacing the RDS for MySQL DB instance with an Amazon Aurora MySQL DB cluster is unnecessary, as the requirement is to ensure the application runs across multiple AZs, not to change the database technology.\n\nC. This solution is inappropriate because:\n- Replacing the NAT gateway with a NAT instance is not a recommended practice, as NAT instances are less scalable and have a higher maintenance overhead compared to NAT gateways.\n- Migrating the RDS for MySQL DB instance to an RDS for PostgreSQL DB instance is unnecessary and unrelated to the requirement of running the application across multiple AZs.\n- Launching a new EC2 instance in the other AZs is not sufficient, as the application needs to be configured to run across multiple AZs.\n\nD. This solution is inappropriate because:\n- Deploying an additional NAT gateway in the other AZs and updating the route tables is correct, but the remaining changes are not necessary.\n-"
  },
  "447": {
    "question": "A company used AWS CloudFormation to create all new infrastructure in its AWS member accounts. The resources rarely change and are properly sized for the expected load. The monthly AWS bill is consistent.Occasionally, a developer creates a new resource for testing and forgets to remove the resource when the test is complete. Most of these tests last a few days before the resources are no longer needed.The company wants to automate the process of finding unused resources. A solutions architect needs to design a solution that determines whether the cost in the AWS bill is increasing. The solution must help identify resources that cause an increase in cost and must automatically notify the company's operations team.Which solution will meet these requirements?",
    "choices": [
      "A. Turn on billing alerts. Use AWS Cost Explorer to determine the costs for the past month. Create an Amazon CloudWatch alarm for total estimated charges. Specify a cost threshold that is higher than the costs that Cost Explorer determined. Add a notification to alert the operations team if the alarm threshold is breached.",
      "B. Turn on billing alerts. Use AWS Cost Explorer to determine the average monthly costs for the past 3 months. Create an Amazon CloudWatch alarm for total estimated charges. Specify a cost threshold that is higher than the costs that Cost Explorer determined. Add a notification to alert the operations team if the alarm threshold is breached.",
      "C. Use AWS Cost Anomaly Detection to create a cost monitor that has a monitor type of Linked account. Create a subscription to send daily AWS cost summaries to the operations team. Specify a threshold for cost variance.",
      "D. Use AWS Cost Anomaly Detection to create a cost monitor that has a monitor type of AWS services. Create a subscription to send daily AWS cost summaries to the operations team. Specify a threshold for cost variance."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (D):\n\nThe correct answer is D, which proposes using AWS Cost Anomaly Detection to create a cost monitor with a monitor type of \"AWS services\". This solution is the most appropriate to meet the requirements stated in the question.\n\nThe key advantages of this solution are:\n- It provides granular visibility into the cost of individual AWS services within the linked account, rather than just the overall account-level costs.\n- This granularity allows the solution to detect anomalies or spikes in the spending for specific services, even if there are offsetting decreases in other services.\n- By creating a subscription to send daily AWS cost summaries to the operations team and specifying a threshold for cost variance, the company can be proactively notified when there is an increase in the cost of any individual service that exceeds the defined threshold.\n- This enables the company to quickly identify the specific resources or services causing the cost increase and take appropriate action, such as removing unused resources.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This solution uses a CloudWatch alarm based on the total estimated charges for the account, as determined by AWS Cost Explorer. While this can detect overall cost increases, it does not provide the granular visibility required to identify the specific resources or services causing the cost increase.\n\nB. Similar to choice A, this solution uses a CloudWatch alarm based on the average monthly costs for the past 3 months. Again, this does not offer the necessary granularity to pinpoint the individual services or resources driving the cost increase.\n\nC. This solution uses a cost monitor with a \"Linked account\" monitor type, which would only provide visibility into the total account-level costs, not the individual service-level costs. This does not meet the requirement of identifying the specific resources or services causing the cost increase."
  },
  "448": {
    "question": "A company is deploying a new web-based application and needs a storage solution for the Linux application servers. The company wants to create a single location for updates to application data for all instances. The active dataset will be up to 100 GB in size. A solutions architect has determined that peak operations will occur for 3 hours daily and will require a total of 225 MiBps of read throughput.The solutions architect must design a Multi-AZ solution that makes a copy of the data available in another AWS Region for disaster recovery (DR). The DR copy has an RPO of less than 1 hour.Which solution will meet these requirements?",
    "choices": [
      "A. Deploy a new Amazon Elastic File System (Amazon EFS) Multi-AZ file system. Configure the file system for 75 MiBps of provisioned throughput. Implement replication to a file system in the DR Region.",
      "B. Deploy a new Amazon FSx for Lustre file system. Configure Bursting Throughput mode for the file system. Use AWS Backup to back up the file system to the DR Region.",
      "C. Deploy a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume with 225 MiBps of throughput. Enable Multi-Attach for the EBS volume. Use AWS Elastic Disaster Recovery to replicate the EBS volume to the DR Region.",
      "D. Deploy an Amazon FSx for OpenZFS file system in both the production Region and the DR Region. Create an AWS DataSync scheduled task to replicate the data from the production file system to the DR file system every 10 minutes."
    ],
    "answer": "A",
    "explanation": "1. Clear explanation of why the correct answer (A) is right:\n\nThe correct answer is A, which is to deploy a new Amazon Elastic File System (Amazon EFS) Multi-AZ file system. This solution meets the requirements in the following ways:\n\n- EFS supports Multi-AZ configuration, which provides high availability and durability of the file system across multiple Availability Zones within the same AWS Region.\n- EFS can be configured for provisioned throughput, which allows the solution to meet the required 225 MiBps of read throughput for the peak operations.\n- EFS supports cross-Region replication, which allows creating a copy of the data in another AWS Region for disaster recovery (DR) purposes.\n- The DR copy can be configured to have an RPO (Recovery Point Objective) of less than 1 hour, which meets the requirement.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nB. Deploy a new Amazon FSx for Lustre file system:\n- Amazon FSx for Lustre does not have a \"Bursting Throughput\" mode. This is a feature of Amazon EFS.\n- Using AWS Backup to back up the FSx for Lustre file system would not meet the RPO requirement of less than 1 hour.\n\nC. Deploy a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume:\n- EBS volumes, even in Multi-Attach mode, cannot be shared across multiple application servers simultaneously, which is a requirement in the question.\n- Using AWS Elastic Disaster Recovery to replicate the EBS volume to the DR Region would not meet the RPO requirement of less than 1 hour.\n\nD. Deploy an Amazon FSx for OpenZFS file system in both the production Region and the DR Region:\n- AWS DataSync tasks cannot be scheduled for a frequency less than 1 hour, which does not meet the RPO requirement of less than 1 hour."
  },
  "449": {
    "question": "A global ecommerce company has many data centers around the world. With the growth of its stored data, the company needs to set up a solution to provide scalable storage for legacy on-premises file applications. The company must be able to take point-in-time copies of volumes by using AWS Backup and must retain low-latency access to frequently accessed data. The company also needs to have storage volumes that can be mounted as Internet Small Computer System Interface (iSCSI) devices from the company\u2019s on-premises application servers.Which solution will meet these requirements?",
    "choices": [
      "A. Provision an AWS Storage Gateway tape gateway. Configure the tape gateway to store data in an Amazon S3 bucket. Deploy AWS Backup to take point-in-time copies of the volumes.",
      "B. Provision an Amazon FSx File Gateway and an Amazon S3 File Gateway. Deploy AWS Backup to take point-in-time copies of the data.",
      "C. Provision an AWS Storage Gateway volume gateway in cache mode. Back up the on-premises Storage Gateway volumes with AWS Backup.",
      "D. Provision an AWS Storage Gateway file gateway in cache mode. Deploy AWS Backup to take point-in-time copies of the volumes."
    ],
    "answer": "C",
    "explanation": "1. Clear explanation of why the correct answer (C) is right:\n\nThe correct answer is C, which is to provision an AWS Storage Gateway volume gateway in cache mode. This solution meets all the requirements stated in the question:\n\n- Scalable storage for legacy on-premises file applications: The Storage Gateway volume gateway in cache mode can provide scalable storage by caching frequently accessed data on-premises and storing the rest in AWS, allowing for virtually unlimited storage capacity.\n\n- Taking point-in-time copies of volumes using AWS Backup: The Storage Gateway volumes can be backed up using AWS Backup, which can take point-in-time copies of the data.\n\n- Retaining low-latency access to frequently accessed data: The \"cache\" mode of the Storage Gateway volume gateway ensures that frequently accessed data is cached on-premises, providing low-latency access.\n\n- Mounting storage volumes as iSCSI devices from on-premises application servers: The Storage Gateway volume gateway supports mounting storage volumes as iSCSI devices, which can be accessed by the on-premises application servers.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nA. Incorrect: Provisioning an AWS Storage Gateway tape gateway and storing data in an S3 bucket does not meet the requirement of providing low-latency access to frequently accessed data, as tape gateways are primarily for archiving data.\n\nB. Incorrect: Provisioning an Amazon FSx File Gateway and an Amazon S3 File Gateway does not provide the ability to mount storage volumes as iSCSI devices, which is a requirement in the question.\n\nD. Incorrect: Provisioning an AWS Storage Gateway file gateway in cache mode does not allow for mounting storage volumes as iSCSI devices, which is a requirement in the question."
  },
  "450": {
    "question": "A company that has multiple business units is using AWS Organizations with all features enabled. The company has implemented an account structure in which each business unit has its own AWS account. Administrators in each AWS account need to view detailed cost and utilization data for their account by using Amazon Athena.Each business unit can have access to only its own cost and utilization data. The IAM policies that govern the ability to set up AWS Cost and Usage Reports are in place. A central Cost and Usage Report that contains all data for the organization is already available in an Amazon S3 bucket.Which solution will meet these requirements with the LEAST operational complexity?",
    "choices": [
      "A. In the organization's management account, use AWS Resource Access Manager (AWS RAM) to share the Cost and Usage Report data with each member account.",
      "B. In the organization's management account, configure an S3 event to invoke an AWS Lambda function each time a new file arrives in the S3 bucket that contains the central Cost and Usage Report. Configure the Lambda function to extract each member account\u2019s data and to place the data in Amazon S3 under a separate prefix. Modify the S3 bucket policy to allow each member account to access its own prefix.",
      "C. In each member account, access AWS Cost Explorer. Create a new report that contains relevant cost information for the account. Save the report in Cost Explorer. Provide instructions that the account administrators can use to access the saved report.",
      "D. In each member account, create a new S3 bucket to store Cost and Usage Report data. Set up a Cost and Usage Report to deliver the data to the new S3 bucket."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B because it provides the least operational complexity solution to meet the requirements. Here's why:\n\n- The central Cost and Usage Report is already available in an Amazon S3 bucket, so there's no need to set up a new report in each member account (as in option D).\n- Using AWS RAM to share the Cost and Usage Report data with each member account (option A) would require additional setup and maintenance, as the access permissions would need to be managed separately for each member account.\n- Accessing the Cost and Usage Report data through AWS Cost Explorer (option C) would require manual effort by each member account administrator, which adds operational complexity.\n\nOption B, on the other hand, automates the process of extracting the data for each member account and placing it in a separate S3 prefix. This allows the member account administrators to access only their own cost and utilization data, meeting the requirement of restricting access to each business unit's data. The initial setup of the Lambda function and S3 bucket policy modifications may require some effort, but after that, the process is fully automated, making it the least operationally complex solution.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Using AWS RAM to share the Cost and Usage Report data: This solution would require additional setup and maintenance, as the access permissions would need to be managed separately for each member account. It adds operational complexity compared to the automated solution in option B.\n\nC. Accessing the Cost and Usage Report data through AWS Cost Explorer: This solution requires manual effort by each member account administrator to create and access the reports, which adds operational complexity compared to the automated solution in option B.\n\nD. Setting up a new Cost and Usage Report in each member account: This solution would require setting up and configuring a new report in each member account, which is more operationally complex than the centralized solution in option B."
  },
  "451": {
    "question": "A company is designing an AWS environment for a manufacturing application. The application has been successful with customers, and the application's user base has increased. The company has connected the AWS environment to the company's on-premises data center through a 1 Gbps AWS Direct Connect connection. The company has configured BGP for the connection.The company must update the existing network connectivity solution to ensure that the solution is highly available, fault tolerant, and secure.Which solution will meet these requirements MOST cost-effectively?",
    "choices": [
      "A. Add a dynamic private IP AWS Site-to-Site VPN as a secondary path to secure data in transit and provide resilience for the Direct Connect connection. Configure MACsec to encrypt traffic inside the Direct Connect connection.",
      "B. Provision another Direct Connect connection between the company's on-premises data center and AWS to increase the transfer speed and provide resilience. Configure MACsec to encrypt traffic inside the Direct Connect connection.",
      "C. Configure multiple private VIFs. Load balance data across the VIFs between the on-premises data center and AWS to provide resilience.",
      "D. Add a static AWS Site-to-Site VPN as a secondary path to secure data in transit and to provide resilience for the Direct Connect connection."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (D):\n\nThe correct answer is option D, which is to \"Add a static AWS Site-to-Site VPN as a secondary path to secure data in transit and to provide resilience for the Direct Connect connection.\"\n\nThis solution is the most cost-effective because it meets the requirements of high availability, fault tolerance, and security, without the need for additional Direct Connect connections or configuring complex load balancing across multiple VIFs.\n\nThe static AWS Site-to-Site VPN provides a secondary, redundant path for the connection between the on-premises data center and AWS, ensuring high availability and fault tolerance. The VPN connection can secure the data in transit, providing the necessary security.\n\nAdditionally, static VPN is a more cost-effective solution compared to adding another Direct Connect connection, as it does not require additional monthly fees for the physical circuit.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option is incorrect because MACsec encryption is only supported on 10Gbps and 100Gbps Direct Connect connections, not on the 1Gbps connection mentioned in the question.\n\nB. This option is incorrect because adding another Direct Connect connection would be more costly than the static VPN solution, without providing significantly more benefits in terms of availability, fault tolerance, or security.\n\nC. This option is incorrect because configuring multiple private VIFs and load balancing data across them does not provide the same level of redundancy and fault tolerance as a secondary VPN connection. Additionally, it may be more complex to configure and manage.\n\nIn summary, the correct answer (D) is the most cost-effective solution that meets the requirements of high availability, fault tolerance, and security, without the need for additional costly infrastructure or complex configurations."
  },
  "452": {
    "question": "A company needs to modernize an application and migrate the application to AWS. The application stores user profile data as text in a single table in an on-premises MySQL database.After the modernization, users will use the application to upload video files that are up to 4 GB in size. Other users must be able to download the video files from the application. The company needs a video storage solution that provides rapid scaling. The solution must not affect application performance.Which solution will meet these requirements?",
    "choices": [
      "A. Migrate the database to Amazon Aurora PostgreSQL by using AWS Database Migration Service (AWS DMS). Store the videos as base64-encoded strings in a TEXT column in the database.",
      "B. Migrate the database to Amazon DynamoDB by using AWS Database Migration Service (AWS DMS) with the AWS Schema Conversion Tool (AWS SCT). Store the videos as objects in Amazon S3. Store the S3 key in the corresponding DynamoDB item.",
      "C. Migrate the database to Amazon Keyspaces (for Apache Cassandra) by using AWS Database Migration Service (AWS DMS) with the AWS Schema Conversion Tool (AWS SCT). Store the videos as objects in Amazon S3. Store the S3 object identifier in the corresponding Amazon Keyspaces entry.",
      "D. Migrate the database to Amazon DynamoDB by using AWS Database Migration Service (AWS DMS) with the AWS Schema Conversion Tool (AWS SCT). Store the videos as base64-encoded strings in the corresponding DynamoDB item."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B because it provides the most appropriate solution for the given requirements:\n\n- Storing the video files as objects in Amazon S3 allows for rapid scaling and handles the large file size of up to 4 GB. S3 is designed to handle large files and provides reliable and scalable storage.\n- Storing the S3 object key in the corresponding DynamoDB item allows for efficient retrieval and management of the video metadata, without directly storing the large video files in the database. This approach separates the storage of the video files (in S3) and the metadata (in DynamoDB), which helps to maintain application performance.\n- Using AWS Database Migration Service (AWS DMS) with the AWS Schema Conversion Tool (AWS SCT) to migrate the existing MySQL database to DynamoDB ensures a smooth transition to the new architecture.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option is not suitable because storing the large video files as base64-encoded strings in the database would significantly impact the database's performance and scalability, especially for files up to 4 GB in size.\n\nC. Amazon Keyspaces (for Apache Cassandra) is not the most appropriate choice for this use case, as Keyspaces is designed for high-throughput, low-latency applications, which may not be the primary requirement here. Additionally, the need to store the S3 object identifier in the Keyspaces entry adds an unnecessary layer of complexity.\n\nD. Similar to option A, storing the large video files as base64-encoded strings in DynamoDB items would not be scalable and would likely affect the application's performance due to the size of the data."
  },
  "453": {
    "question": "A company stores and manages documents in an Amazon Elastic File System (Amazon EFS) file system. The file system is encrypted with an AWS Key Management Service (AWS KMS) key. The file system is mounted to an Amazon EC2 instance that runs proprietary software.The company has enabled automatic backups for the file system. The automatic backups use the AWS Backup default backup plan.A solutions architect must ensure that deleted documents can be recovered within an RPO of 100 minutes.Which solution will meet these requirements?",
    "choices": [
      "A. Create a new IAM role. Create a new backup plan. Use the new IAM role to create backups. Update the KMS key policy to allow the new IAM role to use the key. Implement an hourly backup schedule for the file system.",
      "B. Create a new backup plan. Update the KMS key policy to allow the AWSServiceRoleForBackup IAM role to use the key. Implement a custom cron expression to run a backup of the file system every 30 minutes.",
      "C. Create a new IAM role. Use the existing backup plan. Update the KMS key policy to allow the new IAM role to use the key. Enable continuous backups for point-in-time recovery.",
      "D. Use the existing backup plan. Update the KMS key policy to allow the AWSServiceRoleForBackup IAM role to use the key. Enable Cross-Region Replication for the file system."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\n\nThe correct answer is A because it addresses the key requirements stated in the question:\n\n- The RPO needs to be 100 minutes, which means the backups need to be taken more frequently than the default daily backup plan provided by AWS Backup.\n- To achieve this, the solution needs to create a new backup plan with an hourly backup schedule, which can be done by creating a new IAM role and updating the KMS key policy to allow the new role to use the key for backup operations.\n\n2. Explanations of the incorrect choices:\n\nB. This is incorrect because the question states that the minimum backup interval in AWS Backup is 1 hour, not 30 minutes as suggested in this choice.\n\nC. This is incorrect because the question states that the company has already enabled automatic backups using the AWS Backup default backup plan, and the requirement is to ensure that deleted documents can be recovered within an RPO of 100 minutes. Enabling continuous backups for point-in-time recovery would not meet the specific RPO requirement.\n\nD. This is incorrect because Cross-Region Replication for the file system would not provide the necessary backup frequency to meet the 100-minute RPO requirement. Cross-Region Replication is used for disaster recovery purposes, not for frequent backups."
  },
  "454": {
    "question": "A solutions architect must provide a secure way for a team of cloud engineers to use the AWS CLI to upload objects into an Amazon S3 bucket. Each cloud engineer has an IAM user, IAM access keys, and a virtual multi-factor authentication (MFA) device. The IAM users for the cloud engineers are in a group that is named S3-access. The cloud engineers must use MFA to perform any actions in Amazon S3.Which solution will meet these requirements?",
    "choices": [
      "A. Attach a policy to the S3 bucket to prompt the IAM user for an MFA code when the IAM user performs actions on the S3 bucket. Use IAM access keys with the AWS CLI to call Amazon S3.",
      "B. Update the trust policy for the S3-access group to require principals to use MFA when principals assume the group. Use IAM access keys with the AWS CLI to call Amazon S3.",
      "C. Attach a policy to the S3-access group to deny all S3 actions unless MFA is present. Use IAM access keys with the AWS CLI to call Amazon S3.",
      "D. Attach a policy to the S3-access group to deny all S3 actions unless MFA is present. Request temporary credentials from AWS Security Token Service (AWS STS). Attach the temporary credentials in a profile that Amazon S3 will reference when the user performs actions in Amazon S3."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (D):\nThe correct answer is D because it provides the most secure solution that meets all the requirements stated in the question:\n\n- It uses AWS Security Token Service (AWS STS) to request temporary credentials that include the requirement for multi-factor authentication (MFA). This ensures that the cloud engineers must use MFA when performing any actions in Amazon S3, as per the requirement.\n- It then attaches the temporary credentials to a named profile in the AWS CLI, which allows the cloud engineers to use the AWS CLI to interact with Amazon S3 in a secure manner.\n- This approach follows the principle of least privilege, as the cloud engineers are not using their long-term IAM access keys directly, which would grant broader permissions.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option uses IAM access keys with the AWS CLI, which does not enforce the use of MFA for all S3 actions. The bucket policy may not work consistently with the AWS CLI.\n\nB. This option updates the trust policy for the S3-access group, but does not enforce the use of MFA specifically for S3 actions. It may not work as intended with the AWS CLI.\n\nC. This option uses IAM access keys with the AWS CLI, which does not provide temporary credentials that include the MFA requirement. This would require the cloud engineers to use their long-term IAM access keys, which is less secure and does not follow the principle of least privilege."
  },
  "455": {
    "question": "A company needs to migrate 60 on-premises legacy applications to AWS. The applications are based on the NET Framework and run on Windows.The company needs a solution that minimizes migration time and requires no application code changes. The company also does not want to manage the infrastructure.Which solution will meet these requirements?",
    "choices": [
      "A. Refactor the applications and containerize them by using AWS Toolkit for NET Refactoring. Use Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type to host the containerized applications.",
      "B. Use the Windows Web Application Migration Assistant to migrate the applications to AWS Elastic Beanstalk. Use Elastic Beanstalk to deploy and manage the applications.",
      "C. Use the Windows Web Application Migration Assistant to migrate the applications to Amazon EC2 instances. Use the EC2 instances to deploy and manage the applications.",
      "D. Refactor the applications and containerize them by using AWS Toolkit for NET Refactoring. Use Amazon Elastic Kubernetes Service (Amazon EKS) with the Fargate launch type to host the containerized applications."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B, which is to use the Windows Web Application Migration Assistant to migrate the .NET Framework-based applications to AWS Elastic Beanstalk. This solution meets all the requirements stated in the question:\n\n- Minimizes migration time: The Windows Web Application Migration Assistant is specifically designed to migrate .NET Framework applications to AWS with minimal effort, avoiding the need for time-consuming code changes.\n- Requires no application code changes: The migration assistant allows the applications to be migrated without any modifications to the application code.\n- Does not require managing the infrastructure: AWS Elastic Beanstalk abstracts the underlying infrastructure management, allowing the company to focus on deploying and managing their applications without having to worry about the underlying EC2 instances, scaling, load balancing, and other infrastructure-related tasks.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Refactor the applications and containerize them using AWS Toolkit for .NET Refactoring, and use Amazon ECS with Fargate:\nThis option involves refactoring the applications, which goes against the requirement of \"no application code changes.\" Containerizing the applications would also require more effort compared to the Elastic Beanstalk approach.\n\nC. Use the Windows Web Application Migration Assistant to migrate the applications to Amazon EC2 instances:\nThis option would still require the company to manage the EC2 instances, configure networking, load balancing, and auto-scaling, which contradicts the requirement of not managing the infrastructure.\n\nD. Refactor the applications and containerize them using AWS Toolkit for .NET Refactoring, and use Amazon EKS with Fargate:\nSimilar to option A, this option involves refactoring the applications, which goes against the requirement of \"no application code changes.\" Additionally, using Amazon EKS would require more infrastructure management compared to the Elastic Beanstalk approach."
  },
  "456": {
    "question": "A company needs to run large batch-processing jobs on data that is stored in an Amazon S3 bucket. The jobs perform simulations. The results of the jobs are not time sensitive, and the process can withstand interruptions.Each job must process 15-20 GB of data when the data is stored in the S3 bucket. The company will store the output from the jobs in a different Amazon S3 bucket for further analysis.Which solution will meet these requirements MOST cost-effectively?",
    "choices": [
      "A. Create a serverless data pipeline. Use AWS Step Functions for orchestration. Use AWS Lambda functions with provisioned capacity to process the data.",
      "B. Create an AWS Batch compute environment that includes Amazon EC2 Spot Instances. Specify the SPOT_CAPACITY_OPTIMIZED allocation strategy.",
      "C. Create an AWS Batch compute environment that includes Amazon EC2 On-Demand Instances and Spot Instances. Specify the SPOT_CAPACITY_OPTIMIZED allocation strategy for the Spot Instances.",
      "D. Use Amazon Elastic Kubernetes Service (Amazon EKS) to run the processing jobs. Use managed node groups that contain a combination of Amazon EC2 On-Demand Instances and Spot Instances."
    ],
    "answer": "B",
    "explanation": "1. Explanation of why the correct answer (B) is right:\n\nThe correct answer is B, which is to create an AWS Batch compute environment that includes Amazon EC2 Spot Instances and specifies the SPOT_CAPACITY_OPTIMIZED allocation strategy.\n\nThis solution is the most cost-effective because:\n- AWS Batch is well-suited for running large batch-processing jobs, which aligns with the requirements.\n- Utilizing Spot Instances provides significant cost savings compared to On-Demand Instances, as the question states that the jobs are not time-sensitive and can withstand interruptions.\n- The SPOT_CAPACITY_OPTIMIZED allocation strategy ensures that AWS Batch selects the most cost-effective and available Spot Instances, further optimizing the cost-effectiveness.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Create a serverless data pipeline using AWS Step Functions and AWS Lambda:\n   - This solution is more complex and may not be as cost-effective as the Batch-based approach, as it requires additional service usage (Step Functions, Lambda) and potentially more overhead.\n   - The question specifically mentions batch-processing jobs, which are better suited for a Batch-based solution than a serverless pipeline.\n\nC. Create an AWS Batch compute environment with a mix of On-Demand and Spot Instances:\n   - This solution is less cost-effective than the Spot-only approach in option B, as it still incurs the higher cost of On-Demand Instances.\n   - The SPOT_CAPACITY_OPTIMIZED allocation strategy is only available for Spot Instances, not for a mix of On-Demand and Spot Instances.\n\nD. Use Amazon EKS with a mix of On-Demand and Spot Instances:\n   - This solution is more complex and may not be as cost-effective as the Batch-based approach, as it requires the management of a Kubernetes cluster.\n   - The question does not indicate a requirement for containerization or a Kubernetes-based solution, so a simpler Batch-based approach is likely more appropriate."
  },
  "457": {
    "question": "A company has an application that analyzes and stores image data on premises. The application receives millions of new image files every day. Files are an average of 1 MB in size. The files are analyzed in batches of 1 GB. When the application analyzes a batch, the application zips the images together. The application then archives the images as a single file in an on-premises NFS server for long-term storage.The company has a Microsoft Hyper-V environment on premises and has compute capacity available. The company does not have storage capacity and wants to archive the images on AWS. The company needs the ability to retrieve archived data within 1 week of a request.The company has a 10 Gbps AWS Direct Connect connection between its on-premises data center and AWS. The company needs to set bandwidth limits and schedule archived images to be copied to AWS during non-business hours.Which solution will meet these requirements MOST cost-effectively?",
    "choices": [
      "A. Deploy an AWS DataSync agent on a new GPU-based Amazon EC2 instance. Configure the DataSync agent to copy the batch of files from the NFS on-premises server to Amazon S3 Glacier Instant Retrieval. After the successful copy, delete the data from the on-premises storage.",
      "B. Deploy an AWS DataSync agent as a Hyper-V VM on premises. Configure the DataSync agent to copy the batch of files from the NFS on-premises server to Amazon S3 Glacier Deep Archive. After the successful copy, delete the data from the on-premises storage.",
      "C. Deploy an AWS DataSync agent on a new general purpose Amazon EC2 instance. Configure the DataSync agent to copy the batch of files from the NFS on-premises server to Amazon S3 Standard. After the successful copy, delete the data from the on-premises storage. Create an S3 Lifecycle rule to transition objects from S3 Standard to S3 Glacier Deep Archive after 1 day.",
      "D. Deploy an AWS Storage Gateway Tape Gateway on premises in the Hyper-V environment. Connect the Tape Gateway to AWS. Use automatic tape creation. Specify an Amazon S3 Glacier Deep Archive pool. Eject the tape after the batch of images is copied."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (Option B):\n\nOption B is the correct answer because it meets the key requirements of the scenario in the most cost-effective way:\n\n- Deploys the AWS DataSync agent as a Hyper-V VM on-premises, leveraging the existing Hyper-V environment and compute capacity.\n- Configures the DataSync agent to copy the batch of image files directly from the on-premises NFS server to Amazon S3 Glacier Deep Archive.\n- Deleting the data from the on-premises storage after successful copy to S3 Glacier Deep Archive, which provides long-term, low-cost storage for the archived data.\n- S3 Glacier Deep Archive meets the requirement of retrieving the archived data within 1 week, as it has a retrieval time of 12-24 hours.\n\nThis solution is the most cost-effective as it utilizes the existing on-premises Hyper-V environment and the lower-cost S3 Glacier Deep Archive storage class, without the need for additional compute resources or transitioning data through more expensive storage tiers.\n\n2. Explanations of why the incorrect choices are wrong:\n\nOption A:\n- AWS DataSync with a GPU-based EC2 instance is unnecessary for this use case, as the image analysis and processing is performed on-premises.\n- S3 Glacier Instant Retrieval is not required, as the requirement is to retrieve data within 1 week, which S3 Glacier Deep Archive can provide.\n\nOption C:\n- Storing the data in S3 Standard for 1 day before transitioning to S3 Glacier Deep Archive is an unnecessary additional cost.\n- The requirement is to archive the data, not to store it in S3 Standard long-term.\n\nOption D:\n- Using the AWS Storage Gateway Tape Gateway is an overly complex solution for this use case, as it involves the additional step of ejecting tapes.\n- The requirement can be met more simply and cost-effectively using the AWS DataSync agent to directly copy the data to S3 Glacier Deep Archive."
  },
  "458": {
    "question": "A company wants to record key performance indicators (KPIs) from its application as part of a strategy to convert to a user-based licensing schema. The application is a multi-tier application with a web-based UI. The company saves all log files to Amazon CloudWatch by using the CloudWatch agent. All logins to the application are saved in a log file.As part of the new license schema, the company needs to find out how many unique users each client has on a daily basis, weekly basis, and monthly basis.Which solution will provide this information with the LEAST change to the application?",
    "choices": [
      "A. Configure an Amazon CloudWatch Logs metric filter that saves each successful login as a metric. Configure the user name and client name as dimensions for the metric.",
      "B. Change the application logic to make each successful login generate a call to the AWS SDK to increment a custom metric that records user name and client name dimensions in CloudWatch.",
      "C. Configure the CloudWatch agent to extract successful login metrics from the logs. Additionally, configure the CloudWatch agent to save the successful login metrics as a custom metric that uses the user name and client name as dimensions for the metric.",
      "D. Configure an AWS Lambda function to consume an Amazon CloudWatch Logs stream of the application logs. Additionally, configure the Lambda function to increment a custom metric in CloudWatch that uses the user name and client name as dimensions for the metric."
    ],
    "answer": "D",
    "explanation": "1. Clear explanation of why the correct answer (D) is right:\n\nThe correct answer is D, which involves configuring an AWS Lambda function to consume the Amazon CloudWatch Logs stream of the application logs, and then using the Lambda function to increment a custom metric in CloudWatch that uses the user name and client name as dimensions.\n\nThis solution is the best choice because it requires the least change to the existing application. The application's logging setup (saving logs to CloudWatch) remains unchanged, and the custom metric generation is handled entirely in the Lambda function, without requiring any modifications to the application code.\n\nThe Lambda function can process the log events, deduplicate the logins to track unique users, and then emit the custom CloudWatch metrics with the necessary user and client dimensions. This approach provides full control over the unique user tracking logic and ensures accurate results, as it can leverage a database (e.g., DynamoDB) to store temporary state and handle the deduplication.\n\nAdditionally, the flexibility of CloudWatch metrics allows for easy aggregation of the data over daily, weekly, and monthly periods, which is a key requirement for the license schema.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nA. Configure an Amazon CloudWatch Logs metric filter: This approach is not suitable because metric filters can only count the total number of logins, not the unique users. Metric filters lack the ability to deduplicate the logins and track unique user-client combinations.\n\nB. Change the application logic to make each successful login generate a call to the AWS SDK: This solution would require changes to the application code, which goes against the \"least change\" requirement specified in the question.\n\nC. Configure the CloudWatch agent to extract successful login metrics from the logs: The CloudWatch agent cannot natively deduplicate logins or track unique users. It would require additional custom logic to achieve the desired functionality, which is not as efficient as the Lambda-based solution."
  },
  "459": {
    "question": "A company is using GitHub Actions to run a CI/CD pipeline that accesses resources on AWS. The company has an IAM user that uses a secret key in the pipeline to authenticate to AWS. An existing IAM role with an attached policy grants the required permissions to deploy resources.The company\u2019s security team implements a new requirement that pipelines can no longer use long-lived secret keys. A solutions architect must replace the secret key with a short-lived solution.Which solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      "A. Create an IAM SAML 2.0 identity provider (IdP) in AWS Identity and Access Management (IAM). Create a new IAM role with the appropriate trust policy that allows the sts:AssumeRole API call. Attach the existing IAM policy to the new IAM role. Update GitHub to use SAML authentication for the pipeline.",
      "B. Create an IAM OpenID Connect (OIDC) identity provider (IdP) in AWS Identity and Access Management (IAM). Create a new IAM role with the appropriate trust policy that allows the sts:AssumeRoleWithWebIdentity API call from the GitHub OIDC IdP. Update GitHub to assume the role for the pipeline.",
      "C. Create an Amazon Cognito identity pool. Configure the authentication provider to use GitHub. Create a new IAM role with the appropriate trust policy that allows the sts:AssumeRoleWithWebIdentity API call from the GitHub authentication provider. Configure the pipeline to use Cognito as its authentication provider.",
      "D. Create a trust anchor to AWS Private Certificate Authority. Generate a client certificate to use with AWS IAM Roles Anywhere. Create a new IAM role with the appropriate trust policy that allows the sts:AssumeRole API call. Attach the existing IAM policy to the new IAM role. Configure the pipeline to use the credential helper tool and to reference the client certificate public key to assume the new IAM role."
    ],
    "answer": "B",
    "explanation": "1. Clear explanation of why the correct answer (B) is right:\n\nThe correct answer is B because it involves the use of IAM OpenID Connect (OIDC) identity provider, which is the recommended approach for integrating GitHub Actions with AWS resources. OIDC provides a short-lived token for authentication, which meets the security team's new requirement of not using long-lived secret keys. By creating an IAM role with the appropriate trust policy that allows the sts:AssumeRoleWithWebIdentity API call from the GitHub OIDC IdP, the pipeline can assume the necessary permissions to access AWS resources without using long-lived credentials. This solution has the least operational overhead compared to the other options.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nA. This option is incorrect because GitHub does not support the aging SAML protocol, which is required for the SAML 2.0 identity provider.\n\nC. This option is incorrect because it involves creating an Amazon Cognito identity pool, which is an overengineered solution for the given use case. The requirement can be met more efficiently using the IAM OIDC provider.\n\nD. This option is incorrect because it involves creating a trust anchor to AWS Private CA and using IAM Roles Anywhere, which is an unnecessarily complex solution for the given use case. The requirement can be met more efficiently using the IAM OIDC provider."
  },
  "460": {
    "question": "A company is running a web-crawling process on a list of target URLs to obtain training documents for machine learning training algorithms. A fleet of Amazon EC2 t2.micro instances pulls the target URLs from an Amazon Simple Queue Service (Amazon SQS) queue. The instances then write the result of the crawling algorithm as a .csv file to an Amazon Elastic File System (Amazon EFS) volume. The EFS volume is mounted on all instances of the fleet.A separate system adds the URLs to the SQS queue at infrequent rates. The instances crawl each URL in 10 seconds or less.Metrics indicate that some instances are idle when no URLs are in the SQS queue. A solutions architect needs to redesign the architecture to optimize costs.Which combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
    "choices": [
      "A. Use m5.8xlarge instances instead of t2.micro instances for the web-crawling process. Reduce the number of instances in the fleet by 50%.",
      "B. Convert the web-crawling process into an AWS Lambda function. Configure the Lambda function to pull URLs from the SQS queue.",
      "C. Modify the web-crawling process to store results in Amazon Neptune.",
      "D. Modify the web-crawling process to store results in an Amazon Aurora Serverless MySQL instance.",
      "E. Modify the web-crawling process to store results in Amazon S3."
    ],
    "answer": "BE",
    "explanation": "1. Explanation of the correct answer (BE):\n\nThe correct answer is B and E, which is the most cost-effective approach to optimizing the architecture.\n\nB. Convert the web-crawling process into an AWS Lambda function. Configure the Lambda function to pull URLs from the SQS queue.\nThis is the most cost-effective solution because it eliminates the need to manage and maintain a fleet of EC2 instances. With Lambda, the company only pays for the compute time used by the function, which can be more cost-effective than running EC2 instances, especially when the instances are idle. Lambda also automatically scales based on the incoming workload, further optimizing costs.\n\nE. Modify the web-crawling process to store results in Amazon S3.\nStoring the crawling results in Amazon S3 is a cost-effective solution compared to using EFS or other file storage services. S3 is an object storage service that is designed for large-scale, high-throughput workloads, making it well-suited for storing the crawling results. It also provides high durability and availability, meeting the requirements of the production-grade system.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Use m5.8xlarge instances instead of t2.micro instances for the web-crawling process. Reduce the number of instances in the fleet by 50%.\nThis solution is not the most cost-effective because it involves using larger and more powerful EC2 instances, which would likely increase the overall cost of the system. Reducing the number of instances by 50% may help, but it does not address the issue of instances being idle when there are no URLs in the SQS queue.\n\nC. Modify the web-crawling process to store results in Amazon Neptune.\nStoring the crawling results in Amazon Neptune, a graph database service, is not the most cost-effective solution for this use case. Graph databases are typically more expensive and complex than simpler storage options like S3, and the requirements of the system do not seem to call for the use of a graph database.\n\nD. Modify the web-crawling process to store results in an Amazon Aurora Serverless MySQL instance.\nUsing an Amazon Aurora Serverless MySQL instance to store the crawling results is also not the most cost"
  },
  "461": {
    "question": "A company needs to implement disaster recovery for a critical application that runs in a single AWS Region. The application's users interact with a web frontend that is hosted on Amazon EC2 instances behind an Application Load Balancer (ALB). The application writes to an Amazon RDS for MySQL DB instance. The application also outputs processed documents that are stored in an Amazon S3 bucket.The company\u2019s finance team directly queries the database to run reports. During busy periods, these queries consume resources and negatively affect application performance.A solutions architect must design a solution that will provide resiliency during a disaster. The solution must minimize data loss and must resolve the performance problems that result from the finance team's queries.Which solution will meet these requirements?",
    "choices": [
      "A. Migrate the database to Amazon DynamoDB and use DynamoDB global tables. Instruct the finance team to query a global table in a separate Region. Create an AWS Lambda function to periodically synchronize the contents of the original S3 bucket to a new S3 bucket in the separate Region. Launch EC2 instances and create an ALB in the separate Region. Configure the application to point to the new S3 bucket.",
      "B. Launch additional EC2 instances that host the application in a separate Region. Add the additional instances to the existing ALIn the separate Region, create a read replica of the RDS DB instance. Instruct the finance team to run queries against the read replica. Use S3 Cross-Region Replication (CRR) from the original S3 bucket to a new S3 bucket in the separate Region. During a disaster, promote the read replica to a standalone DB instance. Configure the application to point to the new S3 bucket and to the newly promoted read replica.",
      "C. Create a read replica of the RDS DB instance in a separate Region. Instruct the finance team to run queries against the read replica. Create AMIs of the EC2 instances that host the application frontend. Copy the AMIs to the separate Region. Use S3 Cross-Region Replication (CRR) from the original S3 bucket to a new S3 bucket in the separate Region. During a disaster, promote the read replica to a standalone DB instance. Launch EC2 instances from the AMIs and create an ALB to present the application to end users. Configure the application to point to the new S3 bucket.",
      "D. Create hourly snapshots of the RDS DB instance. Copy the snapshots to a separate Region. Add an Amazon ElastiCache cluster in front of the existing RDS database. Create AMIs of the EC2 instances that host the application frontend. Copy the AMIs to the separate Region. Use S3 Cross-Region Replication (CRR) from the original S3 bucket to a new S3 bucket in the separate Region. During a disaster, restore the database from the latest RDS snapshot. Launch EC2 instances from the AMIs and create an ALB to present the application to end users. Configure the application to point to the new S3 bucket."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (Option C):\n\nOption C is the correct answer because it provides the most comprehensive and resilient disaster recovery solution for the given requirements.\n\nKey points:\n\n- It creates a read replica of the RDS DB instance in a separate AWS Region. This addresses the performance issue caused by the finance team's queries, as they can be directed to the read replica instead of the primary DB.\n- It uses S3 Cross-Region Replication (CRR) to synchronize the data in the original S3 bucket to a new S3 bucket in the separate Region. This ensures minimal data loss in case of a disaster.\n- It creates AMIs (Amazon Machine Images) of the EC2 instances hosting the application frontend and copies them to the separate Region. This allows for quick deployment of the application in the disaster recovery Region.\n- During a disaster, the solution can promote the read replica to a standalone DB instance and launch the EC2 instances from the copied AMIs, configuring the application to point to the new S3 bucket and the newly promoted DB instance.\n\n2. Explanations of why the other options are incorrect:\n\nOption A:\n- Migrating the database to DynamoDB and using global tables may not be the best solution, as the company's finance team directly queries the database, and DynamoDB may not be the most suitable for their reporting needs.\n- Periodically synchronizing the S3 bucket contents with a Lambda function can lead to data loss, as the solution does not provide a real-time synchronization mechanism.\n\nOption B:\n- Adding additional EC2 instances in a separate Region and associating them with the existing ALB is not possible, as the ALB is a regional service and cannot span multiple Regions.\n- Promoting the read replica to a standalone DB instance during a disaster is a good approach, but the solution does not mention configuring the application to point to the new DB instance.\n\nOption D:\n- Hourly snapshots of the RDS DB instance can lead to data loss, as the solution does not provide a real-time data synchronization mechanism.\n- Adding an Amazon ElastiCache cluster in front of the RDS database may not address the performance issues caused by the finance team's queries, as the ElastiCache would be in the same"
  },
  "462": {
    "question": "A company needs to implement disaster recovery for a critical application that runs in a single AWS Region. The application's users interact with a web frontend that is hosted on Amazon EC2 instances behind an Application Load Balancer (ALB). The application writes to an Amazon RDS for MySQL DB instance. The application also outputs processed documents that are stored in an Amazon S3 bucket.The company\u2019s finance team directly queries the database to run reports. During busy periods, these queries consume resources and negatively affect application performance.A solutions architect must design a solution that will provide resiliency during a disaster. The solution must minimize data loss and must resolve the performance problems that result from the finance team's queries.Which solution will meet these requirements?",
    "choices": [
      "A. Migrate the database to Amazon DynamoDB and use DynamoDB global tables. Instruct the finance team to query a global table in a separate Region. Create an AWS Lambda function to periodically synchronize the contents of the original S3 bucket to a new S3 bucket in the separate Region. Launch EC2 instances and create an ALB in the separate Region. Configure the application to point to the new S3 bucket.",
      "B. Launch additional EC2 instances that host the application in a separate Region. Add the additional instances to the existing ALIn the separate Region, create a read replica of the RDS DB instance. Instruct the finance team to run queries against the read replica. Use S3 Cross-Region Replication (CRR) from the original S3 bucket to a new S3 bucket in the separate Region. During a disaster, promote the read replica to a standalone DB instance. Configure the application to point to the new S3 bucket and to the newly promoted read replica.",
      "C. Create a read replica of the RDS DB instance in a separate Region. Instruct the finance team to run queries against the read replica. Create AMIs of the EC2 instances that host the application frontend. Copy the AMIs to the separate Region. Use S3 Cross-Region Replication (CRR) from the original S3 bucket to a new S3 bucket in the separate Region. During a disaster, promote the read replica to a standalone DB instance. Launch EC2 instances from the AMIs and create an ALB to present the application to end users. Configure the application to point to the new S3 bucket.",
      "D. Create hourly snapshots of the RDS DB instance. Copy the snapshots to a separate Region. Add an Amazon ElastiCache cluster in front of the existing RDS database. Create AMIs of the EC2 instances that host the application frontend. Copy the AMIs to the separate Region. Use S3 Cross-Region Replication (CRR) from the original S3 bucket to a new S3 bucket in the separate Region. During a disaster, restore the database from the latest RDS snapshot. Launch EC2 instances from the AMIs and create an ALB to present the application to end users. Configure the application to point to the new S3 bucket."
    ],
    "answer": "C",
    "explanation": "Correct Answer: C\n\nExplanation:\n\n1. Why the correct answer (C) is right:\n   - The question requires a solution that provides resiliency during a disaster, minimizes data loss, and resolves the performance problems caused by the finance team's database queries.\n   - Option C addresses these requirements by:\n     - Creating a read replica of the RDS DB instance in a separate Region, which allows the finance team to run queries against the read replica without affecting the application's performance.\n     - Using S3 Cross-Region Replication (CRR) to synchronize data between the original S3 bucket and a new S3 bucket in the separate Region, ensuring minimal data loss in case of a disaster.\n     - Creating AMIs of the EC2 instances that host the application frontend and copying them to the separate Region. This allows for the quick launch of EC2 instances in the separate Region during a disaster.\n     - During a disaster, the read replica can be promoted to a standalone DB instance, and the application can be reconfigured to point to the new S3 bucket and the newly promoted DB instance.\n\nIncorrect Choices:\n\nA. This option is not optimal because:\n   - Migrating the database to DynamoDB and using DynamoDB global tables may not be the best solution for this scenario, as the application is already using an RDS MySQL database.\n   - Periodically synchronizing the S3 bucket contents using a Lambda function can lead to data loss, as the function may not be able to keep up with the rate of changes during a disaster.\n\nB. This option is not optimal because:\n   - Adding the additional EC2 instances in the separate Region to the existing ALB is not possible, as ALBs are regional resources and cannot span multiple Regions.\n   - Promoting the read replica to a standalone DB instance during a disaster is not explicitly mentioned in this option.\n\nD. This option is not optimal because:\n   - Hourly snapshots of the RDS DB instance may result in data loss, as the data between the last snapshot and the disaster event could be lost.\n   - The use of an Amazon ElastiCache cluster to address the performance issues caused by the finance team's queries is not a direct solution to the problem and may introduce additional complexity."
  },
  "463": {
    "question": "A company needs to implement disaster recovery for a critical application that runs in a single AWS Region. The application's users interact with a web frontend that is hosted on Amazon EC2 instances behind an Application Load Balancer (ALB). The application writes to an Amazon RDS for MySQL DB instance. The application also outputs processed documents that are stored in an Amazon S3 bucket.The company\u2019s finance team directly queries the database to run reports. During busy periods, these queries consume resources and negatively affect application performance.A solutions architect must design a solution that will provide resiliency during a disaster. The solution must minimize data loss and must resolve the performance problems that result from the finance team's queries.Which solution will meet these requirements?",
    "choices": [
      "A. Migrate the database to Amazon DynamoDB and use DynamoDB global tables. Instruct the finance team to query a global table in a separate Region. Create an AWS Lambda function to periodically synchronize the contents of the original S3 bucket to a new S3 bucket in the separate Region. Launch EC2 instances and create an ALB in the separate Region. Configure the application to point to the new S3 bucket.",
      "B. Launch additional EC2 instances that host the application in a separate Region. Add the additional instances to the existing ALIn the separate Region, create a read replica of the RDS DB instance. Instruct the finance team to run queries against the read replica. Use S3 Cross-Region Replication (CRR) from the original S3 bucket to a new S3 bucket in the separate Region. During a disaster, promote the read replica to a standalone DB instance. Configure the application to point to the new S3 bucket and to the newly promoted read replica.",
      "C. Create a read replica of the RDS DB instance in a separate Region. Instruct the finance team to run queries against the read replica. Create AMIs of the EC2 instances that host the application frontend. Copy the AMIs to the separate Region. Use S3 Cross-Region Replication (CRR) from the original S3 bucket to a new S3 bucket in the separate Region. During a disaster, promote the read replica to a standalone DB instance. Launch EC2 instances from the AMIs and create an ALB to present the application to end users. Configure the application to point to the new S3 bucket.",
      "D. Create hourly snapshots of the RDS DB instance. Copy the snapshots to a separate Region. Add an Amazon ElastiCache cluster in front of the existing RDS database. Create AMIs of the EC2 instances that host the application frontend. Copy the AMIs to the separate Region. Use S3 Cross-Region Replication (CRR) from the original S3 bucket to a new S3 bucket in the separate Region. During a disaster, restore the database from the latest RDS snapshot. Launch EC2 instances from the AMIs and create an ALB to present the application to end users. Configure the application to point to the new S3 bucket."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C because it addresses all the requirements mentioned in the question:\n\n- It creates a read replica of the RDS DB instance in a separate Region, which resolves the performance issues caused by the finance team's queries. The finance team can be instructed to run their queries against the read replica.\n- It uses S3 Cross-Region Replication (CRR) to synchronize data between the original S3 bucket and a new S3 bucket in the separate Region, ensuring minimal data loss in case of a disaster.\n- It creates AMIs of the EC2 instances hosting the application frontend and copies them to the separate Region. This allows for launching EC2 instances from the AMIs and creating an ALB to present the application to end users during a disaster.\n- During a disaster, the solution promotes the read replica to a standalone DB instance and configures the application to point to the new S3 bucket, providing a resilient and fully functional application.\n\n2. Explanations of why the other choices are incorrect:\n\nA. This solution is not ideal because it migrates the database to DynamoDB, which may not be the best fit for the existing MySQL-based application. Additionally, the periodic Lambda function to synchronize the S3 bucket introduces the risk of data loss, as it may not always capture the latest data.\n\nB. This solution is incorrect because it cannot add the EC2 instances in the separate Region to the existing ALB, as Application Load Balancers are regional services. Additionally, promoting the read replica to a standalone DB instance during a disaster is not explicitly mentioned.\n\nD. This solution is not optimal because the hourly snapshots of the RDS DB instance may result in data loss, as the finance team's queries can cause the database to be updated more frequently than hourly. The use of Amazon ElastiCache to address the performance issue is not directly relevant to the disaster recovery aspect of the problem."
  },
  "464": {
    "question": "A company uses AWS Organizations to manage its AWS accounts. A solutions architect must design a solution in which only administrator roles are allowed to use IAM actions. However, the solutions architect does not have access to all the AWS accounts throughout the company.Which solution meets these requirements with the LEAST operational overhead?",
    "choices": [
      "A. Create an SCP that applies to all the AWS accounts to allow IAM actions only for administrator roles. Apply the SCP to the root OU.",
      "B. Configure AWS CloudTrail to invoke an AWS Lambda function for each event that is related to IAM actions. Configure the function to deny the action if the user who invoked the action is not an administrator.",
      "C. Create an SCP that applies to all the AWS accounts to deny IAM actions for all users except for those with administrator roles. Apply the SCP to the root OU.",
      "D. Set an IAM permissions boundary that allows IAM actions. Attach the permissions boundary to every administrator role across all the AWS accounts."
    ],
    "answer": "C",
    "explanation": "1. Clear explanation of why the correct answer (C) is right:\n\nThe correct answer is C. Create an SCP that applies to all the AWS accounts to deny IAM actions for all users except for those with administrator roles. Apply the SCP to the root OU.\n\nThis is the best solution because:\n- SCPs in AWS Organizations are the most effective way to centrally manage and enforce permissions across multiple AWS accounts.\n- By creating an SCP that denies IAM actions for all users except administrators, you are ensuring that only administrator roles can perform IAM actions, which meets the stated requirement.\n- Applying the SCP to the root OU will ensure that the policy is enforced across all child OUs and their respective AWS accounts, without the need for the solutions architect to have access to all the individual accounts.\n- This is a preventative, scalable, and centralized approach to managing the required permissions, with the least operational overhead.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nA. Create an SCP that applies to all the AWS accounts to allow IAM actions only for administrator roles. Apply the SCP to the root OU.\n- This is incorrect because SCPs in AWS Organizations work by defining the maximum permissions, not the allowed permissions. An SCP that \"allows\" certain actions would not effectively restrict non-administrator roles from performing those actions.\n\nB. Configure AWS CloudTrail to invoke an AWS Lambda function for each event that is related to IAM actions. Configure the function to deny the action if the user who invoked the action is not an administrator.\n- This is a reactive, rather than preventative, approach. It would require more operational overhead to manage the Lambda function and CloudTrail configuration, compared to a centralized SCP solution.\n\nD. Set an IAM permissions boundary that allows IAM actions. Attach the permissions boundary to every administrator role across all the AWS accounts.\n- IAM permissions boundaries set the maximum permissions for a user or role, not the allowed permissions. This would not effectively restrict non-administrator roles from performing IAM actions, similar to the issue with option A."
  },
  "465": {
    "question": "A company uses an organization in AWS Organizations to manage multiple AWS accounts. The company hosts some applications in a VPC in the company's shared services account.The company has attached a transit gateway to the VPC in the shared services account.The company is developing a new capability and has created a development environment that requires access to the applications that are in the shared services account. The company intends to delete and recreate resources frequently in the development account. The company also wants to give a development team the ability to recreate the team's connection to the shared services account as required.Which solution will meet these requirements?",
    "choices": [
      "A. Create a transit gateway in the development account. Create a transit gateway peering request to the shared services account. Configure the shared services transit gateway to automatically accept peering connections.",
      "B. Turn on automatic acceptance for the transit gateway in the shared services account. Use AWS Resource Access Manager (AWS RAM) to share the transit gateway resource in the shared services account with the development account. Accept the resource in the development account. Create a transit gateway attachment in the development account.",
      "C. Turn on automatic acceptance for the transit gateway in the shared services account. Create a VPC endpoint. Use the endpoint policy to grant permissions on the VPC endpoint for the development account. Configure the endpoint service to automatically accept connection requests. Provide the endpoint details to the development team.",
      "D. Create an Amazon EventBridge rule to invoke an AWS Lambda function that accepts the transit gateway attachment when the development account makes an attachment request. Use AWS Network Manager to share the transit gateway in the shared services account with the development account. Accept the transit gateway in the development account."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (Option B):\n\nOption B is the correct answer because it uses AWS Resource Access Manager (RAM) to share the transit gateway resource from the shared services account with the development account. This approach has the following advantages:\n\n- It eliminates the need for manual peering requests between the two accounts, which is important given the requirement for the development team to be able to recreate the connection as needed.\n- By turning on automatic acceptance for the transit gateway in the shared services account, the development team can quickly establish the connection without requiring intervention on both sides.\n- The use of RAM simplifies the process of granting permissions and managing resources, making it a suitable solution for this use case where the development environment has frequent changes.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option involves creating a separate transit gateway in the development account and setting up peering. This adds unnecessary complexity and management overhead, which goes against the requirement for the development team to be able to recreate the connection easily.\n\nC. This option involves creating a VPC endpoint and configuring an endpoint service. However, the question does not mention the use of a Network Load Balancer or a Gateway Load Balancer, which are typically required for endpoint services. Additionally, this approach does not align well with the requirement to access applications in a VPC through a transit gateway.\n\nD. This option is overly complicated and involves using AWS EventBridge, AWS Lambda, and AWS Network Manager to manage the transit gateway attachment process. The requirement can be more easily addressed using the simpler approach in Option B."
  },
  "466": {
    "question": "A company wants to migrate virtual Microsoft workloads from an on-premises data center to AWS. The company has successfully tested a few sample workloads on AWS. The company also has created an AWS Site-to-Site VPN connection to a VPC. A solutions architect needs to generate a total cost of ownership (TCO) report for the migration of all the workloads from the data center.Simple Network Management Protocol (SNMP) has been enabled on each VM in the data center. The company cannot add more VMs in the data center and cannot install additional software on the VMs. The discovery data must be automatically imported into AWS Migration Hub.Which solution will meet these requirements?",
    "choices": [
      "A. Use the AWS Application Migration Service agentless service and the AWS Migration Hub Strategy Recommendations to generate the TCO report.",
      "B. Launch a Windows Amazon EC2 instance. Install the Migration Evaluator agentless collector on the EC2 instance. Configure Migration Evaluator to generate the TCO report.",
      "C. Launch a Windows Amazon EC2 instance. Install the Migration Evaluator agentless collector on the EC2 instance. Configure Migration Hub to generate the TCO report.",
      "D. Use the AWS Migration Readiness Assessment tool inside the VPC. Configure Migration Evaluator to generate the TCO report."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B. This solution uses the Migration Evaluator agentless collector to gather data from the on-premises VMs using SNMP, and then generates the TCO report.\n\nThe key reasons why this is the right solution:\n- The company has SNMP enabled on each VM, but cannot install additional software on the VMs. The Migration Evaluator agentless collector can collect the necessary data without requiring any agent installation on the VMs.\n- The agentless collector can automatically import the discovery data into AWS Migration Hub, as required.\n- Migration Evaluator can generate the TCO report based on the collected data, which meets the requirement of generating a TCO report for the migration.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option uses the AWS Application Migration Service agentless service, which is not the right tool for generating a TCO report. The AWS Migration Hub Strategy Recommendations can provide migration recommendations, but not a comprehensive TCO report.\n\nC. This option also uses the Migration Evaluator agentless collector, but it configures Migration Hub to generate the TCO report. However, Migration Hub is not the tool that generates the TCO report; that functionality is provided by the Migration Evaluator.\n\nD. This option uses the AWS Migration Readiness Assessment tool, which is designed to assess the readiness of an organization for cloud migration. It does not have the capability to generate a TCO report.\n\nIn summary, the correct solution (B) leverages the Migration Evaluator agentless collector to gather the necessary data and generate the TCO report, which meets all the requirements specified in the question."
  },
  "467": {
    "question": "A company that is developing a mobile game is making game assets available in two AWS Regions. Game assets are served from a set of Amazon EC2 instances behind an Application Load Balancer (ALB) in each Region. The company requires game assets to be fetched from the closest Region. If game assets become unavailable in the closest Region, they should be fetched from the other Region.What should a solutions architect do to meet these requirements?",
    "choices": [
      "A. Create an Amazon CloudFront distribution. Create an origin group with one origin for each ALB. Set one of the origins as primary.",
      "B. Create an Amazon Route 53 health check for each ALCreate a Route 53 failover routing record pointing to the two ALBs. Set the Evaluate Target Health value to Yes.",
      "C. Create two Amazon CloudFront distributions, each with one ALB as the origin. Create an Amazon Route 53 failover routing record pointing to the two CloudFront distributions. Set the Evaluate Target Health value to Yes.",
      "D. Create an Amazon Route 53 health check for each ALB. Create a Route 53 latency alias record pointing to the two ALBs. Set the Evaluate Target Health value to Yes."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (D):\n\nThe correct answer is option D: Create an Amazon Route 53 health check for each ALB, and then create a Route 53 latency alias record pointing to the two ALBs. Set the Evaluate Target Health value to Yes.\n\nThis solution meets the given requirements:\n\n- The latency-based routing policy in Route 53 will direct users to the ALB in the closest AWS Region, ensuring that game assets are fetched from the closest Region.\n- The health checks on the ALBs will monitor their availability, and if one Region becomes unavailable, Route 53 will automatically route traffic to the ALB in the other Region, ensuring that game assets can still be fetched.\n- The Evaluate Target Health setting in the Route 53 record ensures that Route 53 will only route traffic to healthy ALBs, preventing users from accessing unavailable game assets.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option is incorrect because it involves using an Amazon CloudFront distribution. While CloudFront can be used to improve content delivery, it does not automatically choose the closest origin based on user location. The solution would need to set up an origin group with a primary and failover origin, but this does not guarantee that the closest origin will be used.\n\nB. This option is incorrect because Route 53 failover routing records cannot point directly to ALBs. Failover routing is intended for use with a primary and secondary resource, not multiple origins in different Regions.\n\nC. This option is incorrect because it involves creating two separate CloudFront distributions, each with a single ALB as the origin. While this setup would provide failover functionality, it does not guarantee that the closest CloudFront distribution will be used. The traffic would still be distributed based on the CloudFront distribution's settings, not the user's location."
  },
  "468": {
    "question": "A company deploys workloads in multiple AWS accounts. Each account has a VPC with VPC flow logs published in text log format to a centralized Amazon S3 bucket. Each log file is compressed with gzip compression. The company must retain the log files indefinitely.A security engineer occasionally analyzes the logs by using Amazon Athena to query the VPC flow logs. The query performance is degrading over time as the number of ingested logs is growing. A solutions architect must improve the performance of the log analysis and reduce the storage space that the VPC flow logs use.Which solution will meet these requirements with the LARGEST performance improvement?",
    "choices": [
      "A. Create an AWS Lambda function to decompress the gzip files and to compress the files with bzip2 compression. Subscribe the Lambda function to an s3:ObjectCreated:Put S3 event notification for the S3 bucket.",
      "B. Enable S3 Transfer Acceleration for the S3 bucket. Create an S3 Lifecycle configuration to move files to the S3 Intelligent-Tiering storage class as soon as the files are uploaded.",
      "C. Update the VPC flow log configuration to store the files in Apache Parquet format. Specify hourly partitions for the log files.",
      "D. Create a new Athena workgroup without data usage control limits. Use Athena engine version 2."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C, which is to \"Update the VPC flow log configuration to store the files in Apache Parquet format. Specify hourly partitions for the log files.\"\n\nThis solution provides the largest performance improvement for the log analysis for the following reasons:\n\n- Apache Parquet is a columnar data format that is optimized for efficient querying and storage. Compared to row-based formats like CSV or JSON, Parquet allows Athena to selectively read only the required columns, leading to faster query performance and reduced storage costs.\n- Storing the logs in hourly partitions enables Athena to quickly scan through the relevant partitions, instead of having to read through all the log files. This partitioning structure significantly improves the query performance.\n\n2. Explanations of the incorrect choices:\n\nA. Create an AWS Lambda function to decompress the gzip files and to compress the files with bzip2 compression. Subscribe the Lambda function to an s3:ObjectCreated:Put S3 event notification for the S3 bucket.\n   - This solution does not provide the largest performance improvement. While bzip2 compression can reduce storage space, it is not as efficient as the Parquet format for querying.\n\nB. Enable S3 Transfer Acceleration for the S3 bucket. Create an S3 Lifecycle configuration to move files to the S3 Intelligent-Tiering storage class as soon as the files are uploaded.\n   - This solution focuses on improving the data ingestion and storage optimization, but does not directly address the query performance issues. The Parquet format and partitioning are more effective for improving the Athena query performance.\n\nD. Create a new Athena workgroup without data usage control limits. Use Athena engine version 2.\n   - Removing the data usage control limits and using the newer Athena engine version may provide some performance improvements, but they are not as significant as the benefits of the Parquet format and partitioning (choice C)."
  },
  "469": {
    "question": "A company wants to establish a dedicated connection between its on-premises infrastructure and AWS. The company is setting up a 1 Gbps AWS Direct Connect connection to its account VPC. The architecture includes a transit gateway and a Direct Connect gateway to connect multiple VPCs and the on-premises infrastructure.The company must connect to VPC resources over a transit VIF by using the Direct Connect connection.Which combination of steps will meet these requirements? (Choose two.)",
    "choices": [
      "A. Update the 1 Gbps Direct Connect connection to 10 Gbps.",
      "B. Advertise the on-premises network prefixes over the transit VIF.",
      "C. Advertise the VPC prefixes from the Direct Connect gateway to the on-premises network over the transit VIF.",
      "D. Update the Direct Connect connection's MACsec encryption mode attribute to must_encrypt.",
      "E. Associate a MACsec Connection Key Name/Connectivity Association Key (CKN/CAK) pair with the Direct Connect connection."
    ],
    "answer": "BC",
    "explanation": "1. Explanation of the correct answer (B and C):\n\nThe correct combination of steps is:\nB. Advertise the on-premises network prefixes over the transit VIF.\nC. Advertise the VPC prefixes from the Direct Connect gateway to the on-premises network over the transit VIF.\n\nThis is because the requirements state that the company must connect to VPC resources over a transit VIF by using the Direct Connect connection. To achieve this, the following steps are necessary:\n\nB. Advertising the on-premises network prefixes over the transit VIF allows the VPC resources to reach the on-premises infrastructure. This enables bidirectional connectivity.\n\nC. Advertising the VPC prefixes from the Direct Connect gateway to the on-premises network over the transit VIF allows the on-premises infrastructure to reach the VPC resources. This completes the connectivity between the on-premises and VPC environments.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Updating the 1 Gbps Direct Connect connection to 10 Gbps is not necessary to meet the requirements. The 1 Gbps connection is sufficient to establish the dedicated connection between the on-premises infrastructure and the AWS environment.\n\nD. Updating the Direct Connect connection's MACsec encryption mode attribute to \"must_encrypt\" is not required to meet the stated requirements. The question does not mention the need for encryption.\n\nE. Associating a MACsec Connection Key Name/Connectivity Association Key (CKN/CAK) pair with the Direct Connect connection is not necessary to meet the requirements. MACsec encryption is not mentioned as a requirement in this scenario."
  },
  "470": {
    "question": "A company wants to use Amazon WorkSpaces in combination with thin client devices to replace aging desktops. Employees use the desktops to access applications that work with Clinical trial data. Corporate security policy states that access to the applications must be restricted to only company branch office locations. The company is considering adding an additional branch office in the next 6 months.Which solution meets these requirements with the MOST operational efficiency?",
    "choices": [
      "A. Create an IP access control group rule with the list of public addresses from the branch offices. Associate the IP access control group with the WorkSpaces directory.",
      "B. Use AWS Firewall Manager to create a web ACL rule with an IPSet with the list of public addresses from the branch office locations. Associate the web ACL with the WorkSpaces directory.",
      "C. Use AWS Certificate Manager (ACM) to issue trusted device certificates to the machines deployed in the branch office locations. Enable restricted access on the WorkSpaces directory.",
      "D. Create a custom WorkSpace image with Windows Firewall configured to restrict access to the public addresses of the branch offices. Use the image to deploy the WorkSpaces."
    ],
    "answer": "A",
    "explanation": "1. Clear explanation of why the correct answer (A) is right:\n\nThe correct answer is A because it is the most operationally efficient solution to meet the requirements. By creating an IP access control group rule with the list of public addresses from the branch office locations and associating it with the WorkSpaces directory, the company can easily restrict access to the WorkSpaces to only the authorized branch office locations. This is a simple and straightforward approach that is easy to manage, as the IP access control group can be updated as new branch offices are added.\n\n2. Brief explanations of why the incorrect choices are wrong:\n\nB. Use AWS Firewall Manager to create a web ACL rule with an IPSet with the list of public addresses from the branch office locations. Associate the web ACL with the WorkSpaces directory.\nThis is incorrect because web ACLs are used to manage web traffic, and they are not suitable for controlling access to Amazon WorkSpaces. Web ACLs are primarily used to protect resources like Amazon CloudFront, Amazon API Gateway, and Application Load Balancer, but not Amazon WorkSpaces.\n\nC. Use AWS Certificate Manager (ACM) to issue trusted device certificates to the machines deployed in the branch office locations. Enable restricted access on the WorkSpaces directory.\nThis is incorrect because using device certificates to restrict access is a more complex and less operationally efficient solution compared to using an IP access control group. Enabling restricted access on the WorkSpaces directory would still require the company to manage the device certificates, which adds additional overhead.\n\nD. Create a custom WorkSpace image with Windows Firewall configured to restrict access to the public addresses of the branch offices. Use the image to deploy the WorkSpaces.\nThis is incorrect because it requires the creation and management of a custom WorkSpaces image, which is more complex and less operationally efficient than using an IP access control group. Maintaining and updating the custom image as new branch offices are added would be more time-consuming and error-prone."
  },
  "471": {
    "question": "A company uses AWS Organizations. The company runs two firewall appliances in a centralized networking account. Each firewall appliance runs on a manually configured highly available Amazon EC2 instance. A transit gateway connects the VPC from the centralized networking account to VPCs of member accounts. Each firewall appliance uses a static private IP address that is then used to route traffic from the member accounts to the internet.During a recent incident, a badly configured script initiated the termination of both firewall appliances. During the rebuild of the firewall appliances, the company wrote a new script to configure the firewall appliances at startup.The company wants to modernize the deployment of the firewall appliances. The firewall appliances need the ability to scale horizontally to handle increased traffic when the network expands. The company must continue to use the firewall appliances to comply with company policy. The provider of the firewall appliances has confirmed that the latest version of the firewall code will work with all AWS services.Which combination of steps should the solutions architect recommend to meet these requirements MOST cost-effectively? (Choose three.)",
    "choices": [
      "A. Deploy a Gateway Load Balancer in the centralized networking account. Set up an endpoint service that uses AWS PrivateLink.",
      "B. Deploy a Network Load Balancer in the centralized networking account. Set up an endpoint service that uses AWS PrivateLink.",
      "C. Create an Auto Scaling group and a launch template that uses the new script as user data to configure the firewall appliances. Create a target group that uses the instance target type.",
      "D. Create an Auto Scaling group. Configure an AWS Launch Wizard deployment that uses the new script as user data to configure the firewall appliances. Create a target group that uses the IP target type.",
      "E. Create VPC endpoints in each member account. Update the route tables to point to the VPC endpoints.",
      "F. Create VPC endpoints in the centralized networking account. Update the route tables in each member account to point to the VPC endpoints."
    ],
    "answer": "ACF",
    "explanation": "1. Explanation of the correct answer (ACF):\n\nA. Deploy a Gateway Load Balancer in the centralized networking account. Set up an endpoint service that uses AWS PrivateLink.\nThis is the correct choice because the Gateway Load Balancer (GWLB) is designed to handle traffic inspection use cases like firewalls. It uses the GENEVE protocol to redirect traffic to the firewall appliances, enabling horizontal scalability.\n\nC. Create an Auto Scaling group and a launch template that uses the new script as user data to configure the firewall appliances. Create a target group that uses the instance target type.\nThis is also correct because the Auto Scaling group will allow the firewall appliances to scale horizontally to handle increased traffic. The launch template with the new script will ensure consistent configuration of the firewall appliances.\n\nF. Create VPC endpoints in the centralized networking account. Update the route tables in each member account to point to the VPC endpoints.\nThis is the correct choice because it allows traffic from the member accounts to be routed to the centralized firewall appliances through the VPC endpoints, without the need for individual VPC endpoints in each member account, which would be less cost-effective.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. Deploy a Network Load Balancer in the centralized networking account. Set up an endpoint service that uses AWS PrivateLink.\nIncorrect because the Network Load Balancer (NLB) does not support the GENEVE protocol required for traffic inspection with the firewall appliances.\n\nD. Create an Auto Scaling group. Configure an AWS Launch Wizard deployment that uses the new script as user data to configure the firewall appliances. Create a target group that uses the IP target type.\nIncorrect because the AWS Launch Wizard is not designed to work with Auto Scaling groups, which is the recommended approach for scaling the firewall appliances.\n\nE. Create VPC endpoints in each member account. Update the route tables to point to the VPC endpoints.\nIncorrect because this would be less cost-effective than the recommended approach of creating VPC endpoints in the centralized networking account. Having individual VPC endpoints in each member account would incur additional costs."
  },
  "472": {
    "question": "A solutions architect must implement a multi-Region architecture for an Amazon RDS for PostgreSQL database that supports a web application. The database launches from an AWS CloudFormation template that includes AWS services and features that are present in both the primary and secondary Regions.The database is configured for automated backups, and it has an RTO of 15 minutes and an RPO of 2 hours. The web application is configured to use an Amazon Route 53 record to route traffic to the database.Which combination of steps will result in a highly available architecture that meets all the requirements? (Choose two.)",
    "choices": [
      "A. Create a cross-Region read replica of the database in the secondary Region. Configure an AWS Lambda function in the secondary Region to promote the read replica during a failover event.",
      "B. In the primary Region, create a health check on the database that will invoke an AWS Lambda function when a failure is detected. Program the Lambda function to recreate the database from the latest database snapshot in the secondary Region and update the Route 53 host records for the database.",
      "C. Create an AWS Lambda function to copy the latest automated backup to the secondary Region every 2 hours.",
      "D. Create a failover routing policy in Route 53 for the database DNS record. Set the primary and secondary endpoints to the endpoints in each Region.",
      "E. Create a hot standby database in the secondary Region. Use an AWS Lambda function to restore the secondary database to the latest RDS automatic backup in the event that the primary database fails."
    ],
    "answer": "AD",
    "explanation": "1. Clear explanation of the correct answer (A and D):\n\nA. Create a cross-Region read replica of the database in the secondary Region. Configure an AWS Lambda function in the secondary Region to promote the read replica during a failover event.\nThis step ensures continuous data replication between the primary and secondary Regions, minimizing data loss and meeting the RPO requirement of 2 hours. The read replica in the secondary Region can be quickly promoted to become the primary database during a failover event, meeting the RTO requirement of 15 minutes.\n\nD. Create a failover routing policy in Route 53 for the database DNS record. Set the primary and secondary endpoints to the endpoints in each Region.\nThis step ensures that the application's traffic is automatically routed to the secondary database in the event of a failure in the primary Region. The failover routing policy in Route 53 allows for swift failover, minimizing the impact on the web application and meeting the RTO requirement.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. This option does not meet the RTO requirement of 15 minutes, as recreating the database from a snapshot in the secondary Region and updating the Route 53 records would likely take longer than 15 minutes.\n\nC. This option only meets the RPO requirement, but does not provide a standby database instance in the secondary Region, which is necessary to meet the RTO requirement.\n\nE. This option is incomplete and does not provide details on how the backups would be restored to the secondary database or how the failover would be triggered. It also does not specify how the RTO and RPO requirements would be met."
  },
  "473": {
    "question": "An ecommerce company runs an application on AWS. The application has an Amazon API Gateway API that invokes an AWS Lambda function. The data is stored in an Amazon RDS for PostgreSQL DB instance.During the company\u2019s most recent flash sale, a sudden increase in API calls negatively affected the application's performance. A solutions architect reviewed the Amazon CloudWatch metrics during that time and noticed a significant increase in Lambda invocations and database connections. The CPU utilization also was high on the DB instance.What should the solutions architect recommend to optimize the application's performance?",
    "choices": [
      "A. Increase the memory of the Lambda function. Modify the Lambda function to close the database connections when the data is retrieved.",
      "B. Add an Amazon ElastiCache for Redis cluster to store the frequently accessed data from the RDS database.",
      "C. Create an RDS proxy by using the Lambda console. Modify the Lambda function to use the proxy endpoint.",
      "D. Modify the Lambda function to connect to the database outside of the function's handler. Check for an existing database connection before creating a new connection."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C. Create an RDS proxy by using the Lambda console. Modify the Lambda function to use the proxy endpoint.\n\nThe key reasons why this is the right recommendation:\n\n- The problem statement indicates that the database connections were a major issue, leading to high CPU utilization on the RDS instance during the flash sale.\n- Using an RDS proxy can help address this issue by managing the database connections more efficiently. The RDS proxy will handle connection pooling and management, reducing the load on the RDS instance.\n- Creating the RDS proxy directly from the Lambda console is a convenient solution, as it automatically configures the necessary network settings (VPC, subnets, security groups) to enable the Lambda function to connect to the proxy.\n- Modifying the Lambda function to use the proxy endpoint instead of directly connecting to the RDS database will allow the application to take advantage of the connection pooling and management provided by the RDS proxy.\n\n2. Explanations of why the other choices are incorrect:\n\nA. Increase the memory of the Lambda function. Modify the Lambda function to close the database connections when the data is retrieved.\n- Increasing the memory of the Lambda function may not address the root cause of the issue, which is the high number of database connections.\n- Manually closing the database connections in the Lambda function may help, but it introduces more complexity and doesn't provide the same level of connection management as using an RDS proxy.\n\nB. Add an Amazon ElastiCache for Redis cluster to store the frequently accessed data from the RDS database.\n- Adding a caching layer like ElastiCache can help improve performance, but it doesn't directly address the issue of the high number of database connections.\n- Caching can be a good complementary solution, but it doesn't solve the core problem highlighted in the question.\n\nD. Modify the Lambda function to connect to the database outside of the function's handler. Check for an existing database connection before creating a new connection.\n- This approach can help mitigate the issue of creating new connections for each invocation, but it still requires manual connection management in the Lambda function.\n- An RDS proxy provides a more robust and managed solution for connection pooling and reuse, reducing the complexity of the Lambda function."
  },
  "474": {
    "question": "A retail company wants to improve its application architecture. The company's applications register new orders, handle returns of merchandise, and provide analytics. The applications store retail data in a MySQL database and an Oracle OLAP analytics database. All the applications and databases are hosted on Amazon EC2 instances.Each application consists of several components that handle different parts of the order process. These components use incoming data from different sources. A separate ETL job runs every week and copies data from each application to the analytics database.A solutions architect must redesign the architecture into an event-driven solution that uses serverless services. The solution must provide updated analytics in near real time.Which solution will meet these requirements?",
    "choices": [
      "A. Migrate the individual applications as microservices to Amazon Elastic Container Service (Amazon ECS) containers that use AWS Fargate. Keep the retail MySQL database on Amazon EC2. Move the analytics database to Amazon Neptune. Use Amazon Simple Queue Service (Amazon SQS) to send all the incoming data to the microservices and the analytics database.",
      "B. Create an Auto Scaling group for each application. Specify the necessary number of EC2 instances in each Auto Scaling group. Migrate the retail MySQL database and the analytics database to Amazon Aurora MySQL. Use Amazon Simple Notification Service (Amazon SNS) to send all the incoming data to the correct EC2 instances and the analytics database.",
      "C. Migrate the individual applications as microservices to Amazon Elastic Kubernetes Service (Amazon EKS) containers that use AWS Fargate. Migrate the retail MySQL database to Amazon Aurora Serverless MySQL. Migrate the analytics database to Amazon Redshift Serverless. Use Amazon EventBridge to send all the incoming data to the microservices and the analytics database.",
      "D. Migrate the individual applications as microservices to Amazon AppStream 2.0. Migrate the retail MySQL database to Amazon Aurora MySQL. Migrate the analytics database to Amazon Redshift Serverless. Use AWS IoT Core to send all the incoming data to the microservices and the analytics database."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C because it meets all the key requirements of the problem statement:\n\n- Migrating the individual applications as microservices to Amazon EKS containers that use AWS Fargate provides a serverless, event-driven architecture.\n- Migrating the retail MySQL database to Amazon Aurora Serverless MySQL and the analytics database to Amazon Redshift Serverless also provides a fully serverless database solution.\n- Using Amazon EventBridge allows for near real-time data processing and analytics, as it can trigger the microservices and send data to the analytics database immediately when new events occur.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This solution is not fully serverless, as it keeps the retail MySQL database on Amazon EC2 instances. Additionally, Amazon SQS is not suitable for near real-time data processing, as it is designed for asynchronous message queuing.\n\nB. This solution is not serverless, as it uses Auto Scaling groups with EC2 instances. Additionally, using Amazon SNS for data distribution may not provide the real-time processing required.\n\nD. Using AWS IoT Core is not appropriate for this use case, as it is primarily designed for IoT device management and communication. Amazon AppStream 2.0 is for desktop application streaming, which is not relevant here.\n\nIn summary, the correct answer (C) provides a fully serverless, event-driven architecture that meets the requirements of near real-time analytics and data processing, while the other choices do not fully address the problem statement."
  },
  "475": {
    "question": "A company is planning a migration from an on-premises data center to the AWS Cloud. The company plans to use multiple AWS accounts that are managed in an organization in AWS Organizations. The company will create a small number of accounts initially and will add accounts as needed. A solutions architect must design a solution that turns on AWS CloudTrail in all AWS accounts.What is the MOST operationally efficient solution that meets these requirements?",
    "choices": [
      "A. Create an AWS Lambda function that creates a new CloudTrail trail in all AWS accounts in the organization. Invoke the Lambda function daily by using a scheduled action in Amazon EventBridge.",
      "B. Create a new CloudTrail trail in the organization's management account. Configure the trail to log all events for all AWS accounts in the organization.",
      "C. Create a new CloudTrail trail in all AWS accounts in the organization. Create new trails whenever a new account is created. Define an SCP that prevents deletion or modification of trails. Apply the SCP to the root OU.",
      "D. Create an AWS Systems Manager Automation runbook that creates a CloudTrail trail in all AWS accounts in the organization. Invoke the automation by using Systems Manager State Manager."
    ],
    "answer": "B",
    "explanation": "1. Explanation of why the correct answer (B) is right:\n\nThe most operationally efficient solution is to create a new CloudTrail trail in the organization's management account and configure it to log events for all AWS accounts in the organization. This is the most efficient approach because it requires setting up the CloudTrail trail in only one account, the management account, rather than having to create and manage trails in multiple individual accounts. By creating the trail in the management account, it will automatically capture events across all accounts in the organization, without the need to set up and maintain trails in each individual account.\n\n2. Brief explanations of why the incorrect choices are wrong:\n\nA. This solution involves creating an AWS Lambda function that creates new CloudTrail trails in all AWS accounts. This would require managing and maintaining the Lambda function, as well as scheduling the invocation of the function. This is not as operationally efficient as the solution in option B, which requires setting up the trail in only one account.\n\nC. This solution involves creating a new CloudTrail trail in all AWS accounts and then creating new trails whenever a new account is added. This approach is more complex and time-consuming than the solution in option B, which only requires setting up the trail in one account.\n\nD. This solution involves using AWS Systems Manager Automation to create CloudTrail trails in all AWS accounts. While this is a valid approach, it is not as operationally efficient as the solution in option B, which requires setting up the trail in only one account."
  },
  "476": {
    "question": "A software development company has multiple engineers who are working remotely. The company is running Active Directory Domain Services (AD DS) on an Amazon EC2 instance. The company's security policy states that all internal, nonpublic services that are deployed in a VPC must be accessible through a VPN. Multi-factor authentication (MFA) must be used for access to a VPN.What should a solutions architect do to meet these requirements?",
    "choices": [
      "A. Create an AWS Site-to-Site VPN connection. Configure integration between a VPN and AD DS. Use an Amazon WorkSpaces client with MFA support enabled to establish a VPN connection.",
      "B. Create an AWS Client VPN endpoint. Create an AD Connector directory for integration with AD DS. Enable MFA for AD Connector. Use AWS Client VPN to establish a VPN connection.",
      "C. Create multiple AWS Site-to-Site VPN connections by using AWS VPN CloudHub. Configure integration between AWS VPN CloudHub and AD DS. Use AWS Copilot to establish a VPN connection.",
      "D. Create an Amazon WorkLink endpoint. Configure integration between Amazon WorkLink and AD DS. Enable MFA in Amazon WorkLink. Use AWS Client VPN to establish a VPN connection."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B because it best addresses the requirements stated in the question:\n\n- Create an AWS Client VPN endpoint: This allows the remote engineers to establish a secure VPN connection to access the internal, non-public services deployed in the VPC.\n- Create an AD Connector directory for integration with AD DS: This allows the Client VPN endpoint to integrate with the existing Active Directory Domain Services (AD DS) running on the Amazon EC2 instance, enabling authentication and authorization of the remote engineers.\n- Enable MFA for AD Connector: This fulfills the requirement for multi-factor authentication (MFA) to be used for access to the VPN, providing an additional layer of security.\n- Use AWS Client VPN to establish a VPN connection: The remote engineers can use the AWS Client VPN client to connect to the VPN endpoint, accessing the internal services within the VPC.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This choice is incorrect because it uses an AWS Site-to-Site VPN connection, which is designed to connect networks, not provide individual user access. The question specifically states that the requirement is for a VPN solution, not a site-to-site VPN.\n\nC. This choice is incorrect because it involves using AWS VPN CloudHub, which is a solution for connecting multiple VPN connections, not a VPN solution for individual users. Additionally, the use of AWS Copilot is not relevant to the given requirements.\n\nD. This choice is incorrect because Amazon WorkLink is a service that provides secure mobile access to internal websites and web applications, not a VPN solution for accessing internal services within a VPC. The requirement is specifically for a VPN solution, not a web-based access solution."
  },
  "477": {
    "question": "A company is running a three-tier web application in an on-premises data center. The frontend is served by an Apache web server, the middle tier is a monolithic Java application, and the storage tier is a PostgreSQL database.During a recent marketing promotion, customers could not place orders through the application because the application crashed. An analysis showed that all three tiers were overloaded. The application became unresponsive, and the database reached its capacity limit because of read operations. The company already has several similar promotions scheduled in the near future.A solutions architect must develop a plan for migration to AWS to resolve these issues. The solution must maximize scalability and must minimize operational effortWhich combination of steps will meet these requirements? (Choose three.)",
    "choices": [
      "A. Refactor the frontend so that static assets can be hosted on Amazon S3. Use Amazon CloudFront to serve the frontend to customers. Connect the frontend to the Java application.",
      "B. Rehost the Apache web server of the frontend on Amazon EC2 instances that are in an Auto Scaling group. Use a load balancer in front of the Auto Scaling group. Use Amazon Elastic File System (Amazon EFS) to host the static assets that the Apache web server needs.",
      "C. Rehost the Java application in an AWS Elastic Beanstalk environment that includes auto scaling.",
      "D. Refactor the Java application, Develop a Docker container to run the Java application. Use AWS Fargate to host the container.",
      "E. Use AWS Database Migration Service (AWS DMS) to replatform the PostgreSQL database to an Amazon Aurora PostgreSQL database. Use Aurora Auto Scaling for read replicas.",
      "F. Rehost the PostgreSQL database on an Amazon EC2 instance that has twice as much memory as the on-premises server."
    ],
    "answer": "ACE",
    "explanation": "1. Explanation of the correct answer (ACE):\n\nA. Refactoring the frontend to use Amazon S3 and Amazon CloudFront is a good choice because it offloads the static asset serving from the web server, which can help reduce the load on the frontend tier. This also allows the frontend to scale independently from the backend Java application.\n\nC. Rehosting the Java application on AWS Elastic Beanstalk is a good choice because it provides a managed platform that handles scaling, patching, and other operational tasks, reducing the overall operational effort required.\n\nE. Migrating the PostgreSQL database to Amazon Aurora is a good choice because it provides a fully managed, scalable, and highly available database solution. The use of Aurora Auto Scaling for read replicas can help handle the increased read load during promotions.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. Rehosting the Apache web server on Amazon EC2 instances with Auto Scaling and Amazon EFS is a less optimal choice because it still requires more operational effort compared to using Amazon S3 and CloudFront for the frontend.\n\nD. Refactoring the Java application to use Docker and AWS Fargate is a good idea, but the question does not mention the use of Fargate's Service Autoscaling feature, which would be necessary to meet the scalability requirement. Without explicit mention of Service Autoscaling, this choice is less optimal.\n\nF. Rehosting the PostgreSQL database on a larger Amazon EC2 instance is a less optimal choice because it does not provide the same level of scalability, availability, and reduced operational effort as migrating to Amazon Aurora."
  },
  "478": {
    "question": "A company is deploying a new application on AWS. The application consists of an Amazon Elastic Kubernetes Service (Amazon EKS) cluster and an Amazon Elastic Container Registry (Amazon ECR) repository. The EKS cluster has an AWS managed node group.The company's security guidelines state that all resources on AWS must be continuously scanned for security vulnerabilities.Which solution will meet this requirement with the LEAST operational overhead?",
    "choices": [
      "A. Activate AWS Security Hub. Configure Security Hub to scan the EKS nodes and the ECR repository.",
      "B. Activate Amazon Inspector to scan the EKS nodes and the ECR repository.",
      "C. Launch a new Amazon EC2 instance and install a vulnerability scanning tool from AWS Marketplace. Configure the EC2 instance to scan the EKS nodes. Configure Amazon ECR to perform a basic scan on push.",
      "D. Install the Amazon CloudWatch agent on the EKS nodes. Configure the CloudWatch agent to scan continuously. Configure Amazon ECR to perform a basic scan on push."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B. Activate Amazon Inspector to scan the EKS nodes and the ECR repository.\n\nAmazon Inspector is the AWS service that is designed for continuous scanning of resources, including Amazon EC2 instances, container images in Amazon ECR, and AWS Lambda functions, for known software vulnerabilities and unintended network exposure. It automatically discovers and scans these resources, making it the most suitable solution to meet the requirement of continuously scanning the EKS nodes and ECR repository with the least operational overhead.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Activate AWS Security Hub: While AWS Security Hub is a useful tool for aggregating security findings from various AWS services, it is not primarily designed for continuous scanning of EKS nodes or ECR repositories. Security Hub is more focused on compliance checks and aggregation of security alerts from multiple sources.\n\nC. Launch a new Amazon EC2 instance and install a vulnerability scanning tool: This solution would require additional operational overhead, as it involves launching and managing a separate EC2 instance and installing a vulnerability scanning tool, which may not be as integrated or optimized as the native AWS services.\n\nD. Install the Amazon CloudWatch agent on the EKS nodes: The CloudWatch agent is primarily used for monitoring and logging purposes and does not natively provide vulnerability scanning capabilities. Relying on the CloudWatch agent for vulnerability scanning would not meet the requirement of continuously scanning the resources."
  },
  "479": {
    "question": "A company needs to improve the reliability of its ticketing application. The application runs on an Amazon Elastic Container Service (Amazon ECS) cluster. The company uses Amazon CloudFront to serve the application. A single ECS service of the ECS cluster is the CloudFront distribution\u2019s origin.The application allows only a specific number of active users to enter a ticket purchasing flow. These users are identified by an encrypted attribute in their JSON Web Token (JWT). All other users are redirected to a waiting room module until there is available capacity for purchasing.The application is experiencing high loads. The waiting room module is working as designed, but load on the waiting room is disrupting the applications availability.This disruption is negatively affecting the application's ticket sale transactions.Which solution will provide the MOST reliability for ticket sale transactions during periods of high load?",
    "choices": [
      "A. Create a separate service in the ECS cluster for the waiting room. Use a separate scaling configuration. Ensure that the ticketing service uses the JWT information and appropriately forwards requests to the waiting room service.",
      "B. Move the application to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. Split the waiting room module into a pod that is separate from the ticketing pod. Make the ticketing pod part of a StatefulSet. Ensure that the ticketing pod uses the JWT information and appropriately forwards requests to the waiting room pod.",
      "C. Create a separate service in the ECS cluster for the waiting room. Use a separate scaling configuration. Create a CloudFront function that inspects the JWT information and appropriately forwards requests to the ticketing service or the waiting room service.",
      "D. Move the application to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. Split the waiting room module into a pod that is separate from the ticketing pod. Use AWS App Mesh by provisioning the App Mesh controller for Kubernetes. Enable mTLS authentication and service-to-service authentication for communication between the ticketing pod and the waiting room pod. Ensure that the ticketing pod uses the JWT information and appropriately forwards requests to the waiting room pod."
    ],
    "answer": "C",
    "explanation": "1. Clear explanation of why the correct answer (C) is right:\n\nThe correct answer is C because it provides the most reliable solution for handling high loads on the ticketing application's waiting room module. By creating a separate service in the ECS cluster for the waiting room and using a separate scaling configuration, the load on the ticketing service can be effectively managed. Additionally, by creating a CloudFront function that inspects the JWT information and appropriately forwards requests to the ticketing service or the waiting room service, the reliability of the ticket sale transactions can be significantly improved. This approach leverages the edge computing capabilities of CloudFront to offload the routing logic from the application, ensuring that the ticketing service remains available and responsive during periods of high load.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nA. This option separates the waiting room service but still relies on the ticketing service to handle the routing logic based on JWT information. This could become a bottleneck during high loads, as the ticketing service would need to process the JWT information and forward the request accordingly.\n\nB. Migrating the application to an Amazon EKS cluster and using a StatefulSet for the ticketing pod may be an overkill for this specific requirement. It introduces additional complexity and operational overhead, which may not be necessary to solve the problem at hand.\n\nD. While using Amazon EKS and AWS App Mesh provides advanced traffic management and security features, it may be an unnecessary level of complexity for this use case. The additional configuration and authentication mechanisms required could add operational overhead and complexity to the solution."
  },
  "480": {
    "question": "A solutions architect is creating an AWS CloudFormation template from an existing manually created non-production AWS environment. The CloudFormation template can be destroyed and recreated as needed. The environment contains an Amazon EC2 instance. The EC2 instance has an instance profile that the EC2 instance uses to assume a role in a parent account.The solutions architect recreates the role in a CloudFormation template and uses the same role name. When the CloudFormation template is launched in the child account, the EC2 instance can no longer assume the role in the parent account because of insufficient permissionsWhat should the solutions architect do to resolve this issue?",
    "choices": [
      "A. In the parent account, edit the trust policy for the role that the EC2 instance needs to assume. Ensure that the target role ARN in the existing statement that allows the sts:AssumeRole action is correct. Save the trust policy.",
      "B. In the parent account, edit the trust policy for the role that the EC2 instance needs to assume. Add a statement that allows the sts:AssumeRole action for the root principal of the child account. Save the trust policy.",
      "C. Update the CloudFormation stack again. Specify only the CAPABILITY_NAMED_IAM capability.",
      "D. Update the CloudFormation stack again. Specify the CAPABILITY_IAM capability and the CAPABILITY_NAMED_IAM capability."
    ],
    "answer": "A",
    "explanation": "1. Clear explanation of why the correct answer (A) is right:\n\nThe correct answer is A because the issue arises due to a broken or misconfigured trust relationship between the child account's EC2 instance and the role in the parent account. When the role was recreated in the CloudFormation template, its ARN (Amazon Resource Name) likely changed, even though the role name remained the same. The trust policy in the parent account's role needs to be updated to reflect the new ARN of the role in the child account, so that the EC2 instance can successfully assume the role.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nB. This is incorrect because allowing the root principal of the child account to assume the role in the parent account is generally not a recommended security practice. The root account is the most powerful identity in an AWS account and should be strictly protected.\n\nC. This is incorrect because specifying the CAPABILITY_NAMED_IAM capability alone is not sufficient to resolve the issue. The trust policy in the parent account still needs to be updated to reflect the correct role ARN.\n\nD. This is incorrect because while specifying both the CAPABILITY_IAM and CAPABILITY_NAMED_IAM capabilities is necessary for CloudFormation to create or update IAM resources, it does not directly address the issue of the broken trust relationship between the EC2 instance and the role in the parent account. The trust policy in the parent account still needs to be updated."
  },
  "481": {
    "question": "A solutions architect has developed a web application that uses an Amazon API Gateway Regional endpoint and an AWS Lambda function. The consumers of the web application are all close to the AWS Region where the application will be deployed. The Lambda function only queries an Amazon Aurora MySQL database. The solutions architect has configured the database to have three read replicas.During testing, the application does not meet performance requirements. Under high load, the application opens a large number of database connections. The solutions architect must improve the application\u2019s performance.Which actions should the solutions architect take to meet these requirements? (Choose two.)",
    "choices": [
      "A. Use the cluster endpoint of the Aurora database.",
      "B. Use RDS Proxy to set up a connection pool to the reader endpoint of the Aurora database.",
      "C. Use the Lambda Provisioned Concurrency feature.",
      "D. Move the code for opening the database connection in the Lambda function outside of the event handler.",
      "E. Change the API Gateway endpoint to an edge-optimized endpoint."
    ],
    "answer": "BD",
    "explanation": "1. Explanation of the correct answer:\n\nB. Use RDS Proxy to set up a connection pool to the reader endpoint of the Aurora database:\nThis is the correct choice because RDS Proxy can help improve the performance of the application by managing the database connection pool and routing connections to the available read replicas. This can reduce the number of connections that need to be opened and closed, which was causing performance issues under high load.\n\nD. Move the code for opening the database connection in the Lambda function outside of the event handler:\nThis is also a correct choice because it can help improve performance by reusing the database connection across multiple requests, instead of opening and closing a new connection for each request, which can be time-consuming and resource-intensive.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Use the cluster endpoint of the Aurora database:\nThis is not correct because the solutions architect is already using read replicas to offload read traffic from the primary instance, so using the cluster endpoint would not help improve performance in this case.\n\nC. Use the Lambda Provisioned Concurrency feature:\nThis is not correct because the problem is related to the number of connections to the database, not the number of instances running the Lambda function. Provisioned Concurrency would not address the connection-related performance issues.\n\nE. Change the API Gateway endpoint to an edge-optimized endpoint:\nThis is not correct because the problem is related to the number of connections to the database, not the location of the API Gateway endpoint. Changing the API Gateway endpoint would not help improve the database-related performance issues."
  },
  "482": {
    "question": "A solutions architect has developed a web application that uses an Amazon API Gateway Regional endpoint and an AWS Lambda function. The consumers of the web application are all close to the AWS Region where the application will be deployed. The Lambda function only queries an Amazon Aurora MySQL database. The solutions architect has configured the database to have three read replicas.During testing, the application does not meet performance requirements. Under high load, the application opens a large number of database connections. The solutions architect must improve the application\u2019s performance.Which actions should the solutions architect take to meet these requirements? (Choose two.)",
    "choices": [
      "A. Use the cluster endpoint of the Aurora database.",
      "B. Use RDS Proxy to set up a connection pool to the reader endpoint of the Aurora database.",
      "C. Use the Lambda Provisioned Concurrency feature.",
      "D. Move the code for opening the database connection in the Lambda function outside of the event handler.",
      "E. Change the API Gateway endpoint to an edge-optimized endpoint."
    ],
    "answer": "BD",
    "explanation": "1. Explanation of the correct answer:\n\nB. Use RDS Proxy to set up a connection pool to the reader endpoint of the Aurora database.\nRDS Proxy is a feature of Amazon RDS that acts as a proxy between your application and the database. It manages a pool of database connections, allowing your application to share and reuse these connections instead of opening and closing new connections for each request. This can significantly improve the performance of your application, especially under high load, by reducing the number of open database connections and the overhead associated with establishing new connections.\n\nD. Move the code for opening the database connection in the Lambda function outside of the event handler.\nOpening a new database connection for each event handler invocation can be inefficient, as it requires establishing a new connection for each request. By moving the connection opening code outside of the event handler, the Lambda function can reuse the same database connection across multiple requests, reducing the overhead and improving the overall performance of the application.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Use the cluster endpoint of the Aurora database.\nUsing the cluster endpoint would not help improve the performance in this case, as the problem is related to the number of connections to the database, not the location of the endpoint. The read replicas are already being used to offload read traffic from the primary instance, so the cluster endpoint would not provide any additional benefit.\n\nC. Use the Lambda Provisioned Concurrency feature.\nThe Lambda Provisioned Concurrency feature is used to maintain a warm pool of Lambda function instances, which can help reduce the cold start latency. However, it does not directly address the issue of the large number of database connections being opened, which is the main performance bottleneck in this scenario.\n\nE. Change the API Gateway endpoint to an edge-optimized endpoint.\nChanging the API Gateway endpoint to an edge-optimized endpoint would not help improve the performance of the application in this case, as the problem is related to the number of database connections, not the location of the API Gateway endpoint. The edge-optimized endpoint is designed to improve performance for clients that are geographically distributed, but it does not address the database connection issue."
  },
  "483": {
    "question": "A company is migrating infrastructure for its massive multiplayer game to AWS. The game\u2019s application features a leaderboard where players can see rankings in real time. The leaderboard requires microsecond reads and single-digit-millisecond write latencies. The datasets are single-digit terabytes in size and must be available to accept writes in less than a minute if a primary node failure occurs.The company needs a solution in which data can persist for further analytical processing through a data pipeline.Which solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      "B. Create an Amazon ROS database with a read replica. Configure the application to point writes to the writer endpoint. Configure the application to point reads to the reader endpoint.",
      "C. Create an Amazon MemoryDB for Redis cluster in Muit-AZ mode Configure the application to interact with the primary node.",
      "D. Create multiple Redis nodes on Amazon EC2 instances that are spread across multiple Availability Zones. Configure backups to Amazon S3."
    ],
    "answer": "C",
    "explanation": "1. Clear explanation of why the correct answer (C) is right:\n\nThe correct answer is C, which is to create an Amazon MemoryDB for Redis cluster in Multi-AZ mode. This solution is the best fit for the given requirements because:\n\n- Amazon MemoryDB for Redis provides microsecond read and single-digit millisecond write latencies, which meets the performance requirements for the leaderboard.\n- The Multi-AZ mode ensures high availability and durability, with automatic failover to a replica node in less than a minute if the primary node fails. This satisfies the need for the dataset to be available for writes in less than a minute in case of a primary node failure.\n- MemoryDB is a fully managed service, which reduces the operational overhead compared to managing multiple Redis nodes on EC2 instances.\n- The data in MemoryDB can be used for further analytical processing through a data pipeline, as required.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nB. Create an Amazon RDS database with a read replica:\n- RDS does not provide the microsecond read and single-digit millisecond write latencies required for the leaderboard.\n- RDS is a traditional relational database, which may not be the best fit for the high-performance requirements of the leaderboard.\n\nD. Create multiple Redis nodes on Amazon EC2 instances with backups to S3:\n- Manually managing multiple Redis nodes on EC2 instances increases the operational overhead, which goes against the requirement of the least operational overhead.\n- While this solution can provide the required performance, it requires more manual configuration and management compared to the fully managed MemoryDB service."
  },
  "484": {
    "question": "A company is running several applications in the AWS Cloud. The applications are specific to separate business units in the company. The company is running the components of the applications in several AWS accounts that are in an organization in AWS Organizations.Every cloud resource in the company\u2019s organization has a tag that is named BusinessUnit. Every tag already has the appropriate value of the business unit name.The company needs to allocate its cloud costs to different business units. The company also needs to visualize the cloud costs for each business unit.Which solution will meet these requirements?",
    "choices": [
      "A. In the organization's management account, create a cost allocation tag that is named BusinessUnit. Also in the management account, create an Amazon S3 bucket and an AWS Cost and Usage Report (AWS CUR). Configure the S3 bucket as the destination for the AWS CUR. From the management account, query the AWS CUR data by using Amazon Athena. Use Amazon QuickSight for visualization.",
      "B. In each member account, create a cost allocation tag that is named BusinessUnit. In the organization\u2019s management account, create an Amazon S3 bucket and an AWS Cost and Usage Report (AWS CUR). Configure the S3 bucket as the destination for the AWS CUR. Create an Amazon CloudWatch dashboard for visualization.",
      "C. In the organization's management account, create a cost allocation tag that is named BusinessUnit. In each member account, create an Amazon S3 bucket and an AWS Cost and Usage Report (AWS CUR). Configure each S3 bucket as the destination for its respective AWS CUR. In the management account, create an Amazon CloudWatch dashboard for visualization.",
      "D. In each member account, create a cost allocation tag that is named BusinessUnit. Also in each member account, create an Amazon S3 bucket and an AWS Cost and Usage Report (AWS CUR). Configure each S3 bucket as the destination for its respective AWS CUR. From the management account, query the AWS CUR data by using Amazon Athena. Use Amazon QuickSight for visualization."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\n\nThe correct answer is A because it meets the requirements of the company to allocate cloud costs to different business units and visualize the cloud costs for each business unit.\n\nKey points:\n- The solution creates a cost allocation tag named \"BusinessUnit\" in the organization's management account, which aligns with the existing tag on all cloud resources.\n- It creates an Amazon S3 bucket and an AWS Cost and Usage Report (AWS CUR) in the management account, simplifying the process of storing and accessing the cost data.\n- By using Amazon Athena to query the AWS CUR data from the management account, the solution enables easy analysis and reporting of costs for each business unit.\n- The use of Amazon QuickSight in the management account provides a powerful visualization tool to present the cost data in a user-friendly manner.\n\nThis centralized approach in the management account ensures consistent tagging, cost data aggregation, and cost analysis across the entire organization, meeting the stated requirements.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. This option creates the cost allocation tag in each member account, which adds complexity and makes it harder to maintain consistency across the organization. Additionally, the use of Amazon CloudWatch dashboard for visualization may not provide the same level of flexibility and reporting capabilities as Amazon Athena and QuickSight.\n\nC. This option also creates the cost allocation tag in the management account, which is correct. However, it requires creating the AWS CUR and S3 buckets in each member account, which adds unnecessary complexity and makes it more difficult to manage and analyze the cost data from a centralized location.\n\nD. This option creates the cost allocation tag and the AWS CUR with S3 buckets in each member account, which again adds complexity and makes it harder to manage the cost data and reporting from a centralized location. While it still allows for using Athena and QuickSight, the distributed nature of the setup reduces the overall efficiency and manageability of the solution."
  },
  "485": {
    "question": "A utility company wants to collect usage data every 5 minutes from its smart meters to facilitate time-of-use metering. When a meter sends data to AWS, the data is sent to Amazon API Gateway, processed by an AWS Lambda function. and stored in an Amazon DynamoDB table. During the pilot phase, the Lambda functions took from 3 to 5 seconds to complete.As more smart meters are deployed, the engineers notice the Lambda functions are taking from 1 to 2 minutes to complete. The functions are also increasing in duration as new types of metrics are collected from the devices. There are many ProvisionedThroughputExceededException errors while performing PUT operations on DynamoDB, and there are also many TooManyRequestsException errors from Lambda.Which combination of changes will resolve these issues? (Choose two.)",
    "choices": [
      "A. Increase the write capacity units to the DynamoDB table.",
      "B. Increase the memory available to the Lambda functions.",
      "C. Increase the payload size from the smart meters to send more data.",
      "D. Stream the data into an Amazon Kinesis data stream from API Gateway and process the data in batches.",
      "E. Collect data in an Amazon SQS FIFO queue, which triggers a Lambda function to process each message"
    ],
    "answer": "AD",
    "explanation": "1. Explanation of the correct answer (A and D):\n\nA. Increase the write capacity units to the DynamoDB table.\nThe issue with the ProvisionedThroughputExceededException errors indicates that the DynamoDB table is hitting its write capacity limits. Increasing the write capacity units will allow the table to handle the higher volume of data being written, resolving the throughput issues.\n\nD. Stream the data into an Amazon Kinesis data stream from API Gateway and process the data in batches.\nThe issue with the TooManyRequestsException errors from Lambda suggests that the Lambda functions are being overwhelmed by the high volume of individual requests. By streaming the data into an Amazon Kinesis data stream, the data can be processed in batches, reducing the number of Lambda function invocations and improving overall performance.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. Increase the memory available to the Lambda functions.\nIncreasing the memory available to the Lambda functions may not resolve the issues caused by the high volume of concurrent requests and the need for batching. The root problem is the throughput and scale, not the individual Lambda function performance.\n\nC. Increase the payload size from the smart meters to send more data.\nIncreasing the payload size from the smart meters is not necessary and may even exacerbate the issues by increasing the processing overhead for each data point. The problem is not the size of the data, but the overall volume and throughput.\n\nE. Collect data in an Amazon SQS FIFO queue, which triggers a Lambda function to process each message.\nCollecting data in an Amazon SQS FIFO queue and triggering a Lambda function for each message would still result in a high number of Lambda invocations and may not provide significant performance improvements compared to processing data in batches from a Kinesis data stream. Kinesis allows for more efficient batch processing, which can help reduce the number of requests and the load on your Lambda functions and DynamoDB."
  },
  "486": {
    "question": "A company recently completed a successful proof of concept of Amazon WorkSpaces. A solutions architect needs to make the solution highly available across two AWS Regions. Amazon WorkSpaces is deployed in a failover Region, and a hosted zone is deployed in Amazon Route 53.What should the solutions architect do to configure high availability for the solution?",
    "choices": [
      "A. Create a connection alias in the primary Region and in the failover Region. Associate the connection aliases with a directory in each Region. Create a Route 53 failover routing policy. Set Evaluate Target Health to Yes.",
      "B. Create a connection alias in the primary Region and in the failover Region. Associate the connection aliases with a directory in the primary Region. Create a Route 53 multivalue answer routing policy.",
      "C. Create a connection alias in the primary Region. Associate the connection alias with a directory in the primary Region. Create a Route 53 weighted routing policy.",
      "D. Create a connection alias in the primary Region Associate the connection alias with a directory in the failover Region. Create a Route 53 failover routing policy. Set Evaluate Target Health to Yes."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\n\nThe correct answer is A because it aligns with the steps required to configure high availability for the Amazon WorkSpaces solution across two AWS Regions:\n\n- Create a connection alias in the primary Region and in the failover Region. This allows users to connect to the appropriate WorkSpaces environment based on the routing policy.\n- Associate the connection aliases with a directory in each Region. This ensures that the users can access the WorkSpaces environment in the respective Regions.\n- Create a Route 53 failover routing policy. This allows automatic failover from the primary Region to the failover Region in case of a failure or high latency in the primary Region.\n- Set Evaluate Target Health to Yes. This option ensures that Route 53 checks the health of the WorkSpaces environments in each Region and routes traffic accordingly.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. This option is incorrect because it does not create a connection alias in the failover Region. Instead, it associates the connection alias with a directory in the primary Region only. This would not provide high availability, as there would be no failover mechanism in place.\n\nC. This option is incorrect because it creates a connection alias only in the primary Region and associates it with a directory in the primary Region. It also uses a Route 53 weighted routing policy, which is not the appropriate choice for high availability. Weighted routing is used for load balancing, not failover.\n\nD. This option is incorrect because it creates a connection alias in the primary Region and associates it with a directory in the failover Region. This would not provide a seamless failover experience, as users would need to connect to a different environment in the failover Region."
  },
  "487": {
    "question": "A company plans to migrate many VMs from an on-premises environment to AWS. The company requires an initial assessment of the on-premises environment before the migration, a visualization of the dependencies between applications that run on the VMs, and a report that provides an assessment of the on-premises environment.To get this information, the company has initiated a Migration Evaluator assessment request. The company has the ability to install collector software in its on-premises environment without any constraintsWhich solution will provide the company with the required information with the LEAST operational overhead?",
    "choices": [
      "A. Install the AWS Application Discovery Agent on each on-premises VM. After the data collection period ends, use AWS Migration Hub to view the application dependencies. Download the Quick insights assessment report from Migration Hub.",
      "B. Install the Migration Evaluator Collector on each on-premises VM. After the data collection period ends, use Migration Evaluator to view the application dependencies. Download and export the discovered server list from Migration Evaluator. Upload the list to Amazon QuickSight When the QuickSight report is generated, download the Quick Insights assessment report.",
      "C. Setup the AWS Application Discovery Service Agentless Collector in the on-premises environment. After the data collection period ends, use AWS Migration Hub to view the application dependencies. Export the discovered server list from Application Discovery Service. Upload the list to Migration Evaluator. When the Migration Evaluator report is generated, download the Quick Insights assessment.",
      "D. Set up the Migration Evaluator Collector in the on-premises environment. Install the AWS Application Discovery Agent on each VM. After the data collection period ends, use AWS Migration Hub to view the application dependencies. Download the Quick Insights assessment report from Migration Evaluator."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\n\nThe correct answer is A because it provides the required information with the least operational overhead. Here's why:\n\n- Install the AWS Application Discovery Agent on each on-premises VM: This allows the system to discover and map the dependencies between applications running on the VMs, which is one of the key requirements.\n- Use AWS Migration Hub to view the application dependencies: Migration Hub provides a centralized view of the application dependencies, fulfilling the visualization requirement.\n- Download the Quick insights assessment report from Migration Hub: This provides the required assessment report of the on-premises environment, as requested in the question.\n\nThis approach uses the AWS-provided tools (Application Discovery Agent and Migration Hub) to collect the necessary information, which minimizes the operational overhead on the customer's side compared to the other options.\n\n2. Explanations of why the other choices are incorrect:\n\nB. This option involves using the Migration Evaluator Collector, which adds an additional tool to the mix and increases the operational overhead. Additionally, the requirement to export the discovered server list and upload it to Amazon QuickSight adds an extra step compared to the direct integration provided by option A.\n\nC. This option involves setting up the AWS Application Discovery Service Agentless Collector, which is not the most efficient approach for collecting application dependency data. The agentless collector may not gather all the necessary information, and the additional step of exporting the server list and uploading it to Migration Evaluator increases the operational overhead.\n\nD. This option is similar to A, but it adds the extra step of setting up the Migration Evaluator Collector, which is unnecessary and increases the operational overhead compared to the direct use of the AWS Application Discovery Agent and Migration Hub in option A."
  },
  "488": {
    "question": "A company hosts its primary API on AWS by using an Amazon API Gateway API and AWS Lambda functions that contain the logic for the API methods. The company\u2019s internal applications use the API for core functionality and business logic. The company\u2019s customers use the API to access data from their accounts. Several customers also have access to a legacy API that is running on a single standalone Amazon EC2 instance.The company wants to increase the security for these APIs to better prevent denial of service (DoS) attacks, check for vulnerabilities, and guard against common exploits.What should a solutions architect do to meet these requirements?",
    "choices": [
      "A. Use AWS WAF to protect both APIs. Configure Amazon Inspector to analyze the legacy API. Configure Amazon GuardDuty to monitor for malicious attempts to access the APIs.",
      "B. Use AWS WAF to protect the API Gateway API. Configure Amazon Inspector to analyze both APIs. Configure Amazon GuardDuty to block malicious attempts to access the APIs.",
      "C. Use AWS WAF to protect the API Gateway API. Configure Amazon Inspector to analyze the legacy API. Configure Amazon GuardDuty to monitor for malicious attempts to access the APIs.",
      "D. Use AWS WAF to protect the API Gateway AP! Configure Amazon Inspector to protect the legacy API. Configure Amazon GuardDuty to block malicious attempts to access the APIs."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C because it correctly addresses the requirements of the question:\n\n- Use AWS WAF to protect the API Gateway API: This is the right approach to secure the primary API hosted on API Gateway against common web-based attacks, DoS attacks, and other exploits.\n\n- Configure Amazon Inspector to analyze the legacy API: Since the legacy API is running on a standalone EC2 instance, Amazon Inspector is the right tool to assess the security posture and vulnerabilities of the EC2 instance and the underlying application.\n\n- Configure Amazon GuardDuty to monitor for malicious attempts to access the APIs: GuardDuty is a continuous security monitoring service that can detect and alert on unusual and potentially malicious activity across both the API Gateway API and the legacy API. While GuardDuty does not directly block attacks, it provides valuable threat detection capabilities.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option is incorrect because it suggests using AWS WAF to protect both APIs, but the question states that the legacy API is running on a standalone EC2 instance, and WAF cannot be directly attached to an EC2 instance. Additionally, the option suggests using Inspector to analyze the legacy API, which is correct, but it also suggests using GuardDuty to block malicious attempts, which is not the correct capability of GuardDuty.\n\nB. This option is incorrect because it suggests using Inspector to analyze both APIs, but Inspector cannot directly monitor the API Gateway API. The option also suggests using GuardDuty to block malicious attempts, which is not the correct capability of GuardDuty.\n\nD. This option is incorrect because it suggests using Inspector to protect the legacy API, but Inspector is an analysis tool and cannot directly protect the API. The option also suggests using GuardDuty to block malicious attempts, which is not the correct capability of GuardDuty."
  },
  "489": {
    "question": "A company is running a serverless ecommerce application on AWS. The application uses Amazon API Gateway to invoke AWS Lambda Java functions. The Lambda functions connect to an Amazon RDS for MySQL database to store data.During a recent sale event, a sudden increase in web traffic resulted in poor API performance and database connection failures. The company needs to implement a solution to minimize the latency for the Lambda functions and to support bursts in traffic.Which solution will meet these requirements with the LEAST amount of change to the application?",
    "choices": [
      "A. Update the code of the Lambda functions so that the Lambda functions open the database connection outside of the function handler. Increase the provisioned concurrency for the Lambda functions.",
      "B. Create an RDS Proxy endpoint for the database. Store database secrets in AWS Secrets Manager. Set up the required IAM permissions. Update the Lambda functions to connect to the RDS Proxy endpoint. Increase the provisioned concurrency for the Lambda functions.",
      "C. Create a custom parameter group. Increase the value of the max_connections parameter. Associate the custom parameter group with the RDS DB instance and schedule a reboot. Increase the reserved concurrency for the Lambda functions.",
      "D. Create an RDS Proxy endpoint for the database. Store database secrets in AWS Secrets Manager. Set up the required IAM permissions. Update the Lambda functions to connect to the RDS Proxy endpoint. Increase the reserved concurrency for the Lambda functions."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B because it provides the least amount of change to the existing application while addressing the key requirements of minimizing latency for the Lambda functions and supporting bursts in traffic.\n\nThe key aspects of this solution are:\n\n- Creating an RDS Proxy endpoint for the database: This allows the Lambda functions to connect to the database through the proxy, which can efficiently manage the database connections and provide better scalability during traffic surges.\n- Storing database secrets in AWS Secrets Manager: This improves the security of the database credentials, which is an important consideration, without significantly altering the existing application.\n- Updating the Lambda functions to connect to the RDS Proxy endpoint: This is a relatively minor change compared to other options, as it only requires updating the connection details in the Lambda function code.\n- Increasing the provisioned concurrency for the Lambda functions: This ensures that the Lambda functions can handle bursts in traffic without experiencing performance degradation or connection failures.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This option involves opening the database connection outside of the function handler and increasing the provisioned concurrency for the Lambda functions. While this can help with the performance of the Lambda functions, it does not address the database connection failures, which is a key requirement.\n\nC. This option involves creating a custom parameter group, increasing the max_connections parameter, and associating it with the RDS DB instance. While this can increase the number of concurrent database connections, it does not address the scalability and latency issues with the Lambda functions, which are also important requirements.\n\nD. This option is similar to the correct answer (B), but it suggests using reserved concurrency for the Lambda functions instead of provisioned concurrency. While reserved concurrency can also help handle bursts in traffic, it is generally a more cost-effective solution for stable, predictable workloads. Provisioned concurrency is better suited for handling unpredictable traffic surges, which is the key requirement in this scenario."
  },
  "490": {
    "question": "A company requires that all internal application connectivity use private IP addresses. To facilitate this policy, a solutions architect has created interface endpoints to connect to AWS Public services. Upon testing, the solutions architect notices that the service names are resolving to public IP addresses, and that internal services cannot connect to the interface endpoints.Which step should the solutions architect take to resolve this issue?",
    "choices": [
      "A. Update the subnet route table with a route to the interface endpoint.",
      "B. Enable the private DNS option on the VPC attributes.",
      "C. Configure the security group on the interface endpoint to allow connectivity to the AWS services.",
      "D. Configure an Amazon Route 53 private hosted zone with a conditional forwarder for the internal application."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (Option B):\n\nThe correct answer is Option B: Enable the private DNS option on the VPC attributes. This is the appropriate step to resolve the issue because:\n\n- When you create a VPC endpoint, AWS automatically generates a private DNS hostname for the service that resolves to the private IP address of the VPC endpoint.\n- However, by default, the private DNS option is disabled on the VPC, which means that DNS queries for the service name will be resolved using the public DNS instead of the private DNS provided by the VPC endpoint.\n- Enabling the private DNS option on the VPC attributes instructs the VPC to use the private DNS names provided by the VPC endpoints for the specified AWS services.\n- This ensures that the service names resolve to the private IP addresses of the VPC endpoints, allowing internal services within the VPC to connect to the AWS services using private IP addresses, as per the company's policy.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Update the subnet route table with a route to the interface endpoint:\n- Updating the subnet route table is not the correct step, as the issue is related to DNS resolution, not routing.\n- The route table configuration does not affect how the service name is resolved.\n\nC. Configure the security group on the interface endpoint to allow connectivity to the AWS services:\n- Configuring the security group is not the appropriate step, as the issue is related to DNS resolution, not security group settings.\n- The security group configuration does not affect how the service name is resolved.\n\nD. Configure an Amazon Route 53 private hosted zone with a conditional forwarder for the internal application:\n- Configuring a private hosted zone in Route 53 is not necessary, as the issue can be resolved by enabling the private DNS option on the VPC attributes.\n- A conditional forwarder in Route 53 is not required, as the private DNS provided by the VPC endpoint should be able to resolve the service name correctly once the private DNS option is enabled."
  },
  "491": {
    "question": "A global ecommerce company has many data centers around the world. With the growth of its stored data, the company needs to set up a solution to provide scalable storage for legacy on-premises file applications. The company must be able to take point-in-time copies of volumes by using AWS Backup and must retain low-latency access to frequently accessed data. The company also needs to have storage volumes that can be mounted as Internet Small Computer System Interface (iSCSI) devices from the company\u2019s on-premises application servers.Which solution will meet these requirements?",
    "choices": [
      "A. Provision an AWS Storage Gateway tape gateway. Configure the tape gateway to store data in an Amazon S3 bucket. Deploy AWS Backup to take point-in-time copies of the volumes.",
      "B. Provision an Amazon FSx File Gateway and an Amazon S3 File Gateway. Deploy AWS Backup to take point-in-time copies of the data.",
      "C. Provision an AWS Storage Gateway volume gateway in cache mode. Back up the on-premises Storage Gateway volumes with AWS Backup.",
      "D. Provision an AWS Storage Gateway file gateway in cache mode. Deploy AWS Backup to take point-in-time copies of the volumes."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C, which is to provision an AWS Storage Gateway volume gateway in cache mode. This solution meets all the requirements specified in the question:\n\n- Scalable storage for legacy on-premises file applications: The AWS Storage Gateway volume gateway can provide scalable storage by integrating with Amazon S3 as the backend storage.\n- Point-in-time copies using AWS Backup: The Storage Gateway volumes can be backed up using AWS Backup, allowing the company to take point-in-time copies of the data.\n- Low-latency access to frequently accessed data: The \"cache\" mode of the Storage Gateway volume gateway keeps frequently accessed data in a local cache, providing low-latency access.\n- Mounting storage volumes as iSCSI devices: The Storage Gateway file gateway can be used to mount the storage volumes as iSCSI devices from the on-premises application servers.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This choice is incorrect because the AWS Storage Gateway tape gateway is primarily used for archiving data to Amazon S3, and it does not provide the low-latency access required for the frequently accessed data.\n\nB. This choice is incorrect because the Amazon FSx File Gateway and Amazon S3 File Gateway do not provide the capability to mount storage volumes as iSCSI devices, which is a requirement for this scenario.\n\nD. This choice is incorrect because the AWS Storage Gateway file gateway is primarily used for file-level storage and does not provide the same level of integration with Amazon S3 and the ability to mount storage volumes as iSCSI devices, as the volume gateway does."
  },
  "492": {
    "question": "A solutions architect is importing a VM from an on-premises environment by using the Amazon EC2 VM Import feature of AWS Import/Export. The solutions architect has created an AMI and has provisioned an Amazon EC2 instance that is based on that AMI. The EC2 instance runs inside a public subnet in a VPC and has a public IP address assigned.The EC2 instance does not appear as a managed instance in the AWS Systems Manager console.Which combination of steps should the solutions architect take to troubleshoot this issue? (Choose two.)",
    "choices": [
      "A. Verify that Systems Manager Agent is installed on the instance and is running.",
      "B. Verify that the instance is assigned an appropriate IAM role for Systems Manager.",
      "C. Verify the existence of a VPC endpoint on the VPC.",
      "D. Verity that the AWS Application Discovery Agent is configured.",
      "E. Verify the correct configuration of service-linked roles for Systems Manager."
    ],
    "answer": "AB",
    "explanation": "1. Explanation of the correct answer (A and B):\n\nA. Verify that Systems Manager Agent is installed on the instance and is running.\nThe Systems Manager Agent is a necessary component for an EC2 instance to be managed by AWS Systems Manager. If the agent is not installed or not running, the instance will not appear as a managed instance in the Systems Manager console.\n\nB. Verify that the instance is assigned an appropriate IAM role for Systems Manager.\nThe EC2 instance needs an IAM role with the necessary permissions to allow the Systems Manager Agent to interact with the Systems Manager service. Without the appropriate IAM role, the agent will not be able to perform management tasks and report back to the Systems Manager console.\n\n2. Explanations of why the incorrect choices are wrong:\n\nC. Verify the existence of a VPC endpoint on the VPC.\nThis is not a necessary step for the instance to appear as a managed instance in the Systems Manager console. The instance has a public IP address, so it can directly communicate with the Systems Manager service without the need for a VPC endpoint.\n\nD. Verify the AWS Application Discovery Agent is configured.\nThe AWS Application Discovery Agent is used for a different purpose, which is to discover and analyze on-premises applications for migration to AWS. It is not required for the instance to be managed by Systems Manager.\n\nE. Verify the correct configuration of service-linked roles for Systems Manager.\nService-linked roles are used to grant Systems Manager access to other AWS services on your behalf. While these roles are important for certain Systems Manager features, they are not directly related to the instance being managed by Systems Manager. The issue here is with the instance-level configuration, not the service-level configuration."
  },
  "493": {
    "question": "A company is using AWS CloudFormation as its deployment tool for all applications. It stages all application binaries and templates within Amazon S3 buckets with versioning enabled. Developers have access to an Amazon EC2 instance that hosts the integrated development environment (IDE). The developers download the application binaries from Amazon S3 to the EC2 instance, make changes, and upload the binaries to an S3 bucket after running the unit tests locally. The developers want to improve the existing deployment mechanism and implement CI/CD using AWS CodePipeline.The developers have the following requirements:\u2022\tUse AWS CodeCommit for source control.\u2022\tAutomate unit testing and security scanning.\u2022\tAlert the developers when unit tests fail.\u2022\tTurn application features on and off, and customize deployment dynamically as part of CI/CD.\u2022\tHave the lead developer provide approval before deploying an application.Which solution will meet these requirements?",
    "choices": [
      "A. Use AWS CodeBuild to run unit tests and security scans. Use an Amazon EventBridge rule to send Amazon SNS alerts to the developers when unit tests fail. Write AWS Cloud Development Kit (AWS CDK) constructs for different solution features, and use a manifest file to tum features on and off in the AWS CDK application. Use a manual approval stage in the pipeline to allow the lead developer to approve applications.",
      "B. Use AWS Lambda to run unit tests and security scans. Use Lambda in a subsequent stage in the pipeline to send Amazon SNS alerts to the developers when unit tests fail. Write AWS Amplify plugins for different solution features and utilize user prompts to tum features on and off. Use Amazon SES in the pipeline to allow the lead developer to approve applications.",
      "C. Use Jenkins to run unit tests and security scans. Use an Amazon EventBridge rule in the pipeline to send Amazon SES alerts to the developers when unit tests fail Use AWS CloudFormation nested stacks for different solution features and parameters to turn features on and off. Use AWS Lambda in the pipeline to allow the lead developer to approve applications.",
      "D. Use AWS CodeDeploy to run unit tests and security scans. Use an Amazon CloudWatch alarm in the pipeline to send Amazon SNS alerts to the developers when unit tests fail. Use Docker images for different solution features and the AWS CLI to turn features on and off. Use a manual approval stage in the pipeline to allow the lead developer to approve applications."
    ],
    "answer": "A",
    "explanation": "1. Explanation of why the correct answer (A) is right:\n\nAnswer A is the correct solution because it meets all the requirements stated in the question:\n\n- **Use AWS CodeCommit for source control**: This is specified as a requirement, and Answer A includes using AWS CodeCommit.\n- **Automate unit testing and security scanning**: Answer A uses AWS CodeBuild to run unit tests and security scans, which meets this requirement.\n- **Alert the developers when unit tests fail**: Answer A uses an Amazon EventBridge rule to send Amazon SNS alerts to the developers when unit tests fail.\n- **Turn application features on and off, and customize deployment dynamically as part of CI/CD**: Answer A uses AWS CDK constructs and a manifest file to allow for dynamic feature toggling and customization of the deployment.\n- **Have the lead developer provide approval before deploying an application**: Answer A includes a manual approval stage in the pipeline to allow the lead developer to approve applications before deployment.\n\nBy leveraging AWS services like CodeBuild, EventBridge, SNS, CDK, and the manual approval stage in CodePipeline, Answer A provides a comprehensive and integrated solution that meets all the stated requirements.\n\n2. Explanations of why the incorrect choices are wrong:\n\n**B. Use AWS Lambda to run unit tests and security scans**:\n- Lambda is not the best choice for running unit tests and security scans, as it may not provide the necessary resources and flexibility required for these tasks.\n\n**C. Use Jenkins to run unit tests and security scans**:\n- Using a separate tool like Jenkins introduces additional complexity and management overhead, as it needs to be set up and maintained separately from the AWS services.\n- The use of nested stacks and parameters for feature toggling may not be as flexible as the solution proposed in Answer A.\n\n**D. Use AWS CodeDeploy to run unit tests and security scans**:\n- CodeDeploy is primarily designed for application deployment, not for running unit tests and security scans.\n- The use of Docker images and the AWS CLI for feature toggling may not be as flexible as the solution proposed in Answer A.\n\nIn summary, Answer A provides the most comprehensive and integrated solution that leverages native AWS services to meet all the stated requirements, making it the correct choice."
  },
  "494": {
    "question": "A global ecommerce company has many data centers around the world. With the growth of its stored data, the company needs to set up a solution to provide scalable storage for legacy on-premises file applications. The company must be able to take point-in-time copies of volumes by using AWS Backup and must retain low-latency access to frequently accessed data. The company also needs to have storage volumes that can be mounted as Internet Small Computer System Interface (iSCSI) devices from the company\u2019s on-premises application servers.Which solution will meet these requirements?",
    "choices": [
      "A. Provision an AWS Storage Gateway tape gateway. Configure the tape gateway to store data in an Amazon S3 bucket. Deploy AWS Backup to take point-in-time copies of the volumes.",
      "B. Provision an Amazon FSx File Gateway and an Amazon S3 File Gateway. Deploy AWS Backup to take point-in-time copies of the data.",
      "C. Provision an AWS Storage Gateway volume gateway in cache mode. Back up the on-premises Storage Gateway volumes with AWS Backup.",
      "D. Provision an AWS Storage Gateway file gateway in cache mode. Deploy AWS Backup to take point-in-time copies of the volumes."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C, which is to provision an AWS Storage Gateway volume gateway in cache mode. This solution meets all the requirements of the global e-commerce company:\n\n- Scalable storage for legacy on-premises file applications: The AWS Storage Gateway volume gateway in cache mode provides scalable storage by caching frequently accessed data on-premises and storing the rest in Amazon S3, which can scale to accommodate the growing data volumes.\n\n- Low-latency access to frequently accessed data: The cache mode of the Storage Gateway volume gateway ensures low-latency access to frequently accessed data, as it is stored locally on-premises.\n\n- Point-in-time copies of volumes using AWS Backup: AWS Backup can be deployed to take point-in-time copies of the on-premises Storage Gateway volumes, providing the required backup and recovery capabilities.\n\n- Mounting storage volumes as iSCSI devices: The Storage Gateway volume gateway can be configured to expose iSCSI devices, allowing the company's on-premises application servers to mount the storage volumes.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Incorrect: Provisioning an AWS Storage Gateway tape gateway and storing data in an S3 bucket does not provide the low-latency access to frequently accessed data required by the company.\n\nB. Incorrect: Provisioning an Amazon FSx File Gateway and an Amazon S3 File Gateway does not provide the ability to mount storage volumes as iSCSI devices, which is a requirement for the company's on-premises application servers.\n\nD. Incorrect: Provisioning an AWS Storage Gateway file gateway in cache mode does not provide the ability to mount storage volumes as iSCSI devices, which is a requirement for the company's on-premises application servers."
  },
  "495": {
    "question": "A company has an application that uses AWS Key Management Service (AWS KMS) to encrypt and decrypt data. The application stores data in an Amazon S3 bucket in an AWS Region. Company security policies require the data to be encrypted before the data is placed into the S3 bucket. The application must decrypt the data when the application reads files from the S3 bucket.The company replicates the S3 bucket to other Regions. A solutions architect must design a solution so that the application can encrypt and decrypt data across Regions. The application must use the same key to decrypt the data in each Region.Which solution will meet these requirements?",
    "choices": [
      "A. Create a KMS multi-Region primary key. Use the KMS multi-Region primary key to create a KMS multi-Region replica key in each additional Region where the application is running. Update the application code to use the specific replica key in each Region.",
      "B. Create a new customer managed KMS key in each additional Region where the application is running. Update the application code to use the specific KMS key in each Region.",
      "C. Use AWS Private Certificate Authority to create a new certificate authority (CA) in the primary Region. Issue a new private certificate from the CA for the application\u2019s website URL. Share the CA with the additional Regions by using AWS Resource Access Manager (AWS RAM). Update the application code to use the shared CA certificates in each Region.",
      "D. Use AWS Systems Manager Parameter Store to create a parameter in each additional Region where the application is running. Export the key material from the KMS key in the primary Region. Store the key material in the parameter in each Region. Update the application code to use the key data from the parameter in each Region."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\n\nThe correct answer is A because it directly addresses the key requirements of the scenario:\n\n- The application needs to use the same encryption key to encrypt and decrypt data across multiple AWS Regions.\n- AWS KMS multi-Region keys allow you to create a primary key in one Region and then replicate it as a replica key in other Regions.\n- By using the multi-Region primary key to create replica keys in each additional Region, the application can use the same key material to encrypt and decrypt data, meeting the requirement of using the same key across Regions.\n- Updating the application code to use the specific replica key in each Region ensures that the correct key is used for the corresponding Region.\n\n2. Explanations of why the other choices are incorrect:\n\nB. This option suggests creating a new KMS key in each additional Region, which would not allow the application to use the same key across Regions for encryption and decryption.\n\nC. Using a private Certificate Authority (CA) and sharing it across Regions using AWS Resource Access Manager (RAM) does not address the requirement of using the same encryption key across Regions. This solution is focused on managing SSL/TLS certificates, not encryption keys.\n\nD. Storing the encryption key material in AWS Systems Manager Parameter Store and retrieving it in each Region would not provide the same level of key management and control as using KMS multi-Region keys. It also introduces an additional dependency on Parameter Store, which may not be necessary.\n\nIn summary, option A is the correct choice because it directly addresses the requirement of using the same encryption key across multiple Regions by leveraging the AWS KMS multi-Region key functionality."
  },
  "496": {
    "question": "A company hosts an application that uses several Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB). During the initial startup of the EC2 instances, the EC2 instances run user data scripts to download critical content for the application from an Amazon S3 bucket.The EC2 instances are launching correctly. However, after a period of time, the EC2 instances are terminated with the following error message: \u201cAn instance was taken out of service in response to an ELB system health check failure.\u201d EC2 instances continue to launch and be terminated because of Auto Scaling events in an endless loop.The only recent change to the deployment is that the company added a large amount of critical content to the S3 bucket. The company does not want to alter the user data scripts in production.What should a solutions architect do so that the production environment can deploy successfully?",
    "choices": [
      "A. Increase the size of the EC2 instances.",
      "B. Increase the health check timeout for the ALB.",
      "C. Change the health check path for the ALB.",
      "D. Increase the health check grace period for the Auto Scaling group."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (D):\n\nThe correct answer is D. Increase the health check grace period for the Auto Scaling group.\n\nThe issue is that the EC2 instances are being terminated due to an ELB (Elastic Load Balancing) system health check failure. This is likely because the user data scripts that download critical content from the S3 bucket are still running when the health checks are performed, causing the instances to be marked as unhealthy.\n\nBy increasing the health check grace period for the Auto Scaling group, you give the EC2 instances more time to complete their startup process, including the execution of the user data scripts, before being considered healthy or unhealthy. This should prevent the endless loop of launching and terminating EC2 instances due to health check failures.\n\n2. Explanations of the incorrect choices:\n\nA. Increase the size of the EC2 instances:\nThis is not the correct solution because the issue is not related to the instance size. Increasing the instance size may not help with the startup time or the time required to download the content from the S3 bucket.\n\nB. Increase the health check timeout for the ALB:\nIncreasing the health check timeout for the Application Load Balancer (ALB) may not be the best solution. Even if the ALB waits longer for a response, the EC2 instances may still not be ready to serve requests due to the long initialization time, and the health checks will still fail.\n\nC. Change the health check path for the ALB:\nChanging the health check path for the ALB is also not the correct solution. The issue is not related to the health check path, but rather the time required for the EC2 instances to complete their startup process, including the download of content from the S3 bucket."
  },
  "497": {
    "question": "A company needs to move some on-premises Oracle databases to AWS. The company has chosen to keep some of the databases on premises for business compliance reasons.The on-premises databases contain spatial data and run cron jobs for maintenance. The company needs to connect to the on-premises systems directly from AWS to query data as a foreign table.Which solution will meet these requirements?",
    "choices": [
      "A. Create Amazon DynamoDB global tables with auto scaling enabled. Use the AWS Schema Conversion Tool (AWS SCT) and AWS Database Migration Service (AWS DMS) to move the data from on premises to DynamoDB. Create an AWS Lambda function to move the spatial data to Amazon S3. Query the data by using Amazon Athena. Use Amazon EventBridge to schedule jobs in DynamoDB for maintenance. Use Amazon API Gateway for foreign table support.",
      "B. Create an Amazon RDS for Microsoft SQL Server DB instance. Use native replication to move the data from on premises to the DB instance. Use the AWS Schema Conversion Tool (AWS SCT) to modify the SQL Server schema as needed after replication. Move the spatial data to Amazon Redshift. Use stored procedures for system maintenance. Create AWS Glue crawlers to connect to the on-premises Oracle databases for foreign table support.",
      "C. Launch Amazon EC2 instances to host the Oracle databases. Place the EC2 instances in an Auto Scaling group. Use AWS Application Migration Service to move the data from on premises to the EC2 instances and for real-time bidirectional change data capture (CDC) synchronization. Use Oracle native spatial data support. Create an AWS Lambda function to run maintenance jobs as part of an AWS Step Functions workflow. Create an internet gateway for foreign table support.",
      "D. Create an Amazon RDS for PostgreSQL DB instance. Use the AWS Schema Conversion Tool (AWS SCT) and AWS Database Migration Service (AWS DMS) to move the data from on premises to the DB instance. Use PostgreSQL native spatial data support. Run cron jobs on the DB instance for maintenance. Use AWS Direct Connect to connect the DB instance to the on-premises environment for foreign table support."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (D):\n\nThe correct answer is D because it provides the most appropriate solution to meet the requirements outlined in the question:\n\n- It creates an Amazon RDS for PostgreSQL DB instance, which natively supports spatial data types through the PostGIS extension. This aligns with the requirement to handle spatial data from the on-premises Oracle databases.\n- It uses the AWS Schema Conversion Tool (SCT) and AWS Database Migration Service (DMS) to migrate the data from the on-premises Oracle databases to the Amazon RDS for PostgreSQL instance. This ensures a smooth and efficient data migration process.\n- It allows running cron jobs directly on the RDS for PostgreSQL instance using extensions like pg_cron, fulfilling the requirement for maintenance tasks.\n- It uses AWS Direct Connect to establish a dedicated, secure connection between the on-premises environment and the AWS environment. This allows querying data from the on-premises Oracle databases as foreign tables in the PostgreSQL instance, meeting the requirement for direct data access.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This choice is incorrect because:\n- It uses Amazon DynamoDB, which is not well-suited for handling spatial data types.\n- It uses Amazon Athena and Amazon API Gateway, which do not directly address the requirement for querying on-premises data as foreign tables.\n\nB. This choice is incorrect because:\n- It uses Amazon RDS for Microsoft SQL Server, which does not natively support spatial data types like PostgreSQL with PostGIS.\n- It moves the spatial data to Amazon Redshift, which adds complexity and does not directly address the requirement for querying on-premises data as foreign tables.\n\nC. This choice is incorrect because:\n- It uses Amazon EC2 instances to host the Oracle databases, which adds complexity and overhead compared to using a managed database service like Amazon RDS.\n- It uses Oracle's native spatial data support, but does not address the requirement for querying on-premises data as foreign tables."
  },
  "498": {
    "question": "Accompany runs an application on Amazon EC2 and AWS Lambda. The application stores temporary data in Amazon S3. The S3 objects are deleted after 24 hours.The company deploys new versions of the application by launching AWS CloudFormation stacks. The stacks create the required resources. After validating a new version, the company deletes the old stack. The deletion of an old development stack recently failed. A solutions architect needs to resolve this issue without major architecture changes.Which solution will meet these requirements?",
    "choices": [
      "A. Create a Lambda function to delete objects from an S3 bucket. Add the Lambda function as a custom resource in the CloudFormation stack with a DependsOn attribute that points to the S3 bucket resource.",
      "B. Modify the CloudFormation stack to attach a DeletionPolicy attribute with a value of Delete to the S3 bucket.",
      "C. Update the CloudFormation stack to add a DeletionPolicy attribute with a value of Snapshot for the S3 bucket resource",
      "D. Update the CloudFormation template to create an Amazon Elastic File System (Amazon EFS) file system to store temporary files instead of Amazon S3. Configure the Lambda functions to run in the same VPC as the EFS file system."
    ],
    "answer": "A"
  },
  "499": {
    "question": "A company has an application that stores user-uploaded videos in an Amazon S3 bucket that uses S3 Standard storage. Users access the videos frequently in the first 180 days after the videos are uploaded. Access after 180 days is rare. Named users and anonymous users access the videos.Most of the videos are more than 100 MB in size. Users often have poor internet connectivity when they upload videos, resulting in failed uploads. The company uses multipart uploads for the videos.A solutions architect needs to optimize the S3 costs of the application.Which combination of actions will meet these requirements? (Choose two.)",
    "choices": [
      "A. Configure the S3 bucket to be a Requester Pays bucket.",
      "B. Use S3 Transfer Acceleration to upload the videos to the S3 bucket.",
      "C. Create an S3 Lifecycle configuration o expire incomplete multipart uploads 7 days after initiation.",
      "D. Create an S3 Lifecycle configuration to transition objects to S3 Glacier Instant Retrieval after 1 day.",
      "E. Create an S3 Lifecycle configuration to transition objects to S3 Standard-infrequent Access (S3 Standard- IA) after 180 days."
    ],
    "answer": "CE"
  },
  "500": {
    "question": "A company runs an ecommerce web application on AWS. The web application is hosted as a static website on Amazon S3 with Amazon CloudFront for content delivery. An Amazon APIGateway API invokes AWS Lambda functions to handle user requests and order processing for the web application The Lambda functions store data in an Amazon ROS for MySQL DB cluster that uses On-Demand instances. The DB cluster usage has been consistent in the past 12 months.Recently, the website has experienced SQL injection and web exploit attempts. Customers also report that order processing time has increased during periods of peak usage. During these periods, the Lambda functions often have cold starts. As the company grows, the company needs to ensure scalability and low-latency access during traffic peaks. The company also must optimize the database costs and add protection against the SQL injection and web exploit attempts.Which solution will meet these requirements?",
    "choices": [
      "A. Configure the Lambda functions to have an increased timeout value during peak periods. Use RDS Reserved Instances for the database. Use CloudFront and subscribe to AWS Shield Advanced to protect against the SQL injection and web exploit attempts.",
      "B. Increase the memory of the Lambda functions, Transition to Amazon Redshift for the database. Integrate Amazon Inspector with CloudFront to protect against the SQL injection and web exploit attempts.",
      "C. Use Lambda functions with provisioned concurrency for compute during peak periods, Transition to Amazon Aurora Serverless for the database. Use CloudFront and subscribe to AWS Shield Advanced to protect against the SQL injection and web exploit attempts.",
      "D. Use Lambda functions with provisioned concurrency for compute during peak periods. Use RDS Reserved Instances for the database. Integrate AWS WAF with CloudFront to protect against the SQL injection and web exploit attempts."
    ],
    "answer": "D"
  },
  "501": {
    "question": "A company runs a web application on a single Amazon EC2 instance. End users experience slow application performance during times of peak usage, when CPU utilization is consistently more than 95%.A user data script installs required custom packages on the EC2 instance. The process of launching the instance takes several minutes.The company is creating an Auto Scaling group that has mixed instance groups, varied CPUs, and a maximum capacity limit. The Auto Scaling group will use a launch template for various configuration options. The company needs to decrease application latency when new instances are launched during auto scaling.Which solution will meet these requirements?",
    "choices": [
      "A. Use a predictive scaling policy. Use an instance maintenance policy to run the user data script. Set the default instance warmup time to 0 seconds.",
      "B. Use a dynamic scaling policy. Use lifecycle hooks to run the user data script. Set the default instance warmup time to 0 seconds.",
      "C. Use a predictive scaling policy. Enable warm pools for the Auto Scaling group. Use an instance maintenance policy to run the user data script.",
      "D. Use a dynamic scaling policy. Enable warm pools for the Auto Scaling group. Use lifecycle hooks to run the user data script."
    ],
    "answer": "D"
  },
  "502": {
    "question": "A retail company is operating its ecommerce application on AWS. The application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The company uses an Amazon RDS DB instance as the database backend. Amazon CloudFront is configured with one origin that points to the ALB. Static content is cached. Amazon Route 53 is used to host all public zones.After an update of the application, the ALB occasionally returns a 502 status code (Bad Gateway) error. The root cause is malformed HTTP headers that are returned to the ALB. The webpage returns successfully when a solutions architect reloads the webpage immediately after the error occurs.While the company is working on the problem, the solutions architect needs to provide a custom error page instead of the standard ALB error page to visitors.Which combination of steps will meet this requirement with the LEAST amount of operational overhead? (Choose two.)",
    "choices": [
      "A. Create an Amazon S3 bucket. Configure the S3 bucket to host a static webpage. Upload the custom error pages to Amazon S3.",
      "B. Create an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health check response Target.FailedHealthChecks is greater than 0. Configure the Lambda function to modify the forwarding rule at the ALB to point to a publicly accessible web server.",
      "C. Modify the existing Amazon Route 53 records by adding health checks. Configure a fallback target if the health check fails. Modify DNS records to point to a publicly accessible webpage.",
      "D. Create an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health check response Elb.InternalError is greater than 0. Configure the Lambda function to modify the forwarding rule at the ALB to point to a public accessible web server.",
      "E. Add a custom error response by configuring a CloudFront custom error page. Modify DNS records to point to a publicly accessible web page."
    ],
    "answer": "AE"
  },
  "503": {
    "question": "A company runs a web application on AWS. The web application delivers static content from an Amazon S3 bucket that is behind an Amazon CloudFront distribution. The application serves dynamic content by using an Application Load Balancer (ALB) that distributes requests to a fleet of Amazon EC2 instances in Auto Scaling groups. The application uses a domain name setup in Amazon Route 53.Some users reported occasional issues when the users attempted to access the website during peak hours. An operations team found that the ALB sometimes returned HTTP 503 Service Unavailable errors. The company wants to display a custom error message page when these errors occur. The page should be displayed immediately for this error code.Which solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      "A. Set up a Route 53 failover routing policy. Configure a health check to determine the status of the ALB endpoint and to fail over to the failover S3 bucket endpoint.",
      "B. Create a second CloudFront distribution and an S3 static website to host the custom error page. Set up a Route 53 failover routing policy. Use an active-passive configuration between the two distributions.",
      "C. Create a CloudFront origin group that has two origins. Set the ALB endpoint as the primary origin. For the secondary origin, set an S3 bucket that is configured to host a static website Set up origin failover for the CloudFront distribution. Update the S3 static website to incorporate the custom error page.",
      "D. Create a CloudFront function that validates each HTTP response code that the ALB returns. Create an S3 static website in an S3 bucket. Upload the custom error page to the S3 bucket as a failover. Update the function to read the S3 bucket and to serve the error page to the end users."
    ],
    "answer": "C"
  },
  "504": {
    "question": "A company plans to migrate a legacy on-premises application to AWS. The application is a Java web application that runs on Apache Tomcat with a PostgreSQL database.The company does not have access to the source code but can deploy the application Java Archive (JAR) files. The application has increased traffic at the end of each month.Which solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      "A. Launch Amazon EC2 instances in multiple Availability Zones. Deploy Tomcat and PostgreSQL to all the instances by using Amazon Elastic File System (Amazon EFS) mount points. Use AWS Step Functions to deploy additional EC2 instances to scale for increased traffic.",
      "B. Provision Amazon Elastic Kubernetes Service (Amazon EKS) in an Auto Scaling group across multiple AWS Regions. Deploy Tomcat and PostgreSQL in the container images. Use a Network Load Balancer to scale for increased traffic.",
      "C. Refactor the Java application into Python-based containers. Use AWS Lambda functions for the application logic. Store application data in Amazon DynamoDB global tables. Use AWS Storage Gateway and Lambda concurrency to scale for increased traffic.",
      "D. Use AWS Elastic Beanstalk to deploy the Tomcat servers with auto scaling in multiple Availability Zones. Store application data in an Amazon RDS for PostgreSQL database. Deploy Amazon CloudFront and an Application Load Balancer to scale for increased traffic."
    ],
    "answer": "D"
  },
  "505": {
    "question": "A company is migrating its on-premises IoT platform to AWS. The platform consists of the following components:\u2022\tA MongoDB cluster as a data store for all collected and processed IoT data.\u2022\tAn application that uses Message Queuing Telemetry Transport (MQTT) to connect to IoT devices every 5 minutes to collect data.\u2022\tAn application that runs jobs periodically to generate reports from the IoT data. The jobs take 120-600 seconds to finish running.\u2022\tA web application that runs on a web server. End users use the web application to generate reports that are accessible to the general public.The company needs to migrate the platform to AWS to reduce operational overhead while maintaining performance.Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose three.)",
    "choices": [
      "A. Create AWS Step Functions state machines with AUS Lambda tasks to prepare the reports and to write the reports to Amazon S3. Configure an Amazon CloudFront distribution that has an S3 origin to serve the reports",
      "B. Create an AWS Lambda function. Program the Lambda function to connect to the IoT devices. process the data, and write the data to the data store. Configure a Lambda layer to temporarily store messages for processing.",
      "C. Configure an Amazon Elastic Kubernetes Service (Amazon EKS) cluster with Amazon EC2 instances to prepare the reports. Create an ingress controller on the EKS cluster to serve the reports.",
      "D. Connect the IoT devices to AWS IoT Core to publish messages. Create an AWS IoT rule that runs when a message is received. Configure the rule to call an AWS Lambda function. Program the Lambda function to parse, transform, and store device message data to the data store.",
      "E. Migrate the MongoDB cluster to Amazon DocumentDB (with MongoDB compatibility).",
      "F. Migrate the MongoDB cluster to Amazon EC2 instances."
    ],
    "answer": "ADE"
  },
  "506": {
    "question": "A company creates an Amazon API Gateway API and shares the API with an external development team. The API uses AWS Lambda functions and is deployed to a stage that is named Production.The external development team is the sole consumer of the API. The API experiences sudden increases of usage at specific times, leading to concerns about increased costs. The company needs to limit cost and usage without reworking the Lambda functions.Which solution will meet these requirements MOST cost-effectively?",
    "choices": [
      "A. Configure the API to send requests to Amazon Simple Queue Service (Amazon SQS) queues instead of directly to the Lambda functions. Update the Lambda functions to consume messages from the queues and to process the requests. Set up the queues to invoke the Lambda functions when new messages arrive.",
      "B. Configure provisioned concurrency for each Lambda function. Use AWS Application Auto Scaling to register the Lambda functions as targets. Set up scaling schedules to increase and decrease capacity to match changes in API usage.",
      "C. Create an API Gateway API key and an AWS WAF Regional web ACL. Associate the web ACL with the Production stage. Add a rate-based rule to the web ACL. In the rule, specify the rate limit and a custom request aggregation that uses the X-API-Key header. Share the API key with the external development team.",
      "D. Create an API Gateway API Key and usage plan. Define throttling limits and quotas in the usage plan. Associate the usage plan with the Production stage and the API key. Share the API key with the external development team."
    ],
    "answer": "D"
  },
  "507": {
    "question": "An entertainment company hosts a ticketing service on a fleet of Linux Amazon EC2 instances that are in an Auto Scaling group. The ticketing service uses a pricing file. The pricing file is stored in an Amazon S3 bucket that has S3 Standard storage. A central pricing solution that is hosted by a third party updates the pricing file.The pricing file is updated every 1-15 minutes and has several thousand line items. The pricing file is downloaded to each EC2 instance when the instance launches.The EC2 instances occasionally use outdated pricing information that can result in incorrect charges for customers.Which solution will resolve this problem MOST cost-effectively?",
    "choices": [
      "A. Create an AWS Lambda function to update an Amazon DynamoDB table with new prices each time the pricing file is updated. Update the ticketing service to use DynramoDB to look up pricing",
      "B. Create an AWS Lambda function to update an Amazon Elastic File System (Amazon EFS) file share with the pricing file each time the file is updated. Update the ticketing service to use Amazon EFS to access the pricing file.",
      "C. Load Mountpoint for Amazon S3 onto the AMI of the EC2 instances. Configure Mountpoint for Amazon S3 to mount the S3 bucket that contains the pricing file. Update the ticketing service to point to the mount point and path to access the $3 object,",
      "D. Create an Amazon Elastic Block Store (Amazon EBS) volume. Use EBS Multi-Attach to attach the volume to every EC2 instance. When a new EC2 instance launches, configure the new instance to update the pricing file on the EBS volume. Update the ticketing service to point to the new local source."
    ],
    "answer": "C"
  },
  "508": {
    "question": "A company has deployed applications to thousands of Amazon EC2 instances in an AWS account. A security audit discovers that several unencrypted Amazon Elastic Block Store (Amazon EBS) volumes are attached to the EC2 instances. The company\u2019s security policy requires the EBS volumes to be encrypted.The company needs to implement an automated solution to encrypt the EBS volumes. The solution also must prevent development teams from creating unencrypted EBS volumes.Which solution will meet these requirements?",
    "choices": [
      "A. Configure the AWS Config managed rule that identifies unencrypted EBS volumes. Configure an automatic remediation action. Associate an AWS Systems Manager Automation runbook that includes the steps to create a new encrypted EBS volume. Create an AWS Key Management Service (AWS KMS) customer managed key. In the key policy, include a statement to deny the creation of unencrypted EBS volumes.",
      "B. Use AWS Systems Manager Fleet Manager to create a list of unencrypted EBS volumes, Create a Systems Manager Automation runbook that includes the steps to create a new encrypted EBS volume. Create an SCP to deny the creation of unencrypted EBS volumes.",
      "C. Use AWS Systems Manager Fleet Manager to create a list of unencrypted EBS volumes. Create a Systems Manager Automation runbook that includes the steps to create a new encrypted EBS volume. Modify the AWS account setting for EBS encryption to always encrypt new EBS volumes.",
      "D. Configure the AWS Config managed rule that identifies unencrypted EBS volumes. Configure an automatic remediation action. Associate an AWS Systems Manager Automation runbook that includes the steps to create a new encrypted EBS volume. Modify the AWS account setting for EBS encryption to always encrypt new EBS volumes."
    ],
    "answer": "D"
  },
  "509": {
    "question": "A company is using a single AWS Region for its ecommerce website. The website includes a web application that runs on several Amazon EC2 instances behind an Application Load Balancer (ALB). The website also includes an Amazon DynamoDB table. A custom domain name in Amazon Route 53 is linked to the ALB. The company created an SSL/TLS certificate in AWS Certificate Manager (ACM) and attached the certificate to the ALB. The company is not using a content delivery network as part of its design.The company wants to replicate its entire application stack in a second Region to provide disaster recovery, plan for future growth, and provide improved access time to users. A solutions architect needs to implement a solution that achieves these goals and minimizes administrative overhead.Which combination of steps should the solutions architect take to meet these requirements? (Choose three.)",
    "choices": [
      "A. Create an AWS CloudFormation template for the current infrastructure design. Use parameters for important system values, including Region. Use the CloudFormation template to create the new infrastructure in the second Region.",
      "B. Use the AWS Management Console to document the existing infrastructure design in the first Region and to create the new infrastructure in the second Region.",
      "C. Update the Route 53 hosted zone record for the application to use weighted routing. Send 50% of the traffic to the ALB in each Region.",
      "D. Update the Route 53 hosted zone record for the application to use latency-based routing. Send traffic to the ALB in each Region.",
      "E. Update the configuration of the existing DynamoDB table by enabling DynamoDB Streams. Add the second Region to create a global table.",
      "F. Create a new DynamoDB table. Enable DynamoDB Streams for the new table. Add the second Region to create a global table. Copy the data from the existing DynamoDB table to the new table as a one-time operation."
    ],
    "answer": "ADE"
  },
  "510": {
    "question": "A company is using a single AWS Region for its ecommerce website. The website includes a web application that runs on several Amazon EC2 instances behind an Application Load Balancer (ALB). The website also includes an Amazon DynamoDB table. A custom domain name in Amazon Route 53 is linked to the ALB. The company created an SSL/TLS certificate in AWS Certificate Manager (ACM) and attached the certificate to the ALB. The company is not using a content delivery network as part of its design.The company wants to replicate its entire application stack in a second Region to provide disaster recovery, plan for future growth, and provide improved access time to users. A solutions architect needs to implement a solution that achieves these goals and minimizes administrative overhead.Which combination of steps should the solutions architect take to meet these requirements? (Choose three.)",
    "choices": [
      "A. Create an AWS CloudFormation template for the current infrastructure design. Use parameters for important system values, including Region. Use the CloudFormation template to create the new infrastructure in the second Region.",
      "B. Use the AWS Management Console to document the existing infrastructure design in the first Region and to create the new infrastructure in the second Region.",
      "C. Update the Route 53 hosted zone record for the application to use weighted routing. Send 50% of the traffic to the ALB in each Region.",
      "D. Update the Route 53 hosted zone record for the application to use latency-based routing. Send traffic to the ALB in each Region.",
      "E. Update the configuration of the existing DynamoDB table by enabling DynamoDB Streams. Add the second Region to create a global table.",
      "F. Create a new DynamoDB table. Enable DynamoDB Streams for the new table. Add the second Region to create a global table. Copy the data from the existing DynamoDB table to the new table as a one-time operation."
    ],
    "answer": "ADE"
  },
  "511": {
    "question": "A company hosts a data-processing application on Amazon EC2 instances. The application polls an Amazon Elastic File System (Amazon EFS) file system for newly uploaded files. When a new file is detected, the application extracts data from the file and runs logic to select a Docker container image to process the file. The application starts the appropriate container image and passes the file location as a parameter.The data processing that the container performs can take up to 2 hours. When the processing is complete, the code that runs inside the container writes the file back to Amazon EFS and exits.The company needs to refactor the application to eliminate the EC2 instances that are running the containers.Which solution will meet these requirements?",
    "choices": [
      "A. Create an Amazon Elastic Container Service (Amazon ECS) cluster. Configure the processing to run as AWS Fargate tasks. Extract the container selection logic to run as an Amazon EventBridge rule that starts the appropriate Fargate task. Configure the EventBridge rule to run when files are added to the EFS file system.",
      "B. Create an Amazon Elastic Container Service (Amazon ECS) cluster. Configure the processing to run as AWS Fargate tasks. Update and containerize the container selection logic to run as a Fargate service that starts the appropriate Fargate task. Configure an EFS event notification to invoke the Fargate service when files are added to the EFS file system.",
      "C. Create an Amazon Elastic Container Service (Amazon ECS) cluster. Configure the processing to run as AWS Fargate tasks. Extract the container selection logic to run as an AWS Lambda function that starts the appropriate Fargate task. Migrate the storage of file uploads to an Amazon S3 bucket. Update the processing code to use Amazon S3. Configure an S3 event notification to invoke the Lambda function when objects are created.",
      "D. Create AWS Lambda container images for the processing. Configure Lambda functions to use the container images. Extract the container selection logic to run as a decision Lambda function that invokes the appropriate Lambda processing function. Migrate the storage of file uploads to an Amazon S3 bucket. Update the processing code to use Amazon S3. Configure an S3 event notification to invoke the decision Lambda function when objects are created."
    ],
    "answer": "C"
  },
  "512": {
    "question": "A media company has a 30-T8 repository of digital news videos. These videos are stored on tape in an on-premises tape library and referenced by a Media Asset Management (MAM) system. The company wants to enrich the metadata for these videos in an automated fashion and put them into a searchable catalog by using a MAM feature. The company must be able to search based on information in the video, such as objects, scenery items, or people\u2019s faces. A catalog is available that contains faces of people who have appeared in the videos that include an image of each person. The company would like to migrate these videos to AWS.The company has a high-speed AWS Direct Connect connection with AWS and would like to move the MAM solution video content directly from its current file system.How can these requirements be met by using the LEAST amount of ongoing management overhead and causing MINIMAL disruption to the existing system?",
    "choices": [
      "A. Set up an AWS Storage Gateway, file gateway appliance on-premises. Use the MAM solution to extract the videos from the current archive and push them into the file gateway. Use the catalog of faces to build a collection in Amazon Rekognition. Build an AWS Lambda function that invokes the Rekognition Javascript SDK to have Rekognition pull the video from the Amazon S3 files backing the file gateway, retrieve the required metadata, and push the metadata into the MAM solution.",
      "B. Set up an AWS Storage Gateway, tape gateway appliance on-premises. Use the MAM solution to extract the videos from the current archive and push them into the tape gateway. Use the catalog of faces to build a collection in Amazon Rekognition. Build an AWS Lambda function that invokes the Rekognition Javascript SDK to have Amazon Rekognition process the video in the tape gateway, retrieve the required metadata, and push the metadata into the MAM solution.",
      "C. Configure a video ingestion stream by using Amazon Kinesis Video Streams. Use the catalog of faces to build a collection in Amazon Rekognition. Stream the videos from the MAM solution into Kinesis Video Streams. Configure Amazon Rekognition to process the streamed videos. Then, use a stream consumer to retrieve the required metadata, and push the metadata into the MAM solution. Configure the stream to store the videos in Amazon S3.",
      "D. Set up an Amazon EC2 instance that runs the OpenCV libraries. Copy the videos, images, and face catalog from the on-premises library into an Amazon EBS volume mounted on this EC2 instance. Process the videos to retrieve the required metadata, and push the metadata into the MAM solution, while also copying the video files to an Amazon S3 bucket."
    ],
    "answer": "A"
  },
  "513": {
    "question": "A company needs to optimize the cost of an AWS environment that contains multiple accounts in an organization in AWS Organizations. The company conducted cost optimization activities 3 years ago and purchased Amazon EC2 Standard Reserved Instances that recently expired.The company needs EC2 instances for 3 more years. Additionally, the company has deployed a new serverless workload.Which strategy will provide the company with the MOST cost savings?",
    "choices": [
      "A. Purchase the same Reserved Instances for an additional 3-year term with All Upfront payment. Purchase a 3-year Compute Savings Plan with All Upfront payment in the management account to cover any additional compute costs",
      "B. Purchase a 1-year Compute Savings Plan with No Upfront payment in each member account. Use the Savings Plans recommendations in the AWS Cost Management console to choose the Compute Savings Plan.",
      "C. Purchase a 3-year EC2 Instance Savings Plan with No Upfront payment in the management account to cover EC2 costs in each AWS Region. Purchase a 3-year Compute Savings Plan with No Upfront payment in the management account to cover any additional compute costs.",
      "D. Purchase a 3-year EC2 Instance Savings Plan with All Upfront payment in each member account. Use the Savings Plans recommendations in the AWS Cost Management console to choose the EC2 Instance Savings Plan."
    ],
    "answer": "A"
  },
  "514": {
    "question": "A company operates a static content distribution platform that serves customers globally. The customers consume content from their own AWS accounts.The company serves its content from an Amazon S3 bucket. The company uploads the content from its on-premises environment to the S3 bucket by using an S3 File Gateway.The company wants to improve the platform\u2019s performance and reliability by serving content from the AWS Region that is geographically closest to customers. The company must route the on-premises data to Amazon S3 with minimal latency and without public internet exposure.Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose two.)",
    "choices": [
      "A. Implement S3 Multi-Region Access Points",
      "B. Use S3 Cross-Region Replication (CRR) to copy content to different Regions",
      "C. Create an AWS Lambda function that tracks the routing of clients to Regions",
      "D. Use an AWS Site-to-Site VPN connection to connect to a Multi-Region Access Point.",
      "E. Use AWS PrivateLink and AWS Direct Connect to connect to a Multi-Region Access Point."
    ],
    "answer": "AE"
  },
  "515": {
    "question": "A global media company is planning a multi-Region deployment of an application. Amazon DynamoDB global tables will back the deployment to keep the user experience consistent across the two continents where users are concentrated. Each deployment will have a public Application Load Balancer (ALB). The company manages public DNS internally. The company wants to make the application available through an apex domain.Which solution will meet these requirements with the LEAST effort?",
    "choices": [
      "A. Migrate public DNS to Amazon Route 53. Create CNAME records for the apex domain to point to the ALB. Use a geolocation routing policy to route traffic based on user location.",
      "B. Place a Network Load Balancer (NLB) in front of the ALMigrate public DNS to Amazon Route 53. Create a CNAME record for the apex domain to point to the NLB\u2019s static IP address. Use a geolocation routing policy to route traffic based on user location.",
      "C. Create an AWS Global Accelerator accelerator with multiple endpoint groups that target endpoints in appropriate AWS Regions. Use the accelerator\u2019s static IP address to create a record in public DNS for the apex domain.",
      "D. Create an Amazon API Gateway API that is backed by AWS Lambda in one of the AWS Regions. Configure a Lambda function to route traffic to application deployments by using the round robin method. Create CNAME records for the apex domain to point to the API's URL."
    ],
    "answer": "C"
  },
  "516": {
    "question": "A company has multiple AWS accounts that are in an organization in AWS Organizations. The company needs to store AWS account activity and query the data from a central location by using SQL.Which solution will meet these requirements?",
    "choices": [
      "A. Create an AWS CloudTraii trail in each account. Specify CloudTrail management events for the trail. Configure CloudTrail to send the events to Amazon CloudWatch Logs. Configure CloudWatch cross-account observability. Query the data in CloudWatch Logs Insights.",
      "B. Use a delegated administrator account to create an AWS CloudTrail Lake data store. Specify CloudTrail management events for the data store. Enable the data store for all accounts in the organization. Query the data in CloudTrail Lake.",
      "C. Use a delegated administrator account to create an AWS CloudTral trail. Specify CloudTrail management events for the trail. Enable the trail for all accounts in the organization. Keep all other settings as default. Query the CloudTrail data from the CloudTrail event history page.",
      "D. Use AWS CloudFormation StackSets to deploy AWS CloudTrail Lake data stores in each account. Specify CloudTrail management events for the data stores. Keep all other settings as default, Query the data in CloudTrail Lake."
    ],
    "answer": "B"
  },
  "517": {
    "question": "A company is using AWS to develop and manage its production web application. The application includes an Amazon API Gateway HTTP API that invokes an AWS Lambda function. The Lambda function processes and then stores data in a database.The company wants to implement user authorization for the web application in an integrated way. The company already uses a third-party identity provider that issues OAuth tokens for the company\u2019s other applications.Which solution will meet these requirements?",
    "choices": [
      "A. Integrate the company\u2019s third-party identity provider with API Gateway. Configure an API Gateway Lambda authorizer to validate tokens from the identity provider. Require the Lambda authorizer on all API routes. Update the web application to get tokens from the identity provider and include the tokens in the Authorization header when calling the API Gateway HTTP API.",
      "B. Integrate the company's third-party identity provider with AWS Directory Service. Configure Directory Service as an API Gateway authorizer to validate tokens from the identity provider. Require the Directory Service authorizer on all API routes. Configure AWS IAM Identity Center as a SAML 2.0 identity Provider. Configure the web application as a custom SAML 2.0 application.",
      "C. Integrate the company\u2019s third-party identity provider with AWS IAM Identity Center. Configure API Gateway to use IAM Identity Center for zero-configuration authentication and authorization. Update the web application to retrieve AWS Security Token Service (AWS STS) tokens from IAM Identity Center and include the tokens in the Authorization header when calling the API Gateway HTTP API.",
      "D. Integrate the company\u2019s third-party identity provider with AWS IAM Identity Center. Configure IAM users with permissions to call the API Gateway HTTP API. Update the web application to extract request parameters from the IAM users and include the parameters in the Authorization header when calling the API Gateway HTTP API."
    ],
    "answer": "A"
  },
  "518": {
    "question": "A company has deployed applications to thousands of Amazon EC2 instances in an AWS account. A security audit discovers that several unencrypted Amazon Elastic Block Store (Amazon EBS) volumes are attached to the EC2 instances. The company\u2019s security policy requires the EBS volumes to be encrypted.The company needs to implement an automated solution to encrypt the EBS volumes. The solution also must prevent development teams from creating unencrypted EBS volumes.Which solution will meet these requirements?",
    "choices": [
      "A. Configure the AWS Config managed rule that identifies unencrypted EBS volumes. Configure an automatic remediation action. Associate an AWS Systems Manager Automation runbook that includes the steps to create a new encrypted EBS volume. Create an AWS Key Management Service (AWS KMS) customer managed key. In the key policy, include a statement to deny the creation of unencrypted EBS volumes.",
      "B. Use AWS Systems Manager Fleet Manager to create a list of unencrypted EBS volumes, Create a Systems Manager Automation runbook that includes the steps to create a new encrypted EBS volume. Create an SCP to deny the creation of unencrypted EBS volumes.",
      "C. Use AWS Systems Manager Fleet Manager to create a list of unencrypted EBS volumes. Create a Systems Manager Automation runbook that includes the steps to create a new encrypted EBS volume. Modify the AWS account setting for EBS encryption to always encrypt new EBS volumes.",
      "D. Configure the AWS Config managed rule that identifies unencrypted EBS volumes. Configure an automatic remediation action. Associate an AWS Systems Manager Automation runbook that includes the steps to create a new encrypted EBS volume. Modify the AWS account setting for EBS encryption to always encrypt new EBS volumes."
    ],
    "answer": "D"
  },
  "519": {
    "question": "A company is running a large containerized workload in the AWS Cloud. The workload consists of approximately 100 different services. The company uses Amazon Elastic Container Service (Amazon ECS) to orchestrate the workload.Recently the company\u2019s development team started using AWS Fargate instead of Amazon EC2 instances in the ECS cluster. In the past, the workload has come close to running the maximum number of EC2 instances that are available in the account.The company is worried that the workload could reach the maximum number of ECS tasks that are allowed. A solutions architect must implement a solution that will notify the development team when Fargate reaches 80% of the maximum number of tasks.What should the solutions architect do to meet this requirement?",
    "choices": [
      "A. Use Amazon CloudWatch to monitor the Sample Count statistic for each service in the ECS cluster. Set an alarm for when the math expression sample count/SERVICE_QUOTA(service)*100 is greater than 80. Notify the development team by using Amazon Simple Notification Service (Amazon SNS).",
      "B. Use Amazon CloudWatch to monitor service quotas that are published under the AWS/Usage metric namespace. Set an alarm for when the math expression metric/SERVICE_QUOTA(metric)*100 is greater than 80. Notify the development team by using Amazon Simple Notification Service (Amazon SNS).",
      "C. Create an AWS Lambda function to poll detailed metrics from the ECS cluster. When the number of running Fargate tasks is greater than 80, invoke Amazon Simple Email Service (Amazon SES) to notify the development team.",
      "D. Create an AWS Config rule to evaluate whether the Fargate SERVICE_QUOTA is greater than 80. Use Amazon Simple Email Service (Amazon SES) to notify the development team when the AWS Config rule is not compliant."
    ],
    "answer": "B"
  },
  "520": {
    "question": "A company has several AWS Lambda functions written in Python. The functions are deployed with the .zip package deployment type. The functions use a Lambda layer that contains common libraries and packages in a .zip file. The Lambda .zip packages and Lambda layer .zip file are stored in an Amazon S3 bucket.The company must implement automatic scanning of the Lambda functions and the Lambda layer to identify CVEs. A subset of the Lambda functions must receive automated code scans to detect potential data leaks and other vulnerabilities. The code scans must occur only for selected Lambda functions, not all the Lambda functions.Which combination of actions will meet these requirements? (Choose three.)",
    "choices": [
      "A. Activate Amazon Inspector. Start automated CVE scans.",
      "B. Activate Lambda standard scanning and Lambda code scanning in Amazon Inspector.",
      "C. Enable Amazon GuardDuty. Enable the Lambda Protection feature in GuardDuty.",
      "D. Enable scanning in the Monitor settings of the Lambda functions that need code scans.",
      "E. Tag Lambda functions that do not need code scans. In the tag, include a key of InspectorCodeExclusion and a value of LambdaCodeScanning.",
      "F. Use Amazon Inspector to scan the 3 bucket that contains the Lambda .zip packages and the Lambda layer .zip file for code scans."
    ],
    "answer": "ABE"
  },
  "521": {
    "question": "A company is changing the way that it handles patching of Amazon EC2 instances in its application account. The company currently patches instances over the internet by using a NAT gateway in a VPC in the application account.The company has EC2 instances set up as a patch source repository in a dedicated private VPC in a core account. The company wants to use AWS Systems Manager Patch Manager and the patch source repository in the core account to patch the EC2 instances in the application account. The company must prevent all EC2 instances in the application account from accessing the internet.The EC2 instances in the application account need to access Amazon S3, where the application data is stored. These EC2 instances need connectivity to Systems Manager and to the patch source repository in the private VPC in the core account.Which solution will meet these requirements?",
    "choices": [
      "A. Create a network ACL that blocks outbound traffic on port 80. Associate the network ACL with all subnets in the application account. In the application account and the core account, deploy one EC2 instance that runs a custom VPN server. Create a VPN tunnel to access the private VPC. Update the route table in the application account.",
      "B. Create private VIFs for Systems Manager and Amazon S3. Delete the NAT gateway from the VPC in the application account. Create a transit gateway to access the patch source repository EC2 instances in the core account. Update the route table in the core account.",
      "C. Create VPC endpoints for Systems Manager and Amazon S3. Delete the NAT gateway from the VPC in the application account. Create a VPC peering connection to access the patch source repository EC2 instances in the core account. Update the route tables in both accounts.",
      "D. Create a network ACL that blocks inbound traffic on port 80. Associate the network ACL with all subnets in the application account. Create a transit gateway to access the patch source repository EC2 instances in the core account. Update the route tables in both accounts."
    ],
    "answer": "C"
  },
  "522": {
    "question": "A company in the United States (US) has acquired a company in Europe. Both companies use the AWS Cloud. The US company has built a new application with a microservices architecture. The US company is hosting the application across five VPCs in the us-east-2 Region. The application must be able to access resources in one VPC in the eu-west-1 Region.However, the application must not be able to access any other VPCs.The VPCs in both Regions have no overlapping CIDR ranges. All accounts are already consolidated in one organization in AWS Organizations.Which solution will meet these requirements MOST cost-effectively?",
    "choices": [
      "A. Create one transit gateway in eu-west-1. Attach the VPCs in us-east-2 and the VPC in eu-west-1 to the transit gateway. Create the necessary route entries in each VPC so that the traffic is routed through the transit gateway.",
      "B. Create one transit gateway in each Region. Attach the involved subnets to the regional transit gateway. Create the necessary route entries in the associated route tables for each subnet so that the traffic is routed through the regional transit gateway. Peer the two transit gateways.",
      "C. Create a full mesh VPC peering connection configuration between all the VPCs. Create the necessary route entries in each VPC so that the traffic is routed through the VPC peering connection.",
      "D. Create one VPC peering connection for each VPC in us-east-2 to the VPC in eu-west-1. Create the necessary route entries in each VPC so that the traffic is routed through the VPC peering connection."
    ],
    "answer": "D"
  },
  "523": {
    "question": "A travel company built a web application that uses Amazon Simple Email Service (Amazon SES) to send email notifications to users. The company needs to enable logging to help troubleshoot email delivery issues. The company also needs the ability to do searches that are based on recipient, subject, and time sent.Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)",
    "choices": [
      "A. Create an Amazon SES configuration set with Amazon Data Firehose as the destination. Choose to send logs to an Amazon S3 bucket.",
      "B. Enable AWS CloudTrail logging. Specify an Amazon S3 bucket as the destination for the logs.",
      "C. Use Amazon Athena to query the logs in the Amazon S3 bucket for recipient, subject, and time sent.",
      "D. Create an Amazon CloudWatch log group. Configure Amazon SES to send logs to the log group.",
      "E. Use Amazon Athena to query the logs in Amazon CloudWatch for recipient, subject, and time sent."
    ],
    "answer": "AC"
  },
  "524": {
    "question": "A company migrated to AWS and uses AWS Business Support. The company wants to monitor the cost-effectiveness of Amazon EC2 instances across AWS accounts. The EC2 instances have tags for department, business unit, and environment. Development EC2 instances have high cost but low utilization.The company needs to detect and stop any underutilized development EC2 instances. Instances are underutilized if they had 10% or less average daily CPU utilization and 5 MB or less network I/O for at least 4 of the past 14 days.Which solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      "A. Configure Amazon CloudWatch dashboards to monitor EC2 instance utilization based on tags for department, business unit, and environment. Create an Amazon EventBridge rule that invokes an AWS Lambda function to stop underutilized development EC2 instances.",
      "B. Configure AWS Systems Manager to track EC2 instance utilization and report underutilized instances to Amazon CloudWatch. Filter the CloudWatch data by tags for department, business unit, and environment. Create an Amazon EventBridge rule that invokes an AWS Lambda function to stop underutilized development EC2 instances.",
      "C. Create an Amazon EventBridge rule to detect low utilization of EC2 instances reported by AWS Trusted Advisor. Configure the rule to invoke an AWS Lambda function that filters the data by tags for department, business unit, and environment and stops underutilized development EC2 instances.",
      "D. Create an AWS Lambda function to run daily to retrieve utilization data for all EC2 instances. Save the data to an Amazon DynamoDB table. Create an Amazon QuickSight dashboard that uses the DynamoDB table as a data source to identify and stop underutilized development EC2 instances."
    ],
    "answer": "C"
  },
  "525": {
    "question": "Accompany is building an application to collect and transmit sensor data from a factory. The application will use AWS IoT Core to send data from hundreds of devices to an Amazon S3 data lake. The company must enrich the data before loading the data into Amazon S3.The application will transmit the sensor data every 5 seconds. New sensor data must be available in Amazon S3 less than 30 minutes after the application collects the data. No other applications are processing the sensor data from AWS IoT Core.Which solution will meet these requirements MOST cost-effectively?",
    "choices": [
      "A. Create a topic in AWS IoT Core to ingest the sensor data. Create an AWS Lambda function to enrich the data and to write the data to Amazon S3. Configure an AWS IoT rule action to invoke the Lambda function.",
      "B. Use AWS IoT Core Basic Ingest to ingest the sensor data. Configure an AWS IoT rule action to write the data to Amazon Kinesis Data Firehose. Set the Kinesis Data Firehose buffering interval to 900 seconds. Use Kinesis Data Firehose to invoke an AWS Lambda function to enrich the data, Configure Kinesis Data Firehose to deliver the data to Amazon S3.",
      "C. Create a topic in AWS IoT Core to ingest the sensor data. Configure an AWS IoT rule action to send the data to an Amazon Timestream table. Create an AWS Lambda, function to read the data from Timestream. Configure the Lambda function to enrich the data and to write the data to Amazon S3.",
      "D. Use AWS loT Core Basic Ingest to ingest the sensor data. Configure an AWS IoT rule action to write the data to Amazon Kinesis Data Streams. Create a consumer AWS Lambda function to process the data from Kinesis Data Streams and to enrich the data. Call the S3 PutObject API operation from the Lambda function to write the data to Amazon S3."
    ],
    "answer": "B"
  },
  "526": {
    "question": "Accompany is building an application to collect and transmit sensor data from a factory. The application will use AWS IoT Core to send data from hundreds of devices to an Amazon S3 data lake. The company must enrich the data before loading the data into Amazon S3.The application will transmit the sensor data every 5 seconds. New sensor data must be available in Amazon S3 less than 30 minutes after the application collects the data. No other applications are processing the sensor data from AWS IoT Core.Which solution will meet these requirements MOST cost-effectively?",
    "choices": [
      "A. Create a topic in AWS IoT Core to ingest the sensor data. Create an AWS Lambda function to enrich the data and to write the data to Amazon S3. Configure an AWS IoT rule action to invoke the Lambda function.",
      "B. Use AWS IoT Core Basic Ingest to ingest the sensor data. Configure an AWS IoT rule action to write the data to Amazon Kinesis Data Firehose. Set the Kinesis Data Firehose buffering interval to 900 seconds. Use Kinesis Data Firehose to invoke an AWS Lambda function to enrich the data, Configure Kinesis Data Firehose to deliver the data to Amazon S3.",
      "C. Create a topic in AWS IoT Core to ingest the sensor data. Configure an AWS IoT rule action to send the data to an Amazon Timestream table. Create an AWS Lambda, function to read the data from Timestream. Configure the Lambda function to enrich the data and to write the data to Amazon S3.",
      "D. Use AWS loT Core Basic Ingest to ingest the sensor data. Configure an AWS IoT rule action to write the data to Amazon Kinesis Data Streams. Create a consumer AWS Lambda function to process the data from Kinesis Data Streams and to enrich the data. Call the S3 PutObject API operation from the Lambda function to write the data to Amazon S3."
    ],
    "answer": "B"
  },
  "527": {
    "question": "A company is collecting data from a large set of IoT devices. The data is stored in an Amazon S3 data lake. Data scientists perform analytics on Amazon EC2 instances that run in two public subnets in a VPC in a separate AWS account.The data scientists need access to the data lake from the EC2 instances. The EC2 instances already have an assigned role with permissions to access Amazon S3.According to company policies, only authorized networks are allowed to have access to the IoT data.Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)",
    "choices": [
      "A. Create a gateway VPC endpoint for Amazon S3 in the data scientists\u2019 VPC.",
      "B. Create an S3 access point in the data scientists' AWS account for the data lake.",
      "C. Update the EC2 instance role. Add a policy with a condition that allows the s3:GetObject action when the value for the s3:DataAccessPointArn condition key is a valid access point ARN.",
      "D. Update the VPC route table to route S3 traffic to an S3 access point.",
      "E. Add an S3 bucket policy with a condition that allows the s3:GetObject action when the value for the s3:DataAccessPointArn condition key is a valid access point ARN."
    ],
    "answer": "AE"
  },
  "528": {
    "question": "A company wants to migrate its website to AWS. The website uses containers that are deployed in an on-premises, self-managed Kubernetes cluster. All data for the website is stored in an on-premises PostgreSQL database.The company has decided to migrate the on-premises Kubernetes cluster to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The EKS cluster will use EKS managed node groups with a static number of nodes. The company will also migrate the on-premises database to an Amazon RDS for PostgreSQL database.A solutions architect needs to estimate the total cost of ownership (TCO) for this workload before the migration.Which solution will provide the required TCO information?",
    "choices": [
      "A. Request access to Migration Evaluator. Run the Migration Evaluator Collector and import the data. Configure a scenario. Export a Quick Insights report from Migration Evaluator.",
      "B. Launch AWS Database Migration Service (AWS DMS) for the on-premises database. Generate an assessment report. Create an estimate in AWS Pricing Calculator for the costs of the EKS migration.",
      "C. Initialize AWS Application Migration Service. Add the on-premises servers as source servers. Launch a test instance. Output a TCO report from Application Migration Service.",
      "D. Access the AWS Cloud Economics Center webpage to assess the AWS Cloud Value Framework. Create an AWS Cost and Usage report from the Cloud Value Framework."
    ],
    "answer": "A"
  },
  "529": {
    "question": "An events company runs a ticketing platform on AWS. The company\u2019s customers configure and schedule their events on the platform. The events result in large increases of traffic to the platform. The company knows the date and time of each customer\u2019s events.The company runs the platform on an Amazon Elastic Container Service (Amazon ECS) cluster. The ECS cluster consists of Amazon EC2 On-Demand Instances that are in an Auto Scaling group. The Auto Scaling group uses a predictive scaling policy.The ECS cluster makes frequent requests to an Amazon S3 bucket to download ticket assets. The ECS cluster and the S3 bucket are in the same AWS Region and the same AWS account. Traffic between the ECS cluster and the S3 bucket flows across a NAT gateway.The company needs to optimize the cost of the platform without decreasing the platform's availability.Which combination of steps will meet these requirements? (Choose two.)",
    "choices": [
      "A. Create a gateway VPC endpoint for the S3 bucket.",
      "B. Add another ECS capacity provider that uses an Auto Scaling group of Spot Instances. Configure the new capacity provider strategy to have the same weight as the existing capacity provider strategy.",
      "C. Create On-Demand Capacity Reservations for the applicable instance type for the time period of the scheduled scaling policies.",
      "D. Enable S3 Transfer Acceleration on the S3 bucket.",
      "E. Replace the predictive scaling policy with scheduled scaling policies for the scheduled events."
    ],
    "answer": "AE"
  },
  "1": {
    "question": "A company needs to architect a hybrid DNS solution. This solution will use an Amazon Route 53 private hosted zone for the domain cloud.example.com for the resources stored within VPCs.The company has the following DNS resolution requirements:On-premises systems should be able to resolve and connect to cloud.example.com.All VPCs should be able to resolve cloud.example.com.There is already an AWS Direct Connect connection between the on-premises corporate network and AWS Transit Gateway.Which architecture should the company use to meet these requirements with the HIGHEST performance?",
    "choices": [
      "A. Associate the private hosted zone to all the VPCs. Create a Route 53 inbound resolver in the shared services VPC. Attach all VPCs to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the inbound resolver.",
      "B. Associate the private hosted zone to all the VPCs. Deploy an Amazon EC2 conditional forwarder in the shared services VPC. Attach all VPCs to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the conditional forwarder.",
      "C. Associate the private hosted zone to the shared services VPCreate a Route 53 outbound resolver in the shared services VPAttach all VPCs to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the outbound resolver.",
      "D. Associate the private hosted zone to the shared services VPC. Create a Route 53 inbound resolver in the shared services VPC. Attach the shared services VPC to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the inbound resolver."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\n\nThe correct answer is A because it addresses all the requirements in the most performant way:\n\n- It associates the private hosted zone for \"cloud.example.com\" to all VPCs, ensuring that all VPCs can resolve this domain.\n- It creates a Route 53 inbound resolver in the shared services VPC, which allows on-premises systems to resolve and connect to \"cloud.example.com\" through the transit gateway.\n- By attaching all VPCs to the transit gateway and creating forwarding rules in the on-premises DNS server to point to the inbound resolver, it provides a centralized and efficient DNS resolution mechanism.\n\nThis approach leverages the benefits of Route 53 inbound resolver, which provides low-latency, high-performance DNS resolution for on-premises systems, while also allowing all VPCs to access the private hosted zone directly.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. This choice uses an EC2-based conditional forwarder, which may not provide the highest performance compared to the Route 53 inbound resolver.\n\nC. This choice does not associate the private hosted zone to all VPCs, which means that not all VPCs would be able to resolve \"cloud.example.com\". The use of a Route 53 outbound resolver would also not provide the same level of performance as the inbound resolver.\n\nD. Similar to C, this choice does not associate the private hosted zone to all VPCs, which means that not all VPCs would be able to resolve \"cloud.example.com\". Additionally, by only attaching the shared services VPC to the transit gateway, it would not provide a centralized DNS resolution mechanism for the on-premises systems."
  },
  "2": {
    "question": "A company is providing weather data over a REST-based API to several customers. The API is hosted by Amazon API Gateway and is integrated with different AWS Lambda functions for each API operation. The company uses Amazon Route 53 for DNS and has created a resource record of weather.example.com. The company stores data for the API in Amazon DynamoDB tables. The company needs a solution that will give the API the ability to fail over to a different AWS Region.Which solution will meet these requirements?",
    "choices": [
      "A. Deploy a new set of Lambda functions in a new Region. Update the API Gateway API to use an edge-optimized API endpoint with Lambda functions from both Regions as targets. Convert the DynamoDB tables to global tables.",
      "B. Deploy a new API Gateway API and Lambda functions in another Region. Change the Route 53 DNS record to a multivalue answer. Add both API Gateway APIs to the answer. Enable target health monitoring. Convert the DynamoDB tables to global tables.",
      "C. Deploy a new API Gateway API and Lambda functions in another Region. Change the Route 53 DNS record to a failover record. Enable target health monitoring. Convert the DynamoDB tables to global tables.",
      "D. Deploy a new API Gateway API in a new Region. Change the Lambda functions to global functions. Change the Route 53 DNS record to a multivalue answer. Add both API Gateway APIs to the answer. Enable target health monitoring. Convert the DynamoDB tables to global tables."
    ],
    "answer": "C",
    "explanation": "1. Explanation of the correct answer (C):\n\nThe correct answer is C because it provides the best solution to meet the company's requirements of having the ability to fail over the weather data API to a different AWS Region.\n\nThe key aspects of this solution are:\n\n- Deploying a new API Gateway API and Lambda functions in another AWS Region. This ensures there is a redundant set of resources in a different Region to handle the API traffic in case of a failure in the primary Region.\n- Changing the Route 53 DNS record to a failover record. This allows Route 53 to automatically route traffic to the secondary API endpoint in the event the primary endpoint becomes unhealthy, providing failover capability.\n- Enabling target health monitoring. This ensures that Route 53 can accurately detect the health of the primary and secondary API endpoints, triggering the failover when necessary.\n- Converting the DynamoDB tables to global tables. This enables cross-Region replication of the data, ensuring data consistency and availability across Regions.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This solution is incorrect because it uses an edge-optimized API endpoint, which is not the best choice for failover. Edge-optimized endpoints are designed for low-latency access from different geographic locations, but they do not provide the same level of failover capabilities as a solution using Route 53 failover records.\n\nB. This solution is incorrect because it uses a multivalue answer for the Route 53 DNS record. Multivalue answers distribute traffic across multiple endpoints, but they do not provide automatic failover capabilities. The company would need to implement additional logic to handle failover, which is not as seamless as the Route 53 failover solution in choice C.\n\nD. This solution is incorrect because it uses global Lambda functions, which are not necessary for the failover requirement. Additionally, the use of a multivalue answer DNS record, similar to choice B, does not provide the same level of automatic failover capabilities as the Route 53 failover solution in choice C."
  },
  "3": {
    "question": "A company uses AWS Organizations with a single OU named Production to manage multiple accounts. All accounts are members of the Production OU. Administrators use deny list SCPs in the root of the organization to manage access to restricted services.The company recently acquired a new business unit and invited the new unit\u2019s existing AWS account to the organization. Once onboarded, the administrators of the new business unit discovered that they are not able to update existing AWS Config rules to meet the company\u2019s policies.Which option will allow administrators to make changes and continue to enforce the current policies without introducing additional long-term maintenance?",
    "choices": [
      "A. Remove the organization\u2019s root SCPs that limit access to AWS Config. Create AWS Service Catalog products for the company\u2019s standard AWS Config rules and deploy them throughout the organization, including the new account.",
      "B. Create a temporary OU named Onboarding for the new account. Apply an SCP to the Onboarding OU to allow AWS Config actions. Move the new account to the Production OU when adjustments to AWS Config are complete.",
      "C. Convert the organization\u2019s root SCPs from deny list SCPs to allow list SCPs to allow the required services only. Temporarily apply an SCP to the organization\u2019s root that allows AWS Config actions for principals only in the new account.",
      "D. Create a temporary OU named Onboarding for the new account. Apply an SCP to the Onboarding OU to allow AWS Config actions. Move the organization\u2019s root SCP to the Production OU. Move the new account to the Production OU when adjustments to AWS Config are complete."
    ],
    "answer": "D",
    "explanation": "1. Explanation of the correct answer (D):\n\nThe correct answer is D because it provides the most effective and long-term maintainable solution to the problem.\n\nThe key points are:\n\n- The organization uses deny list SCPs at the root level to manage access to restricted services, including AWS Config.\n- When a new account is added from the acquired business unit, the administrators find they cannot update the existing AWS Config rules to meet the company's policies.\n- To address this, the solution involves creating a temporary \"Onboarding\" OU for the new account, applying a more permissive SCP to that OU to allow the necessary AWS Config actions, and then moving the new account to the \"Production\" OU once the necessary changes are complete.\n- This approach preserves the existing deny list SCPs at the root level, which is the standard policy for the organization, and avoids the need for long-term maintenance or changes to the overall policy model.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. Remove the organization's root SCPs and use Service Catalog products: This introduces unnecessary complexity and long-term maintenance overhead, as the organization would need to manage and update the Service Catalog products across all accounts.\n\nB. Create a temporary \"Onboarding\" OU with a permissive SCP: This approach would not work, as the deny list SCP at the root level would still take precedence and continue to block the AWS Config actions, even in the temporary OU.\n\nC. Convert the root SCPs from deny list to allow list: This would also introduce unnecessary complexity and long-term maintenance overhead, as the organization would need to carefully manage the allow list SCP to ensure it covers all the required services, and update it as needed."
  },
  "4": {
    "question": "A company is running a two-tier web-based application in an on-premises data center. The application layer consists of a single server running a stateful application. The application connects to a PostgreSQL database running on a separate server. The application\u2019s user base is expected to grow significantly, so the company is migrating the application and database to AWS. The solution will use Amazon Aurora PostgreSQL, Amazon EC2 Auto Scaling, and Elastic Load Balancing.Which solution will provide a consistent user experience that will allow the application and database tiers to scale?",
    "choices": [
      "A. Enable Aurora Auto Scaling for Aurora Replicas. Use a Network Load Balancer with the least outstanding requests routing algorithm and sticky sessions enabled.",
      "B. Enable Aurora Auto Scaling for Aurora writers. Use an Application Load Balancer with the round robin routing algorithm and sticky sessions enabled.",
      "C. Enable Aurora Auto Scaling for Aurora Replicas. Use an Application Load Balancer with the round robin routing and sticky sessions enabled.",
      "D. Enable Aurora Scaling for Aurora writers. Use a Network Load Balancer with the least outstanding requests routing algorithm and sticky sessions enabled."
    ],
    "answer": "C",
    "explanation": "Here's a clear explanation of why the correct answer is right, and brief explanations of why each incorrect choice is wrong:\n\n1. Explanation of the correct answer (C):\n   - Option C is correct because it aligns with the requirements of the question.\n   - Enabling Aurora Auto Scaling for Aurora Replicas allows the database tier to scale by automatically adding and removing read replicas to match the application's workload.\n   - Using an Application Load Balancer with the round-robin routing algorithm and sticky sessions enabled ensures a consistent user experience. The round-robin algorithm distributes traffic evenly across the application instances, while sticky sessions maintain session affinity for each user.\n   - This solution provides horizontal scaling for both the application and database tiers, ensuring the application can handle the expected growth in the user base while maintaining a consistent user experience.\n\n2. Explanations of the incorrect choices:\n   - Option A is incorrect because using a Network Load Balancer (NLB) does not support the round-robin routing algorithm. NLBs also do not have the ability to enable sticky sessions.\n   - Option B is incorrect because enabling Aurora Auto Scaling for Aurora writers is not the optimal choice, as it only scales the write capacity of the database. The question mentions a stateful application, which suggests the need to scale the read capacity as well.\n   - Option D is incorrect because using a Network Load Balancer (NLB) does not support the round-robin routing algorithm, and the least outstanding requests routing algorithm is not suitable for a stateful application that requires sticky sessions.\n\nIn summary, the correct answer (C) provides the best solution to meet the requirements of the question, as it leverages Aurora Auto Scaling for Replicas and an Application Load Balancer with the appropriate routing and session management settings."
  },
  "5": {
    "question": "A company uses a service to collect metadata from applications that the company hosts on premises. Consumer devices such as TVs and internet radios access the applications. Many older devices do not support certain HTTP headers and exhibit errors when these headers are present in responses. The company has configured an on-premises load balancer to remove the unsupported headers from responses sent to older devices, which the company identified by the User-Agent headers.The company wants to migrate the service to AWS, adopt serverless technologies, and retain the ability to support the older devices. The company has already migrated the applications into a set of AWS Lambda functions.Which solution will meet these requirements?",
    "choices": [
      "A. Create an Amazon CloudFront distribution for the metadata service. Create an Application Load Balancer (ALB). Configure the CloudFront distribution to forward requests to the ALB. Configure the ALB to invoke the correct Lambda function for each type of request. Create a CloudFront function to remove the problematic headers based on the value of the User-Agent header.",
      "B. Create an Amazon API Gateway REST API for the metadata service. Configure API Gateway to invoke the correct Lambda function for each type of request. Modify the default gateway responses to remove the problematic headers based on the value of the User-Agent header.",
      "C. Create an Amazon API Gateway HTTP API for the metadata service. Configure API Gateway to invoke the correct Lambda function for each type of request. Create a response mapping template to remove the problematic headers based on the value of the User-Agent. Associate the response data mapping with the HTTP API.",
      "D. Create an Amazon CloudFront distribution for the metadata service. Create an Application Load Balancer (ALB). Configure the CloudFront distribution to forward requests to the ALB. Configure the ALB to invoke the correct Lambda function for each type of request. Create a Lambda@Edge function that will remove the problematic headers in response to viewer requests based on the value of the User-Agent header."
    ],
    "answer": "A",
    "explanation": "1. Explanation of the correct answer (A):\n\nThe correct answer is A. This solution best meets the requirements outlined in the question:\n\n- It creates an Amazon CloudFront distribution to serve the metadata service, which can handle the response header modification for older devices.\n- It creates an Application Load Balancer (ALB) to forward requests to the correct Lambda function.\n- The CloudFront function can be used to remove the problematic headers based on the User-Agent header, without the need for a more complex Lambda@Edge function.\n- This approach allows the company to migrate the service to AWS, adopt serverless technologies (Lambda), and retain the ability to support the older devices.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. This choice uses Amazon API Gateway REST API, which does not provide the same level of control over the response headers as the CloudFront solution in A. Modifying the default gateway responses may not be as flexible or performant as using a CloudFront function.\n\nC. Similar to B, this choice uses Amazon API Gateway HTTP API, which also does not provide the same level of control over the response headers as the CloudFront solution in A. The response mapping template may not be as efficient as a CloudFront function.\n\nD. This choice uses a Lambda@Edge function, which is more complex to set up and manage than a CloudFront function. While both solutions can remove the problematic headers, the CloudFront function is a simpler and more lightweight option."
  },
  "6": {
    "question": "A company owns a chain of travel agencies and is running an application in the AWS Cloud. Company employees use the application to search for information about travel destinations. Destination content is updated four times each year.Two fixed Amazon EC2 instances serve the application. The company uses an Amazon Route 53 public hosted zone with a multivalue record of travel.example.com that returns the Elastic IP addresses for the EC2 instances. The application uses Amazon DynamoDB as its primary data store. The company uses a self-hosted Redis instance as a caching solution.During content updates, the load on the EC2 instances and the caching solution increases drastically. This increased load has led to downtime on several occasions. A solutions architect must update the application so that the application is highly available and can handle the load that is generated by the content updates.Which solution will meet these requirements?",
    "choices": [
      "A. Set up DynamoDB Accelerator (DAX) as in-memory cache. Update the application to use DAX. Create an Auto Scaling group for the EC2 instances. Create an Application Load Balancer (ALB). Set the Auto Scaling group as a target for the ALB. Update the Route 53 record to use a simple routing policy that targets the ALB's DNS alias. Configure scheduled scaling for the EC2 instances before the content updates.",
      "B. Set up Amazon ElastiCache for Redis. Update the application to use ElastiCache. Create an Auto Scaling group for the EC2 instances. Create an Amazon CloudFront distribution, and set the Auto Scaling group as an origin for the distribution. Update the Route 53 record to use a simple routing policy that targets the CloudFront distribution\u2019s DNS alias. Manually scale up EC2 instances before the content updates.",
      "C. Set up Amazon ElastiCache for Memcached. Update the application to use ElastiCache. Create an Auto Scaling group for the EC2 instances. Create an Application Load Balancer (ALB). Set the Auto Scaling group as a target for the ALB. Update the Route 53 record to use a simple routing policy that targets the ALB's DNS alias. Configure scheduled scaling for the application before the content updates.",
      "D. Set up DynamoDB Accelerator (DAX) as in-memory cache. Update the application to use DAX. Create an Auto Scaling group for the EC2 instances. Create an Amazon CloudFront distribution, and set the Auto Scaling group as an origin for the distribution. Update the Route 53 record to use a simple routing policy that targets the CloudFront distribution's DNS alias. Manually scale up EC2 instances before the content updates."
    ],
    "answer": "A",
    "explanation": "Explanation of the correct answer (A):\n\nThe key aspects of the correct answer (A) are:\n\n1. Using DynamoDB Accelerator (DAX) as an in-memory cache: DAX is specifically optimized for DynamoDB, providing low-latency caching and offloading read traffic from the DynamoDB tables. This helps handle the increased load during content updates.\n\n2. Creating an Auto Scaling group for the EC2 instances: This allows the application to automatically scale the EC2 instances up and down based on the load, ensuring high availability and handling the increased load during content updates.\n\n3. Creating an Application Load Balancer (ALB): The ALB distributes the incoming traffic across the EC2 instances in the Auto Scaling group, further improving the application's scalability and high availability.\n\n4. Updating the Route 53 record to use a simple routing policy that targets the ALB's DNS alias: This ensures that the application's DNS record points to the scalable and highly available ALB, providing a reliable entry point for users.\n\n5. Configuring scheduled scaling for the EC2 instances before the content updates: This proactively scales up the EC2 instances before the content updates, ensuring that the application can handle the anticipated increase in load.\n\nExplanations of the incorrect choices:\n\nB. This choice uses ElastiCache for Redis, which is a good caching solution, but it lacks the DynamoDB integration that DAX provides, making it less optimal for this scenario.\n\nC. This choice uses ElastiCache for Memcached, which is a simpler caching solution compared to Redis and does not have the same level of feature-richness and DynamoDB integration that DAX provides.\n\nD. This choice uses CloudFront, which is a content delivery network and is not primarily designed for handling database load and scaling EC2 instances. While CloudFront can be used in conjunction with other solutions, it is not the optimal choice on its own for this scenario."
  },
  "7": {
    "question": "A company is running a traditional web application on Amazon EC2 instances. The company needs to refactor the application as microservices that run on containers. Separate versions of the application exist in two distinct environments: production and testing. Load for the application is variable, but the minimum load and the maximum load are known. A solutions architect needs to design the updated application with a serverless architecture that minimizes operational complexity.Which solution will meet these requirements MOST cost-effectively?",
    "choices": [
      "A. Upload the container images to AWS Lambda as functions. Configure a concurrency limit for the associated Lambda functions to handle the expected peak load. Configure two separate Lambda integrations within Amazon API Gateway: one for production and one for testing.",
      "B. Upload the container images to Amazon Elastic Container Registry (Amazon ECR). Configure two auto scaled Amazon Elastic Container Service (Amazon ECS) clusters with the Fargate launch type to handle the expected load. Deploy tasks from the ECR images. Configure two separate Application Load Balancers to direct traffic to the ECS clusters.",
      "C. Upload the container images to Amazon Elastic Container Registry (Amazon ECR). Configure two auto scaled Amazon Elastic Kubernetes Service (Amazon EKS) clusters with the Fargate launch type to handle the expected load. Deploy tasks from the ECR images. Configure two separate Application Load Balancers to direct traffic to the EKS clusters.",
      "D. Upload the container images to AWS Elastic Beanstalk. In Elastic Beanstalk, create separate environments and deployments for production and testing. Configure two separate Application Load Balancers to direct traffic to the Elastic Beanstalk deployments."
    ],
    "answer": "B",
    "explanation": "1. Clear explanation of why the correct answer (B) is right:\n\nThe correct answer is B because it best meets the requirements of the problem statement. Specifically:\n\n- It uses a serverless architecture by leveraging Amazon ECS with the Fargate launch type, which allows for automatic scaling of the containers based on the expected load, minimizing operational complexity.\n- It configures separate ECS clusters and Application Load Balancers for the production and testing environments, allowing for distinct deployments.\n- Compared to the other options, ECS with Fargate is the most cost-effective serverless containerization solution, as it does not require managing the underlying infrastructure.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nA. This option uses AWS Lambda, which may not be the most cost-effective solution for containerized applications, especially if the container images are large. Lambda has strict limits on image size (10 GB), which may not be sufficient for the company's needs.\n\nC. This option uses Amazon EKS, which adds an additional layer of complexity compared to ECS. EKS requires more operational overhead for managing the Kubernetes control plane, which may not be the most cost-effective solution for the given requirements.\n\nD. This option uses AWS Elastic Beanstalk, which is a higher-level PaaS service and may not provide the same level of granular control and cost optimization as using ECS directly.\n\nIn summary, the correct answer (B) is the most cost-effective solution because it leverages the serverless and auto-scaling capabilities of Amazon ECS with Fargate, while allowing for separate deployments for the production and testing environments."
  },
  "8": {
    "question": "A company has a multi-tier web application that runs on a fleet of Amazon EC2 instances behind an Application Load Balancer (ALB). The instances are in an Auto Scaling group. The ALB and the Auto Scaling group are replicated in a backup AWS Region. The minimum value and the maximum value for the Auto Scaling group are set to zero. An Amazon RDS Multi-AZ DB instance stores the application\u2019s data. The DB instance has a read replica in the backup Region. The application presents an endpoint to end users by using an Amazon Route 53 record.The company needs to reduce its RTO to less than 15 minutes by giving the application the ability to automatically fail over to the backup Region. The company does not have a large enough budget for an active-active strategy.What should a solutions architect recommend to meet these requirements?",
    "choices": [
      "A. Reconfigure the application\u2019s Route 53 record with a latency-based routing policy that load balances traffic between the two ALBs. Create an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group values. Create an Amazon CloudWatch alarm that is based on the HTTPCode_Target_5XX_Count metric for the ALB in the primary Region. Configure the CloudWatch alarm to invoke the Lambda function.",
      "B. Create an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group values. Configure Route 53 with a health check that monitors the web application and sends an Amazon Simple Notification Service (Amazon SNS) notification to the Lambda function when the health check status is unhealthy. Update the application\u2019s Route 53 record with a failover policy that routes traffic to the ALB in the backup Region when a health check failure occurs.",
      "C. Configure the Auto Scaling group in the backup Region to have the same values as the Auto Scaling group in the primary Region. Reconfigure the application\u2019s Route 53 record with a latency-based routing policy that load balances traffic between the two ALBs. Remove the read replica. Replace the read replica with a standalone RDS DB instance. Configure Cross-Region Replication between the RDS DB instances by using snapshots and Amazon S3.",
      "D. Configure an endpoint in AWS Global Accelerator with the two ALBs as equal weighted targets. Create an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group values. Create an Amazon CloudWatch alarm that is based on the HTTPCode_Target_5XX_Count metric for the ALB in the primary Region. Configure the CloudWatch alarm to invoke the Lambda function."
    ],
    "answer": "B",
    "explanation": "1. Explanation of the correct answer (B):\n\nThe correct answer is B because it meets the company's requirements for reducing the Recovery Time Objective (RTO) to less than 15 minutes and not having a large budget for an active-active strategy.\n\nIn this solution, the key steps are:\n\n- Create an AWS Lambda function in the backup region to promote the read replica and modify the Auto Scaling group values in the backup region.\n- Configure Route 53 with a health check that monitors the web application and sends an Amazon SNS notification to the Lambda function when the health check status is unhealthy.\n- Update the application's Route 53 record with a failover policy that routes traffic to the ALB in the backup region when a health check failure occurs.\n\nThis setup allows for automatic failover to the backup region in the event of a failure in the primary region, meeting the RTO requirement of less than 15 minutes. It is a cost-effective active-passive strategy, which aligns with the company's budget constraint.\n\n2. Explanations of why the incorrect choices are wrong:\n\nA. This solution relies on a latency-based routing policy, which is more suitable for an active-active strategy. It also uses a CloudWatch alarm to trigger the failover, which may not be as reliable as the health check-based approach in option B.\n\nC. This solution attempts to create a standalone RDS instance in the backup region, which is not recommended for a Multi-AZ setup. It also does not provide the automatic failover capabilities required to meet the RTO.\n\nD. This solution uses AWS Global Accelerator, which is an additional service and may add unnecessary complexity and cost. The use of CloudWatch alarms is similar to option A and may not be as reliable as the health check-based approach in option B."
  },
  "9": {
    "question": "A company is hosting a critical application on a single Amazon EC2 instance. The application uses an Amazon ElastiCache for Redis single-node cluster for an in-memory data store. The application uses an Amazon RDS for MariaDB DB instance for a relational database. For the application to function, each piece of the infrastructure must be healthy and must be in an active state.A solutions architect needs to improve the application's architecture so that the infrastructure can automatically recover from failure with the least possible downtime.Which combination of steps will meet these requirements? (Choose three.)",
    "choices": [
      "A. Use an Elastic Load Balancer to distribute traffic across multiple EC2 instances. Ensure that the EC2 instances are part of an Auto Scaling group that has a minimum capacity of two instances.",
      "B. Use an Elastic Load Balancer to distribute traffic across multiple EC2 instances. Ensure that the EC2 instances are configured in unlimited mode.",
      "C. Modify the DB instance to create a read replica in the same Availability Zone. Promote the read replica to be the primary DB instance in failure scenarios.",
      "D. Modify the DB instance to create a Multi-AZ deployment that extends across two Availability Zones.",
      "E. Create a replication group for the ElastiCache for Redis cluster. Configure the cluster to use an Auto Scaling group that has a minimum capacity of two instances.",
      "F. Create a replication group for the ElastiCache for Redis cluster. Enable Multi-AZ on the cluster."
    ],
    "answer": "ADF",
    "explanation": "1. Explanation of the correct answer (ADF):\n\nA. Using an Elastic Load Balancer (ELB) to distribute traffic across multiple EC2 instances, and ensuring that the EC2 instances are part of an Auto Scaling group with a minimum capacity of two instances, helps ensure that the application remains available in the event of a failure of one of the instances. This provides redundancy and high availability for the compute layer.\n\nD. Modifying the DB instance to create a Multi-AZ deployment that extends across two Availability Zones helps ensure that the database remains available in the event of a failure in one of the Availability Zones. Multi-AZ deployments automatically failover to the secondary Availability Zone, reducing downtime.\n\nF. Creating a replication group for the ElastiCache for Redis cluster and enabling Multi-AZ ensures that the in-memory data store remains available in the event of a failure. This allows traffic to be automatically directed to the secondary Availability Zone, reducing downtime.\n\n2. Explanations of why the incorrect choices are wrong:\n\nB. Configuring the EC2 instances in unlimited mode does not ensure that there is always at least one healthy instance to handle traffic if there is a failure. The minimum capacity of the Auto Scaling group must be set to at least two instances to provide redundancy.\n\nC. Modifying the DB instance to create a read replica in the same Availability Zone does not provide the same level of redundancy and high availability as a Multi-AZ deployment. If the primary Availability Zone fails, the read replica would also be unavailable, leading to potential downtime.\n\nE. Creating a replication group for the ElastiCache for Redis cluster without configuring an Auto Scaling group with a minimum capacity of two instances does not provide redundancy for the compute layer. If the single EC2 instance hosting the ElastiCache cluster fails, the application will be unavailable."
  },
  "10": {
    "question": "A retail company is operating its ecommerce application on AWS. The application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The company uses an Amazon RDS DB instance as the database backend. Amazon CloudFront is configured with one origin that points to the ALB. Static content is cached. Amazon Route 53 is used to host all public zones.After an update of the application, the ALB occasionally returns a 502 status code (Bad Gateway) error. The root cause is malformed HTTP headers that are returned to the ALB. The webpage returns successfully when a solutions architect reloads the webpage immediately after the error occurs.While the company is working on the problem, the solutions architect needs to provide a custom error page instead of the standard ALB error page to visitors.Which combination of steps will meet this requirement with the LEAST amount of operational overhead? (Choose two.)",
    "choices": [
      "A. Create an Amazon S3 bucket. Configure the S3 bucket to host a static webpage. Upload the custom error pages to Amazon S3.",
      "B. Create an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health check response Target.FailedHealthChecks is greater than 0. Configure the Lambda function to modify the forwarding rule at the ALB to point to a publicly accessible web server.",
      "C. Modify the existing Amazon Route 53 records by adding health checks. Configure a fallback target if the health check fails. Modify DNS records to point to a publicly accessible webpage.",
      "D. Create an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health check response Elb.InternalError is greater than 0. Configure the Lambda function to modify the forwarding rule at the ALB to point to a public accessible web server.",
      "E. Add a custom error response by configuring a CloudFront custom error page. Modify DNS records to point to a publicly accessible web page."
    ],
    "answer": "AE",
    "explanation": "1. Clear explanation of why the correct answer (A and E) is right:\n\nThe correct answer is A and E because this combination of steps provides the least amount of operational overhead to serve a custom error page when the Application Load Balancer (ALB) returns a 502 Bad Gateway error.\n\nOption A: Create an Amazon S3 bucket to host a static webpage for the custom error page. This is a simple and cost-effective solution, as S3 provides highly available and durable storage for static content.\n\nOption E: Add a custom error response by configuring an Amazon CloudFront custom error page. CloudFront can intercept the 502 error from the ALB and redirect users to the custom error page hosted on S3, without affecting the overall application behavior for users who do not encounter the 502 error.\n\nThis combination of S3 and CloudFront custom error pages is the least operationally complex solution, as it does not require modifying the ALB forwarding rules or using Route 53 health checks, which would impact all users, even those who did not encounter the 502 error.\n\n2. Brief explanations of why each incorrect choice is wrong:\n\nB. This option requires setting up a CloudWatch alarm and a Lambda function to modify the ALB forwarding rules, which is more operationally complex than the correct solution.\n\nC. Modifying DNS records to point to a publicly accessible webpage would affect all users, even those who did not encounter the 502 error, which is not the desired behavior.\n\nD. This option is similar to B, requiring a CloudWatch alarm and a Lambda function to modify the ALB forwarding rules, which is more operationally complex than the correct solution."
  }
}